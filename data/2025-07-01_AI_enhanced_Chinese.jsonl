{"id": "2506.22466", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22466", "abs": "https://arxiv.org/abs/2506.22466", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u535a\u7269\u9986\u4e2d\u81ea\u4e3b\u5bf9\u8bdd\u673a\u5668\u4ebaAndrea\u7684\u516c\u4f17\u4e92\u52a8\uff0c\u53d1\u73b0\u5176\u6027\u522b\u8868\u73b0\u5bf9\u6574\u4f53\u611f\u77e5\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u591a\u6570\u8bbf\u5ba2\u5e0c\u671b\u5176\u63d0\u4f9b\u5c55\u54c1\u4fe1\u606f\uff0c\u540c\u65f6\u6539\u8fdb\u9700\u6c42\u5305\u62ec\u591a\u8bed\u8a00\u652f\u6301\u548c\u66f4\u5feb\u54cd\u5e94\u3002", "motivation": "\u63a2\u7d22\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u53ca\u7528\u6237\u9700\u6c42\uff0c\u4ee5\u4f18\u5316\u672a\u6765\u8bbe\u8ba1\u3002", "method": "\u5728\u535a\u7269\u9986\u8bbe\u7f6e\u81ea\u4e3b\u5bf9\u8bdd\u673a\u5668\u4ebaAndrea\uff0c\u8fdb\u884c\u4e3a\u671f\u516d\u5929\u7684\u65e0\u504f\u89c1\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5e76\u5206\u6790\u7cfb\u7edf\u65e5\u5fd7\u6570\u636e\u3002", "result": "\u6027\u522b\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u8bbf\u5ba2\u4e3b\u8981\u9700\u6c42\u4e3a\u5c55\u54c1\u4fe1\u606f\u3001\u591a\u8bed\u8a00\u652f\u6301\u548c\u66f4\u5feb\u54cd\u5e94\uff1b\u7cfb\u7edf\u65e5\u5fd7\u9a8c\u8bc1\u4e86\u8bbf\u8c08\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u81ea\u4e3b\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u672a\u6765\u5c06\u4f18\u5316\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2506.22473", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22473", "abs": "https://arxiv.org/abs/2506.22473", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "comment": "8 pages with 6 figures", "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u673a\u5668\u4eba\u4ee3\u7406\u591a\u6a21\u6001\u611f\u5b98\u4fe1\u53f7\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4f20\u611f\u5668\u8fd0\u52a8\u5173\u7cfb\uff0c\u5e76\u8bc6\u522b\u4e86\u8fd0\u52a8\u57fa\u5143\u3002", "motivation": "\u7814\u7a76\u52a8\u7269\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u4ea7\u751f\u7684\u9ad8\u7ef4\u611f\u5b98\u4fe1\u606f\u6d41\uff0c\u5e2e\u52a9\u7406\u89e3\u672a\u5904\u7406\u7684\u4f20\u611f\u5668\u8fd0\u52a8\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u77ac\u65f6\u4e92\u4fe1\u606f\u6355\u6349\u52a8\u6001\u529f\u80fd\u8fde\u63a5\uff0c\u7ed3\u5408\u65e0\u9650\u5173\u7cfb\u6a21\u578b\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u8bc6\u522b\u4f20\u611f\u5668\u8fd0\u52a8\u6a21\u5757\u53ca\u5176\u6f14\u5316\u3002", "result": "\u63ed\u793a\u4e86\u4f20\u611f\u5668\u8fd0\u52a8\u5173\u7cfb\uff0c\u5e76\u5206\u89e3\u51fa\u8fd0\u52a8\u57fa\u5143\uff0c\u53ef\u7528\u4e8e\u884c\u4e3a\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u53ca\u4eba\u7c7b\u8fd0\u52a8\u8f68\u8ff9\u6216\u8111\u4fe1\u53f7\u5206\u6790\u3002"}}
{"id": "2506.22494", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "AI": {"tldr": "DriveBLIP2\u6846\u67b6\u57fa\u4e8eBLIP2-OPT\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u89e3\u91ca\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u5feb\u901f\u8bc6\u522b\u5173\u952e\u5bf9\u8c61\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\uff0c\u7a81\u51fa\u5173\u952e\u89c6\u9891\u5e27\u4e2d\u5bf9\u9a7e\u9a76\u51b3\u7b56\u91cd\u8981\u7684\u5bf9\u8c61\uff0c\u4ee5\u751f\u6210\u6e05\u6670\u76f8\u5173\u7684\u89e3\u91ca\u3002", "result": "\u5728DRAMA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cDriveBLIP2\u5728BLEU\u3001ROUGE\u3001CIDEr\u548cSPICE\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u76ee\u6807\u6ce8\u610f\u529b\u673a\u5236\u53ef\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.22572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22572", "abs": "https://arxiv.org/abs/2506.22572", "authors": ["Mrunmayi Mungekar", "Sanjith Menon", "M. Ravi Shankar", "M. Khalid Jawed"], "title": "Directed Shape Morphing using Kirigami-enhanced Thermoplastics", "comment": "Software and Data: https://github.com/structuresComp/Shrinky-Dink", "summary": "We present a simple, accessible method for autonomously transforming flat\nplastic sheets into intricate three-dimensional structures using only uniform\nheating and common tools such as household ovens and scissors. Our approach\ncombines heat-shrinkable thermoplastics with Kirigami patterns tailored to the\ntarget 3D shape, creating bilayer composites that morph into a wide range of\ncomplex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces\nlike mouse covers. Critically, the transformation is driven by a\nlow-information stimulus (uniform heat) yet produces highly intricate shapes\nthrough programmed geometric design. The morphing behavior, confirmed by finite\nelement simulations, arises from strain mismatch between the contracting\nthermoplastic layer and the constraining Kirigami layer. By decoupling material\ncomposition from mechanical response, this method avoids detailed process\ncontrol and enables a broad class of self-morphing structures, offering a\nversatile platform for adaptive design and scalable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u5747\u5300\u52a0\u70ed\u548c\u5e38\u89c1\u5de5\u5177\u5c06\u5851\u6599\u7247\u81ea\u4e3b\u8f6c\u5316\u4e3a\u590d\u67423D\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u590d\u6742\u63a7\u5236\u5373\u53ef\u5b9e\u73b0\u5851\u6599\u7247\u81ea\u4e3b\u53d8\u5f62\u7684\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316\u5236\u9020\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u70ed\u6536\u7f29\u70ed\u5851\u6027\u5851\u6599\u548c\u5b9a\u5236Kirigami\u56fe\u6848\uff0c\u901a\u8fc7\u53cc\u5c42\u590d\u5408\u6750\u6599\u5728\u5747\u5300\u52a0\u70ed\u4e0b\u53d8\u5f62\u3002", "result": "\u6210\u529f\u5236\u9020\u51fa\u591a\u79cd\u590d\u6742\u7ed3\u6784\uff08\u5982\u7897\u3001\u91d1\u5b57\u5854\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u9a8c\u8bc1\u53d8\u5f62\u673a\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u8bbe\u8ba1\u5b9e\u73b0\u4f4e\u4fe1\u606f\u523a\u6fc0\u4e0b\u7684\u590d\u6742\u53d8\u5f62\uff0c\u4e3a\u81ea\u9002\u5e94\u8bbe\u8ba1\u548c\u89c4\u6a21\u5316\u5236\u9020\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002"}}
{"id": "2506.22437", "categories": ["cs.CV", "68T45 (Computer Vision)"], "pdf": "https://arxiv.org/pdf/2506.22437", "abs": "https://arxiv.org/abs/2506.22437", "authors": ["Xinxin Sun", "Peter Chang"], "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u56fe\u50cf\u5bf9\u9f50\u6846\u67b6\uff0c\u9488\u5bf9\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u88c2\u7eb9\u5b9a\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6269\u6563\u548cRANSAC\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982SIFT\u3001SURF\uff09\u5728\u9ad8\u9891\u8fb9\u7f18\u6291\u5236\u548c\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u88c2\u7eb9\u5b9a\u4f4d\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u5404\u5411\u5f02\u6027\u6269\u6563\u6784\u5efa\u88c2\u7eb9\u4fdd\u7559\u5c3a\u5ea6\u7a7a\u95f4\uff0c\u7ed3\u5408RANSAC\u8fdb\u884c\u5355\u5e94\u6027\u4f30\u8ba1\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\uff0c\u88c2\u7eb9\u9762\u79ef\u548c\u957f\u5ea6\u8bef\u5dee\u5206\u522b\u51cf\u5c1170%\u548c90%\uff0c\u5bf9\u9f50\u8bef\u5dee\u4f4e\u4e8e5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u88c2\u7eb9\u6f14\u5316\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u3001\u8f7b\u91cf\u4e14\u7269\u7406\u57fa\u7840\u660e\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2506.22441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9608\u503c\u8ddd\u79bb\u52a0\u6743\u635f\u5931\uff08TDW\uff09\u7684\u6f5c\u5728\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff08TDWLFT\uff09\uff0c\u7528\u4e8e\u5904\u7406\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u7f3a\u5931\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\uff0c\u6709\u6548\u964d\u4f4e\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "motivation": "\u73b0\u5b9e\u4ea4\u901a\u6570\u636e\u5e38\u56e0\u901a\u4fe1\u6545\u969c\u6216\u4f20\u611f\u5668\u95ee\u9898\u4e0d\u5b8c\u6574\u6216\u635f\u574f\uff0c\u5f71\u54cd\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u6027\u80fd\u3002\u73b0\u6709\u6f5c\u5728\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff08LFT\uff09\u5bf9\u5f02\u5e38\u503c\u654f\u611f\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faTDWLFT\u6a21\u578b\uff0c\u91c7\u7528\u9608\u503c\u8ddd\u79bb\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u4e3a\u4e0d\u540c\u6837\u672c\u5206\u914d\u5dee\u5f02\u5316\u6743\u91cd\uff0c\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u7684\u4ea4\u901a\u901f\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cTDWLFT\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TDWLFT\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\u8865\u5168\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2506.22593", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22593", "abs": "https://arxiv.org/abs/2506.22593", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5Pix2G\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u50cf\u7d20\u548cLiDAR\u5730\u56fe\u5b9e\u65f6\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u56fe\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u672a\u77e5\u73af\u5883\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u5728\u73af\u5883\u7406\u89e3\u4e0a\u7684\u5dee\u5f02\uff0c\u901a\u8fc73D\u573a\u666f\u56fe\u6865\u63a5\u4eba\u7c7b\u53ef\u8bfb\u76842D BIM\u4e0e\u673a\u5668\u4eba3D\u5730\u56fe\u3002", "method": "Pix2G\u65b9\u6cd5\u5728CPU\u4e0a\u5b9e\u65f6\u5904\u7406\u56fe\u50cf\u548cLiDAR\u6570\u636e\uff0c\u751f\u6210\u53bb\u566a\u76842D\u5730\u56fe\u548c\u7ed3\u6784\u5206\u5272\u76843D\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u56fe\u8fde\u63a5\u3002", "result": "\u5728NASA JPL NeBula-Spot\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u6742\u4e71\u8f66\u5e93\u548c\u57ce\u5e02\u529e\u516c\u5ba4\u73af\u5883\u7684\u5b9e\u65f6\u81ea\u4e3b\u63a2\u7d22\u4e0e\u5730\u56fe\u6784\u5efa\u3002", "conclusion": "Pix2G\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u73af\u5883\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2506.22438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22438", "abs": "https://arxiv.org/abs/2506.22438", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u6570\u7ed3\u679c\u4fe1\u606f\u548c\u5916\u90e8\u73af\u5883\u6761\u4ef6\u7684\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u56e0\u7d20\u654f\u611f\u6027\u5206\u6790\u548c\u81ea\u9002\u5e94DBSCAN\u805a\u7c7b\u7b97\u6cd5\u4f18\u5316\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u5bb3\u866b\u8ba1\u6570\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u573a\u666f\u4e2d\u8ba1\u6570\u7ed3\u679c\u53ef\u9760\u6027\u7684\u8bc4\u4f30\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u5bb3\u866b\u68c0\u6d4b\u7f51\u7edc\u3001\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u590d\u6742\u5ea6\u8bc4\u4f30\u548c\u5206\u5e03\u5747\u5300\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\u96c6\u4e0aMSE\u964d\u4f4e31.7%\uff0cR2\u63d0\u9ad815.2%\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u91cf\u5316\u5f71\u54cd\u56e0\u7d20\u4e0e\u7f6e\u4fe1\u5ea6\u7684\u5173\u7cfb\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2506.22442", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22442", "abs": "https://arxiv.org/abs/2506.22442", "authors": ["Piotr Makarevich"], "title": "Features-based embedding or Feature-grounding", "comment": "13 pages, 12 figures", "summary": "In everyday reasoning, when we think about a particular object, we associate\nit with a unique set of expected properties such as weight, size, or more\nabstract attributes like density or horsepower. These expectations are shaped\nby our prior knowledge and the conceptual categories we have formed through\nexperience. This paper investigates how such knowledge-based structured\nthinking can be reproduced in deep learning models using features based\nembeddings. Specially, it introduces an specific approach to build\nfeature-grounded embedding, aiming to align shareable representations of\noperable dictionary with interpretable domain-specific conceptual features.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u57fa\u4e8e\u7279\u5f81\u7684\u5d4c\u5165\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u590d\u73b0\u57fa\u4e8e\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u601d\u7ef4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u7279\u5f81\u63a5\u5730\u5d4c\u5165\u7684\u5177\u4f53\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5c06\u4eba\u7c7b\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u548c\u6982\u5ff5\u7c7b\u522b\u7684\u7ed3\u6784\u5316\u601d\u7ef4\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u9886\u57df\u7279\u5b9a\u7279\u5f81\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u7279\u5f81\u63a5\u5730\u5d4c\u5165\u7684\u5177\u4f53\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u53ef\u64cd\u4f5c\u5b57\u5178\u7684\u53ef\u5171\u4eab\u8868\u793a\u4e0e\u53ef\u89e3\u91ca\u7684\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u57fa\u4e8e\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2506.22766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22766", "abs": "https://arxiv.org/abs/2506.22766", "authors": ["Yiting Chen", "Kenneth Kimble", "Howard H. Qian", "Podshara Chanrungmaneekul", "Robert Seney", "Kaiyu Hang"], "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025; 16 pages, 10\n  figures", "summary": "Robust and adaptive robotic peg-in-hole assembly under tight tolerances is\ncritical to various industrial applications. However, it remains an open\nchallenge due to perceptual and physical uncertainties from contact-rich\ninteractions that easily exceed the allowed clearance. In this paper, we study\nhow to leverage contact between the peg and its matching hole to eliminate\nuncertainties in the assembly process under unstructured settings. By examining\nthe role of compliance under contact constraints, we present a manipulation\nsystem that plans collision-inclusive interactions for the peg to 1)\niteratively identify its task environment to localize the target hole and 2)\nexploit environmental contact constraints to refine insertion motions into the\ntarget hole without relying on precise perception, enabling a robust solution\nto peg-in-hole assembly. By conceptualizing the above process as the\ncomposition of funneling in different state spaces, we present a formal\napproach to constructing manipulation funnels as an uncertainty-absorbing\nparadigm for peg-in-hole assembly. The proposed system effectively generalizes\nacross diverse peg-in-hole scenarios across varying scales, shapes, and\nmaterials in a learning-free manner. Extensive experiments on a NIST Assembly\nTask Board (ATB) and additional challenging scenarios validate its robustness\nin real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u673a\u5668\u4eba\u63d2\u5b54\u88c5\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\u548c\u6f0f\u6597\u5316\u7b56\u7565\uff0c\u5728\u4e0d\u4f9d\u8d56\u7cbe\u786e\u611f\u77e5\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u88c5\u914d\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u5e94\u7528\u4e2d\u56e0\u63a5\u89e6\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u7684\u63d2\u5b54\u88c5\u914d\u96be\u9898\u3002", "method": "\u5229\u7528\u63a5\u89e6\u7ea6\u675f\u548c\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\uff0c\u901a\u8fc7\u6f0f\u6597\u5316\u7b56\u7565\u5728\u72b6\u6001\u7a7a\u95f4\u4e2d\u9010\u6b65\u5b9a\u4f4d\u548c\u63d2\u5165\u76ee\u6807\u5b54\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u79cd\u5c3a\u5bf8\u3001\u5f62\u72b6\u548c\u6750\u6599\u7684\u63d2\u5b54\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5b66\u4e60\u5373\u53ef\u6cdb\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d2\u5b54\u88c5\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.22463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22463", "abs": "https://arxiv.org/abs/2506.22463", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "comment": "26 pages, accepted by ICML 2025", "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "AI": {"tldr": "MoDiff\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709\u52a0\u901f\u6280\u672f\uff08\u5982\u7f13\u5b58\u548c\u91cf\u5316\uff09\u5728\u8ba1\u7b97\u8bef\u5dee\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faMoDiff\u6846\u67b6\uff0c\u7ed3\u5408\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\uff0c\u9002\u7528\u4e8e\u6240\u6709\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728CIFAR-10\u548cLSUN\u4e0a\uff0cMoDiff\u5c06\u6fc0\u6d3b\u91cf\u5316\u4ece8\u4f4d\u964d\u81f33\u4f4d\u4e14\u6027\u80fd\u65e0\u635f\u3002", "conclusion": "MoDiff\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2506.22443", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22443", "abs": "https://arxiv.org/abs/2506.22443", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "AI": {"tldr": "RL-Net\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\u7f51\u7edc\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\uff0893.03% F1\uff09\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u89c4\u5219\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u6027\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u89c4\u5219\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u4e0a\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u6027\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faRL-Net\uff0c\u901a\u8fc7\u795e\u7ecf\u4f18\u5316\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u5217\u8868\uff0c\u5e76\u4e0e\u900f\u660e\u89c4\u5219\u7cfb\u7edf\uff08MIRA\uff09\u548c\u53ef\u89e3\u91ca\u9ed1\u76d2\u6a21\u578b\uff08XentricAI\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "RL-Net\u5728\u6027\u80fd\uff0893.03% F1\uff09\u548c\u89c4\u5219\u590d\u6742\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u4e8eMIRA\u548cXentricAI\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "RL-Net\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u5728\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5728\u8fb9\u7f18\u4f20\u611f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.22769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22769", "abs": "https://arxiv.org/abs/2506.22769", "authors": ["Changshi Zhou", "Feng Luan", "Jiarui Hu", "Shaoqiang Meng", "Zhipeng Wang", "Yanchao Dong", "Yanmin Zhou", "Bin He"], "title": "Learning Efficient Robotic Garment Manipulation with Standardization", "comment": null, "summary": "Garment manipulation is a significant challenge for robots due to the complex\ndynamics and potential self-occlusion of garments. Most existing methods of\nefficient garment unfolding overlook the crucial role of standardization of\nflattened garments, which could significantly simplify downstream tasks like\nfolding, ironing, and packing. This paper presents APS-Net, a novel approach to\ngarment manipulation that combines unfolding and standardization in a unified\nframework. APS-Net employs a dual-arm, multi-primitive policy with dynamic\nfling to quickly unfold crumpled garments and pick-and-place (p and p) for\nprecise alignment. The purpose of garment standardization during unfolding\ninvolves not only maximizing surface coverage but also aligning the garment's\nshape and orientation to predefined requirements. To guide effective robot\nlearning, we introduce a novel factorized reward function for standardization,\nwhich incorporates garment coverage (Cov), keypoint distance (KD), and\nintersection-over-union (IoU) metrics. Additionally, we introduce a spatial\naction mask and an Action Optimized Module to improve unfolding efficiency by\nselecting actions and operation points effectively. In simulation, APS-Net\noutperforms state-of-the-art methods for long sleeves, achieving 3.9 percent\nbetter coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09\npercent relative reduction). Real-world folding tasks further demonstrate that\nstandardization simplifies the folding process. Project page: see\nhttps://hellohaia.github.io/APS/", "AI": {"tldr": "APS-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c55\u5f00\u548c\u6807\u51c6\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u81c2\u591a\u57fa\u5143\u7b56\u7565\u548c\u52a8\u6001\u629b\u63b7\u6280\u672f\u9ad8\u6548\u5c55\u5f00\u8863\u7269\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u5956\u52b1\u51fd\u6570\u4f18\u5316\u6807\u51c6\u5316\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8863\u7269\u5c55\u5f00\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6807\u51c6\u5316\u7684\u91cd\u8981\u6027\uff0c\u800c\u6807\u51c6\u5316\u80fd\u663e\u8457\u7b80\u5316\u540e\u7eed\u4efb\u52a1\uff08\u5982\u6298\u53e0\u3001\u71a8\u70eb\u548c\u6253\u5305\uff09\u3002", "method": "\u91c7\u7528\u53cc\u81c2\u591a\u57fa\u5143\u7b56\u7565\uff08\u52a8\u6001\u629b\u63b7\u5c55\u5f00\u548c\u7cbe\u51c6\u653e\u7f6e\u5bf9\u9f50\uff09\uff0c\u5f15\u5165\u56e0\u5b50\u5316\u5956\u52b1\u51fd\u6570\uff08\u8986\u76d6\u7387\u3001\u5173\u952e\u70b9\u8ddd\u79bb\u548cIoU\uff09\u548c\u7a7a\u95f4\u52a8\u4f5c\u63a9\u7801\u4f18\u5316\u52a8\u4f5c\u9009\u62e9\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cAPS-Net\u5728\u957f\u8896\u8863\u7269\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8986\u76d6\u7387\u63d0\u9ad83.9%\uff0cIoU\u63d0\u9ad85.2%\uff0c\u5173\u952e\u70b9\u8ddd\u79bb\u51cf\u5c110.14\u3002", "conclusion": "\u6807\u51c6\u5316\u663e\u8457\u7b80\u5316\u4e86\u6298\u53e0\u4efb\u52a1\uff0cAPS-Net\u4e3a\u8863\u7269\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u6210\u672c\u8d1f\u8f7d\u4f20\u611f\u5668\u7684\u5e8a\u79bb\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u878d\u5408\u548c\u53cc\u6d41Transformer\u6a21\u578bViFusionTST\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u533b\u9662\u548c\u957f\u671f\u62a4\u7406\u673a\u6784\u4e2d\uff0c\u5e8a\u76f8\u5173\u8dcc\u5012\u662f\u4e00\u4e2a\u4e3b\u8981\u4f24\u5bb3\u6765\u6e90\uff0c\u73b0\u6709\u8b66\u62a5\u7cfb\u7edf\u5f80\u5f80\u6ede\u540e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u65e9\u671f\u9884\u6d4b\u5e8a\u79bb\u610f\u56fe\u6765\u9884\u9632\u8dcc\u5012\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4f4e\u6210\u672c\u8d1f\u8f7d\u4f20\u611f\u5668\u91c7\u96c6\u4fe1\u53f7\uff0c\u8f6c\u6362\u4e3aRGB\u7ebf\u56fe\u548c\u4e09\u79cd\u7eb9\u7406\u56fe\uff08\u9012\u5f52\u56fe\u3001\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\u573a\u3001\u683c\u62c9\u7c73\u5b89\u89d2\u573a\uff09\uff0c\u5e76\u901a\u8fc7\u53cc\u6d41Swin Transformer\u6a21\u578bViFusionTST\u8fdb\u884c\u5e76\u884c\u5904\u7406\u548c\u8de8\u6ce8\u610f\u529b\u878d\u5408\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cViFusionTST\u8fbe\u52300.885\u7684\u51c6\u786e\u7387\u548c0.794\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u67091D\u548c2D\u65f6\u95f4\u5e8f\u5217\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u878d\u5408\u7684\u8d1f\u8f7d\u4f20\u611f\u5668\u4fe1\u53f7\u5206\u7c7b\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u5b9e\u65f6\u9690\u79c1\u4fdd\u62a4\u8dcc\u5012\u9884\u9632\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22444", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.22444", "abs": "https://arxiv.org/abs/2506.22444", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "comment": null, "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e3b\u52a8\u6ce8\u610f\u529b\u7f51\u7edc\u7684PASC\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "PASC\u7684\u957f\u671f\u5f71\u54cd\u5bf9\u5168\u7403\u533b\u7597\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5176\u590d\u6742\u8fdb\u5c55\uff0c\u9700\u66f4\u7cbe\u51c6\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Llama-3.1-70B-Instruct\u751f\u6210\u6587\u672c\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff0c\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u6807\u6ce8\uff0c\u63d0\u51fa\u4e3b\u52a8\u6ce8\u610f\u529b\u7f51\u7edc\u9884\u6d4b\u98ce\u9669\u3002", "result": "\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u8bc6\u522b\u4e0e\u98ce\u9669\u76f8\u5173\u7684\u8fdb\u5c55\u4e8b\u4ef6\u3002", "conclusion": "\u7814\u7a76\u76ee\u6807\u662f\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u4e3b\u52a8\u5b66\u4e60\uff0c\u6539\u5584SARS-CoV-2\u60a3\u8005\u7684\u62a4\u7406\u548c\u51b3\u7b56\u3002"}}
{"id": "2506.22788", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22788", "abs": "https://arxiv.org/abs/2506.22788", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "comment": null, "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0eTransformer\u67b6\u6784\u7684SPI-BoTER\u65b9\u6cd5\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u5668\u4eba\u8f68\u8ff9\u8bef\u5dee\u8865\u507f\uff0c\u5728\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u7cbe\u5ea6\u9700\u6c42\u65e5\u76ca\u4e25\u683c\uff0c\u73b0\u6709\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u5b58\u5728\u5efa\u6a21\u7b80\u5316\u3001\u6570\u636e\u9a71\u52a8\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u53ca\u6570\u636e\u9700\u6c42\u5927\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u65b9\u7a0b\u4e0e\u7a00\u758f\u81ea\u6ce8\u610f\u529b\u63a9\u7801\u7684Transformer\u67b6\u6784\uff0c\u91c7\u7528\u53c2\u6570\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u7f51\u7edc\uff0c\u5e76\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u9006\u5173\u8282\u89d2\u8865\u507f\u3002", "result": "\u5728UR5\u673a\u68b0\u81c2\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0c3D\u7edd\u5bf9\u5b9a\u4f4d\u8bef\u5dee\u4e3a0.2515 mm\uff08\u6807\u51c6\u5dee0.15 mm\uff09\uff0c\u6bd4\u4f20\u7edfDNN\u65b9\u6cd5\u8bef\u5dee\u964d\u4f4e35.16%\uff0c\u9006\u89d2\u5ea6\u8865\u507f\u7b97\u6cd5\u5e73\u5747147\u6b21\u8fed\u4ee3\u6536\u655b\u81f30.01 mm\u7cbe\u5ea6\u3002", "conclusion": "SPI-BoTER\u65b9\u6cd5\u7ed3\u5408\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u6570\u636e\u9002\u5e94\u6027\uff0c\u4e3a\u5de5\u4e1a\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u5236\u9020\u4e2d\u7684\u7cbe\u5bc6\u4efb\u52a1\u3002"}}
{"id": "2506.22499", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.22499", "abs": "https://arxiv.org/abs/2506.22499", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "comment": null, "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548c\u4f20\u7edf\u4ea4\u901a\u6570\u636e\u7684\u591a\u7c7b\u522b\u52a8\u6001\u8d77\u8bab\u70b9\u9700\u6c42\u4f30\u8ba1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u672c\u5730\u4f20\u611f\u5668\u8def\u6bb5\u7684\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u672c\u5730\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\uff0c\u536b\u661f\u56fe\u50cf\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u57ce\u5e02\u4ea4\u901a\u4fe1\u606f\uff0c\u514b\u670d\u6570\u636e\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u6d41\u7a0b\u63d0\u53d6\u8f66\u8f86\u7c7b\u522b\u4fe1\u606f\uff0c\u6784\u5efa\u57fa\u4e8e\u8ba1\u7b97\u56fe\u7684DODE\u6a21\u578b\uff0c\u8054\u5408\u6821\u51c6\u4ea4\u901a\u8ba1\u6570\u548c\u65c5\u884c\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u536b\u661f\u6570\u636e\u663e\u8457\u63d0\u5347\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u65e0\u672c\u5730\u4f20\u611f\u5668\u7684\u8def\u6bb5\uff0c\u4e14\u6846\u67b6\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u57ce\u5e02\uff0c\u6570\u636e\u8d28\u91cf\u5bf9\u536b\u661f\u56fe\u50cf\u5f71\u54cd\u9700\u8fdb\u4e00\u6b65\u5206\u6790\u3002"}}
{"id": "2506.22445", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22445", "abs": "https://arxiv.org/abs/2506.22445", "authors": ["Saad Alqithami"], "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "comment": null, "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5206\u5c42\u5bf9\u6297\u5f39\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08HAMARL\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u5728\u591a\u4e2a\u5173\u952e\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u65e5\u76ca\u589e\u957f\u7684\u8fde\u63a5\u6027\u4f7f\u5176\u6613\u53d7\u590d\u6742\u7f51\u7edc\u5a01\u80c1\u653b\u51fb\uff0c\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "HAMARL\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff0c\u5305\u62ec\u4e13\u6ce8\u4e8e\u5b50\u7cfb\u7edf\u5b89\u5168\u7684\u672c\u5730\u667a\u80fd\u4f53\u548c\u5168\u5c40\u534f\u8c03\u5668\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u5faa\u73af\u4ee5\u6a21\u62df\u548c\u9884\u6d4b\u5a01\u80c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHAMARL\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u51cf\u5c11\u4e86\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u786e\u4fdd\u4e86\u64cd\u4f5c\u8fde\u7eed\u6027\u3002", "conclusion": "\u7ed3\u5408\u5206\u5c42\u591a\u667a\u80fd\u4f53\u534f\u8c03\u548c\u5bf9\u6297\u611f\u77e5\u8bad\u7ec3\uff0c\u53ef\u6709\u6548\u63d0\u5347\u4e0b\u4e00\u4ee3CPS\u7684\u5f39\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22827", "abs": "https://arxiv.org/abs/2506.22827", "authors": ["Andr\u00e9 Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u53ef\u9760\u7684\u591a\u6b65\u64cd\u4f5c\u4efb\u52a1\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u548c\u5bb6\u5ead\u73af\u5883\u4e2d\u53ef\u9760\u6267\u884c\u590d\u6742\u591a\u6b65\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u5c42\u7cfb\u7edf\u5305\u62ec\u4f4e\u5c42RL\u63a7\u5236\u5668\u3001\u4e2d\u5c42\u6a21\u4eff\u5b66\u4e60\u6280\u80fd\u7b56\u7565\u548c\u9ad8\u5c42\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\u6a21\u5757\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u5b8c\u6210\u975e\u6293\u53d6\u5f0f\u62fe\u653e\u4efb\u52a1\uff0c\u6210\u529f\u7387\u4e3a72.5%\u3002", "conclusion": "\u5206\u5c42\u7cfb\u7edf\u53ef\u884c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2506.22500", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u624b\u672f\u5ba4\u98ce\u9669\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff08VS-KC\uff09\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6OR-VSKC\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u672f\u98ce\u9669\u8bc6\u522b\u5bf9\u60a3\u8005\u5b89\u5168\u548c\u51cf\u5c11\u53ef\u9884\u9632\u533b\u7597\u9519\u8bef\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u5b89\u5168\u8fdd\u89c4\u68c0\u6d4b\u4e2d\u5b58\u5728\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u621034,000\u591a\u5f20\u5408\u6210\u56fe\u50cf\uff0c\u6a21\u62df\u624b\u672f\u5ba4\u5b89\u5168\u8fdd\u89c4\u573a\u666f\uff0c\u5e76\u8f85\u4ee5214\u5f20\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\u4f5c\u4e3a\u9a8c\u8bc1\u57fa\u51c6\u3002", "result": "\u5728OR-VSKC\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684MLLMs\u663e\u8457\u63d0\u5347\u4e86\u5df2\u77e5\u51b2\u7a81\u5b9e\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u5bf9\u672a\u8bad\u7ec3\u5b9e\u4f53\u7c7b\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u8d21\u732e\u5305\u62ec\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3001\u5f00\u6e90\u6570\u636e\u96c6OR-VSKC\u53ca\u57fa\u51c6\uff0c\u4ee5\u53ca\u5bf9MLLMs\u77e5\u8bc6\u4e00\u81f4\u6027\u7684\u5b9e\u8bc1\u5206\u6790\u3002"}}
{"id": "2506.22446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22446", "abs": "https://arxiv.org/abs/2506.22446", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "comment": null, "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "AI": {"tldr": "EAGLE\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u6a21\u6001\u878d\u5408\u89e3\u51b3\u4e86\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5e76\u5728\u4e09\u79cd\u764c\u75c7\u7c7b\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u5b58\u5728\u7b80\u5355\u7b56\u7565\u3001\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002", "method": "EAGLE\u91c7\u7528\u52a8\u6001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u3001\u5927\u89c4\u6a21\u964d\u7ef4\u3001\u4e09\u79cd\u4e92\u8865\u5f52\u56e0\u65b9\u6cd5\u548c\u7edf\u4e00\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728911\u540d\u60a3\u8005\u4e2d\u9a8c\u8bc1\uff0cEAGLE\u80fd\u8bc6\u522b\u9ad8\u98ce\u9669\u548c\u4f4e\u98ce\u9669\u7fa4\u4f53\uff0c\u751f\u5b58\u5dee\u5f02\u663e\u8457\uff084-5\u500d\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u8d21\u732e\u5206\u6790\u3002", "conclusion": "EAGLE\u7ed3\u5408\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u4fe1\u4efb\u3002"}}
{"id": "2506.22894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6f02\u79fb\uff0c\u7ed3\u5408\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\u548c\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff08PSF\uff09\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u7a33\u5b9a\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u4e0d\u7a33\u5b9a\u6027\u6f02\u79fb\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u63a2\u7d22\u80fd\u529b\u6709\u9650\u4e14\u672a\u5145\u5206\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "method": "\u96c6\u6210RL\u4ee3\u7406\u4e0e\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\uff0c\u4f7f\u7528PSF\u5728\u7ebf\u8c03\u6574\u52a8\u4f5c\u4ee5\u907f\u514d\u4e0d\u5b89\u5168\u72b6\u6001\u3002", "result": "\u5728Matlab-Carsim\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u6f02\u79fb\u6027\u80fd\u3001\u51cf\u5c11\u8ddf\u8e2a\u8bef\u5dee\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2506.22501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22501", "abs": "https://arxiv.org/abs/2506.22501", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatialNet-ViT\u7684\u65b0\u6a21\u578b\uff0c\u7ed3\u5408Vision Transformers\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u9065\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u6216\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u9065\u611f\u5206\u7c7b\u6311\u6218\u3002", "method": "\u5229\u7528Vision Transformers\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u3001\u8fc1\u79fb\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "SpatialNet-ViT\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2506.22447", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22447", "abs": "https://arxiv.org/abs/2506.22447", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "comment": null, "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u3001\u591a\u53d8\u91cf\u7684Vision Transformer\u67b6\u6784\uff081EMD\uff09\uff0c\u7528\u4e8e\u8054\u5408\u9884\u6d4b\u4e09\u4e2a\u5173\u952e\u6c14\u5019\u53d8\u91cf\uff0c\u4f18\u4e8e\u5355\u53d8\u91cf\u57fa\u7ebf\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5168\u7403\u6c14\u5019\u6a21\u578b\uff08GCMs\uff09\u5206\u8fa8\u7387\u4f4e\uff0c\u533a\u57df\u6c14\u5019\u6a21\u578b\uff08RCMs\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7075\u6d3b\u6027\u6709\u9650\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u591a\u4e3a\u5355\u53d8\u91cf\u6a21\u578b\uff0c\u7f3a\u4e4f\u8de8\u53d8\u91cf\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7f16\u7801\u5668\u548c\u53d8\u91cf\u7279\u5b9a\u89e3\u7801\u5668\u7684ViT\u67b6\u6784\uff0c\u8054\u5408\u9884\u6d4b\u6e29\u5ea6\u3001\u98ce\u901f\u548c500 hPa\u4f4d\u52bf\u9ad8\u5ea6\u3002", "result": "\u591a\u53d8\u91cf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8de8\u53d8\u91cf\u77e5\u8bc6\u8f6c\u79fb\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u53d8\u91cf\u57fa\u7ebf\uff0c\u5e76\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u591a\u53d8\u91cf\u5efa\u6a21\u5728\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u964d\u5c3a\u5ea6\u4e2d\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2506.22942", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22942", "abs": "https://arxiv.org/abs/2506.22942", "authors": ["Kartik A. Pant", "Jaehyeok Kim", "James M. Goppert", "Inseok Hwang"], "title": "Energy-Constrained Resilient Multi-Robot Coverage Control", "comment": "6 pages, 4 figures", "summary": "The problem of multi-robot coverage control becomes significantly challenging\nwhen multiple robots leave the mission space simultaneously to charge their\nbatteries, disrupting the underlying network topology for communication and\nsensing. To address this, we propose a resilient network design and control\napproach that allows robots to achieve the desired coverage performance while\nsatisfying energy constraints and maintaining network connectivity throughout\nthe mission. We model the combined motion, energy, and network dynamics of the\nmultirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,\nreturn-to-base, and recharge, respectively. We show that ensuring the energy\nconstraints can be transformed into designing appropriate guard conditions for\nmode transition between each of the three modes. Additionally, we present a\nsystematic procedure to design, maintain, and reconfigure the underlying\nnetwork topology using an energy-aware bearing rigid network design, enhancing\nthe structural resilience of the MRS even when a subset of robots departs to\ncharge their batteries. Finally, we validate our proposed method using\nnumerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u8986\u76d6\u63a7\u5236\u7684\u5f39\u6027\u7f51\u7edc\u8bbe\u8ba1\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u540c\u65f6\u79bb\u5f00\u4efb\u52a1\u7a7a\u95f4\u5145\u7535\u65f6\u7684\u7f51\u7edc\u62d3\u6251\u95ee\u9898\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u540c\u65f6\u79bb\u5f00\u4efb\u52a1\u7a7a\u95f4\u5145\u7535\u4f1a\u7834\u574f\u901a\u4fe1\u548c\u611f\u77e5\u7684\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\uff0c\u5f71\u54cd\u8986\u76d6\u6027\u80fd\u3002", "method": "\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u3001\u80fd\u91cf\u548c\u7f51\u7edc\u52a8\u6001\u5efa\u6a21\u4e3a\u6df7\u5408\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u6a21\u5f0f\u8f6c\u6362\u7684\u5b88\u536b\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u80fd\u91cf\u611f\u77e5\u7684\u8f74\u627f\u521a\u6027\u7f51\u7edc\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u6ee1\u8db3\u80fd\u91cf\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u8fde\u63a5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ed3\u6784\u5f39\u6027\uff0c\u786e\u4fdd\u8986\u76d6\u6027\u80fd\u548c\u7f51\u7edc\u8fde\u63a5\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.22503", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22503", "abs": "https://arxiv.org/abs/2506.22503", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "comment": null, "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4e09\u7ef4\u59ff\u6001\u8ddf\u8e2a\u6570\u636e\u63d0\u5347\u5bf9\u8db3\u7403\u4e2d\u76d8\u5e26\u6280\u80fd\u7684\u7406\u89e3\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\uff0c\u65b0\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e8c\u7ef4\u4f4d\u7f6e\u6570\u636e\u65e0\u6cd5\u5168\u9762\u6355\u6349\u76d8\u5e26\u4e2d\u7684\u5e73\u8861\u3001\u65b9\u5411\u6027\u548c\u63a7\u7403\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u5206\u6790\u7684\u6df1\u5ea6\u3002", "method": "\u4ece2022/23\u8d5b\u5b63\u6b27\u51a0\u76841,736\u6b21\u76d8\u5e26\u4e2d\u63d0\u53d6\u57fa\u4e8e\u59ff\u6001\u7684\u65b0\u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u76d8\u5e26\u6210\u529f\u7684\u5f71\u54cd\u3002", "result": "\u653b\u51fb\u8005\u7684\u5e73\u8861\u53ca\u4e0e\u9632\u5b88\u8005\u7684\u65b9\u5411\u5bf9\u9f50\u7b49\u59ff\u6001\u7279\u5f81\u5bf9\u9884\u6d4b\u76d8\u5e26\u6210\u529f\u5177\u6709\u4fe1\u606f\u91cf\uff0c\u7ed3\u5408\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\u540e\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4e09\u7ef4\u59ff\u6001\u6570\u636e\u4e3a\u8db3\u7403\u76d8\u5e26\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c6\u89d2\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u6280\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2506.22502", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.22502", "abs": "https://arxiv.org/abs/2506.22502", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "title": "Stabilization of industrial processes with time series machine learning", "comment": null, "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u795e\u7ecf\u7f51\u7edc\u7684\u7b80\u5355\u6d41\u7a0b\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u4e863\u500d\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u7a33\u5b9a\u5316\u5728\u5de5\u4e1a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u63d0\u5347\u7a33\u5b9a\u5316\u8d28\u91cf\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff08\u9884\u6d4b\u5668\u548c\u4f18\u5316\u5668\uff09\u7684\u6d41\u7a0b\uff0c\u5c06\u70b9\u503c\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u95ee\u9898\u3002", "result": "\u5728\u6e29\u5ea6\u63a7\u5236\u65b9\u9762\uff0c\u7a33\u5b9a\u6027\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea63\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u7a33\u5b9a\u5316\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22956", "abs": "https://arxiv.org/abs/2506.22956", "authors": ["David Rodr\u00edguez-Mart\u00ednez", "Dave van der Meer", "Junlin Song", "Abishek Bera", "C. J. P\u00e9rez-del-Pulgar", "Miguel Angel Olivares-Mendez"], "title": "SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes", "comment": "10 pages, 8 figures, dataset", "summary": "Exploring high-latitude lunar regions presents an extremely challenging\nvisual environment for robots. The low sunlight elevation angle and minimal\nlight scattering result in a visual field dominated by a high dynamic range\nfeaturing long, dynamic shadows. Reproducing these conditions on Earth requires\nsophisticated simulators and specialized facilities. We introduce a unique\ndataset recorded at the LunaLab from the SnT - University of Luxembourg, an\nindoor test facility designed to replicate the optical characteristics of\nmultiple lunar latitudes. Our dataset includes images, inertial measurements,\nand wheel odometry data from robots navigating seven distinct trajectories\nunder multiple illumination scenarios, simulating high-latitude lunar\nconditions from dawn to night time with and without the aid of headlights,\nresulting in 88 distinct sequences containing a total of 1.3M images. Data was\ncaptured using a stereo RGB-inertial sensor, a monocular monochrome camera, and\nfor the first time, a novel single-photon avalanche diode (SPAD) camera. We\nrecorded both static and dynamic image sequences, with robots navigating at\nslow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,\nand timestamped, providing a valuable resource for validating perception tasks\nfrom vision-based autonomous navigation to scientific imaging for future lunar\nmissions targeting high-latitude regions or those intended for robots operating\nacross perceptually degraded environments. The dataset can be downloaded from\nhttps://zenodo.org/records/13970078?preview=1, and a visual overview is\navailable at https://youtu.be/d7sPeO50_2I. All supplementary material can be\nfound at https://github.com/spaceuma/spice-hl3.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5728LunaLab\u8bbe\u65bd\u4e2d\u8bb0\u5f55\u7684\u9ad8\u7eac\u5ea6\u6708\u7403\u73af\u5883\u6a21\u62df\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u50cf\u3001\u60ef\u6027\u6d4b\u91cf\u548c\u8f6e\u5f0f\u91cc\u7a0b\u6570\u636e\uff0c\u7528\u4e8e\u9a8c\u8bc1\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u9ad8\u7eac\u5ea6\u6708\u7403\u533a\u57df\u7684\u89c6\u89c9\u73af\u5883\u5bf9\u673a\u5668\u4eba\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u5728\u5730\u7403\u4e0a\u6a21\u62df\u8fd9\u4e9b\u6761\u4ef6\u4ee5\u652f\u6301\u672a\u6765\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u4f20\u611f\u5668\uff08\u5305\u62ec\u65b0\u578bSPAD\u76f8\u673a\uff09\u5728LunaLab\u4e2d\u8bb0\u5f55\u4e0d\u540c\u5149\u7167\u548c\u901f\u5ea6\u4e0b\u7684\u673a\u5668\u4eba\u5bfc\u822a\u6570\u636e\u3002", "result": "\u751f\u6210\u4e8688\u4e2a\u5e8f\u5217\u51711.3M\u56fe\u50cf\u7684\u6821\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u5149\u7167\u6761\u4ef6\u548c\u673a\u5668\u4eba\u901f\u5ea6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u4efb\u52a1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.22504", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22504", "abs": "https://arxiv.org/abs/2506.22504", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "comment": null, "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff08Patch2Loc\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4ece\u6b63\u5e38\u8111\u90e8MRI\u4e2d\u5b66\u4e60\uff0c\u68c0\u6d4b\u5f02\u5e38\u8111\u7ec4\u7ec7\u3002", "motivation": "\u8111\u90e8\u75c5\u53d8\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06MRI\u4e2d\u7684\u6b63\u5e38\u8111\u7ec4\u7ec7\u8865\u4e01\u6620\u5c04\u5230\u5176\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u68c0\u6d4b\u5f02\u5e38\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPatch2Loc\u5728\u65e0\u76d1\u7763\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Patch2Loc\u4e3a\u65e0\u76d1\u7763\u8111\u90e8\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22530", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22530", "abs": "https://arxiv.org/abs/2506.22530", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "Task-Agnostic Contrastive Pretraining for Relational Deep Learning", "comment": "arXiv admin note: text overlap with arXiv:2506.22199", "summary": "Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph\nNeural Network principles to learn directly from relational databases by\nrepresenting them as heterogeneous graphs. However, existing RDL models\ntypically rely on task-specific supervised learning, requiring training\nseparate models for each predictive task, which may hamper scalability and\nreuse.\n  In this work, we propose a novel task-agnostic contrastive pretraining\napproach for RDL that enables database-wide representation learning. For that\naim, we introduce three levels of contrastive objectives$-$row-level,\nlink-level, and context-level$-$designed to capture the structural and semantic\nheterogeneity inherent to relational data. We implement the respective\npretraining approach through a modular RDL architecture and an efficient\nsampling strategy tailored to the heterogeneous database setting. Our\npreliminary results on standard RDL benchmarks demonstrate that fine-tuning the\npretrained models measurably outperforms training from scratch, validating the\npromise of the proposed methodology in learning transferable representations\nfor relational data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\uff0c\u901a\u8fc7\u4e09\u4e2a\u5c42\u6b21\u7684\u5bf9\u6bd4\u76ee\u6807\u5b66\u4e60\u6570\u636e\u5e93\u8303\u56f4\u5185\u7684\u8868\u793a\u3002", "motivation": "\u73b0\u6709RDL\u6a21\u578b\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u91cd\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u884c\u7ea7\u3001\u94fe\u63a5\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u4e09\u4e2a\u5bf9\u6bd4\u76ee\u6807\uff0c\u8bbe\u8ba1\u6a21\u5757\u5316RDL\u67b6\u6784\u548c\u9ad8\u6548\u91c7\u6837\u7b56\u7565\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5173\u7cfb\u6570\u636e\u5b66\u4e60\u53ef\u8fc1\u79fb\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23023", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23023", "abs": "https://arxiv.org/abs/2506.23023", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "comment": "6 pages, 10 figures, submitted to a conference", "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "AI": {"tldr": "SAD-RL\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u573a\u666f\u5316\u73af\u5883\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u884c\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6cdb\u5316\u6027\u548c\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSAD-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u7b56\u7565\uff08\u9ad8\u5c42\u9009\u62e9\u6a21\u677f\uff0c\u4f4e\u5c42\u6267\u884c\uff09\u548c\u573a\u666f\u5316\u73af\u5883\uff0c\u5f15\u5165\u6311\u6218\u6027\u573a\u666f\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSAD-RL\u80fd\u9ad8\u6548\u5b9e\u73b0\u5b89\u5168\u884c\u4e3a\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5206\u5c42\u7b56\u7565\u548c\u573a\u666f\u591a\u6837\u6027\u662f\u5173\u952e\u3002", "conclusion": "SAD-RL\u4e3a\u590d\u6742\u9a7e\u9a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22505", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22505", "abs": "https://arxiv.org/abs/2506.22505", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "comment": null, "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5f31\u76d1\u7763\uff08\u56fe\u50cf\u7ea7\u6807\u7b7e\uff09\u8bad\u7ec3\u4e8c\u503c\u5bf9\u8c61\u5206\u5272\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u80cc\u666f\u56fe\u50cf\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4e13\u4e1a\u56fe\u50cf\u9886\u57df\uff08\u5982\u58f0\u7eb3\u3001\u9065\u611f\u3001\u751f\u7269\u533b\u5b66\u7b49\uff09\uff0c\u50cf\u7d20\u7ea7\u5206\u5272\u6210\u672c\u9ad8\uff0c\u800c\u56fe\u50cf\u7ea7\u6807\u7b7e\u66f4\u6613\u83b7\u53d6\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u80cc\u666f\u56fe\u50cf\uff0c\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\uff08\u5c06\u5206\u5272\u5bf9\u8c61\u7f6e\u4e8e\u76ee\u6807\u80cc\u666f\u4e2d\uff09\uff0c\u5e76\u5229\u7528\u6837\u672c\u5dee\u5f02\u548c\u80cc\u666f\u76d1\u7763\u635f\u5931\u8bad\u7ec3\u7f51\u7edc\u3002", "result": "\u5728\u58f0\u7eb3\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\u4e0a\u5747\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u9884\u8bad\u7ec3\u7f51\u7edc\u6216\u5bf9\u6297\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u4e0b\u6709\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u9886\u57df\uff0c\u4e14\u907f\u514d\u4e86\u590d\u6742\u6a21\u578b\u4f9d\u8d56\u3002"}}
{"id": "2506.22566", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22566", "abs": "https://arxiv.org/abs/2506.22566", "authors": ["Jacob Adamczyk"], "title": "Exploration Behavior of Untrained Policies", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u5728\u8bad\u7ec3\u524d\u5982\u4f55\u9690\u5f0f\u5f71\u54cd\u63a2\u7d22\u884c\u4e3a\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5c55\u793a\u4e86\u672a\u8bad\u7ec3\u7b56\u7565\u5728\u73a9\u5177\u6a21\u578b\u4e2d\u751f\u6210\u5f39\u9053\u6216\u6269\u6563\u8f68\u8ff9\u7684\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u6216\u5bf9\u6297\u6027\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u7406\u89e3\u7b56\u7565\u521d\u59cb\u5316\u5bf9\u63a2\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5229\u7528\u65e0\u9650\u5bbd\u5ea6\u7f51\u7edc\u7406\u8bba\u548c\u8fde\u7eed\u65f6\u95f4\u6781\u9650\uff0c\u5206\u6790\u672a\u8bad\u7ec3\u7b56\u7565\u4ea7\u751f\u7684\u76f8\u5173\u52a8\u4f5c\u548c\u975e\u5e73\u51e1\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u67b6\u6784\u8ba8\u8bba\u8f68\u8ff9\u5206\u5e03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u672a\u8bad\u7ec3\u7b56\u7565\u80fd\u4ea7\u751f\u76f8\u5173\u52a8\u4f5c\u548c\u975e\u5e73\u51e1\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\uff0c\u4e3a\u63a2\u7d22\u884c\u4e3a\u7684\u65e9\u671f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5229\u7528\u7b56\u7565\u521d\u59cb\u5316\u4f5c\u4e3a\u8bbe\u8ba1\u5de5\u5177\u6765\u7406\u89e3\u65e9\u671f\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u884c\u4e3a\u5efa\u7acb\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u6846\u67b6\u3002"}}
{"id": "2506.23078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23078", "abs": "https://arxiv.org/abs/2506.23078", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "comment": null, "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "AI": {"tldr": "Voxel-ESVIO\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f53\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u9ad8\u6548\u7b5b\u9009\u9ad8\u8d28\u91cf3D\u70b9\uff0c\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f18\u5f02\u65f6\u95f4\u5206\u8fa8\u7387\u4f7f\u5176\u6210\u4e3a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u91cd\u8981\u4f20\u611f\u5668\uff0c\u4f46\u4e8b\u4ef6\u6d41\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u5f71\u54cd\u4e86\u9ad8\u8d28\u91cf\u5730\u56fe\u70b9\u7684\u9009\u62e9\uff0c\u4ece\u800c\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u70b9\u9009\u62e9\u548c\u4f53\u7d20\u611f\u77e5\u7684\u70b9\u7ba1\u7406\uff0c\u534f\u540c\u4f18\u5316\u5730\u56fe\u70b9\u7684\u9009\u62e9\u548c\u66f4\u65b0\uff0c\u9ad8\u6548\u63d0\u53d6\u6297\u566a\u58f0\u7684\u5730\u56fe\u70b9\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVoxel-ESVIO\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Voxel-ESVIO\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22509", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff08DNA\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u7edf\u8ba1\uff0c\u589e\u5f3a\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5efa\u6a21\u57df\u4fe1\u606f\u5206\u5e03\u8f6c\u6362\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u566a\u58f0\u7edf\u8ba1\u504f\u5dee\u4f1a\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDomain Noise Alignment\uff08DNA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u566a\u58f0\u7edf\u8ba1\uff0c\u6216\u5728\u65e0\u6e90\u57df\u60c5\u51b5\u4e0b\u5229\u7528\u9ad8\u7f6e\u4fe1\u533a\u57df\u9010\u6b65\u8c03\u6574\u566a\u58f0\u7edf\u8ba1\u3002", "result": "\u5728\u56db\u79cd\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86DNA\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u57df\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DNA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6269\u6563\u5bc6\u96c6\u9884\u6d4b\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22578", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22578", "abs": "https://arxiv.org/abs/2506.22578", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "title": "The Hidden Link Between RLHF and Contrastive Learning", "comment": null, "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4e92\u4fe1\u606f\uff08MI\uff09\u6700\u5927\u5316\u7684\u89c6\u89d2\u91cd\u65b0\u89e3\u91caRLHF\u548cDPO\uff0c\u63d0\u51faMIO\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DPO\u4e2d\u540e\u671f\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u7406\u548c\u6570\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793aRLHF\u548cDPO\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8054\u7cfb\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u4eceMI\u6700\u5927\u5316\u89d2\u5ea6\u5206\u6790RLHF\u548cDPO\uff0c\u63d0\u51fa\u57fa\u4e8eJensen-Shannon MI\u4f30\u8ba1\u5668\u7684MIO\u65b9\u6cd5\u3002", "result": "MIO\u89e3\u51b3\u4e86DPO\u7684\u540e\u671f\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u63a8\u7406\u548c\u6570\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "MIO\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2506.23114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23114", "abs": "https://arxiv.org/abs/2506.23114", "authors": ["Zhanxiang Cao", "Buqing Nie", "Yang Zhang", "Yue Gao"], "title": "Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications", "comment": "8 pages,6 figures, IROS2025", "summary": "Recent advancements in quadruped robot research have significantly improved\ntheir ability to traverse complex and unstructured outdoor environments.\nHowever, the issue of noise generated during locomotion is generally\noverlooked, which is critically important in noise-sensitive indoor\nenvironments, such as service and healthcare settings, where maintaining low\nnoise levels is essential. This study aims to optimize the acoustic noise\ngenerated by quadruped robots during locomotion through the development of\nadvanced motion control algorithms. To achieve this, we propose a novel\napproach that minimizes noise emissions by integrating optimized gait design\nwith tailored control strategies. This method achieves an average noise\nreduction of approximately 8 dBA during movement, thereby enhancing the\nsuitability of quadruped robots for deployment in noise-sensitive indoor\nenvironments. Experimental results demonstrate the effectiveness of this\napproach across various indoor settings, highlighting the potential of\nquadruped robots for quiet operation in noise-sensitive environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u6b65\u6001\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u65f6\u7684\u566a\u97f3\uff0c\u5e73\u5747\u51cf\u5c11\u7ea68 dBA\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u566a\u97f3\u654f\u611f\u7684\u5ba4\u5185\u73af\u5883\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u80fd\u529b\u5df2\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u8fd0\u52a8\u4ea7\u751f\u7684\u566a\u97f3\u5728\u566a\u97f3\u654f\u611f\u7684\u5ba4\u5185\u73af\u5883\uff08\u5982\u670d\u52a1\u548c\u533b\u7597\u573a\u6240\uff09\u4e2d\u88ab\u5ffd\u89c6\uff0c\u800c\u4f4e\u566a\u97f3\u6c34\u5e73\u5bf9\u8fd9\u4e9b\u73af\u5883\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u4f18\u5316\u7684\u6b65\u6001\u8bbe\u8ba1\u548c\u5b9a\u5236\u63a7\u5236\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u566a\u97f3\u6392\u653e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5ba4\u5185\u73af\u5883\u4e2d\u6709\u6548\uff0c\u5e73\u5747\u566a\u97f3\u964d\u4f4e\u7ea68 dBA\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u566a\u97f3\u654f\u611f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u9759\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22511", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22511", "abs": "https://arxiv.org/abs/2506.22511", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "title": "Lightning the Night with Generative Artificial Intelligence", "comment": null, "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8eFY4B\u536b\u661f\u7684\u591a\u6ce2\u6bb5\u70ed\u7ea2\u5916\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u9ad8\u7cbe\u5ea6\u7684\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u68c0\u7d22\u6a21\u578bRefDiff\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u5e76\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u591c\u95f4\u56e0\u7f3a\u4e4f\u53ef\u89c1\u5149\u800c\u65e0\u6cd5\u8fdb\u884c\u5168\u5929\u5019\u6c14\u8c61\u89c2\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eFY4B\u536b\u661f\u7684AGRI\u591a\u6ce2\u6bb5\u70ed\u7ea2\u5916\u6570\u636e\uff0c\u5f00\u53d1\u751f\u6210\u6269\u6563\u6a21\u578bRefDiff\uff0c\u5b9e\u73b0\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u68c0\u7d22\u3002", "result": "RefDiff\u7684SSIM\u6307\u6570\u8fbe0.90\uff0c\u5728\u590d\u6742\u4e91\u7ed3\u6784\u548c\u539a\u4e91\u533a\u57df\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u591c\u95f4\u68c0\u7d22\u80fd\u529b\u4e0e\u767d\u5929\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u68c0\u7d22\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u591c\u95f4\u53ef\u89c1\u5149\u6570\u636e\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22602", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22602", "abs": "https://arxiv.org/abs/2506.22602", "authors": ["Joshua C. Zhao", "Saurabh Bagchi"], "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?", "comment": "13 pages", "summary": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u65b9\u6cd5\uff08FGSM\uff09\u66ff\u4ee3\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0FGSM\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bf9\u6297\u6027\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8fc1\u79fb\u5b66\u4e60\u7684\u5fae\u8c03\u9636\u6bb5\u3002\u7814\u7a76\u65e8\u5728\u964d\u4f4e\u8fd9\u4e00\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1FGSM\u5728\u5bf9\u6297\u6027\u5fae\u8c03\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e0ePGD\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "FGSM\u5728\u03b5=4\u548c\u03b5=8\u7684\u6270\u52a8\u9884\u7b97\u4e0b\uff0c\u4ec5\u635f\u5931\u5c11\u91cf\u9c81\u68d2\u6027\uff080.39%\u548c1.39%\uff09\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c114\u500d\u3002", "conclusion": "FGSM\u5728\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u8fc1\u79fb\u5b66\u4e60\u4e2d\u4e0d\u4ec5\u9ad8\u6548\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1PGD\uff0c\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2506.23125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23125", "abs": "https://arxiv.org/abs/2506.23125", "authors": ["Zhanxiang Cao", "Yang Zhang", "Buqing Nie", "Huangxuan Lin", "Haoyang Li", "Yue Gao"], "title": "Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots", "comment": "8 pages, 8 figures", "summary": "Learning policies for complex humanoid tasks remains both challenging and\ncompelling. Inspired by how infants and athletes rely on external support--such\nas parental walkers or coach-applied guidance--to acquire skills like walking,\ndancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive\nCurriculum Force for humanoid motion learning. A2CF trains a dual-agent system,\nin which a dedicated assistive force agent applies state-dependent forces to\nguide the robot through difficult initial motions and gradually reduces\nassistance as the robot's proficiency improves. Across three\nbenchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves\nconvergence 30% faster than baseline methods, lowers failure rates by over 40%,\nand ultimately produces robust, support-free policies. Real-world experiments\nfurther demonstrate that adaptively applied assistive forces significantly\naccelerate the acquisition of complex skills in high-dimensional robotic\ncontrol.", "AI": {"tldr": "A2CF\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u52a0\u901f\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u52a8\u4f5c\u5b66\u4e60\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\u3002", "motivation": "\u53d7\u5a74\u513f\u548c\u8fd0\u52a8\u5458\u4f9d\u8d56\u5916\u90e8\u652f\u6301\u5b66\u4e60\u590d\u6742\u52a8\u4f5c\u7684\u542f\u53d1\uff0c\u63d0\u51faA2CF\u65b9\u6cd5\u4ee5\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u5b66\u4e60\u7684\u6311\u6218\u3002", "method": "A2CF\u8bad\u7ec3\u53cc\u4ee3\u7406\u7cfb\u7edf\uff0c\u8f85\u52a9\u529b\u4ee3\u7406\u6839\u636e\u72b6\u6001\u65bd\u52a0\u529b\u5f15\u5bfc\u673a\u5668\u4eba\uff0c\u5e76\u968f\u719f\u7ec3\u5ea6\u63d0\u5347\u9010\u6b65\u51cf\u5c11\u8f85\u52a9\u3002", "result": "\u5728\u884c\u8d70\u3001\u821e\u8e48\u548c\u540e\u7a7a\u7ffb\u4efb\u52a1\u4e2d\uff0cA2CF\u6536\u655b\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\uff0c\u751f\u6210\u65e0\u9700\u652f\u6301\u7684\u7a33\u5065\u7b56\u7565\u3002", "conclusion": "\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u663e\u8457\u52a0\u901f\u9ad8\u7ef4\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u590d\u6742\u6280\u80fd\u7684\u83b7\u53d6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.22513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22513", "abs": "https://arxiv.org/abs/2506.22513", "authors": ["Aditya Sharma"], "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "comment": null, "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eNDE 4.0\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c04\u7ebf\u68c0\u6d4b\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u901a\u8fc7\u865a\u62df\u7f3a\u9677\u589e\u5f3a\u548c\u6570\u636e\u6269\u5c55\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5c04\u7ebf\u68c0\u6d4b\u4e2d\u4fe1\u606f\u89e3\u91ca\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u6700\u5927\u5316\u865a\u62df\u7f3a\u9677\u589e\u5f3a\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "method": "\u6536\u96c6\u5e76\u5206\u7c7b223\u5f20\u98de\u673a\u710a\u7f1d\u7684CR\u7167\u7247\uff0c\u4f7f\u7528\u865a\u62df\u7f3a\u9677\u589e\u5f3a\u548c\u6807\u51c6\u589e\u5f3a\u6269\u5c55\u6570\u636e\uff0c\u8bad\u7ec3\u6539\u8fdb\u7684U-net\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7f3a\u9677\u5206\u5272\u3002", "result": "\u6a21\u578b\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7075\u654f\u5ea6\uff0c\u7ed3\u5408\u6269\u5c55\u65b9\u6cd5\u5728\u710a\u7f1d\u533a\u57df\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u5177\u5907\u5feb\u901f\u5904\u7406\u5927\u56fe\u50cf\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u88ab\u4e13\u4e1a\u8bc4\u4f30\u8005\u8ba4\u53ef\uff0c\u5177\u5907\u4f5c\u4e3a\u68c0\u6d4b\u6d41\u7a0b\u652f\u6301\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22621", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22621", "abs": "https://arxiv.org/abs/2506.22621", "authors": ["Paul Saves", "Edward Hall\u00e9-Hannan", "Jasper Bussemaker", "Youssef Diouane", "Nathalie Bartoli"], "title": "Hierarchical Modeling and Architecture Optimization: Review and Unified Framework", "comment": null, "summary": "Simulation-based problems involving mixed-variable inputs frequently feature\ndomains that are hierarchical, conditional, heterogeneous, or tree-structured.\nThese characteristics pose challenges for data representation, modeling, and\noptimization. This paper reviews extensive literature on these structured input\nspaces and proposes a unified framework that generalizes existing approaches.\nIn this framework, input variables may be continuous, integer, or categorical.\nA variable is described as meta if its value governs the presence of other\ndecreed variables, enabling the modeling of conditional and hierarchical\nstructures.\n  We further introduce the concept of partially-decreed variables, whose\nactivation depends on contextual conditions. To capture these inter-variable\nhierarchical relationships, we introduce design space graphs, combining\nprinciples from feature modeling and graph theory. This allows the definition\nof general hierarchical domains suitable for describing complex system\narchitectures. The framework supports the use of surrogate models over such\ndomains and integrates hierarchical kernels and distances for efficient\nmodeling and optimization. The proposed methods are implemented in the\nopen-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are\ndemonstrated through applications in Bayesian optimization for complex system\ndesign, including a case study in green aircraft architecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6df7\u5408\u53d8\u91cf\u8f93\u5165\u7684\u4eff\u771f\u95ee\u9898\uff0c\u652f\u6301\u8fde\u7eed\u3001\u6574\u6570\u548c\u5206\u7c7b\u53d8\u91cf\uff0c\u5e76\u5f15\u5165\u5143\u53d8\u91cf\u548c\u90e8\u5206\u5b9a\u4e49\u53d8\u91cf\u4ee5\u5efa\u6a21\u6761\u4ef6\u4e0e\u5c42\u6b21\u7ed3\u6784\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u53d8\u91cf\u8f93\u5165\u5728\u5c42\u6b21\u5316\u3001\u6761\u4ef6\u5316\u3001\u5f02\u6784\u5316\u6216\u6811\u72b6\u7ed3\u6784\u9886\u57df\u4e2d\u7684\u6570\u636e\u8868\u793a\u3001\u5efa\u6a21\u548c\u4f18\u5316\u6311\u6218\u3002", "method": "\u5f15\u5165\u8bbe\u8ba1\u7a7a\u95f4\u56fe\u7ed3\u5408\u7279\u5f81\u5efa\u6a21\u548c\u56fe\u8bba\uff0c\u63d0\u51fa\u652f\u6301\u5c42\u6b21\u6838\u548c\u8ddd\u79bb\u7684\u4ee3\u7406\u6a21\u578b\u6846\u67b6\u3002", "result": "\u65b9\u6cd5\u5728\u5f00\u6e90\u5de5\u5177SMT 2.0\u4e2d\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\uff08\u5982\u7eff\u8272\u98de\u673a\u67b6\u6784\uff09\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "\u6846\u67b6\u4e3a\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5efa\u6a21\u4e0e\u4f18\u5316\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.23126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23126", "abs": "https://arxiv.org/abs/2506.23126", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "comment": null, "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "AI": {"tldr": "ParticleFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u70b9\u4e91\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u7684\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u65e0\u9700\u590d\u6742\u573a\u666f\u91cd\u5efa\u5373\u53ef\u8bad\u7ec3\uff0c\u5e76\u5728\u52a8\u6001\u9884\u6d4b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u67093D\u4e16\u754c\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u6750\u6599\u52a8\u6001\u4e14\u9700\u8981\u8017\u65f6\u76843D\u91cd\u5efa\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u7684\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u63d0\u51faParticleFormer\uff0c\u7ed3\u5408Transformer\u548c\u70b9\u4e91\u91cd\u5efa\u635f\u5931\uff0c\u76f4\u63a5\u5229\u7528\u771f\u5b9e\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u8bad\u7ec3\uff0c\u6355\u6349\u591a\u6750\u6599\u52a8\u6001\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0cParticleFormer\u5728\u52a8\u6001\u9884\u6d4b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ParticleFormer\u5728\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u52a8\u6001\u6a21\u578b\u3002"}}
{"id": "2506.22517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22517", "abs": "https://arxiv.org/abs/2506.22517", "authors": ["Subhadip Kumar"], "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "comment": null, "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff08Yolov12\u3001Yolov11\u548cRF-DETR\uff09\u5728\u96c6\u88c5\u7bb1\u635f\u4f24\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RF-DETR\u5728\u7f55\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u96c6\u88c5\u7bb1\u635f\u4f24\u662f\u7269\u6d41\u884c\u4e1a\u7684\u5b89\u5168\u9690\u60a3\uff0c\u53ca\u65f6\u68c0\u6d4b\u53ef\u5ef6\u957f\u4f7f\u7528\u5bff\u547d\u5e76\u907f\u514d\u98ce\u9669\u3002", "method": "\u4f7f\u7528278\u5f20\u6807\u6ce8\u56fe\u50cf\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6a21\u578b\uff0c\u6bd4\u8f83mAP\u548c\u7cbe\u5ea6\u3002", "result": "Yolov11\u548c12\u7684mAP@50\u4e3a81.9%\uff0cRF-DETR\u4e3a77.7%\uff0c\u4f46RF-DETR\u5728\u7f55\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RF-DETR\u5728\u7f55\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u9002\u5408\u96c6\u88c5\u7bb1\u635f\u4f24\u68c0\u6d4b\u3002"}}
{"id": "2506.22631", "categories": ["cs.LG", "stat.ML", "68Q32, 68W27, 68W20"], "pdf": "https://arxiv.org/pdf/2506.22631", "abs": "https://arxiv.org/abs/2506.22631", "authors": ["Dmitry B. Rokhlin"], "title": "A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS", "comment": null, "summary": "We study the problem of online regression with the unconstrained quadratic\nloss against a time-varying sequence of functions from a Reproducing Kernel\nHilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a\ndiscounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic\nregret in the finite-dimensional case. In this work, we lift their approach to\nthe non-parametric domain by synthesizing the DVAW framework with a random\nfeature approximation. We propose a fully adaptive, hierarchical algorithm,\nwhich we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that\nlearns both the discount factor and the number of random features. We prove\nthat this algorithm, which has a per-iteration computational complexity of\n$O(T\\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +\n\\sqrt{T}\\ln T)$, where $P_T$ is the functional path length of a comparator\nsequence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aH-VAW-D\u7684\u5206\u5c42\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u56de\u5f52\u95ee\u9898\uff0c\u7ed3\u5408\u4e86DVAW\u6846\u67b6\u548c\u968f\u673a\u7279\u5f81\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u540e\u6094\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u56de\u5f52\u95ee\u9898\uff0c\u9488\u5bf9\u65f6\u95f4\u53d8\u5316\u7684\u51fd\u6570\u5e8f\u5217\uff0c\u6269\u5c55\u4e86DVAW\u9884\u6d4b\u5668\u5230\u975e\u53c2\u6570\u9886\u57df\u3002", "method": "\u63d0\u51faH-VAW-D\u7b97\u6cd5\uff0c\u7ed3\u5408DVAW\u6846\u67b6\u548c\u968f\u673a\u7279\u5f81\u8fd1\u4f3c\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u6298\u6263\u56e0\u5b50\u548c\u968f\u673a\u7279\u5f81\u6570\u91cf\u3002", "result": "\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a$O(T\\ln T)$\uff0c\u52a8\u6001\u540e\u6094\u4e3a$O(T^{2/3}P_T^{1/3} + \\sqrt{T}\\ln T)$\u3002", "conclusion": "H-VAW-D\u5728\u975e\u53c2\u6570\u9886\u57df\u5b9e\u73b0\u4e86\u52a8\u6001\u540e\u6094\u7684\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.23129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23129", "abs": "https://arxiv.org/abs/2506.23129", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jirou\u0161ek", "Naiemeh Ahmadlou", "Veli Bak\u0131rc\u0131o\u011flu", "Martin Saska"], "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "comment": null, "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u5ea6\u7684\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u65b9\u6848\uff0c\u907f\u514d\u4e86\u5bf9\u521d\u59cb\u731c\u6d4b\u654f\u611f\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u7684\u6709\u9650\u65f6\u95f4\u7f16\u961f\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u4e2d\u4f9d\u8d56\u6570\u503c\u65b9\u6cd5\u548c\u5bf9\u521d\u59cb\u731c\u6d4b\u654f\u611f\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u65e0\u4eba\u673a\u52a8\u529b\u5b66\u7684\u5fae\u5206\u5e73\u5766\u6027\uff0c\u8bbe\u8ba1\u6709\u9650\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Pontryagin\u539f\u7406\u6c42\u89e3\uff0c\u7ed3\u5408\u65b9\u5411\u611f\u77e5\u7684\u78b0\u649e\u907f\u514d\u7b56\u7565\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u56db\u65e0\u4eba\u673a\u7f16\u961f\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u7684\u6709\u9650\u65f6\u95f4\u7f16\u961f\u63a7\u5236\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.22531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPreserve Anything\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5bf9\u8c61\u4fdd\u5b58\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7N\u901a\u9053ControlNet\u5b9e\u73b0\u591a\u5bf9\u8c61\u4fdd\u5b58\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u573a\u666f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5bf9\u8c61\u4fdd\u5b58\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u573a\u666f\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528N\u901a\u9053ControlNet\uff0c\u7ed3\u5408\u5bf9\u8c61\u4fdd\u5b58\u6a21\u5757\u3001\u80cc\u666f\u5f15\u5bfc\u6a21\u5757\u548c\u9ad8\u9891\u8986\u76d6\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u5bf9\u8c61\u4fdd\u5b58\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u3002", "result": "\u5728FID\u548cCLIP-S\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5728\u63d0\u793a\u5bf9\u9f50\u3001\u771f\u5b9e\u611f\u7b49\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u5bf9\u8c61\u4fdd\u5b58\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22638", "abs": "https://arxiv.org/abs/2506.22638", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "comment": null, "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5c42\u7ed3\u6784\uff0c\u8fd9\u4e9b\u5c42\u5728\u9884\u8bad\u7ec3\u4e2d\u5f62\u6210\uff0c\u4e14\u5728\u4e0d\u540c\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u5347\u662f\u5426\u6e90\u4e8e\u5c42\u7ed3\u6784\u91cd\u5927\u53d8\u5316\u6216\u5fae\u5c0f\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u5c42\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u57fa\u7840\u6a21\u578b\u53ca\u540e\u8bad\u7ec3\u53d8\u4f53\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4f9d\u8d56\u7279\u5b9a\u5c42\u7ed3\u6784\uff0c\u79fb\u9664\u8fd9\u4e9b\u5c42\u4f1a\u5bfc\u81f4\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d\uff1b\u975e\u6570\u5b66\u4efb\u52a1\u5219\u65e0\u6b64\u73b0\u8c61\u3002", "conclusion": "\u6570\u5b66\u63a8\u7406\u9700\u8981\u9884\u8bad\u7ec3\u4e2d\u5f62\u6210\u7684\u4e13\u7528\u5c42\uff0c\u800c\u975e\u63a8\u7406\u4efb\u52a1\u5219\u65e0\u6b64\u9700\u6c42\u3002"}}
{"id": "2506.23152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23152", "abs": "https://arxiv.org/abs/2506.23152", "authors": ["Youzhuo Wang", "Jiayi Ye", "Chuyang Xiao", "Yiming Zhong", "Heng Tao", "Hang Yu", "Yumeng Liu", "Jingyi Yu", "Yuexin Ma"], "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover", "comment": null, "summary": "Handover between a human and a dexterous robotic hand is a fundamental yet\nchallenging task in human-robot collaboration. It requires handling dynamic\nenvironments and a wide variety of objects and demands robust and adaptive\ngrasping strategies. However, progress in developing effective dynamic\ndexterous grasping methods is limited by the absence of high-quality,\nreal-world human-to-robot handover datasets. Existing datasets primarily focus\non grasping static objects or rely on synthesized handover motions, which\ndiffer significantly from real-world robot motion patterns, creating a\nsubstantial gap in applicability. In this paper, we introduce DexH2R, a\ncomprehensive real-world dataset for human-to-robot handovers, built on a\ndexterous robotic hand. Our dataset captures a diverse range of interactive\nobjects, dynamic motion patterns, rich visual sensor data, and detailed\nannotations. Additionally, to ensure natural and human-like dexterous motions,\nwe utilize teleoperation for data collection, enabling the robot's movements to\nalign with human behaviors and habits, which is a crucial characteristic for\nintelligent humanoid robots. Furthermore, we propose an effective solution,\nDynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art\napproaches, including auto-regressive models and diffusion policy methods,\nproviding a thorough comparison and analysis. We believe our benchmark will\ndrive advancements in human-to-robot handover research by offering a\nhigh-quality dataset, effective solutions, and comprehensive evaluation\nmetrics.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DexH2R\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86DynamicGrasp\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u5728\u52a8\u6001\u73af\u5883\u548c\u591a\u6837\u5316\u7269\u4f53\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u9650\u5236\u4e86\u52a8\u6001\u7075\u5de7\u6293\u53d6\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u9065\u64cd\u4f5c\u6536\u96c6\u6570\u636e\uff0c\u6784\u5efaDexH2R\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faDynamicGrasp\u89e3\u51b3\u65b9\u6848\uff0c\u8bc4\u4f30\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\u3002", "result": "DexH2R\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u7684\u4ea4\u4e92\u5bf9\u8c61\u548c\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\uff0cDynamicGrasp\u5728\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u89e3\u51b3\u65b9\u6848\u63a8\u52a8\u4e86\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u7684\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2506.22554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Seamless Interaction Dataset\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u80fd\u7406\u89e3\u548c\u751f\u6210\u4eba\u9645\u884c\u4e3a\u52a8\u6001\u7684AI\u6280\u672f\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u7cfb\u5217\u76f8\u5173\u6a21\u578b\u53ca\u5176\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u4eba\u9645\u884c\u4e3a\u52a8\u6001\u7684AI\u6280\u672f\uff0c\u4ee5\u63a8\u52a8\u793e\u4ea4\u667a\u80fdAI\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528Seamless Interaction Dataset\u5f00\u53d1\u6a21\u578b\uff0c\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u97f3\u5bf9\u9f50\u7684\u80a2\u4f53\u52a8\u4f5c\u548c\u9762\u90e8\u8868\u60c5\uff0c\u5e76\u6574\u54082D\u548c3D\u6e32\u67d3\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u60c5\u611f\u53ef\u8c03\u8282\u4e14\u8bed\u4e49\u76f8\u5173\u7684\u52a8\u4f5c\uff0c\u5c55\u793a\u4e86\u66f4\u76f4\u89c2\u548c\u54cd\u5e94\u5f0f\u7684\u4eba\u673a\u4ea4\u4e92\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u81ea\u7136\u7684\u865a\u62df\u4ee3\u7406\u548c\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.22645", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22645", "abs": "https://arxiv.org/abs/2506.22645", "authors": ["Amir Hossein Rahmati", "Nathan M. Urban", "Byung-Jun Yoon", "Xiaoning Qian"], "title": "Cost-effective Reduced-Order Modeling via Bayesian Active Learning", "comment": null, "summary": "Machine Learning surrogates have been developed to accelerate solving systems\ndynamics of complex processes in different science and engineering\napplications. To faithfully capture governing systems dynamics, these methods\nrely on large training datasets, hence restricting their applicability in\nreal-world problems. In this work, we propose BayPOD-AL, an active learning\nframework based on an uncertainty-aware Bayesian proper orthogonal\ndecomposition (POD) approach, which aims to effectively learn reduced-order\nmodels from high-fidelity full-order models representing complex systems.\nExperimental results on predicting the temperature evolution over a rod\ndemonstrate BayPOD-AL's effectiveness in suggesting the informative data and\nreducing computational cost related to constructing a training dataset compared\nto other uncertainty-guided active learning strategies. Furthermore, we\ndemonstrate BayPOD-AL's generalizability and efficiency by evaluating its\nperformance on a dataset of higher temporal resolution than the training\ndataset.", "AI": {"tldr": "BayPOD-AL\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65afPOD\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u9ad8\u6548\u5b66\u4e60\u590d\u6742\u7cfb\u7edf\u7684\u964d\u9636\u6a21\u578b\uff0c\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faBayPOD-AL\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8d1d\u53f6\u65afPOD\u65b9\u6cd5\uff0c\u4e3b\u52a8\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBayPOD-AL\u5728\u9884\u6d4b\u6e29\u5ea6\u6f14\u5316\u65f6\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "BayPOD-AL\u5728\u66f4\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2506.23164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u4e2d\u6a21\u5f0f\u5d29\u6e83\u7684\u65b0\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u5173\u952e\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u6a21\u5f0f\u5d29\u6e83\u3001\u6a21\u5f0f\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u591a\u6a21\u6001\u884c\u4e3a\u65f6\u53ef\u80fd\u4ec5\u9884\u6d4b\u6700\u53ef\u80fd\u7684\u6a21\u5f0f\uff08\u6a21\u5f0f\u5d29\u6e83\uff09\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u672a\u5b9a\u91cf\u8bc4\u4f30\u4ea4\u4e92\u6a21\u5f0f\u591a\u6837\u6027\u6216\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u6a21\u5f0f\u5d29\u6e83\u3001\u6a21\u5f0f\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u7684\u6307\u6807\uff0c\u7279\u522b\u5173\u6ce8\u9884\u6d4b\u7684\u65f6\u5e8f\u7ef4\u5ea6\u3002", "result": "\u6d4b\u8bd5\u4e86\u56db\u79cd\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u5f0f\u5d29\u6e83\u786e\u5b9e\u5b58\u5728\uff0c\u4e14\u5373\u4f7f\u63a5\u8fd1\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u6a21\u578b\u4ecd\u53ef\u80fd\u65e0\u6cd5\u9884\u6d4b\u6b63\u786e\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u6df1\u5165\u7406\u89e3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u4e00\u81f4\u548c\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22556", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22556", "abs": "https://arxiv.org/abs/2506.22556", "authors": ["Markus Juvonen", "Samuli Siltanen"], "title": "Recomposed realities: animating still images via patch clustering and randomness", "comment": "22 pages, 19 figures", "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u52a8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u968f\u673a\u91c7\u6837\u5b9e\u73b0\u9759\u6001\u56fe\u50cf\u7684\u52a8\u6001\u5316\u3002", "motivation": "\u5229\u7528\u73b0\u6709\u56fe\u50cf\u6570\u636e\u4e3a\u9759\u6001\u56fe\u50cf\u8d4b\u4e88\u52a8\u6001\u6548\u679c\uff0c\u5f3a\u8c03\u91cd\u65b0\u89e3\u91ca\u800c\u975e\u7b80\u5355\u590d\u5236\u3002", "method": "\u4f7f\u7528k-means\u805a\u7c7b\u5bf9\u56fe\u50cf\u5757\u5206\u7ec4\uff0c\u901a\u8fc7\u5339\u914d\u548c\u968f\u673a\u91c7\u6837\u91cd\u5efa\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u80fd\u591f\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u6982\u5ff5\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\uff0c\u5171\u4eab\u5c40\u90e8\u7ed3\u6784\u5e76\u5b9e\u73b0\u52a8\u6001\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u52a8\u753b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22655", "abs": "https://arxiv.org/abs/2506.22655", "authors": ["Andrew F. Ilersich", "Prasanth B. Nair"], "title": "Learning Stochastic Multiscale Models", "comment": "Body is 9 pages, 13 including acknowledgements and references, 35\n  including appendix. 21 figures and 6 tables. Submitted to NeurIPS 2025", "summary": "The physical sciences are replete with dynamical systems that require the\nresolution of a wide range of length and time scales. This presents significant\ncomputational challenges since direct numerical simulation requires\ndiscretization at the finest relevant scales, leading to a high-dimensional\nstate space. In this work, we propose an approach to learn stochastic\nmultiscale models in the form of stochastic differential equations directly\nfrom observational data. Our method resolves the state on a coarse mesh while\nintroducing an auxiliary state to capture the effects of unresolved scales. We\nlearn the parameters of the multiscale model using a modern forward-solver-free\namortized variational inference method. Our approach draws inspiration from\nphysics-based multiscale modeling approaches, such as large-eddy simulation in\nfluid dynamics, while learning directly from data. We present numerical studies\nto demonstrate that our learned multiscale models achieve superior predictive\naccuracy compared to direct numerical simulation and closure-type models at\nequivalent resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7f51\u683c\u548c\u8f85\u52a9\u72b6\u6001\u6355\u6349\u672a\u89e3\u6790\u5c3a\u5ea6\u6548\u5e94\uff0c\u4f18\u4e8e\u76f4\u63a5\u6570\u503c\u6a21\u62df\u548c\u95ed\u5305\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u9ad8\u7ef4\u8ba1\u7b97\u6311\u6218\uff0c\u907f\u514d\u76f4\u63a5\u6570\u503c\u6a21\u62df\u7684\u6602\u8d35\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u73b0\u4ee3\u53d8\u5206\u63a8\u65ad\u6280\u672f\uff0c\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u53c2\u6570\u3002", "result": "\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u76f4\u63a5\u6570\u503c\u6a21\u62df\u548c\u95ed\u5305\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u5c3a\u5ea6\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23316", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "AI": {"tldr": "InfGen\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u4ea4\u901a\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u7684\u4ea4\u901a\u6a21\u62df\uff0c\u5e76\u80fd\u6301\u7eed\u63d2\u5165\u65b0\u8f66\u8f86\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u6a21\u62df\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u521d\u59cb\u5316\u6216\u65e5\u5fd7\u56de\u653e\uff0c\u96be\u4ee5\u6a21\u62df\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u7684\u573a\u666f\u3002", "method": "InfGen\u5c06\u6574\u4e2a\u573a\u666f\u8868\u793a\u4e3a\u4e00\u7cfb\u5217\u4ee4\u724c\uff08\u5982\u4ea4\u901a\u706f\u4fe1\u53f7\u3001\u8f66\u8f86\u72b6\u6001\u548c\u8fd0\u52a8\u5411\u91cf\uff09\uff0c\u5e76\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInfGen\u80fd\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u81ea\u9002\u5e94\u7684\u4ea4\u901a\u884c\u4e3a\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u5176\u751f\u6210\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "InfGen\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u5ea6\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u73af\u5883\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.22562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22562", "abs": "https://arxiv.org/abs/2506.22562", "authors": ["Abhineet Singh", "Nilanjan Ray"], "title": "Improving Token-based Object Detection with Video", "comment": "Under review for publication in IEEE Access", "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "AI": {"tldr": "\u8bba\u6587\u6539\u8fdb\u4e86Pix2Seq\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5c06\u5176\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7a00\u758f\u635f\u5931\u548c\u542f\u53d1\u5f0f\u540e\u5904\u7406\u95ee\u9898\uff0c\u4e14\u901a\u5e38\u9700\u8981\u5c062D\u6846\u94fe\u63a5\u4e3a\u89c6\u9891\u5bf9\u8c61\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u901a\u8fc7\u5c06\u5bf9\u8c61\u8868\u793a\u4e3a\u53ef\u53d8\u957f\u5ea6\u7684\u79bb\u6563\u6807\u8bb0\u5e8f\u5217\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u68c0\u6d4b\u5668\u7684\u6846\u91c7\u6837\u95ee\u9898\uff1b\u540c\u65f6\u76f4\u63a5\u8f93\u51fa3D\u6846\u6216\u8f68\u8ff9\uff0c\u65e0\u9700\u94fe\u63a52D\u6846\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8ePix2Seq\u9759\u6001\u68c0\u6d4b\u5668\uff0c\u5e76\u5728UA-DETRAC\u4e0a\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2506.22668", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22668", "abs": "https://arxiv.org/abs/2506.22668", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "comment": "12 pages", "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "AI": {"tldr": "DistShap\u662f\u4e00\u79cd\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u8ba1\u7b97GNN\u9884\u6d4b\u7684Shapley\u503c\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740GNN\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u89e3\u91ca\u5176\u9884\u6d4b\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u3002", "method": "DistShap\u901a\u8fc7\u5206\u5e03\u5f0f\u91c7\u6837\u5b50\u56fe\u3001\u5e76\u884c\u6267\u884cGNN\u63a8\u7406\uff0c\u5e76\u89e3\u51b3\u5206\u5e03\u5f0f\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u6765\u8ba1\u7b97\u8fb9\u91cd\u8981\u6027\u5206\u6570\u3002", "result": "DistShap\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u652f\u6301\u767e\u4e07\u7ea7\u7279\u5f81\u7684GNN\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u8fbe128\u4e2aGPU\u3002", "conclusion": "DistShap\u4e3a\u5927\u89c4\u6a21GNN\u89e3\u91ca\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23326", "abs": "https://arxiv.org/abs/2506.23326", "authors": ["Sang-Yoep Lee", "Leonardo Zamora Yanez", "Jacob Rogatinsky", "Vi T. Vo", "Tanvi Shingade", "Tommaso Ranzani"], "title": "Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Soft robotic systems are known for their flexibility and adaptability, but\ntraditional physics-based models struggle to capture their complex, nonlinear\nbehaviors. This study explores a data-driven approach to modeling the\nvolume-flow-pressure relationship in hydraulic soft actuators, focusing on\nlow-complexity models with high accuracy. We perform regression analysis on a\nstacked balloon actuator system using exponential, polynomial, and neural\nnetwork models with or without autoregressive inputs. The results demonstrate\nthat simpler models, particularly multivariate polynomials, effectively predict\npressure dynamics with fewer parameters. This research offers a practical\nsolution for real-time soft robotics applications, balancing model complexity\nand computational efficiency. Moreover, the approach may benefit various\ntechniques that require explicit analytical models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u6db2\u538b\u8f6f\u6267\u884c\u5668\u7684\u4f53\u79ef-\u6d41\u91cf-\u538b\u529b\u5173\u7cfb\uff0c\u91cd\u70b9\u5728\u4e8e\u4f4e\u590d\u6742\u5ea6\u9ad8\u7cbe\u5ea6\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u9879\u5f0f\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u538b\u529b\u52a8\u6001\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u56de\u5f52\u5206\u6790\uff0c\u6bd4\u8f83\u6307\u6570\u3001\u591a\u9879\u5f0f\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5305\u62ec\u6709\u65e0\u81ea\u56de\u5f52\u8f93\u5165\u7684\u60c5\u51b5\u3002", "result": "\u591a\u9879\u5f0f\u6a21\u578b\u80fd\u4ee5\u8f83\u5c11\u53c2\u6570\u6709\u6548\u9884\u6d4b\u538b\u529b\u52a8\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u8f6f\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6a21\u578b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.22567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22567", "abs": "https://arxiv.org/abs/2506.22567", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "comment": null, "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "AI": {"tldr": "MMKD-CLIP\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u6784\u5efa\u9ad8\u6027\u80fd\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u548c\u7edf\u4e00\u6807\u51c6\uff0c\u9650\u5236\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5728290\u4e07\u751f\u7269\u533b\u5b66\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d61920\u4e07\u7279\u5f81\u5bf9\u8fdb\u884c\u7279\u5f81\u7ea7\u84b8\u998f\u3002", "result": "\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d61080\u4e07\u56fe\u50cf\u548c9\u79cd\u6a21\u6001\uff0cMMKD-CLIP\u5728\u516d\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u662f\u6784\u5efa\u9ad8\u6027\u80fd\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.22685", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.22685", "abs": "https://arxiv.org/abs/2506.22685", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "comment": null, "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u4e2a\u6027\u5316\u4e2d\u7684\u8bed\u4e49\u574d\u7f29\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u8c03\u6574\u5d4c\u5165\u7684\u5e45\u5ea6\u548c\u65b9\u5411\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\u3002", "motivation": "\u8bed\u4e49\u574d\u7f29\u95ee\u9898\u5bfc\u81f4\u591a\u6982\u5ff5\u8f93\u5165\u63d0\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u964d\u4f4e\uff0c\u8f93\u51fa\u56fe\u50cf\u65e0\u6cd5\u6355\u6349\u9884\u671f\u6982\u5ff5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5e45\u5ea6\u548c\u65b9\u5411\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e2a\u6027\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u80fd\u5e7f\u6cdb\u89e3\u51b3\u8bed\u4e49\u574d\u7f29\u95ee\u9898\u3002"}}
{"id": "2506.23333", "categories": ["cs.RO", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.23333", "abs": "https://arxiv.org/abs/2506.23333", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "S\u00e1ndor P. Fekete", "Aaron T. Becker"], "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5355\u4e2a\u673a\u5668\u4eba\u91cd\u65b0\u914d\u7f6e\u8fde\u63a5\u74f7\u7816\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u7b97\u6cd5\u4e0e\u4e24\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5728\u4fdd\u6301\u8fde\u63a5\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u5c06\u74f7\u7816\u7ed3\u6784\u91cd\u65b0\u914d\u7f6e\u4e3a\u76ee\u6807\u5f62\u72b6\u3002", "method": "\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86Becker\u7b49\u4eba\u7684\u76f4\u65b9\u56fe\u7b97\u6cd5\uff0c\u4e0e\u4e24\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u76f4\u65b9\u56fe\u7b97\u6cd5\u5728\u8d77\u59cb\u548c\u76ee\u6807\u914d\u7f6e\u5206\u79bb\u826f\u597d\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\u3002", "conclusion": "\u76f4\u65b9\u56fe\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4e3a\u74f7\u7816\u7ed3\u6784\u91cd\u914d\u7f6e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22570", "abs": "https://arxiv.org/abs/2506.22570", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "comment": "17 pages, 7 figures, 6 tables", "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u519c\u4e1a\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7DAS Conv\u6a21\u5757\u548c\u4f18\u5316\u7684\u8df3\u8fde\u7ed3\u6784\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u519c\u4e1a\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u5bf9\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u9700\u9ad8\u6548\u65b9\u6cd5\u4ee5\u652f\u6301\u51b3\u7b56\u548c\u5e72\u9884\u3002", "method": "\u96c6\u6210Dual Atrous Separable Convolution\u6a21\u5757\u4e8eDeepLabV3\u6846\u67b6\uff0c\u5e76\u4f18\u5316\u8df3\u8fde\u7ed3\u6784\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728Agriculture Vision\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6548\u7387\u63d0\u534766%\uff0c\u63a5\u8fd1\u590d\u6742Transformer\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7684\u8bed\u4e49\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u9065\u611f\u5e94\u7528\u3002"}}
{"id": "2506.22696", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22696", "abs": "https://arxiv.org/abs/2506.22696", "authors": ["Brian Mak", "Jeffrey Flanigan"], "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMT\u7684\u6a21\u578b\uff0c\u7528\u5916\u79ef\u8bb0\u5fc6\u77e9\u9635\u66ff\u4ee3\u4f20\u7edfTransformer\u7684\u6b8b\u5dee\u6d41\uff0c\u5177\u6709\u72ec\u7acb\u6269\u5c55\u6b8b\u5dee\u6d41\u5927\u5c0f\u3001\u51cf\u5c11\u8ba1\u7b97\u548c\u53c2\u6570\u9700\u6c42\u7b49\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfTransformer\u7684\u6b8b\u5dee\u6d41\u673a\u5236\u5728\u5b58\u50a8\u548c\u68c0\u7d22\u4fe1\u606f\u65f6\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u673a\u5236\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u7528\u5916\u79ef\u8bb0\u5fc6\u77e9\u9635\uff08Kohonen, 1972, Anderson, 1972\uff09\u66ff\u4ee3Transformer\u7684\u6b8b\u5dee\u6d41\uff0c\u6784\u5efaRMT\u6a21\u578b\u3002", "result": "RMT\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edfTransformer\uff0c\u51cf\u5c11\u4e8658%\u7684FLOPS\u300125%\u7684\u53c2\u6570\u548c41%\u7684\u8bad\u7ec3token\uff0c\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "RMT\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u6b8b\u5dee\u6d41\u6269\u5c55\u548c\u65b9\u5dee\u4f20\u64ad\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.23346", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23346", "abs": "https://arxiv.org/abs/2506.23346", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "comment": "RSS 2025 Workshop on Reliable Robotics", "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMPC\u548cHJ\u53ef\u8fbe\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u81ea\u4e3b\u7cfb\u7edf\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u4f18\u5316\u4efb\u52a1\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u4e24\u8005\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548cHamilton-Jacobi\uff08HJ\uff09\u53ef\u8fbe\u6027\u7406\u8bba\uff0c\u8bbe\u8ba1\u6846\u67b6\u4ee5\u786e\u4fdd\u9012\u5f52\u53ef\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u57284D Dubins Car\u548c6\u81ea\u7531\u5ea6Kuka iiwa\u673a\u68b0\u81c2\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u4f18\u5316\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u3002"}}
{"id": "2506.22589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22589", "abs": "https://arxiv.org/abs/2506.22589", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "comment": "Accepted at ICDAR2025", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "AI": {"tldr": "LIGHT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u7528\u4e8e\u94fe\u63a5\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\u4fe1\u606f\u5bf9\u7814\u7a76\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u94fe\u63a5\u6587\u672c\u7247\u6bb5\uff0c\u5c24\u5176\u662f\u591a\u8bcd\u5730\u540d\u3002", "method": "LIGHT\u6574\u5408\u51e0\u4f55\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u6a21\u5757\u548c\u591a\u6a21\u6001\u5b66\u4e60\u9884\u6d4b\u6587\u672c\u987a\u5e8f\u3002", "result": "LIGHT\u5728ICDAR 2024/2025 MapText\u7ade\u8d5b\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u5386\u53f2\u5730\u56fe\u6587\u672c\u94fe\u63a5\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22708", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2506.22708", "abs": "https://arxiv.org/abs/2506.22708", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "comment": null, "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "AI": {"tldr": "FairMarket-RL\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728P2P\u4ea4\u6613\u4e2d\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709P2P\u4ea4\u6613\u65b9\u6cd5\u7f3a\u4e4f\u786e\u4fdd\u516c\u5e73\u6027\u7684\u7a33\u5065\u6846\u67b6\uff0cFairMarket-RL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6846\u67b6\u4f7f\u7528LLM\u4f5c\u4e3a\u5b9e\u65f6\u516c\u5e73\u6027\u8bc4\u4f30\u5668\uff0c\u7ed3\u5408RL\u8bad\u7ec3\u4ea4\u6613\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\u4f18\u5316\u516c\u5e73\u6027\u3002", "result": "\u4ee3\u7406\u5728\u6a21\u62dfP2P\u5fae\u7535\u7f51\u4e2d\u5b9e\u73b0\u4e8690%\u4ee5\u4e0a\u7684\u4e70\u5bb6\u9700\u6c42\u6ee1\u8db3\u7387\uff0c\u516c\u5e73\u6027\u8bc4\u5206\u9ad8\u4e8e0.80\uff0c\u5e76\u51cf\u5c11\u4e86\u5356\u5bb6\u5229\u6da6\u5dee\u5f02\u3002", "conclusion": "FairMarket-RL\u4e3a\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u516c\u5e73\u9a71\u52a8\u7684\u81ea\u4e3b\u4ea4\u6613\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23351", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23351", "abs": "https://arxiv.org/abs/2506.23351", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RoboTwin\u53cc\u81c2\u534f\u4f5c\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u63a8\u52a8\u53cc\u81c2\u673a\u5668\u4eba\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u7814\u7a76\uff0c\u5438\u5f15\u4e86\u5168\u7403\u56e2\u961f\u53c2\u4e0e\u5e76\u53d6\u5f97\u4e86\u91cd\u8981\u6210\u679c\u3002", "motivation": "\u63a8\u52a8\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u7279\u522b\u662f\u53cc\u81c2\u534f\u4f5c\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "method": "\u57fa\u4e8eRoboTwin\u4eff\u771f\u5e73\u53f0\u548cAgileX COBOT-Magic\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9636\u6bb5\uff08\u4eff\u771f\u4e24\u8f6e\u548c\u73b0\u5b9e\u4e16\u754c\u4e00\u8f6e\uff09\u7684\u6311\u6218\u8d5b\uff0c\u5305\u542b17\u79cd\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5438\u5f15\u4e8664\u4e2a\u5168\u7403\u56e2\u961f\u548c400\u591a\u540d\u53c2\u4e0e\u8005\uff0c\u4ea7\u751f\u4e86\u5982SEM\u548cAnchorDP3\u7b49\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u53cc\u81c2\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "\u6311\u6218\u8d5b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u76ee\u6807\u662f\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u53cc\u81c2\u64cd\u4f5c\u7b56\u7565\u3002"}}
{"id": "2506.22591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22591", "abs": "https://arxiv.org/abs/2506.22591", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "comment": "Accepted at MICCAI 2025", "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "AI": {"tldr": "BrainMT\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5411Mamba\u5757\u548cTransformer\u5757\uff0c\u6709\u6548\u6355\u6349fMRI\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6216Transformer\uff09\u96be\u4ee5\u5efa\u6a21fMRI\u6570\u636e\u4e2d\u7684\u590d\u6742\u65f6\u7a7a\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "BrainMT\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a1\uff09\u53cc\u5411Mamba\u5757\u6355\u83b7\u5168\u5c40\u65f6\u95f4\u4ea4\u4e92\uff1b2\uff09Transformer\u5757\u5efa\u6a21\u5168\u5c40\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728UKBioBank\u548cHuman Connectome Project\u6570\u636e\u96c6\u4e0a\uff0cBrainMT\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "BrainMT\u901a\u8fc7\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.22712", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22712", "abs": "https://arxiv.org/abs/2506.22712", "authors": ["Alexander Theus", "Alessandro Cabodi", "Sotiris Anagnostidis", "Antonio Orvieto", "Sidak Pal Singh", "Valentina Boeva"], "title": "Generalized Linear Mode Connectivity for Transformers", "comment": null, "summary": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u56db\u79cd\u5bf9\u79f0\u6027\u7c7b\u522b\uff0c\u4ece\u800c\u9996\u6b21\u5728\u72ec\u7acb\u8bad\u7ec3\u7684Vision Transformers\u548cGPT-2\u6a21\u578b\u4e4b\u95f4\u53d1\u73b0\u4e86\u4f4e\u6216\u96f6\u969c\u788d\u7684\u7ebf\u6027\u63d2\u503c\u8def\u5f84\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u6df1\u5ea6\u5b66\u4e60\u7684\u6cdb\u5316\u548c\u4f18\u5316\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u662f\u7ebf\u6027\u6a21\u5f0f\u8fde\u901a\u6027\uff08LMC\uff09\u73b0\u8c61\u3002\u7136\u800c\uff0c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\uff08\u5982\u795e\u7ecf\u5143\u6392\u5217\uff09\u4f7f\u5f97\u529f\u80fd\u7b49\u6548\u7684\u6a21\u578b\u663e\u5f97\u4e0d\u540c\uff0c\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6db5\u76d6\u56db\u79cd\u5bf9\u79f0\u6027\u7c7b\u522b\uff1a\u6392\u5217\u3001\u534a\u6392\u5217\u3001\u6b63\u4ea4\u53d8\u6362\u548c\u4e00\u822c\u53ef\u9006\u6620\u5c04\uff0c\u6269\u5c55\u4e86\u6709\u6548\u91cd\u65b0\u53c2\u6570\u5316\u7684\u8303\u56f4\uff0c\u5e76\u5c06\u8bb8\u591a\u5148\u524d\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u5305\u542b\u5728\u5185\u3002", "result": "\u8be5\u6846\u67b6\u9996\u6b21\u5728\u72ec\u7acb\u8bad\u7ec3\u7684Vision Transformers\u548cGPT-2\u6a21\u578b\u4e4b\u95f4\u53d1\u73b0\u4e86\u4f4e\u6216\u96f6\u969c\u788d\u7684\u7ebf\u6027\u63d2\u503c\u8def\u5f84\uff0c\u63ed\u793a\u4e86\u635f\u5931\u666f\u89c2\u4e2d\u66f4\u6df1\u5c42\u6b21\u7684\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u5206\u6790\u5728\u7406\u89e3\u6a21\u578b\u7a7a\u95f4\u51e0\u4f55\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u5bf9\u79f0\u6027\u6846\u67b6\u3002"}}
{"id": "2506.23369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23369", "abs": "https://arxiv.org/abs/2506.23369", "authors": ["Xiao'ao Song", "Konstantinos Karydis"], "title": "GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions", "comment": "Accepted for publication in CASE 2025, 6 pages, 8 figures", "summary": "Efficient identification of picking points is critical for automated fruit\nharvesting. Avocados present unique challenges owing to their irregular shape,\nweight, and less-structured growing environments, which require specific\nviewpoints for successful harvesting. We propose a geometry-based,\nsemantics-aware viewpoint-planning algorithm to address these challenges. The\nplanning process involves three key steps: viewpoint sampling, evaluation, and\nexecution. Starting from a partially occluded view, the system first detects\nthe fruit, then leverages geometric information to constrain the viewpoint\nsearch space to a 1D circle, and uniformly samples four points to balance the\nefficiency and exploration. A new picking score metric is introduced to\nevaluate the viewpoint suitability and guide the camera to the next-best view.\nWe validate our method through simulation against two state-of-the-art\nalgorithms. Results show a 100% success rate in two case studies with\nsignificant occlusions, demonstrating the efficiency and robustness of our\napproach. Our code is available at https://github.com/lineojcd/GSNBV", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u548c\u8bed\u4e49\u611f\u77e5\u7684\u89c6\u70b9\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u4e0d\u89c4\u5219\u5f62\u72b6\u6c34\u679c\uff08\u5982\u725b\u6cb9\u679c\uff09\u7684\u91c7\u6458\u70b9\uff0c\u901a\u8fc7\u89c6\u70b9\u91c7\u6837\u3001\u8bc4\u4f30\u548c\u6267\u884c\u4e09\u6b65\u5b9e\u73b0\uff0c\u9a8c\u8bc1\u663e\u793a100%\u6210\u529f\u7387\u3002", "motivation": "\u81ea\u52a8\u5316\u6c34\u679c\u91c7\u6458\u4e2d\uff0c\u725b\u6cb9\u679c\u56e0\u5176\u4e0d\u89c4\u5219\u5f62\u72b6\u3001\u91cd\u91cf\u548c\u975e\u7ed3\u6784\u5316\u751f\u957f\u73af\u5883\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u9700\u7279\u5b9a\u89c6\u70b9\u4ee5\u6210\u529f\u91c7\u6458\u3002", "method": "\u7b97\u6cd5\u5305\u62ec\u89c6\u70b9\u91c7\u6837\u3001\u8bc4\u4f30\u548c\u6267\u884c\u4e09\u6b65\uff1a\u4ece\u90e8\u5206\u906e\u6321\u89c6\u56fe\u5f00\u59cb\uff0c\u68c0\u6d4b\u6c34\u679c\u540e\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u5c06\u89c6\u70b9\u641c\u7d22\u7a7a\u95f4\u7ea6\u675f\u52301D\u5706\uff0c\u5747\u5300\u91c7\u6837\u56db\u70b9\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u63a2\u7d22\uff0c\u5f15\u5165\u65b0\u91c7\u6458\u8bc4\u5206\u6307\u6807\u8bc4\u4f30\u89c6\u70b9\u9002\u5b9c\u6027\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u663e\u793a\uff0c\u5728\u4e24\u79cd\u4e25\u91cd\u906e\u6321\u6848\u4f8b\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u725b\u6cb9\u679c\u91c7\u6458\u4e2d\u7684\u89c6\u70b9\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u91c7\u6458\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22624", "abs": "https://arxiv.org/abs/2506.22624", "authors": ["Zuyao You", "Zuxuan Wu"], "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "comment": null, "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "AI": {"tldr": "Seg-R1\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7GRPO\u7b56\u7565\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u590d\u6742\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\uff08\u5982\u524d\u666f\u5206\u5272\uff09\u4e2d\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528GRPO\u7b56\u7565\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u548c\u8fb9\u754c\u6846\u63d0\u793a\u5f15\u5bfcSAM2\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u4ec5\u4f7f\u7528RL\u8bad\u7ec3\u3002", "result": "\u5728COD10K\u4e0a\u8fbe\u52300.873 S-measure\uff0c\u5728RefCOCOg\u548cReasonSeg\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6027\u80fd\u8d85\u8d8a\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u7eafRL\u8bad\u7ec3\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u50cf\u7d20\u7ea7\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22716", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R\u00fchle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "AI": {"tldr": "BEST-Route\u662f\u4e00\u79cd\u65b0\u578b\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u67e5\u8be2\u5230\u4e0d\u540c\u6210\u672c\u548c\u8d28\u91cf\u7684\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u591a\u54cd\u5e94\u91c7\u6837\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u56e0\u4ec5\u751f\u6210\u5355\u4e00\u54cd\u5e94\u800c\u8fc7\u5ea6\u4f9d\u8d56\u6602\u8d35\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4f4e\u6210\u672c\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faBEST-Route\u6846\u67b6\uff0c\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u8d28\u91cf\u9608\u503c\u9009\u62e9\u6a21\u578b\u53ca\u91c7\u6837\u54cd\u5e94\u6570\u91cf\uff0c\u5229\u7528\u5c0f\u6a21\u578b\u591a\u54cd\u5e94\u91c7\u6837\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53ef\u964d\u4f4e\u6210\u672c\u8fbe60%\uff0c\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u3002", "conclusion": "BEST-Route\u901a\u8fc7\u667a\u80fd\u8def\u7531\u548c\u591a\u54cd\u5e94\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u4e0e\u6027\u80fd\u7684\u9ad8\u6548\u6743\u8861\u3002"}}
{"id": "2506.23400", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23400", "abs": "https://arxiv.org/abs/2506.23400", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u79fb\u52a8\u589e\u6750\u5236\u9020\u5e73\u53f0\uff0c\u4ee5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u548c\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u589e\u6750\u5236\u9020\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u8bbe\u7f6e\u548c\u4eba\u5de5\u4f9d\u8d56\uff0c\u5bfc\u81f4\u751f\u4ea7\u5468\u671f\u957f\u4e14\u6269\u5c55\u6027\u6709\u9650\u3002\u79fb\u52a8\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u9ad8\u751f\u4ea7\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5ffd\u89c6\u6253\u5370\u8d28\u91cf\u548c\u5c0f\u578b\u7cbe\u5bc6\u90e8\u4ef6\u7684\u751f\u4ea7\u3002", "method": "\u96c6\u6210\u589e\u6750\u5236\u9020\u7cfb\u7edf\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u548c\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u8d28\u91cf\u589e\u6750\u5236\u9020\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22636", "abs": "https://arxiv.org/abs/2506.22636", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757ReCo\uff0c\u7528\u4e8e\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u751f\u6210\u6587\u672c\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u8f93\u5165\u65e0\u5173\u6216\u77db\u76fe\u7684\u6587\u672c\uff0c\u8fd9\u88ab\u5f52\u56e0\u4e8e\u5bf9\u8bed\u8a00\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u89c6\u89c9\u8f93\u5165\u7684\u2018\u8bb0\u5fc6\u6d88\u9000\u6548\u5e94\u2019\u3002", "method": "\u5728\u73b0\u6709VLM\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u540d\u4e3aReCo\u7684\u5c0f\u578b\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u65e0\u9700\u5176\u4ed6\u4fee\u6539\uff0c\u5229\u7528\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u6765\u63a7\u5236\u5e7b\u89c9\u884c\u4e3a\u3002", "result": "\u5728\u4e09\u79cd\u4e3b\u6d41VLM\uff08InstructBLIP\u3001LlaVA\u3001MiniGPT4\uff09\u4e0a\u9a8c\u8bc1\u4e86ReCo\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6709\u63d0\u5347\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u51cf\u5c11\u5e7b\u89c9\u7684\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "conclusion": "ReCo\u6a21\u5757\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u7f13\u89e3VLM\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.22732", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22732", "abs": "https://arxiv.org/abs/2506.22732", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "comment": null, "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRTC-GTNLN\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u5f20\u91cfL1-L2\u8303\u6570\u548c\u68af\u5ea6\u5f20\u91cfL1-L2\u8303\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u6570\u636e\u4e2d\u540c\u65f6\u5b58\u5728\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u4ea4\u901a\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u548c\u901a\u4fe1\u95ee\u9898\u540c\u65f6\u5b58\u5728\u7f3a\u5931\u503c\u548c\u566a\u58f0\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u53cc\u91cd\u9000\u5316\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5f20\u91cfL1-L2\u8303\u6570\u4f5c\u4e3a\u975e\u51f8\u5f20\u91cf\u79e9\u66ff\u4ee3\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u57df\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u68af\u5ea6\u5f20\u91cfL1-L2\u8303\u6570\uff0c\u6700\u7ec8\u6784\u5efa\u4e86RTC-GTNLN\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRTC-GTNLN\u5728\u540c\u65f6\u5904\u7406\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RTC-GTNLN\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u4f4e\u79e9\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u6743\u8861\u53c2\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u6570\u636e\u7684\u53cc\u91cd\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2506.23433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23433", "abs": "https://arxiv.org/abs/2506.23433", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "comment": null, "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u6570\u636e\u96c6\u4e2d\u8bc6\u522b\u6709\u4ef7\u503c\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u91cd\u70b9\u5173\u6ce8\u76f4\u63a5\u548c\u95f4\u63a5\u98ce\u9669\u60c5\u5883\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u9700\u8981\u4e30\u5bcc\u7684\u9053\u8def\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7b5b\u9009\u9ad8\u4ef7\u503c\u573a\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6982\u7387\u98ce\u9669\u6a21\u578b\u68c0\u6d4b\u9ad8\u98ce\u9669\u60c5\u5883\uff0c\u5305\u62ec\u76f4\u63a5\uff08\u4e00\u9636\uff09\u548c\u95f4\u63a5\uff08\u4e8c\u9636\uff09\u5f71\u54cd\u3002", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7b5b\u9009\u51fa\u590d\u6742\u4e14\u4e92\u8865\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6307\u6807Kalman\u96be\u5ea6\u548cTTP\u3002", "conclusion": "\u63d0\u51fa\u7684\u98ce\u9669\u8fc7\u6ee4\u65b9\u6cd5\u4e30\u5bcc\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u6570\u636e\u8d28\u91cf\uff0c\u76f8\u5173\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.22637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22637", "abs": "https://arxiv.org/abs/2506.22637", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaO$_2$\u7684\u4e24\u9636\u6bb5\u6269\u6563\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e2d\u7684\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u84b8\u998f\u6548\u679c\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "CaO$_2$\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6982\u7387\u4fe1\u606f\u6837\u672c\u9009\u62e9\u7ba1\u9053\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u6f5c\u5728\u8868\u793a\u4ee5\u63d0\u9ad8\u6761\u4ef6\u4f3c\u7136\u3002", "result": "CaO$_2$\u5728ImageNet\u53ca\u5176\u5b50\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad82.3%\u3002", "conclusion": "CaO$_2$\u901a\u8fc7\u4f18\u5316\u84b8\u998f\u8fc7\u7a0b\u4e0e\u8bc4\u4f30\u76ee\u6807\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22771", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22771", "abs": "https://arxiv.org/abs/2506.22771", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eINT8\u91cf\u5316\u7684Forward-Forward\uff08FF\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u7b56\u7565\u7a33\u5b9a\u68af\u5ea6\u91cf\u5316\uff0c\u5e76\u5f15\u5165\u201c\u524d\u77bb\u201d\u65b9\u6848\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u80fd\u8017\u548c\u5185\u5b58\u5360\u7528\u4e0a\u5747\u6709\u663e\u8457\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u5728\u65f6\u95f4\u548c\u80fd\u8017\u4e0a\u7684\u4f4e\u6548\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\uff0c\u800c\u4f4e\u7cbe\u5ea6\u91cf\u5316\u5728\u8bad\u7ec3\u4e2d\u7684\u7814\u7a76\u8f83\u5c11\u3002FF\u7b97\u6cd5\u901a\u8fc7\u907f\u514d\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\u503c\u6765\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "method": "\u91c7\u7528INT8\u91cf\u5316\u8bad\u7ec3\uff0c\u7ed3\u5408FF\u7684\u9010\u5c42\u7b56\u7565\u7a33\u5b9a\u68af\u5ea6\u91cf\u5316\uff0c\u5e76\u63d0\u51fa\u201c\u524d\u77bb\u201d\u65b9\u6848\u4ee5\u89e3\u51b3FF\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728NVIDIA Jetson Orin Nano\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53474.6%\uff0c\u80fd\u8017\u964d\u4f4e8.3%\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1127.0%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23514", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23514", "abs": "https://arxiv.org/abs/2506.23514", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "comment": "Accepted to IROS 2025", "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "AI": {"tldr": "MGPRL\u662f\u4e00\u79cd\u57fa\u4e8eWi-Fi\u4fe1\u53f7\u7684\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u548c\u51f8\u5305\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u4f9d\u8d56\u9ad8\u6210\u672c\u6216\u77ed\u7a0b\u4f20\u611f\u5668\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u9884\u6d4bRSSI\u573a\uff0c\u901a\u8fc7\u51f8\u5305\u5bf9\u9f50\u5b9e\u73b0\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "MGPRL\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MGPRL\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u9884\u6821\u51c6\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22678", "abs": "https://arxiv.org/abs/2506.22678", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "title": "3D Shape Generation: A Survey", "comment": "20 pages, 5 figures", "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u56f4\u7ed5\u5f62\u72b6\u8868\u793a\u3001\u751f\u6210\u65b9\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\u5c55\u5f00\u8ba8\u8bba\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u52a8\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u53d1\u5c55\uff0c\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5168\u9762\u7684\u9886\u57df\u6982\u8ff0\u548c\u672a\u6765\u65b9\u5411\u3002", "method": "\u5206\u7c7b\u8ba8\u8bba\u4e86\u663e\u5f0f\u3001\u9690\u5f0f\u548c\u6df7\u5408\u5f62\u72b6\u8868\u793a\uff0c\u4ee5\u53ca\u524d\u9988\u67b6\u6784\u751f\u6210\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7efc\u8ff0\u4e86\u5f53\u524d3D\u5f62\u72b6\u751f\u6210\u7684\u6280\u672f\u73b0\u72b6\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u751f\u6210\u5f62\u72b6\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u63a7\u3001\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.22780", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2506.22780", "abs": "https://arxiv.org/abs/2506.22780", "authors": ["Dibyajyoti Chakraborty", "Haiwen Guan", "Jason Stock", "Troy Arcomano", "Guido Cervone", "Romit Maulik"], "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models", "comment": null, "summary": "Score-based diffusion modeling is a generative machine learning algorithm\nthat can be used to sample from complex distributions. They achieve this by\nlearning a score function, i.e., the gradient of the log-probability density of\nthe data, and reversing a noising process using the same. Once trained,\nscore-based diffusion models not only generate new samples but also enable\nzero-shot conditioning of the generated samples on observed data. This promises\na novel paradigm for data and model fusion, wherein the implicitly learned\ndistributions of pretrained score-based diffusion models can be updated given\nthe availability of online data in a Bayesian formulation. In this article, we\napply such a concept to the super-resolution of a high-dimensional dynamical\nsystem, given the real-time availability of low-resolution and experimentally\nobserved sparse sensor measurements from multimodal data. Additional analysis\non how score-based sampling can be used for uncertainty estimates is also\nprovided. Our experiments are performed for a super-resolution task that\ngenerates the ERA5 atmospheric dataset given sparse observations from a\ncoarse-grained representation of the same and/or from unstructured experimental\nobservations of the IGRA radiosonde dataset. We demonstrate accurate recovery\nof the high dimensional state given multiple sources of low-fidelity\nmeasurements. We also discover that the generative model can balance the\ninfluence of multiple dataset modalities during spatiotemporal reconstructions.", "AI": {"tldr": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u7684\u5bf9\u6570\u6982\u7387\u5bc6\u5ea6\u68af\u5ea6\uff0c\u5b9e\u73b0\u590d\u6742\u5206\u5e03\u7684\u91c7\u6837\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u6761\u4ef6\u751f\u6210\u3002\u672c\u6587\u5c06\u5176\u5e94\u7528\u4e8e\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u7684\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u5b9e\u73b0\u51c6\u786e\u91cd\u5efa\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u4e0e\u6a21\u578b\u878d\u5408\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u548c\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5206\u6570\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u516c\u5f0f\u66f4\u65b0\u9690\u5f0f\u5b66\u4e60\u7684\u5206\u5e03\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982ERA5\u548cIGRA\u6570\u636e\u96c6\uff09\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6062\u590d\u9ad8\u7ef4\u72b6\u6001\uff0c\u5e76\u5728\u65f6\u7a7a\u91cd\u5efa\u4e2d\u5e73\u8861\u591a\u6a21\u6001\u6570\u636e\u7684\u5f71\u54cd\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u4e3a\u6570\u636e\u878d\u5408\u548c\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.23573", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u4eba\u5458\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ee5\u63d0\u5347\u62e5\u6324\u73af\u5883\u4e2d\u673a\u5668\u4eba\u62a4\u9001\u670d\u52a1\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u62a4\u9001\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u5bfc\u822a\u7b56\u7565\uff0c\u5047\u8bbe\u88ab\u62a4\u9001\u8005\u4f1a\u987a\u5229\u8ddf\u968f\uff0c\u4f46\u5728\u62e5\u6324\u73af\u5883\u4e2d\u8fd9\u4e00\u5047\u8bbe\u5e38\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u670d\u52a1\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u540c\u65f6\u8fdb\u884c\u4eba\u5458\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u4ee5\u9002\u5e94\u88ab\u62a4\u9001\u8005\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u5bf9\u6bd4\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u62a4\u9001\u670d\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u62e5\u6324\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u62a4\u9001\u670d\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578bLightBSR\uff0c\u901a\u8fc7\u4f18\u5316\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u533a\u5206\u6027\uff0c\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709IDE-BSR\u65b9\u6cd5\u5ffd\u89c6IDR\u533a\u5206\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u9000\u5316\u5148\u9a8c\u7ea6\u675f\u5bf9\u6bd4\u5b66\u4e60\uff08\u6559\u5e08\u9636\u6bb5\uff09\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff08\u5b66\u751f\u9636\u6bb5\uff09\u3002", "result": "LightBSR\u5728\u591a\u79cd\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u590d\u6742\u5ea6\u4f4e\u3002", "conclusion": "\u4f18\u5316IDR\u533a\u5206\u6027\u53ef\u663e\u8457\u63d0\u5347\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2506.22802", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22802", "abs": "https://arxiv.org/abs/2506.22802", "authors": ["Hae Jin Song", "Laurent Itti"], "title": "Riemannian-Geometric Fingerprints of Generative Models", "comment": null, "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u751f\u6210\u6a21\u578b\u6307\u7eb9\u5b9a\u4e49\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u578b\u5f52\u5c5e\u548c\u533a\u5206\u5408\u6210\u4e0e\u4eba\u7c7b\u6570\u636e\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u548c\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u6a21\u578b\u6307\u7eb9\u548c\u5f52\u5c5e\u95ee\u9898\u7684\u9700\u6c42\uff0c\u5305\u62ec\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u3001\u5185\u5bb9\u6eaf\u6e90\u4ee5\u53ca\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u3002", "method": "\u91c7\u7528\u9ece\u66fc\u51e0\u4f55\u65b9\u6cd5\uff0c\u5b9a\u4e49\u751f\u6210\u6a21\u578b\u7684\u6307\u7eb9\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u68af\u5ea6\u7684\u7b97\u6cd5\u8ba1\u7b97\u6307\u7eb9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u591a\u79cd\u751f\u6210\u6a21\u578b\uff08\u6db5\u76d6\u4e0d\u540c\u6570\u636e\u96c6\u3001\u5206\u8fa8\u7387\u548c\u6a21\u6001\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5f52\u5c5e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9ece\u66fc\u51e0\u4f55\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u6307\u7eb9\u63d0\u4f9b\u4e86\u4e00\u79cd\u666e\u9002\u4e14\u9ad8\u6548\u7684\u5b9a\u4e49\u548c\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2506.23614", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2506.23614", "abs": "https://arxiv.org/abs/2506.23614", "authors": ["Jing Huang", "Hao Su", "Kwok Wai Samuel Au"], "title": "Passage-traversing optimal path planning with sampling-based algorithms", "comment": "30 pages, 22 figures, 6 tables, journal paper", "summary": "This paper introduces a new paradigm of optimal path planning, i.e.,\npassage-traversing optimal path planning (PTOPP), that optimizes paths'\ntraversed passages for specified optimization objectives. In particular, PTOPP\nis utilized to find the path with optimal accessible free space along its\nentire length, which represents a basic requirement for paths in robotics. As\npassages are places where free space shrinks and becomes constrained, the core\nidea is to leverage the path's passage traversal status to characterize its\naccessible free space comprehensively. To this end, a novel passage detection\nand free space decomposition method using proximity graphs is proposed,\nenabling fast detection of sparse but informative passages and environment\ndecompositions. Based on this preprocessing, optimal path planning with\naccessible free space objectives or constraints is formulated as PTOPP problems\ncompatible with sampling-based optimal planners. Then, sampling-based\nalgorithms for PTOPP, including their dependent primitive procedures, are\ndeveloped leveraging partitioned environments for fast passage traversal check.\nAll these methods are implemented and thoroughly tested for effectiveness and\nefficiency validation. Compared to existing approaches, such as clearance-based\nmethods, PTOPP demonstrates significant advantages in configurability, solution\noptimality, and efficiency, addressing prior limitations and incapabilities. It\nis believed to provide an efficient and versatile solution to accessible free\nspace optimization over conventional avenues and more generally, to a broad\nclass of path planning problems that can be formulated as PTOPP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u8303\u5f0fPTOPP\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u7684\u901a\u9053\u904d\u5386\u72b6\u6001\u6765\u5168\u9762\u8868\u5f81\u5176\u53ef\u8bbf\u95ee\u81ea\u7531\u7a7a\u95f4\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u81ea\u7531\u7a7a\u95f4\u7684\u53ef\u8bbf\u95ee\u6027\u662f\u57fa\u672c\u9700\u6c42\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u95f4\u9699\u7684\u65b9\u6cd5\uff09\u5728\u914d\u7f6e\u6027\u3001\u6700\u4f18\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u5c40\u9650\u3002PTOPP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u90bb\u8fd1\u56fe\u7684\u901a\u9053\u68c0\u6d4b\u548c\u81ea\u7531\u7a7a\u95f4\u5206\u89e3\u65b9\u6cd5\uff0c\u5feb\u901f\u8bc6\u522b\u7a00\u758f\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\uff0c\u5e76\u5f00\u53d1\u91c7\u6837\u7b97\u6cd5\u89e3\u51b3PTOPP\u95ee\u9898\u3002", "result": "PTOPP\u5728\u914d\u7f6e\u6027\u3001\u6700\u4f18\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PTOPP\u4e3a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2506.22718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22718", "abs": "https://arxiv.org/abs/2506.22718", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "comment": null, "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8054\u5408\u89e3\u51b3\u90e8\u5206\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc73D\u9ad8\u65af\u6a21\u578b\u8868\u793a\u7269\u4f53\uff0c\u9002\u7528\u4e8e\u70b9\u4e91\u52a8\u6001\u91c7\u6837\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u70b9\u4e91\u5e8f\u5217\u4e2d\u90e8\u5206\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u70b9\u4e91\u52a8\u6001\u91c7\u6837\u6216\u906e\u6321\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u57fa\u4e8e\u70b9\u5bf9\u5e94\u7684\u65b9\u6cd5\u4e0d\u9002\u7528\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6a21\u578b\u8868\u793a\u7269\u4f53\uff0c\u53c2\u6570\u5316\u65f6\u95f4\u4f9d\u8d56\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u7f29\u653e\uff0c\u5efa\u7acb\u70b9\u4e0e\u9ad8\u65af\u7684\u5bf9\u5e94\u5173\u7cfb\u4ee5\u5b9e\u73b0\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u3002", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\uff0c\u90e8\u5206\u5206\u5272\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd513%\uff0c\u4e14\u5bf9\u7f3a\u5931\u70b9\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u91c7\u6837\u548c\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u70b9\u5bf9\u5e94\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.22809", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22809", "abs": "https://arxiv.org/abs/2506.22809", "authors": ["Cooper Doyle"], "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "AI": {"tldr": "BayesLoRA\u662f\u4e00\u79cd\u4efb\u52a1\u7279\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5c06MC-Dropout\u96c6\u6210\u5230LoRA\u4e2d\uff0c\u4e3a\u4e0b\u6e38\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u53d8\u538b\u5668\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u4e0b\u6e38\u4efb\u52a1\u9700\u6c42\uff0cBayesLoRA\u65e8\u5728\u63d0\u4f9b\u66f4\u8d34\u5408\u5b9e\u9645\u5de5\u4f5c\u6d41\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "method": "\u5c06MC-Dropout\u96c6\u6210\u5230LoRA\u4e2d\uff0c\u5229\u7528\u6570\u5b66\u548c\u5b9e\u8bc1\u5206\u6790\u5c55\u793aLoRA\u9002\u914d\u5668\u5728\u5fae\u8c03\u5206\u5e03\u5916\u7684\u65b9\u5dee\u653e\u5927\u7279\u6027\u3002", "result": "BayesLoRA\u80fd\u591f\u4e3a\u4ee3\u7406\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "BayesLoRA\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9a\u5236\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23624", "abs": "https://arxiv.org/abs/2506.23624", "authors": ["Max Grobbel", "Tristan Schneider", "S\u00f6ren Hohmann"], "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649eUR5e\u5173\u8282\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4ee5\u6291\u5236\u73bb\u7483\u4e2d\u6db2\u4f53\u6643\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u975e\u89e6\u89c9VR\u63a7\u5236\u5668\u5265\u593a\u4e86\u64cd\u4f5c\u5458\u7684\u5173\u952e\u8fd0\u52a8\u53cd\u9988\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5d4c\u5165\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649eUR5e\u5173\u8282\u8f68\u8ff9\uff0c\u5e76\u4e3b\u52a8\u6291\u5236\u6db2\u4f53\u6643\u52a8\u3002", "result": "\u63a7\u5236\u5668\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\u4e3a13\u6beb\u79d2\uff0c\u8bc1\u5b9e\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u4ed6\u76ee\u6807\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002"}}
{"id": "2506.22720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22720", "abs": "https://arxiv.org/abs/2506.22720", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Deterministic Object Pose Confidence Region Estimation", "comment": "Accepted by ICCV 2025", "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u9ad8\u65af\u5173\u952e\u70b9\u5206\u5e03\u548c\u4f20\u64ad\u7f6e\u4fe1\u533a\u57df\uff0c\u9ad8\u6548\u4f30\u8ba16D\u4f4d\u59ff\u7f6e\u4fe1\u533a\u57df\uff0c\u907f\u514d\u91c7\u6837\u65b9\u6cd5\u7684\u4f4e\u6548\u548c\u533a\u57df\u81a8\u80c0\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u91c7\u6837\u76846D\u4f4d\u59ff\u7f6e\u4fe1\u533a\u57df\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u91c7\u6837\u901f\u5ea6\u6162\u548c\u7f6e\u4fe1\u533a\u57df\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5f52\u7eb3\u5171\u5f62\u9884\u6d4b\u6821\u51c6\u9ad8\u65af\u5173\u952e\u70b9\u5206\u5e03\u4e3a2D\u5173\u952e\u70b9\u7f6e\u4fe1\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u9690\u51fd\u6570\u5b9a\u7406\u76f4\u63a5\u4f20\u64ad\u52306D\u4f4d\u59ff\u7f6e\u4fe1\u533a\u57df\u3002", "result": "\u5728LineMOD Occlusion\u548cSPEED\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u7f6e\u4fe1\u533a\u57df\u4f53\u79ef\u663e\u8457\u51cf\u5c0f\uff08\u65cb\u8f6c99.9%\uff0c\u5e73\u79fb99.8%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u63d0\u4f9b\u7d27\u51d1\u7684\u7f6e\u4fe1\u533a\u57df\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.22821", "categories": ["cs.LG", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22821", "abs": "https://arxiv.org/abs/2506.22821", "authors": ["Thomas Gaskin", "Guy J. Abel"], "title": "Deep learning 40 years of human migration", "comment": null, "summary": "We present a novel and detailed dataset on origin-destination annual\nmigration flows and stocks between 230 countries and regions, spanning the\nperiod from 1990 to the present. Our flow estimates are further disaggregated\nby country of birth, providing a comprehensive picture of migration over the\nlast 43 years. The estimates are obtained by training a deep recurrent neural\nnetwork to learn flow patterns from 18 covariates for all countries, including\ngeographic, economic, cultural, societal, and political information. The\nrecurrent architecture of the neural network means that the entire past can\ninfluence current migration patterns, allowing us to learn long-range temporal\ncorrelations. By training an ensemble of neural networks and additionally\npushing uncertainty on the covariates through the trained network, we obtain\nconfidence bounds for all our estimates, allowing researchers to pinpoint the\ngeographic regions most in need of additional data collection. We validate our\napproach on various test sets of unseen data, demonstrating that it\nsignificantly outperforms traditional methods estimating five-year flows while\ndelivering a significant increase in temporal resolution. The model is fully\nopen source: all training data, neural network weights, and training code are\nmade public alongside the migration estimates, providing a valuable resource\nfor future studies of human migration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5e74\u5ea6\u79fb\u6c11\u6d41\u52a8\u548c\u5b58\u91cf\u6570\u636e\u96c6\uff0c\u8986\u76d6230\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a1990\u5e74\u81f3\u4eca\u3002\u901a\u8fc7\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u591a\u79cd\u534f\u53d8\u91cf\uff0c\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u79fb\u6c11\u6d41\u52a8\u4f30\u8ba1\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u6570\u636e\u548c\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u79fb\u6c11\u6570\u636e\u7f3a\u4e4f\u5168\u9762\u6027\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u96be\u4ee5\u6ee1\u8db3\u7814\u7a76\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u79fb\u6c11\u6d41\u52a8\u4f30\u8ba1\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u6587\u5316\u7b4918\u79cd\u534f\u53d8\u91cf\uff0c\u8bad\u7ec3\u6a21\u578b\u4ee5\u5b66\u4e60\u79fb\u6c11\u6d41\u52a8\u6a21\u5f0f\u3002\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u4f20\u9012\uff0c\u63d0\u4f9b\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u7f6e\u4fe1\u533a\u95f4\u4ee5\u6307\u5bfc\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u8be5\u6a21\u578b\u548c\u6570\u636e\u4e3a\u79fb\u6c11\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u4fc3\u8fdb\u4e86\u672a\u6765\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2506.23723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23723", "abs": "https://arxiv.org/abs/2506.23723", "authors": ["Jozsef Palmieri", "Paolo Di Lillo", "Stefano Chiaverini", "Alessandro Marino"], "title": "A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings", "comment": null, "summary": "The adoption of mobile robotic platforms in complex environments, such as\nagricultural settings, requires these systems to exhibit a flexible yet\neffective architecture that integrates perception and control. In such\nscenarios, several tasks need to be accomplished simultaneously, ranging from\nmanaging robot limits to performing operational tasks and handling human\ninputs. The purpose of this paper is to present a comprehensive control\narchitecture for achieving complex tasks such as robotized harvesting in\nvineyards within the framework of the European project CANOPIES. In detail, a\n16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical\nQuadratic Programming (HQP) approach capable of handling both equality and\ninequality constraints at various priorities to harvest grape bunches selected\nby the perception system developed within the project. Furthermore, given the\ncomplexity of the scenario and the uncertainty in the perception system, which\ncould potentially lead to collisions with the environment, the handling of\ninteraction forces is necessary. Remarkably, this was achieved using the same\nHQP framework. This feature is further leveraged to enable semi-autonomous\noperations, allowing a human operator to assist the robotic counterpart in\ncompleting harvesting tasks. Finally, the obtained results are validated\nthrough extensive testing conducted first in a laboratory environment to prove\nindividual functionalities, then in a real vineyard, encompassing both\nautonomous and semi-autonomous grape harvesting operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u519c\u4e1a\u73af\u5883\uff08\u5982\u8461\u8404\u56ed\uff09\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5c42\u4e8c\u6b21\u89c4\u5212\uff08HQP\uff09\u65b9\u6cd5\u5904\u7406\u591a\u4efb\u52a1\u548c\u7ea6\u675f\uff0c\u652f\u6301\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u64cd\u4f5c\u3002", "motivation": "\u5728\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u7075\u6d3b\u4e14\u9ad8\u6548\u5730\u6574\u5408\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u4ee5\u5b8c\u6210\u591a\u4efb\u52a1\uff08\u5982\u8461\u8404\u91c7\u6458\uff09\uff0c\u540c\u65f6\u5904\u7406\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u4f7f\u752816\u81ea\u7531\u5ea6\u53cc\u81c2\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u901a\u8fc7HQP\u65b9\u6cd5\u5904\u7406\u4f18\u5148\u7ea7\u4e0d\u540c\u7684\u7b49\u5f0f\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u7ed3\u5408\u611f\u77e5\u7cfb\u7edf\u9009\u62e9\u8461\u8404\u4e32\u3002\u540c\u4e00\u6846\u67b6\u8fd8\u7528\u4e8e\u5904\u7406\u4ea4\u4e92\u529b\u548c\u534a\u81ea\u4e3b\u64cd\u4f5c\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u8461\u8404\u56ed\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u8461\u8404\u91c7\u6458\u7684\u529f\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u67b6\u6784\u5728\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\u548c\u534a\u81ea\u4e3b\u64cd\u4f5c\uff0c\u4e3a\u673a\u5668\u4eba\u5316\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22726", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22726", "abs": "https://arxiv.org/abs/2506.22726", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "comment": null, "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "AI": {"tldr": "XTransfer\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u6a21\u6001\u65e0\u5173\u7684\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u4fee\u590d\u548c\u5c42\u91cd\u7ec4\u89e3\u51b3\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fc1\u79fb\u7684\u6a21\u6001\u504f\u79fb\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u5f00\u53d1\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u8d44\u6e90\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u504f\u79fb\u3001\u8d44\u6e90\u9700\u6c42\u9ad8\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "XTransfer\u901a\u8fc7\u6a21\u578b\u4fee\u590d\uff08\u4fee\u590d\u6a21\u6001\u504f\u79fb\uff09\u548c\u5c42\u91cd\u7ec4\uff08\u9ad8\u6548\u641c\u7d22\u548c\u91cd\u7ec4\u6e90\u6a21\u578b\u5c42\uff09\u5b9e\u73b0\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "XTransfer\u5728\u591a\u79cd\u4eba\u7c7b\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8fb9\u7f18\u90e8\u7f72\u7684\u6210\u672c\u3002", "conclusion": "XTransfer\u4e3a\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u7684\u4eba\u7c7b\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22837", "abs": "https://arxiv.org/abs/2506.22837", "authors": ["Kamil Faber", "Marcin Pietro\u0144", "Dominik \u017burek", "Roberto Corizzo"], "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "comment": null, "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "AI": {"tldr": "xLSTMAD\u662f\u4e00\u79cd\u57fa\u4e8exLSTM\u67b6\u6784\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06xLSTM\u5e94\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1xLSTM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b49\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faxLSTMAD\uff0c\u5305\u542b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5206\u4e3a\u9884\u6d4b\uff08xLSTMAD-F\uff09\u548c\u91cd\u5efa\uff08xLSTMAD-R\uff09\u4e24\u79cd\u53d8\u4f53\uff0c\u5e76\u4f7f\u7528MSE\u548cSoftDTW\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728TSB-AD-M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cxLSTMAD\u4f18\u4e8e23\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86xLSTM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "xLSTMAD\u9996\u6b21\u8bc1\u660e\u4e86xLSTM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.23725", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23725", "abs": "https://arxiv.org/abs/2506.23725", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "AI": {"tldr": "PAC Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u7406\u5c5e\u6027\u3001\u529f\u80fd\u6027\u548c\u7ea6\u675f\uff08PAC\uff09\u7406\u89e3\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u9ad8\u5c42\u6b21\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4f4e\u5c42\u6b21\u7269\u7406\u524d\u63d0\uff08\u5982\u7269\u4f53\u5c5e\u6027\u3001\u529f\u80fd\u6027\u548c\u7ea6\u675f\uff09\u7684\u9a8c\u8bc1\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faPAC Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0830,000+\u6807\u6ce8\uff0c673\u5f20\u771f\u5b9e\u56fe\u50cf\uff0c100\u4e2a\u4eba\u5f62\u89c6\u89d2\u573a\u666f\uff0c120\u4e2a\u6a21\u62df\u7ea6\u675f\u573a\u666f\uff09\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30VLMs\u7684PAC\u7406\u89e3\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524dVLMs\u5728\u57fa\u7840\u7269\u7406\u6982\u5ff5\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u53ef\u9760\u6027\u3002", "conclusion": "PAC Bench\u4e3aVLMs\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u5e76\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2506.22736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "AI": {"tldr": "UniFuse\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u89e3\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\u548cOmni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u672a\u5bf9\u9f50\u6216\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7684\u4e00\u4f53\u5316\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6e90\u56fe\u50cf\u9ad8\u8d28\u91cf\u4e14\u5bf9\u9f50\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u56fe\u50cf\u53ef\u80fd\u672a\u5bf9\u9f50\u6216\u8d28\u91cf\u5dee\uff0c\u5bfc\u81f4\u878d\u5408\u6548\u679c\u4e0b\u964d\u3002", "method": "UniFuse\u7ed3\u5408\u964d\u89e3\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u3001Omni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u548cUniversal Feature Restoration & Fusion\u6a21\u5757\uff0c\u5229\u7528ALSN\u5b9e\u73b0\u81ea\u9002\u5e94\u7279\u5f81\u8868\u793a\u548c\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniFuse\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7684\u4e00\u4f53\u5316\u3002", "conclusion": "UniFuse\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u5bf9\u9f50\u548c\u6062\u590d\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.22845", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.22845", "abs": "https://arxiv.org/abs/2506.22845", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "comment": null, "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNNs\uff09\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u529f\u7387\u8f93\u51fa\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u4e0e\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53\u6216\u7565\u4f18\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u667a\u80fd\u7535\u7f51\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u9700\u6c42\u589e\u52a0\uff0cQNNs\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u671b\u5728\u80fd\u6e90\u9886\u57df\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cd\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u7684QNN\u914d\u7f6e\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u548c\u672a\u89c1\u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u3002", "result": "QNNs\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4e0e\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53\u6216\u7565\u4f18\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7535\u8def\u590d\u6742\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "QNNs\u5728\u80fd\u6e90\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u53c2\u8003\u3002"}}
{"id": "2506.23739", "categories": ["cs.RO", "cs.CE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.23739", "abs": "https://arxiv.org/abs/2506.23739", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M\u00fcller"], "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u4ea4\u4e92\uff0c\u9a8c\u8bc1\u4e86\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u548c\u865a\u62df\u73af\u5883\u4e2d\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0eVRUs\u7684\u5b89\u5168\u548c\u771f\u5b9e\u4ea4\u4e92\u9700\u8981\u5148\u8fdb\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u3002", "method": "\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u548c\u865a\u62dfVRUs\u7684\u6bd4\u8f83\u5206\u6790\u9a8c\u8bc1HPE\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u76ee\u6444\u50cf\u59343D\u9aa8\u9abc\u68c0\u6d4bAI\u548cUnreal Engine 5\u751f\u6210\u865a\u62df\u573a\u666f\u3002", "result": "HPE\u5728\u7a33\u5b9a\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5728\u52a8\u6001\u8fd0\u52a8\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u5b58\u5728\u663e\u8457\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u590d\u6742\u9a91\u884c\u59ff\u52bf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u57fa\u4e8eAI\u7684\u8f66\u8f86\u611f\u77e5\u6d4b\u8bd5\u65b9\u6cd5\u548c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u4e0eVRUs\u7684\u4ea4\u4e92\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2506.22749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22749", "abs": "https://arxiv.org/abs/2506.22749", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "comment": null, "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8054\u5408\u51e0\u4f55\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\uff08JGAU\uff09\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f69\u8272\u70b9\u4e91\u662f3D\u5e94\u7528\u7684\u4e3b\u6d41\u8868\u793a\uff0c\u4f46\u751f\u6210\u5927\u89c4\u6a21\u4e14\u5bc6\u96c6\u7684\u5f69\u8272\u70b9\u4e91\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faJGAU\u6846\u67b6\uff0c\u5305\u62ec\u51e0\u4f55\u4e0a\u91c7\u6837\u7f51\u7edc\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u7c97\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\u548c\u5c5e\u6027\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u57284\u500d\u30018\u500d\u300112\u500d\u548c16\u500d\u4e0a\u91c7\u6837\u7387\u4e0b\uff0cPSNR\u5206\u522b\u8fbe\u523033.90\u300132.10\u300131.10\u548c30.39\u5206\u8d1d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "JGAU\u65b9\u6cd5\u5728\u5f69\u8272\u70b9\u4e91\u4e0a\u91c7\u6837\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a3D\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22848", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22848", "abs": "https://arxiv.org/abs/2506.22848", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "comment": null, "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u7684\u65b9\u6cd5Auto-SLE\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u89c4\u6a21\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5206\u6cbb\u7b56\u7565\uff08D&D\uff09\u5728\u5927\u89c4\u6a21BN\u7ed3\u6784\u5b66\u4e60\u4e2d\u5b50\u95ee\u9898\u5b66\u4e60\u51c6\u786e\u6027\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165SLE\u7ed3\u5408\u591a\u79cdBN\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5b66\u4e60\u8fd1\u6700\u4f18SLE\u7684\u65b9\u6cd5Auto-SLE\uff0c\u5c06\u5176\u96c6\u6210\u5230D&D\u65b9\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6d89\u53ca10,000\u53d8\u91cf\u7684\u6570\u636e\u96c6\u4e0a\u901a\u5e38\u80fd\u63d0\u534730%~225%\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u66f4\u591a\u53d8\u91cf\uff08\u598230,000\uff09\u548c\u4e0d\u540c\u7f51\u7edc\u7279\u6027\u7684\u6570\u636e\u96c6\u3002", "conclusion": "SLE\u53ca\u5176\u81ea\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u7684BN\u7ed3\u6784\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2506.23768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23768", "abs": "https://arxiv.org/abs/2506.23768", "authors": ["Vittorio La Barbera", "Steven Bohez", "Leonard Hasenclever", "Yuval Tassa", "John R. Hutchinson"], "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model", "comment": null, "summary": "We introduce a novel musculoskeletal model of a dog, procedurally generated\nfrom accurate 3D muscle meshes. Accompanying this model is a motion\ncapture-based locomotion task compatible with a variety of control algorithms,\nas well as an improved muscle dynamics model designed to enhance convergence in\ndifferentiable control frameworks. We validate our approach by comparing\nsimulated muscle activation patterns with experimentally obtained\nelectromyography (EMG) data from previous canine locomotion studies. This work\naims to bridge gaps between biomechanics, robotics, and computational\nneuroscience, offering a robust platform for researchers investigating muscle\nactuation and neuromuscular control.We plan to release the full model along\nwith the retargeted motion capture clips to facilitate further research and\ndevelopment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cbe\u786e3D\u808c\u8089\u7f51\u683c\u751f\u6210\u7684\u72d7\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u517c\u5bb9\u591a\u79cd\u63a7\u5236\u7b97\u6cd5\u7684\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4ee5\u9a8c\u8bc1\u6a21\u62df\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8c\u6570\u636e\u7684\u5339\u914d\u6027\u3002", "motivation": "\u65e8\u5728\u586b\u8865\u751f\u7269\u529b\u5b66\u3001\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e4b\u95f4\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u808c\u8089\u9a71\u52a8\u548c\u795e\u7ecf\u808c\u8089\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u5e73\u53f0\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u72d7\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u9a8c\u8bc1\u6a21\u62df\u4e0e\u5b9e\u9a8c\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u62df\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8cEMG\u6570\u636e\u543b\u5408\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u672a\u6765\u5c06\u516c\u5f00\u5b8c\u6574\u6a21\u578b\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2506.22753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22753", "abs": "https://arxiv.org/abs/2506.22753", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "comment": null, "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u8def\u5f84\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u91d1\u5c5e\u900f\u955c\u6444\u5f71\u4e2d\u7684\u5149\u5b66\u9000\u5316\u548c\u8ba1\u7b97\u6062\u590d\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u914d\u5bf9\u6570\u636e\u3002", "motivation": "\u91d1\u5c5e\u900f\u955c\u5728\u8d85\u7d27\u51d1\u8ba1\u7b97\u6210\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u590d\u6742\u5149\u5b66\u9000\u5316\u548c\u8ba1\u7b97\u6062\u590d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u6821\u51c6\u6216\u5927\u91cf\u6570\u636e\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u8def\u5f84\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u6b63\u3001\u4e2d\u3001\u8d1f\u63d0\u793a\u8def\u5f84\u5e73\u8861\u9ad8\u9891\u7ec6\u8282\u751f\u6210\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5e76\u5f15\u5165\u4f2a\u6570\u636e\u589e\u5f3a\u548c\u53ef\u8c03\u89e3\u7801\u5668\u3002SVDA\u6a21\u5757\u81ea\u9002\u5e94\u5efa\u6a21\u5149\u5b66\u548c\u4f20\u611f\u5668\u9000\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u9510\u5229\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u6beb\u7c73\u7ea7MetaCamera\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7136\u56fe\u50cf\u5148\u9a8c\u548c\u53ef\u63a7\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u5c5e\u900f\u955c\u6444\u5f71\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22871", "categories": ["cs.LG", "cs.MM", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22871", "abs": "https://arxiv.org/abs/2506.22871", "authors": ["Homayun Afrabandpey", "Hamed Rezazadegan Tavakoli"], "title": "P$^2$U: Progressive Precision Update For Efficient Model Distribution", "comment": null, "summary": "Efficient model distribution is becoming increasingly critical in\nbandwidth-constrained environments. In this paper, we propose a simple yet\neffective approach called Progressive Precision Update (P$^2$U) to address this\nproblem. Instead of transmitting the original high-precision model, P$^2$U\ntransmits a lower-bit precision model, coupled with a model update representing\nthe difference between the original high-precision model and the transmitted\nlow precision version. With extensive experiments on various model\narchitectures, ranging from small models ($1 - 6$ million parameters) to a\nlarge model (more than $100$ million parameters) and using three different data\nsets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U\nconsistently achieves better tradeoff between accuracy, bandwidth usage and\nlatency. Moreover, we show that when bandwidth or startup time is the priority,\naggressive quantization (e.g., 4-bit) can be used without severely compromising\nperformance. These results establish P$^2$U as an effective and practical\nsolution for scalable and efficient model distribution in low-resource\nsettings, including federated learning, edge computing, and IoT deployments.\nGiven that P$^2$U complements existing compression techniques and can be\nimplemented alongside any compression method, e.g., sparsification,\nquantization, pruning, etc., the potential for improvement is even greater.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6e10\u8fdb\u7cbe\u5ea6\u66f4\u65b0\uff08P$^2$U\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u6a21\u578b\u53ca\u5176\u4e0e\u9ad8\u7cbe\u5ea6\u6a21\u578b\u7684\u5dee\u5f02\u66f4\u65b0\uff0c\u4f18\u5316\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6a21\u578b\u5206\u53d1\u6548\u7387\u3002", "motivation": "\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u9ad8\u6548\u6a21\u578b\u5206\u53d1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "P$^2$U\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u6a21\u578b\u53ca\u5176\u4e0e\u9ad8\u7cbe\u5ea6\u6a21\u578b\u7684\u5dee\u5f02\u66f4\u65b0\uff0c\u800c\u975e\u539f\u59cb\u9ad8\u7cbe\u5ea6\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cP$^2$U\u5728\u51c6\u786e\u6027\u3001\u5e26\u5bbd\u4f7f\u7528\u548c\u5ef6\u8fdf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u4e14\u6fc0\u8fdb\u91cf\u5316\uff08\u59824\u4f4d\uff09\u5728\u5e26\u5bbd\u6216\u542f\u52a8\u65f6\u95f4\u4f18\u5148\u65f6\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "P$^2$U\u662f\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u9ad8\u6548\u6a21\u578b\u5206\u53d1\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c\u7269\u8054\u7f51\u90e8\u7f72\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u538b\u7f29\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2506.23771", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u9ad8\u4f4e\u5c42\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u6548\u7387\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e38\u5ffd\u7565\u7b56\u7565\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u9a7e\u9a76\u884c\u4e3a\u6ce2\u52a8\u6216\u65e0\u6cd5\u7edf\u4e00\u4f18\u5316\u9a7e\u9a76\u884c\u4e3a\u4e0e\u63a7\u5236\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u7ed3\u6784\uff0c\u9ad8\u5c42\u7b56\u7565\u751f\u6210\u957f\u65f6\u95f4\u5c3a\u5ea6\u8fd0\u52a8\u6307\u5bfc\uff0c\u4f4e\u5c42\u7b56\u7565\u751f\u6210\u77ed\u65f6\u95f4\u5c3a\u5ea6\u63a7\u5236\u547d\u4ee4\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u4eff\u771f\u548cHighD\u6570\u636e\u96c6\u7684\u9ad8\u901f\u591a\u8f66\u9053\u573a\u666f\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9a7e\u9a76\u884c\u4e3a\u6ce2\u52a8\u548c\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9a7e\u9a76\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u7684\u53ef\u7f16\u8f91\u89c6\u9891\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4eff\u771f\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u6563\u5c04\u6784\u5efa\u903c\u771f\u4eff\u771f\uff0c\u7ed3\u5408\u589e\u91cf\u8bed\u4e49\u84b8\u998f\u548c3D\u6b63\u5219\u5316NNFM\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u4eff\u771f\u751f\u4ea7\u4e0e\u6027\u80fd\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86RoboPearls\u7684\u6709\u6548\u6027\uff0c\u5305\u62ecRLBench\u3001COLOSSEUM\u7b49\uff0c\u4eff\u771f\u6027\u80fd\u8868\u73b0\u6ee1\u610f\u3002", "conclusion": "RoboPearls\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4eff\u771f\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u8854\u63a5\u80fd\u529b\u3002"}}
{"id": "2506.22895", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22895", "abs": "https://arxiv.org/abs/2506.22895", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "comment": null, "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u56de\u5f52\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u2113\u2080\u8303\u6570\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5229\u7528\u6df7\u5408\u6574\u6570\u4f18\u5316\uff08MIO\uff09\u89e3\u51b3\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u81ea\u56de\u5f52\u6a21\u578b\u5728\u6355\u6349\u5468\u671f\u6027\u548c\u5b63\u8282\u6027\u65f6\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7a00\u758f\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u2113\u2080\u8303\u6570\u7ea6\u675f\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff1b\u9488\u5bf9\u65f6\u53d8\u548c\u591a\u7ef4\u6570\u636e\uff0c\u5206\u522b\u63d0\u51fa\u51b3\u7b56\u53d8\u91cf\u526a\u679d\uff08DVP\uff09\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDVP\u7b56\u7565\u663e\u8457\u52a0\u901fMIO\u6c42\u89e3\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7684\u8d28\u91cf\uff1b\u6a21\u578b\u6210\u529f\u63ed\u793a\u4e86\u4eba\u7c7b\u79fb\u52a8\u7684\u5468\u671f\u6027\u548c\u6c14\u5019\u6570\u636e\u7684\u52a8\u6001\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65f6\u7a7a\u6570\u636e\u7684\u6a21\u5f0f\u53d1\u73b0\u3002"}}
{"id": "2506.23781", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23781", "abs": "https://arxiv.org/abs/2506.23781", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u76843D\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u73b0\u6210\u9ed1\u76d2\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5206\u79bb\uff0c\u4e14\u7f3a\u4e4f\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\uff0c\u9650\u5236\u4e86\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u54083D\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u80cc\u9762\u5254\u9664\u6280\u672f\uff0c\u5b9e\u73b0\u5728\u7ebf\u751f\u6210\u957f\u65f6\u7a0b3D\u68c0\u6d4b\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5df2\u77e5\u65e0\u4eba\u673a\u52a8\u6001\u6a21\u578b\uff0c\u4ec5\u9700\u8f93\u5165\u8f93\u51fa\u6570\u636e\uff0c\u9002\u7528\u4e8e\u73b0\u6210\u9ed1\u76d2\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "VSRM\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u63d0\u53d6\u957f\u8ddd\u79bb\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u548c\u9891\u7387\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u5c40\u90e8\u611f\u53d7\u91ce\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0cMamba\u56e0\u5176\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVSRM\u6846\u67b6\uff0c\u5305\u542b\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u3001\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u53ca\u9891\u7387Charbonnier-like\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VSRM\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2506.22901", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2506.22901", "abs": "https://arxiv.org/abs/2506.22901", "authors": ["Sina Tabakhi", "Haiping Lu"], "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "comment": "15 pages, 7 figures", "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "AI": {"tldr": "MAGNET\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u56fe\u7ed3\u6784\u9002\u5e94\u4e0d\u540c\u7f3a\u5931\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u4e2d\u5e38\u89c1\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u7f3a\u5931\u6a21\u5f0f\u548c\u6a21\u6001\u6570\u91cf\u589e\u52a0\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faMAGNET\uff0c\u7ed3\u5408\u60a3\u8005-\u6a21\u6001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u60a3\u8005\u56fe\u7ed3\u6784\uff0c\u7ebf\u6027\u590d\u6742\u5ea6\u9002\u5e94\u7f3a\u5931\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u591a\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\uff0cMAGNET\u5728\u764c\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "MAGNET\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u590d\u6742\u5ea6\u53ef\u63a7\u3002"}}
{"id": "2506.23919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23919", "abs": "https://arxiv.org/abs/2506.23919", "authors": ["Haonan Chen", "Bangjun Wang", "Jingxiang Guo", "Tianrui Zhang", "Yiwen Hou", "Xuchuan Huang", "Chenrui Tie", "Lin Shao"], "title": "World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation", "comment": null, "summary": "Improving data efficiency and generalization in robotic manipulation remains\na core challenge. We propose a novel framework that leverages a pre-trained\nmultimodal image-generation model as a world model to guide policy learning. By\nexploiting its rich visual-semantic representations and strong generalization\nacross diverse scenes, the model generates open-ended future state predictions\nthat inform downstream manipulation. Coupled with zero-shot low-level control\nmodules, our approach enables general-purpose robotic manipulation without\ntask-specific training. Experiments in both simulation and real-world\nenvironments demonstrate that our method achieves effective performance across\na wide range of manipulation tasks with no additional data collection or\nfine-tuning. Supplementary materials are available on our website:\nhttps://world4omni.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6765\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u662f\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u8868\u793a\u548c\u6cdb\u5316\u80fd\u529b\u751f\u6210\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u4f4e\u7ea7\u63a7\u5236\u6a21\u5757\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22783", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22783", "abs": "https://arxiv.org/abs/2506.22783", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPhonemeFake (PF)\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u63a8\u7406\u64cd\u7eb5\u5173\u952e\u8bed\u97f3\u7247\u6bb5\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u7c7b\u611f\u77e5\u548c\u57fa\u51c6\u51c6\u786e\u7387\uff0c\u5e76\u5f00\u6e90\u4e86\u68c0\u6d4b\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709Deepfake\u6570\u636e\u96c6\u672a\u80fd\u771f\u5b9e\u53cd\u6620\u653b\u51fb\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u9700\u5f00\u53d1\u66f4\u771f\u5b9e\u7684\u653b\u51fb\u5411\u91cf\u3002", "method": "\u5f15\u5165PF\u653b\u51fb\u65b9\u6cd5\uff0c\u64cd\u7eb5\u5173\u952e\u8bed\u97f3\u7247\u6bb5\uff1b\u5f00\u6e90\u68c0\u6d4b\u6a21\u578b\uff0c\u81ea\u9002\u5e94\u4f18\u5148\u8ba1\u7b97\u88ab\u64cd\u7eb5\u533a\u57df\u3002", "result": "PF\u653b\u51fb\u964d\u4f4e\u4eba\u7c7b\u611f\u77e542%\uff0c\u57fa\u51c6\u51c6\u786e\u738794%\uff1b\u68c0\u6d4b\u6a21\u578b\u964d\u4f4eEER 91%\uff0c\u63d0\u901f90%\u3002", "conclusion": "PF\u653b\u51fb\u548c\u68c0\u6d4b\u6a21\u578b\u4e3a\u66f4\u771f\u5b9eDeepfake\u653b\u51fb\u548c\u9ad8\u6548\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22927", "abs": "https://arxiv.org/abs/2506.22927", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "comment": null, "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6269\u6563\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5171\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u56fe\u50cf\u548c\u6587\u672c\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4ecd\u4e0d\u6210\u719f\uff0c\u800c\u65f6\u95f4\u5e8f\u5217\u5728\u91d1\u878d\u3001\u6c14\u5019\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u4ece\u6587\u672c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u662f\u53ef\u884c\u7684\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9a\u5236\u9884\u6d4b\u3001\u6570\u636e\u589e\u5f3a\u7b49\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u5305\u542b63,010\u5bf9\u65f6\u95f4\u5e8f\u5217-\u63cf\u8ff0\u7684\u65b0\u6570\u636e\u96c6\u3002"}}
{"id": "2506.23944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23944", "abs": "https://arxiv.org/abs/2506.23944", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "comment": null, "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u672c\u4f53\u611f\u89c9\u504f\u79fb\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u9002\u5e94\u6846\u67b6\u548cWasserstein\u8ddd\u79bb\u5bf9\u9f50\u8bad\u7ec3\u4e0e\u90e8\u7f72\u5206\u5e03\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u76f4\u63a5\u4f7f\u7528\u6240\u6709\u672c\u4f53\u611f\u89c9\u72b6\u6001\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u4e0e\u90e8\u7f72\u65f6\u7684\u672c\u4f53\u611f\u89c9\u5206\u5e03\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u57df\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528\u90e8\u7f72\u65f6\u7684\u6eda\u52a8\u6570\u636e\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u91cf\u5316\u5dee\u5f02\uff0c\u5e76\u6dfb\u52a0\u566a\u58f0\u5bf9\u9f50\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u672c\u4f53\u611f\u89c9\u5e76\u7f13\u89e3\u5176\u8d1f\u9762\u5f71\u54cd\uff0c\u6027\u80fd\u4f18\u4e8e\u4e22\u5f03\u672c\u4f53\u611f\u89c9\u6216\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u672c\u4f53\u611f\u89c9\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22784", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22784", "abs": "https://arxiv.org/abs/2506.22784", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "comment": null, "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u68c0\u6d4b\u5668\u5339\u914d\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u5b9e\u73b0LiDAR\u70b9\u4e91\u4e0e\u76f8\u673a\u56fe\u50cf\u4e4b\u95f4\u7684\u70b9\u50cf\u7d20\u914d\u51c6\uff0c\u89e3\u51b3\u4e86\u5355\u5e27LiDAR\u7a00\u758f\u6027\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u70b9\u4e91\u4e0e\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u4ee5\u53ca\u5355\u5e27LiDAR\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u662f\u73b0\u6709\u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u5339\u914d\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5c06LiDAR\u5f3a\u5ea6\u56fe\u6295\u5f71\u52302D\u89c6\u56fe\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65e0\u68c0\u6d4b\u5668\u5339\u914d\u7f51\u7edc\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u5e94\u4f30\u8ba1\uff0c\u540c\u65f6\u5f15\u5165\u53ef\u91cd\u590d\u6027\u8bc4\u5206\u673a\u5236\u63d0\u5347\u5339\u914d\u53ef\u9760\u6027\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cMIAS-LCEC-TF70\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f9d\u8d56\u591a\u5e27\u70b9\u4e91\u7d2f\u79ef\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5355\u5e27LiDAR\u4e0b\u7684\u70b9\u50cf\u7d20\u914d\u51c6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5339\u914d\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22929", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22929", "abs": "https://arxiv.org/abs/2506.22929", "authors": ["Chen Zhang"], "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "comment": null, "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u5b8c\u5907\u6027\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u6570\u636e\uff0c\u652f\u6301\u79d1\u5b66\u8ba1\u7b97\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u56e0\u7ef4\u5ea6\u8bc5\u5492\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u5f25\u8865\u5f53\u524d\u5927\u89c4\u6a21\u6570\u636e\u5de5\u5177\u5728\u6570\u5b66\u7edf\u8ba1\u652f\u6301\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c06\u9ad8\u7ef4\u6570\u636e\u5206\u89e3\u4e3a\u7ef4\u5ea6\u65e0\u5173\u7684\u7ed3\u6784\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u5904\u7406\uff0c\u5e76\u6574\u5408\u6570\u636e\u6316\u6398\u4e0e\u5e76\u884c\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u8de8\u6570\u636e\u7c7b\u578b\uff08\u5982\u533b\u5b66\u548c\u81ea\u7136\u56fe\u50cf\uff09\u7684\u79d1\u5b66\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5904\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u4e3a\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23999", "abs": "https://arxiv.org/abs/2506.23999", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "comment": null, "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u98ce\u9669\u5206\u6790\u7684\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u672a\u6765\u8f68\u8ff9\u9884\u6d4b\u548c\u65f6\u7a7a\u79bb\u6563\u5316\u98ce\u9669\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u7406\u8bba\u4ec5\u57fa\u4e8e\u5f53\u524d\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u672a\u6765\u9884\u6d4b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u9700\u6c42\u3002", "method": "\u6846\u67b6\u5305\u62ec\u5c40\u90e8\u98ce\u9669\u611f\u77e5\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u3001\u65f6\u7a7a\u79bb\u6563\u5316\u9884\u6d4b\u98ce\u9669\u5206\u6790\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5206\u6790\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u8f66\u8f86\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u9884\u6d4b\u548c\u98ce\u9669\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2506.22800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22800", "abs": "https://arxiv.org/abs/2506.22800", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "comment": null, "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version\nincorporating reviewer suggestions will be updated soon.)", "AI": {"tldr": "RGE-GS\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u751f\u6210\u4e0e\u5956\u52b1\u5f15\u5bfc\u9ad8\u65af\u79ef\u5206\u7684\u65b0\u578b\u6269\u5c55\u91cd\u5efa\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u4e2d\u7269\u7406\u4e0d\u4e00\u81f4\u6027\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5355\u6b21\u9a7e\u9a76\u7247\u6bb5\u5e38\u5bfc\u81f4\u9053\u8def\u7ed3\u6784\u626b\u63cf\u4e0d\u5b8c\u6574\uff0c\u9700\u6269\u5c55\u91cd\u5efa\u4ee5\u63d0\u5347\u4f20\u611f\u5668\u6a21\u62df\u5668\u7684\u9a7e\u9a76\u52a8\u4f5c\u56de\u5f52\u6548\u679c\u3002", "method": "\u63d0\u51fa\u5956\u52b1\u7f51\u7edc\u7b5b\u9009\u6269\u6563\u8f93\u51fa\u4ee5\u786e\u4fdd\u7a7a\u95f4\u7a33\u5b9a\u6027\uff0c\u5e76\u91c7\u7528\u5dee\u5f02\u5316\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u9ad8\u65af\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cRGE-GS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "RGE-GS\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.22950", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22950", "abs": "https://arxiv.org/abs/2506.22950", "authors": ["Liangyu Wang", "Huanyi Xie", "Xinhai Wang", "Tianjin Huang", "Mengdi Li", "Di Wang"], "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models", "comment": null, "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Infinite Sampling\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7ec4\u5927\u5c0f\u4e0eGPU\u5185\u5b58\u4f7f\u7528\uff0c\u4f18\u5316GRPO\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u56e0\u751f\u6210\u548c\u5b58\u50a8\u591a\u54cd\u5e94\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u53d7\u9650\u65f6\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u5fae\u91c7\u6837\u7ec4\u3001\u8fde\u7eed\u91c7\u6837\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408FPTAS\u548cSJF\u4f18\u5316\u5185\u5b58\u548c\u541e\u5410\u91cf\u3002", "result": "\u5fae\u91c7\u6837\u7ec4\u51cf\u5c11\u5cf0\u503c\u5185\u5b5850%\u4ee5\u4e0a\uff0cInfinite Sampling\u63d0\u5347\u541e\u5410\u91cf25%\uff0c\u4fdd\u6301\u5b8c\u6574\u54cd\u5e94\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "Infinite Sampling\u5728GPU\u5185\u5b58\u53d7\u9650\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684GRPO\u8bad\u7ec3\uff0c\u652f\u6301\u66f4\u5927\u7ec4\u89c4\u6a21\u3002"}}
{"id": "2506.24046", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.24046", "abs": "https://arxiv.org/abs/2506.24046", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "comment": null, "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7ed3\u80a0\u955c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u5bfc\u5e08\u7ed3\u80a0\u955c\u5b9e\u65f6\u6307\u5bfc\u65b0\u624b\uff0c\u52a0\u901f\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7ed3\u80a0\u955c\u8bad\u7ec3\u4f9d\u8d56\u5de5\u5177\u4ea4\u63a5\uff0c\u9650\u5236\u4e86\u65b0\u624b\u5bf9\u8bbe\u5907\u7684\u719f\u7ec3\u5ea6\u53d1\u5c55\uff0c\u9700\u8981\u5b9e\u65f6\u6307\u5bfc\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u63a7\u7ed3\u80a0\u955c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u53ef\u81ea\u52a8\u5207\u6362\u5bfc\u5e08\u548c\u65b0\u624b\u5bf9\u7ed3\u80a0\u955c\u7684\u63a7\u5236\u3002", "result": "\u521d\u6b65\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u65b0\u624b\u6280\u80fd\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u671b\u52a0\u901f\u7ed3\u80a0\u955c\u6280\u80fd\u5b66\u4e60\uff0c\u672a\u6765\u53ef\u5b9e\u73b0\u4e2a\u6027\u5316\u6559\u5b66\u3002"}}
{"id": "2506.22803", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22803", "abs": "https://arxiv.org/abs/2506.22803", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "AI": {"tldr": "\u63d0\u51faCBM-HNMU\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u63d0\u5347\u6df1\u5ea6\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u81ea\u52a8\u8bc6\u522b\u5e76\u4fee\u6b63\u6709\u5bb3\u6982\u5ff5\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u5bfc\u81f4\u89e3\u91ca\u6027\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u5e72\u9884\u6216\u4ec5\u9488\u5bf9\u6837\u672c\u7ea7\u522b\u3002", "method": "\u5229\u7528\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u4f5c\u4e3a\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u8bc6\u522b\u5e76\u4fee\u6b63\u6709\u5bb3\u6982\u5ff5\uff0c\u5c06\u4fee\u6b63\u540e\u7684\u77e5\u8bc6\u84b8\u998f\u56de\u9ed1\u76d2\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u63d0\u53472.64%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.03%\u3002", "conclusion": "CBM-HNMU\u6709\u6548\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22984", "abs": "https://arxiv.org/abs/2506.22984", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "comment": null, "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5806\u53e0LSTM\u548c\u968f\u673a\u68ee\u6797\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u9884\u6d4b\u8f68\u8ff9\u548c\u8bc6\u522b\u5f02\u5e38\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u6613\u53d7\u4f20\u611f\u5668\u6545\u969c\u3001\u7f51\u7edc\u653b\u51fb\u548c\u73af\u5883\u5e72\u6270\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u5f02\u5e38\u68c0\u6d4b\u5bf9\u4fdd\u969c\u4ea4\u901a\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u8f66\u8f86\u884c\u4e3a\u751f\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5806\u53e0LSTM\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5e8f\u5217\u5f02\u5e38\uff0c\u540c\u65f6\u5229\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u7684R2\u4e3a0.9830\uff0cMAE\u4e3a5.746\uff1b\u5806\u53e0LSTM\u6a21\u578b\u7684R2\u4e3a0.9998\uff0cMAE\u4e3a82.425\uff0c\u5747\u80fd\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u80fd\u51c6\u786e\u9884\u6d4b\u8f68\u8ff9\u5e76\u68c0\u6d4b\u5f02\u5e38\uff0c\u4e3aCAVs\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2506.22806", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22806", "abs": "https://arxiv.org/abs/2506.22806", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "comment": null, "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConcept Pinpoint Eraser (CPE)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u5757\u9009\u62e9\u6027\u5220\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u62a4\u5176\u4ed6\u6982\u5ff5\uff0c\u5e76\u5728\u5bf9\u6297\u8bad\u7ec3\u4e2d\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u5fae\u8c03\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u5220\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u6982\u5ff5\u5931\u771f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "CPE\u901a\u8fc7\u6dfb\u52a0\u975e\u7ebf\u6027Residual Attention Gates (ResAGs)\u548c\u6ce8\u610f\u529b\u951a\u5b9a\u635f\u5931\uff0c\u9009\u62e9\u6027\u5220\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPE\u5728\u5220\u9664\u540d\u4eba\u3001\u827a\u672f\u98ce\u683c\u548c\u654f\u611f\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u6982\u5ff5\u7684\u591a\u6837\u6027\u3002", "conclusion": "CPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u9c81\u68d2\u7684\u6982\u5ff5\u5220\u9664\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2506.22994", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22994", "abs": "https://arxiv.org/abs/2506.22994", "authors": ["Can Hakan Da\u011f\u0131d\u0131r", "Mia Hubert", "Peter J. Rousseeuw"], "title": "Kernel Outlier Detection", "comment": null, "summary": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKOD\u7684\u65b0\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u907f\u514d\u4f9d\u8d56\u5206\u5e03\u5047\u8bbe\u6216\u96be\u4ee5\u8c03\u4f18\u7684\u8d85\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u6838\u53d8\u6362\u548c\u6295\u5f71\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u65b0\u7684\u65b9\u5411\u641c\u7d22\u96c6\u6210\u548c\u7ed3\u679c\u7ec4\u5408\u65b9\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u5c0f\u578b\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5927\u578b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86KOD\u7684\u6709\u6548\u6027\u3002", "conclusion": "KOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u8f7b\u91cf\u7ea7\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2506.22807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22807", "abs": "https://arxiv.org/abs/2506.22807", "authors": ["Yueyang Li", "Shengyu Gong", "Weiming Zeng", "Nizhuan Wang", "Wai Ting Siok"], "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition", "comment": null, "summary": "Electroencephalography (EEG) serves as a reliable and objective signal for\nemotion recognition in affective brain-computer interfaces, offering unique\nadvantages through its high temporal resolution and ability to capture\nauthentic emotional states that cannot be consciously controlled. However,\ncross-subject generalization remains a fundamental challenge due to individual\nvariability, cognitive traits, and emotional responses. We propose FreqDGT, a\nfrequency-adaptive dynamic graph transformer that systematically addresses\nthese limitations through an integrated framework. FreqDGT introduces\nfrequency-adaptive processing (FAP) to dynamically weight emotion-relevant\nfrequency bands based on neuroscientific evidence, employs adaptive dynamic\ngraph learning (ADGL) to learn input-specific brain connectivity patterns, and\nimplements multi-scale temporal disentanglement network (MTDN) that combines\nhierarchical temporal transformers with adversarial feature disentanglement to\ncapture both temporal dynamics and ensure cross-subject robustness.\nComprehensive experiments demonstrate that FreqDGT significantly improves\ncross-subject emotion recognition accuracy, confirming the effectiveness of\nintegrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical\nmodeling while ensuring robustness to individual differences. The code is\navailable at https://github.com/NZWANG/FreqDGT.", "AI": {"tldr": "FreqDGT\u662f\u4e00\u79cd\u9891\u7387\u81ea\u9002\u5e94\u52a8\u6001\u56fe\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u6574\u5408\u9891\u7387\u81ea\u9002\u5e94\u5904\u7406\u3001\u52a8\u6001\u56fe\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u89e3\u7f20\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u88ab\u8bd5\u60c5\u7eea\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8111\u7535\u56fe\uff08EEG\uff09\u5728\u60c5\u611f\u8111\u673a\u63a5\u53e3\u4e2d\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6355\u6349\u771f\u5b9e\u60c5\u7eea\u7684\u4f18\u52bf\uff0c\u4f46\u8de8\u88ab\u8bd5\u6cdb\u5316\u4ecd\u56e0\u4e2a\u4f53\u5dee\u5f02\u548c\u8ba4\u77e5\u7279\u5f81\u800c\u53d7\u9650\u3002", "method": "FreqDGT\u7ed3\u5408\u9891\u7387\u81ea\u9002\u5e94\u5904\u7406\uff08FAP\uff09\u3001\u81ea\u9002\u5e94\u52a8\u6001\u56fe\u5b66\u4e60\uff08ADGL\uff09\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u89e3\u7f20\u7f51\u7edc\uff08MTDN\uff09\uff0c\u52a8\u6001\u52a0\u6743\u60c5\u7eea\u76f8\u5173\u9891\u6bb5\u3001\u5b66\u4e60\u7279\u5b9a\u8f93\u5165\u7684\u5927\u8111\u8fde\u63a5\u6a21\u5f0f\uff0c\u5e76\u6355\u83b7\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreqDGT\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u88ab\u8bd5\u60c5\u7eea\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u9891\u7387\u81ea\u9002\u5e94\u3001\u7a7a\u95f4\u52a8\u6001\u548c\u65f6\u95f4\u5206\u5c42\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002", "conclusion": "FreqDGT\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u88ab\u8bd5\u60c5\u7eea\u8bc6\u522b\u7684\u6311\u6218\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.22995", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.22995", "abs": "https://arxiv.org/abs/2506.22995", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov\u00f2", "Marcello Restelli"], "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5fae\u7535\u7f51\u80fd\u6e90\u7ba1\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6a21\u62df\u50a8\u80fd\u7cfb\u7edf\u52a8\u6001\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u5176\u4ed6RL\u57fa\u51c6\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\uff08RESs\uff09\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u4f20\u7edf\u7535\u7f51\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7ba1\u7406\u5206\u6563\u7684\u80fd\u6e90\u751f\u4ea7\u548c\u6d88\u8d39\uff0c\u5fae\u7535\u7f51\uff08MGs\uff09\u56e0\u5176\u672c\u5730\u5316\u63a7\u5236\u80fd\u529b\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdRL\u4ee3\u7406\uff0c\u901a\u8fc7\u5b66\u4e60\u80fd\u6e90\u4ea4\u6613\u548c\u5b58\u50a8\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u6280\u672f\u6a21\u62df\u50a8\u80fd\u7cfb\u7edf\u52a8\u6001\uff0c\u8003\u8651\u9000\u5316\u56e0\u7d20\u4ee5\u63d0\u9ad8\u4eff\u771f\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u610f\u5927\u5229\u7535\u7f51\u7684\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u5176\u4ed6RL\u57fa\u51c6\u3002", "conclusion": "\u8be5RL\u7b56\u7565\u4e3a\u667a\u80fd\u5fae\u7535\u7f51\u7ba1\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23135", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23135", "abs": "https://arxiv.org/abs/2506.23135", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape: Physics-informed Embodied World Model", "comment": "17 pages", "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.", "AI": {"tldr": "RoboScape\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7269\u7406\u611f\u77e5\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60RGB\u89c6\u9891\u751f\u6210\u548c\u7269\u7406\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u57283D\u51e0\u4f55\u548c\u8fd0\u52a8\u52a8\u529b\u5b66\u5efa\u6a21\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7684\u4e16\u754c\u6a21\u578b\u5728\u7269\u7406\u611f\u77e5\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u7279\u522b\u662f\u57283D\u51e0\u4f55\u548c\u8fd0\u52a8\u52a8\u529b\u5b66\u5efa\u6a21\u4e0a\uff0c\u5bfc\u81f4\u63a5\u89e6\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u573a\u666f\u7684\u89c6\u9891\u751f\u6210\u4e0d\u771f\u5b9e\u3002", "method": "RoboScape\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u7269\u7406\u611f\u77e5\u8054\u5408\u8bad\u7ec3\u4efb\u52a1\uff08\u65f6\u95f4\u6df1\u5ea6\u9884\u6d4b\u548c\u5173\u952e\u70b9\u52a8\u529b\u5b66\u5b66\u4e60\uff09\u6765\u589e\u5f3a\u89c6\u9891\u6e32\u67d3\u76843D\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoboScape\u751f\u6210\u7684\u89c6\u9891\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u5728\u673a\u5668\u4eba\u7b56\u7565\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "RoboScape\u4e3a\u6784\u5efa\u9ad8\u6548\u7684\u7269\u7406\u611f\u77e5\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u5177\u8eab\u667a\u80fd\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2506.22814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22814", "abs": "https://arxiv.org/abs/2506.22814", "authors": ["Andrew Hamara", "Andrew C. Freeman"], "title": "Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping", "comment": null, "summary": "Automatic image cropping aims to extract the most visually salient regions\nwhile preserving essential composition elements. Traditional saliency-aware\ncropping methods optimize a single bounding box, making them ineffective for\napplications requiring multiple disjoint crops. In this work, we extend the\nFixed Aspect Ratio Cropping algorithm to efficiently extract multiple\nnon-overlapping crops in linear time. Our approach dynamically adjusts\nattention thresholds and removes selected crops from consideration without\nrecomputing the entire saliency map. We discuss qualitative results and\nintroduce the potential for future datasets and benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u533a\u57df\u56fe\u50cf\u88c1\u526a\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u9608\u503c\u5e76\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u663e\u8457\u6027\u56fe\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f18\u5316\u5355\u4e00\u88c1\u526a\u6846\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u591a\u4e2a\u4e0d\u91cd\u53e0\u88c1\u526a\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u6269\u5c55\u56fa\u5b9a\u6bd4\u4f8b\u88c1\u526a\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u9608\u503c\u5e76\u79fb\u9664\u5df2\u9009\u533a\u57df\uff0c\u7ebf\u6027\u65f6\u95f4\u5185\u63d0\u53d6\u591a\u533a\u57df\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u533a\u57df\u88c1\u526a\uff0c\u65e0\u9700\u91cd\u590d\u8ba1\u7b97\u663e\u8457\u6027\u56fe\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u4e3a\u672a\u6765\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.23024", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.23024", "abs": "https://arxiv.org/abs/2506.23024", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R\u00e9"], "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBarycentric Weight Layer (BWLer)\u4ee5\u89e3\u51b3PINNs\u5728\u6c42\u89e3PDE\u65f6\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u63d2\u503c\u63d0\u5347\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u8fdb\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3PINNs\u5728\u6c42\u89e3PDE\u65f6\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u7cbe\u5ea6\u9650\u5236\u662f\u5426\u6e90\u4e8ePDE\u7684\u75c5\u6001\u6027\u6216MLP\u67b6\u6784\u3002", "method": "\u5f15\u5165BWLer\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u63d2\u503c\u5efa\u6a21PDE\u89e3\uff0c\u53ef\u9644\u52a0\u4e8eMLP\u6216\u5b8c\u5168\u66ff\u4ee3MLP\uff0c\u5e76\u5229\u7528\u8c31\u5bfc\u6570\u548c\u9884\u5904\u7406\u4f18\u5316\u8bad\u7ec3\u3002", "result": "BWLer\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2dRMSE\u6539\u8fdb\u8fbe30x\u81f31800x\uff0c\u751a\u81f3\u63a5\u8fd1\u673a\u5668\u7cbe\u5ea6\u3002", "conclusion": "BWLer\u4e3a\u7ed3\u5408PINNs\u7075\u6d3b\u6027\u548c\u7ecf\u5178\u8c31\u6c42\u89e3\u5668\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.23434", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23434", "abs": "https://arxiv.org/abs/2506.23434", "authors": ["Tianran Liu", "Shengwen Zhao", "Nicholas Rhinehart"], "title": "Towards foundational LiDAR world models with efficient latent flow matching", "comment": "25 pages, 13 figures", "summary": "LiDAR-based world models offer more structured and geometry-aware\nrepresentations than their image-based counterparts. However, existing LiDAR\nworld models are narrowly trained; each model excels only in the domain for\nwhich it was built. Can we develop LiDAR world models that exhibit strong\ntransferability across multiple domains? We conduct the first systematic domain\ntransfer study across three demanding scenarios: (i) outdoor to indoor\ngeneralization, (ii) sparse-beam \\& dense-beam adaptation, and (iii)\nnon-semantic to semantic transfer. Given different amounts of fine-tuning data,\nour experiments show that a single pre-trained model can achieve up to 11%\nabsolute improvement (83\\% relative) over training from scratch and outperforms\ntraining from scratch in 30/36 of our comparisons. This transferability of\ndynamic learning significantly reduces the reliance on manually annotated data\nfor semantic occupancy forecasting: our method exceed the previous semantic\noccupancy forecasting models with only 5% of the labeled training data required\nby prior models. We also observed inefficiencies of current LiDAR world models,\nmainly through their under-compression of LiDAR data and inefficient training\nobjectives. To address this, we propose a latent conditional flow matching\n(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy\nusing only half the training data and a compression ratio 6 times higher than\nthat of prior methods. Our model achieves SOTA performance on\nfuture-trajectory-conditioned semantic occupancy forecasting while being 23x\nmore computationally efficient (a 28x FPS speedup); and achieves SOTA\nperformance on semantic occupancy forecasting while being 2x more\ncomputationally efficient (a 1.1x FPS speedup).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LiDAR\u4e16\u754c\u6a21\u578b\u7684\u8de8\u9886\u57df\u8fc1\u79fb\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6d41\u5339\u914d\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u63a2\u7d22LiDAR\u4e16\u754c\u6a21\u578b\u5728\u591a\u9886\u57df\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u6846\u67b6\uff0c\u4f18\u5316LiDAR\u6570\u636e\u538b\u7f29\u548c\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97005%\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u5148\u524d\u6a21\u578b\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534723\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LiDAR\u4e16\u754c\u6a21\u578b\u7684\u8de8\u9886\u57df\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22817", "abs": "https://arxiv.org/abs/2506.22817", "authors": ["Xingyilang Yin", "Jiale Wang", "Xi Yang", "Mutian Xu", "Xu Gu", "Nannan Wang"], "title": "Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding", "comment": null, "summary": "Recent open-vocabulary 3D scene understanding approaches mainly focus on\ntraining 3D networks through contrastive learning with point-text pairs or by\ndistilling 2D features into 3D models via point-pixel alignment. While these\nmethods show considerable performance in benchmarks with limited vocabularies,\nthey struggle to handle diverse object categories as the limited amount of 3D\ndata upbound training strong open-vocabulary 3d models. We observe that 2D\nmulti-view fusion methods take precedence in understanding diverse concepts in\n3D scenes. However, inherent noises in vision-language models lead multi-view\nfusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel\napproach aimed at unleashing the potential of 2D multi-view fusion for\nopen-vocabulary 3D scene understanding. We focus on reducing the inherent\nnoises without training, thereby preserving the generalizability while\nenhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D\nfeatures by leveraging precise region-level image features and text features\nencoded by CLIP encoders and incorporates 3D geometric priors to optimize\nmulti-view fusion. Extensive experiments on various datasets demonstrate the\neffectiveness of our method. Notably, our MVOV3D achieves a new record with\n14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge\nopen-vocabulary semantic segmentation, outperforming current leading trained 3D\nnetworks by a significant margin.", "AI": {"tldr": "MVOV3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c112D\u591a\u89c6\u56fe\u878d\u5408\u4e2d\u7684\u56fa\u6709\u566a\u58f0\uff0c\u63d0\u5347\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4fdd\u7559\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u8bcd\u6c47\u91cf\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u7684\u5bf9\u8c61\u7c7b\u522b\uff0c\u4e143D\u6570\u636e\u91cf\u6709\u9650\u5236\u7ea6\u4e86\u5f00\u653e\u8bcd\u6c473D\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "MVOV3D\u5229\u7528CLIP\u7f16\u7801\u5668\u63d0\u53d6\u7cbe\u786e\u7684\u533a\u57df\u7ea7\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\uff0c\u5e76\u7ed3\u54083D\u51e0\u4f55\u5148\u9a8c\u4f18\u5316\u591a\u89c6\u56fe\u878d\u5408\u3002", "result": "\u5728ScanNet200\u548cMatterport160\u4e0a\uff0cMVOV3D\u5206\u522b\u5b9e\u73b0\u4e8614.7%\u548c16.2%\u7684mIoU\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MVOV3D\u901a\u8fc7\u4f18\u53162D\u591a\u89c6\u56fe\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23025", "abs": "https://arxiv.org/abs/2506.23025", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u63d0\u51fa2\u4f4d\u548c1.6\u4f4d\u6743\u91cd\u6253\u5305\u65b9\u6848\uff0c\u5e76\u5f00\u53d1\u4e86\u52a0\u901f\u63a8\u7406\u7684GPU\u5185\u6838TriRun\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u4e0d\u8db3\u7684\u74f6\u9888\u3002", "method": "\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7684\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\uff0c\u5206\u6790\u5176\u6269\u5c55\u6027\uff0c\u63d0\u51fa2\u4f4d\u548c1.6\u4f4d\u6743\u91cd\u6253\u5305\u65b9\u6848\uff0c\u5e76\u5f00\u53d1TriRun GPU\u5185\u6838\u3002", "result": "TriLMs\u5728\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u65f6\u8868\u73b0\u66f4\u4f18\uff0cSpectra-1.1\u6a21\u578b\u5c55\u793a\u4e86\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff0cTriRun\u52a0\u901f\u63a8\u7406\u8fbe5\u500d\u3002", "conclusion": "\u4e3a\u9ad8\u6548LLMs\u7684\u6784\u5efa\u548c\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2506.23982", "categories": ["cs.CV", "cs.RO", "I.4.9"], "pdf": "https://arxiv.org/pdf/2506.23982", "abs": "https://arxiv.org/abs/2506.23982", "authors": ["Ruiyang Hao", "Bowen Jing", "Haibao Yu", "Zaiqing Nie"], "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving", "comment": "14 pages, 4 figures", "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff08E2EAD\uff09\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u4eba\u5de5\u9a8c\u8bc1\u751f\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u6807\u6ce8\uff0c\u4e3a\u4e2a\u6027\u5316E2EAD\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4e2a\u6027\u5316\u5728\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff08E2EAD\uff09\u4e2d\u88ab\u5ffd\u89c6\uff0c\u800c\u7528\u6237\u5bf9\u9f50\u884c\u4e3a\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u4fe1\u4efb\u548c\u666e\u53ca\u81f3\u5173\u91cd\u8981\u3002\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u4ece\u771f\u5b9e\u9053\u8def\u62d3\u6251\u4e2d\u63d0\u53d6\u9759\u6001\u73af\u5883\u7279\u5f81\uff0c\u4f7f\u7528\u5fae\u8c03\u7684VLM\u63a8\u65ad\u52a8\u6001\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u901a\u8fc7\u884c\u4e3a\u5206\u5e03\u5206\u6790\u548c\u89c4\u5219\u542f\u53d1\u5f0f\u751f\u6210\u5ba2\u89c2\u504f\u597d\u6807\u6ce8\uff0c\u5e76\u5229\u7528VLM\u751f\u6210\u4e3b\u89c2\u6807\u6ce8\uff0c\u6700\u540e\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u878d\u5408\u4e24\u8005\u3002", "result": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u663e\u793a\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u504f\u597d\u7684\u6a21\u578b\u884c\u4e3a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e2a\u6027\u5316E2EAD\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u3002"}}
{"id": "2506.22819", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22819", "abs": "https://arxiv.org/abs/2506.22819", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "comment": "26 pages", "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\uff08TPT\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63d0\u793a\u521d\u59cb\u5316\u548c\u65b0\u9896\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684TPT\u65b9\u6cd5\u5728\u63d0\u5347\u51c6\u786e\u7387\u65f6\u5ffd\u89c6\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "1. \u4f7f\u7528LLM\u521d\u59cb\u5316\u6d4b\u8bd5\u65f6\u63d0\u793a\u4ee5\u907f\u514d\u8fc7\u62df\u5408\uff1b2. \u63d0\u51fa\u6b63\u5219\u5316\u635f\u5931\u4ee5\u51cf\u5c11\u7c7b\u5185\u8ddd\u79bb\u5e76\u589e\u52a0\u7c7b\u95f4\u8ddd\u79bb\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5e73\u5747\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\u4ece11.7\u964d\u81f34.11\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u7684\u63d0\u793a\u521d\u59cb\u5316\u548c\u6b63\u5219\u5316\uff0cTPT\u7684\u6821\u51c6\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u589e\u5f3a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.23033", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23033", "abs": "https://arxiv.org/abs/2506.23033", "authors": ["Yash Vardhan Tomar"], "title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "comment": null, "summary": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u7279\u5f81\u8868\u793a\u6765\u51cf\u5c11\u4e0a\u4e0b\u6587\u504f\u5dee\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u504f\u5dee\u548c\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u504f\u5dee\u7f13\u89e3\u7b56\u7565\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7279\u5f81\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u56db\u79cd\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528\u504f\u5dee\u654f\u611f\u635f\u5931\u51fd\u6570\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u5e73\u5747\u504f\u5dee\u51cf\u5c1143.35%\uff0c\u5747\u65b9\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u6027\u80fd\u4f18\u4e8eSMOTE\u8fc7\u91c7\u6837\u3002", "conclusion": "\u7279\u5f81\u6df7\u5408\u6846\u67b6\u9ad8\u6548\u4e14\u65e0\u9700\u663e\u5f0f\u8bc6\u522b\u504f\u5dee\u5c5e\u6027\uff0c\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2506.24044", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.24044", "abs": "https://arxiv.org/abs/2506.24044", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "comment": null, "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e8620\u591a\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u53d1\u5c55\uff0cVLA\u8303\u5f0f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6574\u5408\u89c6\u89c9\u611f\u77e5\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u63a7\u5236\u7684\u6f5c\u529b\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u5206\u6563\u4e14\u5feb\u901f\u6269\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u3002", "method": "\u901a\u8fc7\u5206\u6790VLA\u6a21\u578b\u7684\u67b6\u6784\u6f14\u53d8\uff0c\u6bd4\u8f83\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5e76\u6574\u7406\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u603b\u7ed3\u4e86VLA\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u63a8\u8fdb\u53ef\u89e3\u91ca\u4e14\u793e\u4f1a\u5bf9\u9f50\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5168\u9762\u53c2\u8003\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2506.22832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22832", "abs": "https://arxiv.org/abs/2506.22832", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "comment": null, "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542c\u4f17\u589e\u5f3a\u7684GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u6539\u8fdb\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u548c\u63a8\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u76d1\u7763\u5fae\u8c03\u6613\u5bfc\u81f4\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u9700\u8981\u590d\u6742\u6807\u6ce8\u6d41\u7a0b\u3002\u5f3a\u5316\u5b66\u4e60\uff08\u5982GRPO\uff09\u867d\u80fd\u63d0\u5347\u6cdb\u5316\u6027\uff0c\u4f46\u5728\u63a8\u7406\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u5f15\u5165\u542c\u4f17\u589e\u5f3a\u7684GRPO\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u542c\u4f17\uff09\u91cd\u65b0\u8bc4\u4f30\u63a8\u7406\u94fe\uff0c\u751f\u6210\u5bc6\u96c6\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728ImageReward\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523067.4%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u4e0aOOD\u6027\u80fd\u63d0\u53476%\uff0c\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u77db\u76fe\u3002", "conclusion": "\u542c\u4f17\u589e\u5f3a\u7684\u5956\u52b1\u673a\u5236\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6570\u636e\u8def\u5f84\u3002"}}
{"id": "2506.23036", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23036", "abs": "https://arxiv.org/abs/2506.23036", "authors": ["Zain ul Abdeen", "Ming Jin"], "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "comment": null, "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u7f51\u7edc\u53c2\u6570\u5728\u5185\u5916\u538b\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u5206\u7c7b\u7684\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u5185\u5916\u538b\u529b\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u7a81\u89e6\u53ef\u5851\u6027\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u53c2\u6570\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7a81\u89e6\u8fc7\u6ee4\u5f15\u5165\u5185\u90e8\u538b\u529b\uff0c\u5bf9\u6297\u653b\u51fb\u5f15\u5165\u5916\u90e8\u538b\u529b\uff0c\u5b9a\u4e49\u53c2\u6570\u5206\u6570\u5206\u7c7b\u8106\u5f31\u3001\u9c81\u68d2\u6216\u53cd\u8106\u5f31\u53c2\u6570\u3002", "result": "\u5728Mujoco\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u53d1\u73b0\u53cd\u8106\u5f31\u53c2\u6570\u80fd\u63d0\u5347\u7b56\u7565\u5728\u538b\u529b\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u548c\u53cd\u8106\u5f31\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.22833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22833", "abs": "https://arxiv.org/abs/2506.22833", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "comment": null, "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the\nresulting images often lack the capacity for localized editing. In response,\ngenerative radiance manifolds emerge as an efficient approach for constrained\npoint sampling within volumes, effectively reducing computational demands and\nenabling the learning of fine details. This work introduces SemFaceEdit, a\nnovel method that streamlines the appearance and geometric editing process by\ngenerating semantic fields on generative radiance manifolds. Utilizing latent\ncodes, our method effectively disentangles the geometry and appearance\nassociated with different facial semantics within the generated image. In\ncontrast to existing methods that can change the appearance of the entire\nradiance field, our method enables the precise editing of particular facial\nsemantics while preserving the integrity of other regions. Our network\ncomprises two key modules: the Geometry module, which generates semantic\nradiance and occupancy fields, and the Appearance module, which is responsible\nfor predicting RGB radiance. We jointly train both modules in adversarial\nsettings to learn semantic-aware geometry and appearance descriptors. The\nappearance descriptors are then conditioned on their respective semantic latent\ncodes by the Appearance Module, facilitating disentanglement and enhanced\ncontrol. Our experiments highlight SemFaceEdit's superior performance in\nsemantic field-based editing, particularly in achieving improved radiance field\ndisentanglement.", "AI": {"tldr": "SemFaceEdit\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u573a\u5b9e\u73b0\u9762\u90e8\u56fe\u50cf\u7684\u5c40\u90e8\u7f16\u8f91\uff0c\u89e3\u51b3\u4e863D\u611f\u77e5GAN\u5728\u5c40\u90e8\u7f16\u8f91\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u611f\u77e5GAN\u6280\u672f\u867d\u7136\u63d0\u4f9b\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4f46\u7f3a\u4e4f\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u3002\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u867d\u80fd\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u5e76\u5b66\u4e60\u7ec6\u8282\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8bed\u4e49\u7f16\u8f91\u3002", "method": "SemFaceEdit\u901a\u8fc7\u51e0\u4f55\u6a21\u5757\u548c\u5916\u89c2\u6a21\u5757\u8054\u5408\u8bad\u7ec3\uff0c\u751f\u6210\u8bed\u4e49\u8f90\u5c04\u573a\u548cRGB\u8f90\u5c04\u573a\uff0c\u5229\u7528\u6f5c\u5728\u7801\u89e3\u8026\u51e0\u4f55\u4e0e\u5916\u89c2\uff0c\u5b9e\u73b0\u5c40\u90e8\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemFaceEdit\u5728\u8bed\u4e49\u573a\u7f16\u8f91\u548c\u8f90\u5c04\u573a\u89e3\u8026\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u7cbe\u786e\u7f16\u8f91\u7279\u5b9a\u9762\u90e8\u8bed\u4e49\u800c\u4e0d\u5f71\u54cd\u5176\u4ed6\u533a\u57df\u3002", "conclusion": "SemFaceEdit\u901a\u8fc7\u8bed\u4e49\u573a\u548c\u6f5c\u5728\u7801\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9762\u90e8\u56fe\u50cf\u7684\u7cbe\u786e\u5c40\u90e8\u7f16\u8f91\uff0c\u4e3a3D\u611f\u77e5\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23041", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23041", "abs": "https://arxiv.org/abs/2506.23041", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "comment": null, "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\uff08ViT\uff09\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u548cMLP\u5757\u91cd\u52a0\u6743\uff0c\u63d0\u5347\u5c0f\u89c4\u6a21\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u77e5\u8bc6\u84b8\u998f\u6548\u679c\u3002", "motivation": "\u5f53\u4ece\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u5f3a\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u65f6\uff0c\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5c0f\u89c4\u6a21\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u5f15\u5165MLP\u5757\u91cd\u52a0\u6743\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5c0f\u89c4\u6a21\u5b66\u751f\u6a21\u578b\u80fd\u591f\u4ece\u6700\u5f3a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u53d7\u76ca\u3002", "conclusion": "\u901a\u8fc7\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u548cMLP\u5757\u91cd\u52a0\u6743\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002"}}
{"id": "2506.22836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22836", "abs": "https://arxiv.org/abs/2506.22836", "authors": ["Hongyan An", "Kuan Zhu", "Xin He", "Haiyun Guo", "Chaoyang Zhao", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition", "comment": "ICME 2025 Oral", "summary": "Pedestrian attribute recognition (PAR) is a fundamental perception task in\nintelligent transportation and security. To tackle this fine-grained task, most\nexisting methods focus on extracting regional features to enrich attribute\ninformation. However, a regional feature is typically used to predict a fixed\nset of pre-defined attributes in these methods, which limits the performance\nand practicality in two aspects: 1) Regional features may compromise\nfine-grained patterns unique to certain attributes in favor of capturing common\ncharacteristics shared across attributes. 2) Regional features cannot\ngeneralize to predict unseen attributes in the test time. In this paper, we\npropose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C}\ng\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which\nadaptively extracts fine-grained attribute-level features for each attribute\nindividually, regardless of whether the attributes are seen or not during\ntraining. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to\ncapture latent features at varying levels of visual granularity, thereby\nenriching the diversity of the extracted information. Next, we introduce the\nAttribute-guided Visual Feature Extraction (AVFE) module, which leverages\ntextual attributes as queries to retrieve their corresponding visual attribute\nfeatures from the Mix Tokens using a cross-attention mechanism. To ensure that\ntextual attributes focus on the appropriate Mix Tokens, we further incorporate\na Region-Aware Contrastive Learning (RACL) method, encouraging attributes\nwithin the same region to share consistent attention maps. Extensive\nexperiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness\nand strong generalization ability of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFOCUS\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u6df7\u5408\u4ee4\u724c\u548c\u5c5e\u6027\u5f15\u5bfc\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7ea7\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u63d0\u53d6\u533a\u57df\u7279\u5f81\u6765\u9884\u6d4b\u9884\u5b9a\u4e49\u5c5e\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u727a\u7272\u67d0\u4e9b\u5c5e\u6027\u7684\u72ec\u7279\u7ec6\u7c92\u5ea6\u6a21\u5f0f\uff0c\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5c5e\u6027\u3002", "method": "\u63d0\u51faFOCUS\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u7c92\u5ea6\u6df7\u5408\u4ee4\u724c\uff08MGMT\uff09\u6355\u83b7\u4e0d\u540c\u89c6\u89c9\u7c92\u5ea6\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5c5e\u6027\u5f15\u5bfc\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08AVFE\uff09\u5229\u7528\u6587\u672c\u5c5e\u6027\u67e5\u8be2\u5bf9\u5e94\u89c6\u89c9\u7279\u5f81\uff0c\u4ee5\u53ca\u533a\u57df\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff08RACL\uff09\u786e\u4fdd\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u3002", "result": "\u5728PA100K\u3001PETA\u548cRAPv1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FOCUS\u65b9\u6cd5\u80fd\u591f\u81ea\u9002\u5e94\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7ea7\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.23053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23053", "abs": "https://arxiv.org/abs/2506.23053", "authors": ["Hanlin Dong", "Arian Prabowo", "Hao Xue", "Flora D. Salim"], "title": "Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction", "comment": null, "summary": "Air quality prediction is a challenging forecasting task due to its\nspatio-temporal complexity and the inherent dynamics as well as uncertainty.\nMost of the current models handle these two challenges by applying Graph Neural\nNetworks or known physics principles, and quantifying stochasticity through\nprobabilistic networks like Diffusion models. Nevertheless, finding the right\nbalancing point between the certainties and uncertainties remains an open\nquestion. Therefore, we propose Double-Diffusion, a novel diffusion\nprobabilistic model that harnesses the power of known physics to guide air\nquality forecasting with stochasticity. To the best of our knowledge, while\nprecedents have been made of using conditional diffusion models to predict air\npollution, this is the first attempt to use physics as a conditional generative\napproach for air quality prediction. Along with a sampling strategy adopted\nfrom image restoration and a new denoiser architecture, Double-Diffusion ranks\nfirst in most evaluation scenarios across two real-life datasets compared with\nother probabilistic models, it also cuts inference time by 50% to 30% while\nenjoying an increase between 3-12% in Continuous Ranked Probabilistic Score\n(CRPS).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDouble-Diffusion\u7684\u65b0\u578b\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u7ed3\u5408\u5df2\u77e5\u7269\u7406\u539f\u7406\u548c\u968f\u673a\u6027\u6765\u9884\u6d4b\u7a7a\u6c14\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u5e73\u8861\u786e\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u7269\u7406\u539f\u7406\u4f5c\u4e3a\u751f\u6210\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u53cc\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u539f\u7406\u4f5c\u4e3a\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u91c7\u6837\u7b56\u7565\u548c\u65b0\u53bb\u566a\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u8bc4\u4f30\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1130%-50%\uff0cCRPS\u63d0\u53473%-12%\u3002", "conclusion": "Double-Diffusion\u4e3a\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.22843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22843", "abs": "https://arxiv.org/abs/2506.22843", "authors": ["Kien Nguyen", "Clinton Fookes", "Sridha Sridharan", "Huy Nguyen", "Feng Liu", "Xiaoming Liu", "Arun Ross", "Dana Michalski", "Tam\u00e1s Endrei", "Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zijing Gong", "Yuhao Wang", "Xuehu Liu", "Pingping Zhang", "Md Rashidunnabi", "Hugo Proen\u00e7a", "Kailash A. Hambarde", "Saeid Rezaei"], "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results", "comment": null, "summary": "Person re-identification (ReID) across aerial and ground vantage points has\nbecome crucial for large-scale surveillance and public safety applications.\nAlthough significant progress has been made in ground-only scenarios, bridging\nthe aerial-ground domain gap remains a formidable challenge due to extreme\nviewpoint differences, scale variations, and occlusions. Building upon the\nachievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID\n2025 Challenge - the first large-scale video-based competition focused on\nhigh-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID\ndataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7\nmillion frames captured from UAVs, CCTV, and wearable cameras, the challenge\nfeatured four international teams. These teams developed solutions ranging from\nmulti-stream architectures to transformer-based temporal reasoning and\nphysics-informed modeling. The leading approach, X-TFCLIP from UAM, attained\n72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the\nground-to-aerial ReID setting, surpassing existing baselines while highlighting\nthe dataset's complexity. For additional details, please refer to the official\nwebsite at https://agvpreid25.github.io.", "AI": {"tldr": "AG-VPReID 2025 Challenge\u9996\u6b21\u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u7684\u9ad8\u7a7a\uff0880-120\u7c73\uff09\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u89c6\u89d2\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\uff0c\u4f7f\u7528\u65b0\u6570\u636e\u96c6AG-VPReID\uff0c\u5305\u542b3027\u4e2a\u8eab\u4efd\u548c370\u4e07\u5e27\u3002\u9886\u5148\u65b9\u6cd5X-TFCLIP\u8fbe\u523072.28%\u7684Rank-1\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u89c6\u89d2\u95f4\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u586b\u8865\u9ad8\u7a7a\u4e0e\u5730\u9762\u89c6\u89d2\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8eAG-VPReID\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u6d41\u67b6\u6784\u3001\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u63a8\u7406\u548c\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u7b49\u65b9\u6cd5\u3002", "result": "X-TFCLIP\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u5230\u5730\u9762\u548c\u5730\u9762\u5230\u65e0\u4eba\u673a\u7684\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523072.28%\u548c70.77%\u7684Rank-1\u51c6\u786e\u7387\u3002", "conclusion": "AG-VPReID 2025 Challenge\u5c55\u793a\u4e86\u9ad8\u7a7a\u4e0e\u5730\u9762\u89c6\u89d2\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6311\u6218\u6027\uff0cX-TFCLIP\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.23055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23055", "abs": "https://arxiv.org/abs/2506.23055", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u4e0e\u4eba\u7c7b\u5fc3\u7406\u7ef4\u5ea6\u6982\u5ff5\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc743\u4e2a\u6807\u51c6\u5316\u5fc3\u7406\u95ee\u5377\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0GPT-4\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u53cd\u5e94\u76f8\u5173\u6027\u8f83\u9ad8\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u51c6\u786e\u5185\u5316\u4eba\u7c7b\u5fc3\u7406\u6982\u5ff5\uff0c\u4ee5\u63ed\u793a\u5176\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u6f5c\u5728\u504f\u5dee\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u752843\u4e2a\u6807\u51c6\u5316\u5fc3\u7406\u95ee\u5377\uff0c\u901a\u8fc7\u6210\u5bf9\u76f8\u4f3c\u6027\u5206\u6790\u548c\u5c42\u6b21\u805a\u7c7b\uff0c\u6bd4\u8f83\u6a21\u578b\u751f\u6210\u7684\u805a\u7c7b\u7ed3\u6784\u4e0e\u539f\u59cb\u5206\u7c7b\u6807\u7b7e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cGPT-4\u7684\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\uff0866.2%\uff09\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\uff0855.9%\uff09\u548cBERT\uff0848.1%\uff09\uff0c\u4e14\u5176\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u4eba\u7c7b\u53cd\u5e94\u76f8\u5173\u6027\u8f83\u9ad8\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ee5\u53ef\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u8fd1\u4f3c\u4eba\u7c7b\u5fc3\u7406\u6982\u5ff5\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.22850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22850", "abs": "https://arxiv.org/abs/2506.22850", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "title": "DMD-Net: Deep Mesh Denoising Network", "comment": null, "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning\nframework, for solving the mesh denoising problem. DMD-Net consists of a Graph\nConvolutional Neural Network in which aggregation is performed in both the\nprimal as well as the dual graph. This is realized in the form of an asymmetric\ntwo-stream network, which contains a primal-dual fusion block that enables\ncommunication between the primal-stream and the dual-stream. We develop a\nFeature Guided Transformer (FGT) paradigm, which consists of a feature\nextractor, a transformer, and a denoiser. The feature extractor estimates the\nlocal features, that guide the transformer to compute a transformation, which\nis applied to the noisy input mesh to obtain a useful intermediate\nrepresentation. This is further processed by the denoiser to obtain the\ndenoised mesh. Our network is trained on a large scale dataset of 3D objects.\nWe perform exhaustive ablation studies to demonstrate that each component in\nour network is essential for obtaining the best performance. We show that our\nmethod obtains competitive or better results when compared with the\nstate-of-the-art mesh denoising algorithms. We demonstrate that our method is\nrobust to various kinds of noise. We observe that even in the presence of\nextremely high noise, our method achieves excellent performance.", "AI": {"tldr": "DMD-Net\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u683c\u53bb\u566a\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u6d41\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6027\u80fd\u53bb\u566a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7f51\u683c\u53bb\u566a\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u53bb\u566a\u6548\u679c\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u539f\u59cb\u56fe\u548c\u53cc\u56fe\u7684\u53cc\u6d41\u7f51\u7edc\uff0c\u4ee5\u53ca\u7279\u5f81\u5f15\u5bfc\u53d8\u6362\u5668\uff08FGT\uff09\u8303\u5f0f\uff0c\u5b9e\u73b0\u7f51\u683c\u53bb\u566a\u3002", "result": "\u5728\u5927\u89c4\u6a213D\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u5bf9\u9ad8\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "DMD-Net\u5728\u7f51\u683c\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u5904\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23068", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.23068", "abs": "https://arxiv.org/abs/2506.23068", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J\u00fcrgen Schmidhuber", "Mengyue Yang"], "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "comment": "33 pages", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5143\u56e0\u679c\u56fe\u201d\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u8868\u793a\u4e0d\u540c\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u4e0b\u7684\u56e0\u679c\u7ed3\u6784\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u4e0d\u65ad\u4f18\u5316\u6a21\u578b\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u56e0\u679c\u673a\u5236\u5e38\u56e0\u89c2\u5bdf\u7a97\u53e3\u72ed\u7a84\u800c\u663e\u5f97\u6f02\u79fb\uff0c\u4f20\u7edf\u5047\u8bbe\u5355\u4e00\u56fa\u5b9a\u56e0\u679c\u673a\u5236\u7684\u4e16\u754c\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\u3002", "method": "\u5f15\u5165\u5143\u56e0\u679c\u56fe\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\uff0c\u5305\u542b\u591a\u4e2a\u7531\u6f5c\u5728\u72b6\u6001\u89e6\u53d1\u7684\u56e0\u679c\u5b50\u56fe\uff0c\u5e76\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5e72\u9884\u7b56\u7565\u53d1\u73b0\u548c\u4f18\u5316\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u673a\u5668\u4eba\u624b\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u56e0\u679c\u52a8\u6001\u53d8\u5316\u5e76\u6cdb\u5316\u5230\u65b0\u60c5\u5883\u3002", "conclusion": "\u5143\u56e0\u679c\u56fe\u53ca\u5176\u9a71\u52a8\u7684\u56e0\u679c\u63a2\u7d22\u4ee3\u7406\u4e3a\u89e3\u51b3\u52a8\u6001\u56e0\u679c\u673a\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2506.22864", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22864", "abs": "https://arxiv.org/abs/2506.22864", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "comment": "ICMR 2025", "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "AI": {"tldr": "MaTIR\u4efb\u52a1\u7ed3\u5408\u4e86\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u548c\u53c2\u8003\u8868\u8fbe\u5f0f\u5206\u5272\uff08RES\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TIR\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u800cRES\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u96c6\u5408\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u5206\u5272\u611f\u77e5\u7684\u56fe\u50cf\u68c0\u7d22\u7ed3\u679c\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u91cd\u65b0\u6392\u5e8f\u548c\u5bf9\u8c61\u5b9a\u4f4d\u3002\u5229\u7528SAM 2\u548cAlpha-CLIP\u79bb\u7ebf\u751f\u6210\u5bf9\u8c61\u63a9\u7801\u548c\u533a\u57df\u7ea7\u5d4c\u5165\u3002", "result": "\u5728COCO\u548cD$^3$\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u68c0\u7d22\u51c6\u786e\u6027\u548c\u5206\u5272\u8d28\u91cf\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MaTIR\u4efb\u52a1\u548c\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86TIR\u548cRES\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u56fe\u50cf\u68c0\u7d22\u548c\u5bf9\u8c61\u5206\u5272\u3002"}}
{"id": "2506.23145", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23145", "abs": "https://arxiv.org/abs/2506.23145", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "comment": null, "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "AI": {"tldr": "Forget-MI\u662f\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\uff0c\u901a\u8fc7\u635f\u5931\u51fd\u6570\u548c\u6270\u52a8\u6280\u672f\u5b9e\u73b0\u6570\u636e\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u533b\u7597AI\u4e2d\uff0c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u591a\u6a21\u6001\u67b6\u6784\u4e2d\u79fb\u9664\u654f\u611f\u6570\u636e\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u635f\u5931\u51fd\u6570\u548c\u6270\u52a8\u6280\u672f\uff0cForget-MI\u80fd\u591f\u9057\u5fd8\u5355\u6a21\u6001\u548c\u8054\u5408\u8868\u793a\u7684\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6570\u636e\u7684\u77e5\u8bc6\u3002", "result": "Forget-MI\u5728\u9057\u5fd8\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u663e\u8457\u964d\u4f4e\uff08AUC\u548cF1\u5206\u6570\u5206\u522b\u4e0b\u964d0.221\u548c0.305\uff09\uff0c\u540c\u65f6\u6d4b\u8bd5\u96c6\u6027\u80fd\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\uff0c\u4e14MIA\u964d\u4f4e0.202\u3002", "conclusion": "Forget-MI\u5728\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u9057\u5fd8\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.22866", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22866", "abs": "https://arxiv.org/abs/2506.22866", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "comment": null, "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u533a\u57df\u611f\u77e5CAM\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u89e3\u51b3\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u548c\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u4e0e\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u5b9e\u9645\u9700\u6c42\u51b2\u7a81\u3002", "method": "\u5f15\u5165\u8fc7\u6ee4\u5f15\u5bfc\u53cd\u5411\u4f20\u64ad\uff08FGBP\uff09\u4f18\u5316CAM\uff0c\u7ed3\u5408\u533a\u57df\u611f\u77e5\u52a0\u6743\u6a21\u5757\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\u3002", "result": "\u5728\u5de5\u4e1a\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u586b\u8865\u4e86\u5f31\u76d1\u7763\u5b66\u4e60\u4e0e\u9ad8\u7cbe\u5ea6\u7f3a\u9677\u5206\u5272\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23147", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23147", "abs": "https://arxiv.org/abs/2506.23147", "authors": ["Jonathan Schuster", "Fabian Transchel"], "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "comment": "6 pages, 2 figures", "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u9a7e\u9a76\u884c\u4e3a\u5206\u7c7b\u7684maneuverRecognition\u5305\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9884\u5904\u7406\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u4fee\u6539\u7684LSTM\u7f51\u7edc\u7ed3\u6784\u3002", "motivation": "\u9a7e\u9a76\u884c\u4e3a\u8bc6\u522b\u53ef\u63d0\u5347\u4fdd\u9669\u4e2a\u6027\u5316\u3001\u9053\u8def\u5b89\u5168\u548c\u73af\u4fdd\u9a7e\u9a76\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5b9e\u7528\u7684Python\u5de5\u5177\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86maneuverRecognition\u5305\uff0c\u5305\u542b\u9884\u5904\u7406\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\u529f\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8eLSTM\u7684\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u4f7f\u7528\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u8bb0\u5f55\u7684\u4e09\u4eba\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u9a8c\u8bc1\u4e86\u5305\u7684\u6709\u6548\u6027\u3002", "conclusion": "maneuverRecognition\u5305\u4e3a\u9a7e\u9a76\u884c\u4e3a\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u652f\u6301\u5feb\u901f\u5f00\u53d1\u548c\u8bc4\u4f30\u6a21\u578b\u3002"}}
{"id": "2506.22868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22868", "abs": "https://arxiv.org/abs/2506.22868", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "comment": "15 pages, 9 figures, 3 tables", "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "AI": {"tldr": "STR-Match\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7f16\u8f91\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eSTR\u5206\u6570\u7684\u6f5c\u5728\u4f18\u5316\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u3001\u8fd0\u52a8\u626d\u66f2\u548c\u9886\u57df\u8f6c\u6362\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65f6\u7a7a\u50cf\u7d20\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSTR-Match\u7b97\u6cd5\uff0c\u5229\u75282D\u7a7a\u95f4\u6ce8\u610f\u529b\u548c1D\u65f6\u95f4\u6a21\u5757\u8ba1\u7b97STR\u5206\u6570\uff0c\u6307\u5bfc\u6f5c\u5728\u4f18\u5316\u6846\u67b6\u751f\u6210\u65f6\u7a7a\u4e00\u81f4\u7684\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTR-Match\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u663e\u8457\u9886\u57df\u8f6c\u6362\u65f6\u4ecd\u80fd\u4fdd\u6301\u6e90\u89c6\u9891\u7684\u5173\u952e\u89c6\u89c9\u5c5e\u6027\u3002", "conclusion": "STR-Match\u901a\u8fc7\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u50cf\u7d20\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7f16\u8f91\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2506.23165", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.23165", "abs": "https://arxiv.org/abs/2506.23165", "authors": ["David Bossens", "Atsushi Nitanda"], "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "comment": null, "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22880", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22880", "abs": "https://arxiv.org/abs/2506.22880", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "comment": null, "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "AI": {"tldr": "DeSa2VA\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u589e\u5f3a\u7684\u63d0\u793a\u65b9\u6848\uff0c\u901a\u8fc7\u6587\u672c\u9884\u8bad\u7ec3\u548c\u7ebf\u6027\u89e3\u8026\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u5206\u5272\u548c\u63a5\u5730\u65b9\u6cd5\u4e2d\u52a8\u6001\u89c6\u89c9\u4fe1\u606f\u4e0e\u9759\u6001\u8bed\u4e49\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982Sa2VA\uff09\u76f4\u63a5\u5c06\u7279\u5f81\u878d\u5408\u5230\u5206\u5272\u6a21\u578b\u4e2d\uff0c\u5bfc\u81f4\u52a8\u6001\u89c6\u89c9\u4fe1\u606f\u4e0e\u9759\u6001\u8bed\u4e49\u7ea0\u7f20\uff0c\u964d\u4f4e\u4e86\u5206\u5272\u51c6\u786e\u6027\u3002", "method": "1. \u8bbe\u8ba1\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c06\u6587\u672c\u6807\u7b7e\u8f6c\u6362\u4e3a\u70b9\u7ea7\u63d0\u793a\u5e76\u751f\u6210\u6587\u672c\u63a9\u7801\uff1b2. \u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u89e3\u8026\u4e3a\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u5b50\u7a7a\u95f4\uff1b3. \u52a8\u6001\u63a9\u7801\u878d\u5408\u7b56\u7565\u7ed3\u5408\u89e3\u8026\u7279\u5f81\u3002", "result": "\u5728\u56fe\u50cf\u5206\u5272\u3001\u56fe\u50cf\u95ee\u7b54\u3001\u89c6\u9891\u5206\u5272\u548c\u89c6\u9891\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeSa2VA\u901a\u8fc7\u89e3\u8026\u548c\u52a8\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u548c\u63a5\u5730\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23174", "abs": "https://arxiv.org/abs/2506.23174", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "comment": "Published in MobiSys 2025", "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "AI": {"tldr": "\u63d0\u51faSynCheck\u65b9\u6848\uff0c\u901a\u8fc7\u91cf\u5316\u5408\u6210\u6570\u636e\u7684\u4eb2\u548c\u6027\u4e0e\u591a\u6837\u6027\uff0c\u6539\u8fdb\u65e0\u7ebf\u4f20\u611f\u4efb\u52a1\u4e2d\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0d\u53ef\u9884\u6d4b\uff0c\u5f71\u54cd\u65e0\u7ebf\u4f20\u611f\u4efb\u52a1\u6027\u80fd\uff0c\u9700\u91cf\u5316\u8bc4\u4f30\u5e76\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4eb2\u548c\u6027\u4e0e\u591a\u6837\u6027\u6307\u6807\u8bc4\u4f30\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1SynCheck\u65b9\u6848\u4f18\u5316\u6570\u636e\u4f7f\u7528\u3002", "result": "SynCheck\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u53474.3%\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u964d\u4f4e13.4%\u3002", "conclusion": "SynCheck\u6709\u6548\u89e3\u51b3\u5408\u6210\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u65e0\u7ebf\u4f20\u611f\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.22881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22881", "abs": "https://arxiv.org/abs/2506.22881", "authors": ["Fumiya Uchiyama", "Rintaro Yanagi", "Shohei Taniguchi", "Shota Takashiro", "Masahiro Suzuki", "Hirokatsu Kataoka", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings", "comment": null, "summary": "Contrastive learning has the capacity to model multimodal probability\ndistributions by embedding and aligning visual representations with semantics\nfrom captions. This approach enables the estimation of relational semantic\nsimilarity; however, it remains unclear whether it can also represent absolute\nsemantic informativeness. In this work, we introduce a semantic informativeness\nmetric for an image calculated from text samples via a contrastive learning\nmodel; similarly, the informativeness of a text is calculated from image\nsamples. We propose a redefinition of the concept of Information Gain, a\nconcept previously explored in natural language processing, extending its\napplication to the domains of vision and language. Our metric quantifies how\nconditioning on an image distorts the distribution of associated texts, and\nvice versa for text conditioning on image distributions. In OpenCLIP's\nempirical results, we observe that images with the lowest Information Gain\nscores often correspond to placeholder icons such as \"image not found.\"\nFurthermore, we propose to measure a norm-based metric of the embedding to\nestimate the Information Gain, following the theoretical results for Skip-Gram\nwith Negative Sampling (SGNS) word embedding. Information Gain can be measured\nusing either CLIP or SigLIP, and the results demonstrate a strong correlation\nwith a coefficient of determination ranging from 0.98 to 1.00. After obtaining\nthe mean and the covariance of the sample embedding, the computational cost of\nthis method is independent of the sample size, and it is compatible with\npublicly available, open-weight models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bed\u4e49\u4fe1\u606f\u91cf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u548c\u6587\u672c\u7684\u7edd\u5bf9\u8bed\u4e49\u4fe1\u606f\u91cf\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e86\u4fe1\u606f\u589e\u76ca\u7684\u6982\u5ff5\u3002", "motivation": "\u7814\u7a76\u5bf9\u6bd4\u5b66\u4e60\u662f\u5426\u80fd\u8868\u793a\u7edd\u5bf9\u8bed\u4e49\u4fe1\u606f\u91cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u7cfb\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u56fe\u50cf\u548c\u6587\u672c\u7684\u8bed\u4e49\u4fe1\u606f\u91cf\uff0c\u63d0\u51fa\u57fa\u4e8e\u5d4c\u5165\u7684\u4fe1\u606f\u589e\u76ca\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4fe1\u606f\u589e\u76ca\u5206\u6570\u4f4e\u7684\u56fe\u50cf\u901a\u5e38\u4e3a\u5360\u4f4d\u56fe\u6807\uff08\u5982\u201c\u672a\u627e\u5230\u56fe\u50cf\u201d\uff09\uff0c\u4e14\u4fe1\u606f\u589e\u76ca\u4e0e\u5d4c\u5165\u8303\u6570\u5f3a\u76f8\u5173\uff08R\u00b2=0.98-1.00\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u516c\u5f00\u6a21\u578b\uff0c\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u4e49\u4fe1\u606f\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.23182", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23182", "abs": "https://arxiv.org/abs/2506.23182", "authors": ["Robert Frank", "Michael Widrich", "Rahmad Akbar", "G\u00fcnter Klambauer", "Geir Kjetil Sandve", "Philippe A. Robert", "Victor Greiff"], "title": "Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data", "comment": null, "summary": "Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAMA\u7684\u751f\u6210\u6a21\u578b\u5f52\u56e0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5728\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u4e2d\uff0c\u751f\u6210\u6a21\u578b\uff08\u5982LSTM\uff09\u53ef\u4ee5\u4ec5\u7528\u6b63\u6807\u7b7e\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u7f3a\u4e4f\u5f52\u56e0\u65b9\u6cd5\u9650\u5236\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eIntegrated Gradients\u7684GAMA\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u6297\u4f53\u6570\u636e\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "GAMA\u80fd\u591f\u6062\u590d\u751f\u7269\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u5728\u65e0\u9700\u8d1f\u6807\u7b7e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u751f\u6210\u5e8f\u5217\u8bbe\u8ba1\u7b56\u7565\u3002", "conclusion": "GAMA\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u7684\u9a8c\u8bc1\u548c\u4f18\u5316\u3002"}}
{"id": "2506.22890", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.22890", "abs": "https://arxiv.org/abs/2506.22890", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "comment": null, "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard", "AI": {"tldr": "CP-Guard\u662f\u4e00\u4e2a\u9632\u5fa1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u6d88\u9664\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6076\u610f\u4ee3\u7406\uff0c\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u548c\u81ea\u9002\u5e94\u9608\u503c\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "motivation": "\u534f\u4f5c\u611f\u77e5\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u63d0\u5347\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u6613\u53d7\u6076\u610f\u4ee3\u7406\u653b\u51fb\uff0c\u9700\u4e00\u79cd\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51faPASAC\u65b9\u6cd5\u91c7\u6837\u9a8c\u8bc1\u5171\u8bc6\uff0c\u5b9a\u4e49CCLoss\u6355\u6349\u5dee\u5f02\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u9608\u503c\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCP-Guard\u80fd\u6709\u6548\u68c0\u6d4b\u6076\u610f\u4ee3\u7406\u5e76\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "conclusion": "CP-Guard\u4e3a\u534f\u4f5c\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2506.23186", "categories": ["cs.LG", "cs.DM", "math.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23186", "abs": "https://arxiv.org/abs/2506.23186", "authors": ["Marco Bressan", "Victor Chepoi", "Emmanuel Esposito", "Maximilian Thiessen"], "title": "Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs", "comment": null, "summary": "Abstract notions of convexity over the vertices of a graph, and corresponding\nnotions of halfspaces, have recently gained attention from the machine learning\ncommunity. In this work we study monophonic halfspaces, a notion of graph\nhalfspaces defined through closure under induced paths. Our main result is a\n$2$-satisfiability based decomposition theorem, which allows one to represent\nmonophonic halfspaces as a disjoint union of certain vertex subsets. Using this\ndecomposition, we achieve efficient and (nearly) optimal algorithms for various\nlearning problems, such as teaching, active, and online learning. Most notably,\nwe obtain a polynomial-time algorithm for empirical risk minimization.\nIndependently of the decomposition theorem, we obtain an efficient, stable, and\nproper sample compression scheme. This makes monophonic halfspaces efficiently\nlearnable with proper learners and linear error rate $1/\\varepsilon$ in the\nrealizable PAC setting. Our results answer open questions from the literature,\nand show a stark contrast with geodesic halfspaces, for which most of the said\nlearning problems are NP-hard.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u9876\u70b9\u4e0a\u7684\u5355\u97f3\u534a\u7a7a\u95f4\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2-\u53ef\u6ee1\u8db3\u6027\u7684\u5206\u89e3\u5b9a\u7406\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7d22\u56fe\u9876\u70b9\u4e0a\u7684\u51f8\u6027\u6982\u5ff5\u53ca\u5176\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6d4b\u5730\u534a\u7a7a\u95f4\uff09\u7684NP\u96be\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bf1\u5bfc\u8def\u5f84\u5b9a\u4e49\u5355\u97f3\u534a\u7a7a\u95f4\uff0c\u63d0\u51fa\u5206\u89e3\u5b9a\u7406\u5c06\u5176\u8868\u793a\u4e3a\u4e0d\u76f8\u4ea4\u7684\u9876\u70b9\u5b50\u96c6\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6837\u672c\u538b\u7f29\u65b9\u6848\u3002", "conclusion": "\u5355\u97f3\u534a\u7a7a\u95f4\u5728\u53ef\u5b66\u4e60\u6027\u4e0a\u4f18\u4e8e\u6d4b\u5730\u534a\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u6587\u732e\u4e2d\u7684\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2506.22899", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S\u00fcsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u7684\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u4e0b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u751f\u6210\u9ad8\u6e05\u8f93\u51fa\u7684\u80fd\u529b\u3002", "motivation": "NCA\u5728\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u4fe1\u606f\u4f20\u64ad\u53d7\u9650\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5171\u4eab\u9690\u5f0f\u89e3\u7801\u5668\uff0c\u5728\u7c97\u7f51\u683c\u4e0a\u8fdb\u884cNCA\u6f14\u5316\u540e\uff0c\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u6e32\u67d3\u4efb\u610f\u5206\u8fa8\u7387\u7684\u8f93\u51fa\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8d28\u91cf\u3001\u6548\u7387\u548c\u6027\u80fd\uff0c\u652f\u6301\u5b9e\u65f6\u751f\u6210\u9ad8\u6e05\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7ec4\u7ec7\u548c\u6d8c\u73b0\u7279\u6027\u3002", "conclusion": "\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u7684NCA\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u53d8\u4f53\u548c\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.23201", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23201", "abs": "https://arxiv.org/abs/2506.23201", "authors": ["Haoran Li", "Muhao Guo", "Marija Ilic", "Yang Weng", "Guangchun Ruan"], "title": "External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting", "comment": "10 pages", "summary": "Accurate residential load forecasting is critical for power system\nreliability with rising renewable integration and demand-side flexibility.\nHowever, most statistical and machine learning models treat external factors,\nsuch as weather, calendar effects, and pricing, as extra input, ignoring their\nheterogeneity, and thus limiting the extraction of useful external information.\nWe propose a paradigm shift: external data should serve as meta-knowledge to\ndynamically adapt the forecasting model itself. Based on this idea, we design a\nmeta-representation framework using hypernetworks that modulate selected\nparameters of a base Deep Learning (DL) model in response to external\nconditions. This provides both expressivity and adaptability. We further\nintegrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through\nselective expert activation, while improving robustness by filtering redundant\nexternal inputs. The resulting model, dubbed as a Meta Mixture of Experts for\nExternal data (M2oE2), achieves substantial improvements in accuracy and\nrobustness with limited additional overhead, outperforming existing\nstate-of-the-art methods in diverse load datasets. The dataset and source code\nare publicly available at\nhttps://github.com/haorandd/M2oE2\\_load\\_forecast.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2oE2\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u548c\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u52a8\u6001\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4f\u5b85\u8d1f\u8377\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u548c\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\u7684\u589e\u52a0\uff0c\u51c6\u786e\u7684\u4f4f\u5b85\u8d1f\u8377\u9884\u6d4b\u5bf9\u7535\u529b\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u578b\u5ffd\u7565\u5916\u90e8\u56e0\u7d20\u7684\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u6709\u7528\u4fe1\u606f\u7684\u63d0\u53d6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u5143\u8868\u793a\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "M2oE2\u5728\u591a\u79cd\u8d1f\u8377\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u989d\u5916\u5f00\u9500\u6709\u9650\u3002", "conclusion": "M2oE2\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5916\u90e8\u6761\u4ef6\uff0c\u4e3a\u8d1f\u8377\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.22900", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22900", "abs": "https://arxiv.org/abs/2506.22900", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "comment": null, "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "AI": {"tldr": "MOTOR\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08MedVQA\uff09\u7684\u51c6\u786e\u6027\uff0c\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd56.45%\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728MedVQA\u4e2d\u5e38\u751f\u6210\u9519\u8bef\u7b54\u6848\uff0c\u4e14\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u591a\u6a21\u6001\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "MOTOR\u5229\u7528\u57fa\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u68c0\u7d22\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u7840\u63cf\u8ff0\u548c\u6700\u4f18\u4f20\u8f93\u6280\u672f\uff0c\u6355\u6349\u67e5\u8be2\u4e0e\u4e0a\u4e0b\u6587\u7684\u591a\u6a21\u6001\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\uff0cMOTOR\u5728MedVQA\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53476.45%\u3002", "conclusion": "MOTOR\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u663e\u8457\u63d0\u5347\u4e86MedVQA\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002"}}
{"id": "2506.23210", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.23210", "abs": "https://arxiv.org/abs/2506.23210", "authors": ["Taehwan Yoon", "Bongjun Choi"], "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "comment": "6 pages,14 equation", "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u5fae\u8c03\u89e3\u51b3\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4e0d\u8db3\u4e14\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\uff0c\u9700\u901a\u8fc7\u4f18\u5316\u548c\u4e2a\u6027\u5316\u63d0\u5347\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\uff0c\u5f15\u5165\u6700\u4f18\u8fd1\u7aef\u9879\u548c\u53c2\u8003\u6a21\u578b\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u517c\u987e\u6027\u80fd\u548c\u9690\u79c1\u3002"}}
{"id": "2506.22902", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22902", "abs": "https://arxiv.org/abs/2506.22902", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "comment": null, "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u70b9\u4e91\u538b\u7f29\uff08PCC\uff09\u548c\u8d28\u91cf\u8bc4\u4f30\uff08PCQA\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u624b\u5de5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u70b9\u4e91\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\u53ca\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u5e94\u7528\u9700\u6c42\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u538b\u7f29\u548c\u8d28\u91cf\u8bc4\u4f30\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790\u624b\u5de5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684PCC\u7b97\u6cd5\u53caPCQA\u6307\u6807\uff0c\u5e76\u5728\u65b0\u5174\u6570\u636e\u96c6\u4e0a\u5bf9\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6587\u7ae0\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u65b9\u6cd5\u6bd4\u8f83\u548c\u5b9e\u9645\u89c1\u89e3\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u5ef6\u8fdf\u548c\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\u7b49\u6311\u6218\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u6df7\u5408\u538b\u7f29\u6846\u67b6\u548c\u9ad8\u7ea7\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u3002"}}
{"id": "2506.23221", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23221", "abs": "https://arxiv.org/abs/2506.23221", "authors": ["B\u00e1lint Horv\u00e1th", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "comment": "23 pages, 8 figures, 6 tables", "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5SGKI\uff0c\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u4e2d\u7684\u7f3a\u5931\u50cf\u7d20\u4f30\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4e2d\u7f3a\u5931\u50cf\u7d20\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u57fa\u4e8eReproducing Kernel Hilbert Space (RKHS)\u7684\u5047\u8bbe\uff0c\u63d0\u51faSGKI\u65b9\u6cd5\uff0c\u5229\u7528Schur\u8865\u9ad8\u6548\u8ba1\u7b97\u975e\u6e10\u8fd1\u7f6e\u4fe1\u5e26\u3002", "result": "SGKI\u4e0d\u4ec5\u80fd\u4f30\u8ba1\u7f3a\u5931\u50cf\u7d20\uff0c\u8fd8\u80fd\u4e3a\u6240\u6709\u7f3a\u5931\u50cf\u7d20\u6784\u5efa\u540c\u65f6\u4fdd\u8bc1\u7684\u7f6e\u4fe1\u5e26\u3002", "conclusion": "SGKI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002"}}
{"id": "2506.22907", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.22907", "abs": "https://arxiv.org/abs/2506.22907", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "comment": null, "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "AI": {"tldr": "MagShield\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7a00\u758f\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u4e2d\u7684\u78c1\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u7ea0\u6b63\u7b56\u7565\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709IMU\u7cfb\u7edf\u5728\u78c1\u5e72\u6270\u73af\u5883\u4e2d\u6613\u4ea7\u751f\u65b9\u5411\u4f30\u8ba1\u8bef\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u201c\u68c0\u6d4b-\u7ea0\u6b63\u201d\u7b56\u7565\uff0c\u901a\u8fc7\u591aIMU\u8054\u5408\u5206\u6790\u68c0\u6d4b\u78c1\u5e72\u6270\uff0c\u5e76\u5229\u7528\u4eba\u4f53\u8fd0\u52a8\u5148\u9a8c\u7ea0\u6b63\u65b9\u5411\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMagShield\u663e\u8457\u63d0\u9ad8\u4e86\u78c1\u5e72\u6270\u4e0b\u7684\u8fd0\u52a8\u6355\u6349\u51c6\u786e\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "MagShield\u80fd\u6709\u6548\u63d0\u5347\u7a00\u758f\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5728\u78c1\u5e72\u6270\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23225", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23225", "abs": "https://arxiv.org/abs/2506.23225", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "title": "Masked Gated Linear Unit", "comment": null, "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Masked Gated Linear Units (MGLUs)\uff0c\u901a\u8fc7\u5171\u4eab\u6743\u91cd\u77e9\u9635\u548c\u786c\u4ef6\u53cb\u597d\u7684\u5185\u6838\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfGLUs\u7531\u4e8e\u4f7f\u7528\u72ec\u7acb\u7684\u6743\u91cd\u77e9\u9635\u5bfc\u81f4\u5185\u5b58\u8bfb\u53d6\u91cf\u7ffb\u500d\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faMGLUs\uff0c\u5305\u62ecMoEG\u67b6\u6784\u548cFlashMGLU\u5185\u6838\uff0c\u901a\u8fc7\u5171\u4eab\u6743\u91cd\u77e9\u9635\u548c\u4f18\u5316\u5b9e\u73b0\u51cf\u5c11\u5185\u5b58\u4f20\u8f93\u3002", "result": "\u5728RTX5090 GPU\u4e0a\uff0cMGLUs\u6bd4\u6807\u51c6GLUs\u5185\u5b58\u6548\u7387\u63d0\u534747%\uff0c\u901f\u5ea6\u63d0\u534734%\uff0c\u63a8\u7406\u901f\u5ea6\u6700\u9ad8\u63d0\u534719.7\u500d\u3002", "conclusion": "MGLUs\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.22908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22908", "abs": "https://arxiv.org/abs/2506.22908", "authors": ["Yuzhu Wang", "Manni Duan", "Shu Kong"], "title": "Attention to Burstiness: Low-Rank Bilinear Prompt Tuning", "comment": "ICCV 2025", "summary": "Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique\nthat adapts a pre-trained vision Transformer (ViT) by learning a small set of\nparameters in the input space, known as prompts. In VPT, we uncover\n``burstiness'' in the values arising from the interaction of image patch\nembeddings, and the key and query projectors within Transformer's\nself-attention module. Furthermore, the values of patch embeddings and the key\nand query projectors exhibit Laplacian and hyper-Laplacian distribution,\nrespectively. Intuitively, these non-Gaussian distributions pose challenges for\nlearning prompts. To address this, we propose whitening these data,\nde-correlating them and equalizing their variance towards more Gaussian before\nlearning prompts. We derive the whitening matrix over random image patch\nembeddings and ViT's key and query projectors, and multiply it with the prompt\nto be learned in a bilinear manner. Surprisingly, this method significantly\naccelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on\nthe CUB dataset; interestingly, it learns ``bursty prompts''. Extending the\nbilinear model which is known to introduce burstiness, we present a compact,\nlow-rank version by learning two smaller matrices whose multiplication yields\nthe final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).\nExtensive experiments across multiple benchmark datasets demonstrate that BPT\nmethods not only outperform various VPT methods but also reduce parameter count\nand computation overhead.", "AI": {"tldr": "Visual Prompt Tuning (VPT) \u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u6280\u672f\uff0c\u901a\u8fc7\u5728\u5b66\u4e60\u8f93\u5165\u7a7a\u95f4\u4e2d\u7684\u5c11\u91cf\u53c2\u6570\uff08\u79f0\u4e3a\u63d0\u793a\uff09\u6765\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u89c6\u89c9 Transformer (ViT)\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u56fe\u50cf\u5757\u5d4c\u5165\u4e0e Transformer \u81ea\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u7684\u952e\u548c\u67e5\u8be2\u6295\u5f71\u5668\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f1a\u4ea7\u751f\u201c\u7a81\u53d1\u6027\u201d\u503c\uff0c\u4e14\u8fd9\u4e9b\u503c\u7684\u5206\u5e03\u4e3a\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u589e\u52a0\u4e86\u5b66\u4e60\u63d0\u793a\u7684\u96be\u5ea6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u767d\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u76f8\u5173\u548c\u65b9\u5dee\u5747\u8861\u5316\u4f7f\u5176\u66f4\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u53cc\u7ebf\u6027\u63d0\u793a\u8c03\u4f18\uff08BPT\uff09\uff0c\u663e\u8457\u52a0\u901f\u4e86\u63d0\u793a\u5b66\u4e60\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9 Transformer \u5728\u5fae\u8c03\u65f6\u5b58\u5728\u975e\u9ad8\u65af\u5206\u5e03\u7684\u6570\u636e\u7279\u6027\uff0c\u589e\u52a0\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u96be\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u6570\u636e\u5206\u5e03\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u6570\u636e\u767d\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u76f8\u5173\u548c\u65b9\u5dee\u5747\u8861\u5316\u4f7f\u6570\u636e\u66f4\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u53cc\u7ebf\u6027\u63d0\u793a\u8c03\u4f18\uff08BPT\uff09\uff0c\u901a\u8fc7\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "result": "BPT \u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5404\u79cd VPT \u65b9\u6cd5\uff0c\u4f8b\u5982\u5728 CUB \u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 25 \u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "BPT \u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5206\u5e03\u548c\u5f15\u5165\u4f4e\u79e9\u5206\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u89c6\u89c9 Transformer \u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23266", "abs": "https://arxiv.org/abs/2506.23266", "authors": ["Lujun Li", "Zhu Qiyuan", "Jiacheng Wang", "Wei Li", "Hao Gu", "Sirui Han", "Yike Guo"], "title": "Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging", "comment": "Work in progress, revisions ongoing", "summary": "Mixture of Experts (MoE) LLMs face significant obstacles due to their massive\nparameter scale, which imposes memory, storage, and deployment challenges.\nAlthough recent expert merging methods promise greater efficiency by\nconsolidating multiple experts, they are fundamentally hindered by parameter\nconflicts arising from expert specialization. In this paper, we present\nSub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key\ninsight is to perform joint Singular Value Decomposition (SVD) on concatenated\nexpert weights, reducing conflicting parameters by extracting shared\n$U$-matrices while enabling effective merging of the expert-specific $V$\ncomponents. Specifically, Sub-MoE consists of two innovative phases: (1)\nAdaptive Expert Clustering, which groups functionally coherent experts via\nK-means clustering based on cosine similarity of expert outputs; and (2)\nSubspace Expert Merging, which first enforces Experts Union Decomposition to\nderive the shared $U$-matrix across experts in the same group, then pursues\nfrequency-based merging for individual $V$-matrices, and finalizes expert\nreconstruction using the merged $V$-matrix. In this way, we align and fuse\nexperts in a shared subspace, and can be extended with intra-expert compression\nfor further inference optimization. Extensive experiments on Mixtral, DeepSeek,\nand Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms\nexisting expert pruning and merging methods. Notably, our Sub-MoE maintains\n96\\%|86\\% of original performance with 25\\%|50\\% expert reduction on\nMixtral-8x7B in zero-shot benchmarks. Code will be released at\nhttps://github.com/lliai/MoERazor.", "AI": {"tldr": "Sub-MoE\u662f\u4e00\u79cd\u901a\u8fc7\u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\u538b\u7f29MoE\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "MoE\u6a21\u578b\u56e0\u53c2\u6570\u89c4\u6a21\u5e9e\u5927\u9762\u4e34\u5185\u5b58\u3001\u5b58\u50a8\u548c\u90e8\u7f72\u6311\u6218\uff0c\u73b0\u6709\u4e13\u5bb6\u5408\u5e76\u65b9\u6cd5\u56e0\u53c2\u6570\u51b2\u7a81\u53d7\u9650\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u4e13\u5bb6\u805a\u7c7b\u548c\u5b50\u7a7a\u95f4\u4e13\u5bb6\u5408\u5e76\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7SVD\u63d0\u53d6\u5171\u4eabU\u77e9\u9635\u5e76\u5408\u5e76V\u77e9\u9635\u3002", "result": "\u5728Mixtral\u7b49\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e13\u5bb6\u51cf\u5c1125%|50%\u65f6\u6027\u80fd\u4fdd\u630196%|86%\u3002", "conclusion": "Sub-MoE\u5728\u538b\u7f29MoE\u6a21\u578b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u3002"}}
{"id": "2506.22930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22930", "abs": "https://arxiv.org/abs/2506.22930", "authors": ["Yiwei He", "Xiangtai Li", "Zhenglin Huang", "Yi Dong", "Hao Fei", "Jiangning Zhang", "Baoyuan Wu", "Guangliang Cheng"], "title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization", "comment": null, "summary": "The increasing realism of multimodal content has made misinformation more\nsubtle and harder to detect, especially in news media where images are\nfrequently paired with bilingual (e.g., Chinese-English) subtitles. Such\ncontent often includes localized image edits and cross-lingual inconsistencies\nthat jointly distort meaning while remaining superficially plausible. We\nintroduce BiMi, a bilingual multimodal framework that jointly performs\nregion-level localization, cross-modal and cross-lingual consistency detection,\nand natural language explanation for misinformation analysis. To support\ngeneralization, BiMi integrates an online retrieval module that supplements\nmodel reasoning with up-to-date external context. We further release BiMiBench,\na large-scale and comprehensive benchmark constructed by systematically editing\nreal news images and subtitles, comprising 104,000 samples with realistic\nmanipulations across visual and linguistic modalities. To enhance\ninterpretability, we apply Group Relative Policy Optimization (GRPO) to improve\nexplanation quality, marking the first use of GRPO in this domain. Extensive\nexperiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in\nclassification accuracy, +15.9 in localization accuracy, and +2.5 in\nexplanation BERTScore, advancing state-of-the-art performance in realistic,\nmultilingual misinformation detection. Code, models, and datasets will be\nreleased.", "AI": {"tldr": "BiMi\u662f\u4e00\u4e2a\u53cc\u8bed\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u901a\u8fc7\u533a\u57df\u7ea7\u5b9a\u4f4d\u3001\u8de8\u6a21\u6001\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u68c0\u6d4b\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u589e\u5f3a\uff0c\u865a\u5047\u4fe1\u606f\u53d8\u5f97\u66f4\u52a0\u9690\u853d\u4e14\u96be\u4ee5\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5728\u53cc\u8bed\u5b57\u5e55\u65b0\u95fb\u4e2d\uff0c\u5c40\u90e8\u56fe\u50cf\u7f16\u8f91\u548c\u8de8\u8bed\u8a00\u4e0d\u4e00\u81f4\u6027\u5171\u540c\u626d\u66f2\u4e86\u4fe1\u606f\u3002", "method": "BiMi\u7ed3\u5408\u4e86\u533a\u57df\u7ea7\u5b9a\u4f4d\u3001\u8de8\u6a21\u6001\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u68c0\u6d4b\u3001\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5e76\u96c6\u6210\u4e86\u5728\u7ebf\u68c0\u7d22\u6a21\u5757\u4ee5\u8865\u5145\u5916\u90e8\u4e0a\u4e0b\u6587\u3002\u6b64\u5916\uff0c\u5e94\u7528GRPO\u63d0\u5347\u89e3\u91ca\u8d28\u91cf\u3002", "result": "BiMi\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u63d0\u5347\u4e868.9\uff0c\u5b9a\u4f4d\u51c6\u786e\u6027\u63d0\u5347\u4e8615.9\uff0c\u89e3\u91caBERTScore\u63d0\u5347\u4e862.5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "BiMi\u5728\u73b0\u5b9e\u591a\u8bed\u8a00\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86BiMiBench\u57fa\u51c6\u6d4b\u8bd5\u548c\u5de5\u5177\u3002"}}
{"id": "2506.23274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23274", "abs": "https://arxiv.org/abs/2506.23274", "authors": ["Hans Peter Lynsg\u00f8e Raaschou-jensen", "Constanza Fierro", "Anders S\u00f8gaard"], "title": "Predicting thinking time in Reasoning models", "comment": null, "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u601d\u8003\u65f6\u95f4\u4e0d\u53ef\u9884\u6d4b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u9884\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u201c\u8fdb\u5ea6\u6761\u201d\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u601d\u8003\u65f6\u95f4\u4e0d\u53ef\u9884\u6d4b\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u632b\u8d25\u611f\u3002", "method": "\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u9884\u6d4b\u6a21\u578b\u601d\u8003\u65f6\u95f4\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e3a\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u201c\u8fdb\u5ea6\u6761\u201d\u7684\u5b9e\u7528\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u5bf9\u7528\u6237\u4ea4\u4e92\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.22939", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22939", "abs": "https://arxiv.org/abs/2506.22939", "authors": ["Ghufran A. Omran", "Wassan Saad Abduljabbar Hayale", "Ahmad AbdulQadir AlRababah", "Israa Ibraheem Al-Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar", "Harshavardhan Reddy Penubadi"], "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data", "comment": null, "summary": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCO-BRNN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u6570\u636e\u7684\u573a\u666f\u5206\u7c7b\uff0c\u5176\u51c6\u786e\u7387\u9ad8\u8fbe97%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u5728\u591a\u4e2a\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u96be\u4ee5\u5904\u7406\u566a\u58f0\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN)\uff0c\u5e76\u4e0e\u591a\u79cd\u73b0\u6709\u6280\u672f\uff08\u5982MLP-CNN\u3001CNN-LSTM\u7b49\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "CO-BRNN\u8fbe\u523097%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff08\u5982LSTM-CRF\u768490%\u3001MLP-CNN\u768485%\u7b49\uff09\u3002", "conclusion": "CO-BRNN\u5728\u9065\u611f\u573a\u666f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u7269\u7406\u9a8c\u8bc1\u5bf9\u536b\u661f\u6570\u636e\u6548\u7387\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.23280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23280", "abs": "https://arxiv.org/abs/2506.23280", "authors": ["Chaoqun Du", "Yulin Wang", "Shiji Song", "Gao Huang"], "title": "BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition", "comment": null, "summary": "Bayesian decision theory advocates the Bayes classifier as the optimal\napproach for minimizing the risk in machine learning problems. Current deep\nlearning algorithms usually solve for the optimal classifier by\n\\emph{implicitly} estimating the posterior probabilities, \\emph{e.g.}, by\nminimizing the Softmax cross-entropy loss. This simple methodology has been\nproven effective for meticulously balanced academic benchmark datasets.\nHowever, it is not applicable to the long-tailed data distributions in the real\nworld, where it leads to the gradient imbalance issue and fails to ensure the\nBayes optimal decision rule. To address these challenges, this paper presents a\nnovel approach (BAPE) that provides a more precise theoretical estimation of\nthe data distributions by \\emph{explicitly} modeling the parameters of the\nposterior probabilities and solving them with point estimation. Consequently,\nour method directly learns the Bayes classifier without gradient descent based\non Bayes' theorem, simultaneously alleviating the gradient imbalance and\nensuring the Bayes optimal decision rule. Furthermore, we propose a\nstraightforward yet effective \\emph{distribution adjustment} technique. This\nmethod enables the Bayes classifier trained from the long-tailed training set\nto effectively adapt to the test data distribution with an arbitrary imbalance\nfactor, thereby enhancing performance without incurring additional\ncomputational costs. In addition, we demonstrate the gains of our method are\northogonal to existing learning approaches for long-tailed scenarios, as they\nare mostly designed under the principle of \\emph{implicitly} estimating the\nposterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method\nsignificantly improves the generalization performance of popular deep networks,\ndespite its simplicity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08BAPE\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u540e\u9a8c\u6982\u7387\u53c2\u6570\u5e76\u76f4\u63a5\u5b66\u4e60\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\uff0c\u89e3\u51b3\u4e86\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u7684\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u4e86\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u89c4\u5219\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u901a\u8fc7\u9690\u5f0f\u4f30\u8ba1\u540e\u9a8c\u6982\u7387\uff08\u5982\u6700\u5c0f\u5316Softmax\u4ea4\u53c9\u71b5\u635f\u5931\uff09\u6765\u6c42\u89e3\u6700\u4f18\u5206\u7c7b\u5668\uff0c\u4f46\u5728\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u4f1a\u5bfc\u81f4\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u6cd5\u786e\u4fdd\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u89c4\u5219\u3002", "method": "BAPE\u65b9\u6cd5\u663e\u5f0f\u5efa\u6a21\u540e\u9a8c\u6982\u7387\u53c2\u6570\uff0c\u901a\u8fc7\u70b9\u4f30\u8ba1\u76f4\u63a5\u5b66\u4e60\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\uff0c\u65e0\u9700\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u8c03\u6574\u6280\u672f\u4ee5\u9002\u5e94\u4efb\u610f\u4e0d\u5e73\u8861\u7684\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728CIFAR-10-LT\u3001CIFAR-100-LT\u3001ImageNet-LT\u548ciNaturalist\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "BAPE\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u540e\u9a8c\u6982\u7387\u53c2\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u7684\u95ee\u9898\uff0c\u4e14\u5176\u589e\u76ca\u4e0e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u6b63\u4ea4\u3002"}}
{"id": "2506.22955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22955", "abs": "https://arxiv.org/abs/2506.22955", "authors": ["Haniyeh Nikkhah", "Jafar Tanha", "Mahdi Zarrin", "SeyedEhsan Roshan", "Amin Kazempour"], "title": "YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging", "comment": "Accepted at The 7th International conference on Pattern Recognition\n  and Image Analysis (IPRIA 2025)", "summary": "Medical image segmentation poses significant challenges due to class\nimbalance and the complex structure of medical images. To address these\nchallenges, this study proposes YM-WML, a novel model for cardiac image\nsegmentation. The model integrates a robust backbone for effective feature\nextraction, a YOLOv11 neck for multi-scale feature aggregation, and an\nattention-based segmentation head for precise and accurate segmentation. To\naddress class imbalance, we introduce the Weighted Multi-class Exponential\n(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity\nCoefficient of 91.02, outperforming state-of-the-art methods. The model\ndemonstrates stable training, accurate segmentation, and strong generalization,\nsetting a new benchmark in cardiac segmentation tasks.", "AI": {"tldr": "\u63d0\u51faYM-WML\u6a21\u578b\u7528\u4e8e\u5fc3\u810f\u56fe\u50cf\u5206\u5272\uff0c\u7ed3\u5408YOLOv11\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\uff0c\u5f15\u5165WME\u635f\u5931\u51fd\u6570\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728ACDC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7ed3\u6784\u590d\u6742\u7684\u6311\u6218\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u6a21\u578b\u3002", "method": "YM-WML\u6a21\u578b\u7ed3\u5408\u4e86\u5f3a\u5065\u7684\u9aa8\u5e72\u7f51\u7edc\u3001YOLOv11\u9888\u90e8\u6a21\u5757\u548c\u6ce8\u610f\u529b\u5206\u5272\u5934\uff0c\u5e76\u5f15\u5165WME\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728ACDC\u6570\u636e\u96c6\u4e0aDice\u7cfb\u6570\u8fbe91.02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "YM-WML\u5728\u5fc3\u810f\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3001\u51c6\u786e\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u4e3a\u65b0\u6807\u6746\u3002"}}
{"id": "2506.23286", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23286", "abs": "https://arxiv.org/abs/2506.23286", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "comment": "Accepted at ICML 2025 for oral presentation", "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u53cd\u76f4\u89c9\u73b0\u8c61\uff08\u5982\u53cc\u4e0b\u964d\u3001grokking\u548c\u5f69\u7968\u5047\u8bbe\uff09\u7684\u7814\u7a76\u5e94\u907f\u514d\u5b64\u7acb\u89e3\u91ca\uff0c\u800c\u5e94\u4f5c\u4e3a\u9a8c\u8bc1\u901a\u7528\u7406\u8bba\u7684\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4e2d\u53cd\u76f4\u89c9\u73b0\u8c61\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u907f\u514d\u4f4e\u6548\u7684\u5b64\u7acb\u7814\u7a76\u3002", "method": "\u5206\u6790\u591a\u4e2a\u53cd\u76f4\u89c9\u73b0\u8c61\u7684\u7814\u7a76\u6210\u679c\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "result": "\u8ba4\u4e3a\u8fd9\u4e9b\u73b0\u8c61\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7f3a\u4e4f\u8bc1\u636e\uff0c\u4f46\u53ef\u4f5c\u4e3a\u9a8c\u8bc1\u901a\u7528\u7406\u8bba\u7684\u72ec\u7279\u573a\u666f\u3002", "conclusion": "\u5efa\u8bae\u7814\u7a76\u793e\u533a\u8c03\u6574\u65b9\u5411\uff0c\u786e\u4fdd\u6df1\u5ea6\u5b66\u4e60\u73b0\u8c61\u7814\u7a76\u4e0e\u9886\u57df\u6574\u4f53\u76ee\u6807\u4e00\u81f4\u3002"}}
{"id": "2506.22960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22960", "abs": "https://arxiv.org/abs/2506.22960", "authors": ["Shreyas Dixit", "Ashhar Aziz", "Shashwat Bajpai", "Vasu Sharma", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images", "comment": null, "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.", "AI": {"tldr": "\u6b27\u76df\u9884\u6d4b\u52302026\u5e7490%\u7684\u5728\u7ebf\u5185\u5bb9\u53ef\u80fd\u662fAI\u751f\u6210\u7684\uff0c\u5f15\u53d1\u5bf9\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u7684\u62c5\u5fe7\u3002\u52a0\u5ddeAB 3211\u6cd5\u6848\u8981\u6c42\u5bf9AI\u751f\u6210\u5185\u5bb9\u52a0\u6c34\u5370\uff0c\u4f46\u73b0\u6709\u6280\u672f\u6613\u88ab\u7be1\u6539\u3002\u672c\u6587\u63d0\u51faPECCAVI\uff0c\u4e00\u79cd\u6297\u89c6\u89c9\u8f6c\u8ff0\u653b\u51fb\u7684\u65e0\u5931\u771f\u6c34\u5370\u6280\u672f\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u5185\u5bb9\u7684\u6c34\u5370\u6613\u88ab\u7be1\u6539\u548c\u7ed5\u8fc7\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u89c9\u8f6c\u8ff0\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "method": "PECCAVI\u901a\u8fc7\u5728\u591a\u901a\u9053\u9891\u57df\u5d4c\u5165\u6c34\u5370\uff0c\u5e76\u5229\u7528\u566a\u58f0\u629b\u5149\u6280\u672f\u4fdd\u62a4\u975e\u7194\u5316\u70b9\uff08NMPs\uff09\uff0c\u9632\u6b62\u6c34\u5370\u88ab\u79fb\u9664\u3002", "result": "PECCAVI\u80fd\u6709\u6548\u62b5\u6297\u89c6\u89c9\u8f6c\u8ff0\u653b\u51fb\uff0c\u4fdd\u6301\u56fe\u50cf\u65e0\u5931\u771f\uff0c\u4e14\u6a21\u578b\u65e0\u5173\u3002", "conclusion": "PECCAVI\u4e3aAI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u8010\u7528\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.23287", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23287", "abs": "https://arxiv.org/abs/2506.23287", "authors": ["Zelin Zang", "WenZhe Li", "Fei Chen", "Yongjie Xu", "Chang Yu", "Zhen Lei", "Stan Z. Li"], "title": "Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis", "comment": "9 pages, 6 figures, under review", "summary": "In single-cell research, tracing and analyzing high-throughput single-cell\ndifferentiation trajectories is crucial for understanding complex biological\nprocesses. Key to this is the modeling and generation of hierarchical data that\nrepresents the intrinsic structure within datasets. Traditional methods face\nlimitations in terms of computational cost, performance, generative capacity,\nand stability. Recent VAEs based approaches have made strides in addressing\nthese challenges but still require specialized network modules for each tree\nbranch, limiting their stability and ability to capture deep hierarchical\nrelationships. To overcome these challenges, we introduce diffusion-based\napproach called HDTree. HDTree captures tree relationships within a\nhierarchical latent space using a unified hierarchical codebook and quantized\ndiffusion processes to model tree node transitions. This method improves\nstability by eliminating branch-specific modules and enhancing generative\ncapacity through gradual hierarchical changes simulated by the diffusion\nprocess. HDTree's effectiveness is demonstrated through comparisons on both\ngeneral-purpose and single-cell datasets, where it outperforms existing methods\nin terms of accuracy and performance. These contributions provide a new tool\nfor hierarchical lineage analysis, enabling more accurate and efficient\nmodeling of cellular differentiation paths and offering insights for downstream\nbiological tasks. The code of HDTree is available at anonymous link\nhttps://anonymous.4open.science/r/code_HDTree_review-A8DB.", "AI": {"tldr": "HDTree\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5c42\u6b21\u7801\u672c\u548c\u91cf\u5316\u6269\u6563\u8fc7\u7a0b\u6539\u8fdb\u751f\u6210\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u6027\u80fd\u548c\u751f\u6210\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u73b0\u6709VAE\u65b9\u6cd5\u9700\u8981\u5206\u652f\u7279\u5b9a\u6a21\u5757\uff0c\u9650\u5236\u4e86\u7a33\u5b9a\u6027\u548c\u6df1\u5ea6\u5c42\u6b21\u5173\u7cfb\u7684\u6355\u6349\u3002", "method": "HDTree\u5229\u7528\u5c42\u6b21\u6f5c\u5728\u7a7a\u95f4\u548c\u91cf\u5316\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u6811\u8282\u70b9\u8f6c\u6362\uff0c\u6d88\u9664\u5206\u652f\u7279\u5b9a\u6a21\u5757\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u6a21\u62df\u6e10\u8fdb\u5c42\u6b21\u53d8\u5316\u3002", "result": "\u5728\u901a\u7528\u548c\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\uff0cHDTree\u5728\u51c6\u786e\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HDTree\u4e3a\u5c42\u6b21\u8c31\u7cfb\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u80fd\u66f4\u51c6\u786e\u9ad8\u6548\u5730\u5efa\u6a21\u7ec6\u80de\u5206\u5316\u8def\u5f84\uff0c\u652f\u6301\u4e0b\u6e38\u751f\u7269\u4efb\u52a1\u3002"}}
{"id": "2506.22967", "categories": ["cs.CV", "cs.LG", "cs.MM", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22967", "abs": "https://arxiv.org/abs/2506.22967", "authors": ["Amir Aghdam", "Vincent Tao Hu"], "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment", "comment": "Preprint manuscript - Project page:\n  https://github.com/aghdamamir/act-align", "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.", "AI": {"tldr": "ActAlign\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u89c6\u9891\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5bf9\u9f50\u548c\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5339\u914d\u89c6\u9891\u5e27\u4e0e\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5b50\u52a8\u4f5c\u5e8f\u5217\uff0c\u65e0\u9700\u89c6\u9891\u6587\u672c\u76d1\u7763\u6216\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u5e8f\u5b50\u52a8\u4f5c\u5e8f\u5217\uff0c\u901a\u8fc7DTW\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u89c6\u9891\u5e27\u4e0e\u5b50\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728ActionAtlas\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523030.5%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5341\u4ebf\u53c2\u6570\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u91cf\u51cf\u5c118\u500d\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u8bed\u8a00\u5148\u9a8c\u548c\u7ecf\u5178\u5bf9\u9f50\u6280\u672f\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2506.23339", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23339", "abs": "https://arxiv.org/abs/2506.23339", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "AI": {"tldr": "VALID-Mol\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u5316\u5b66\u9a8c\u8bc1\u548c\u5fae\u8c03LLM\uff0c\u5c06\u751f\u6210\u6709\u6548\u5316\u5b66\u7ed3\u6784\u7684\u6bd4\u4f8b\u4ece3%\u63d0\u5347\u81f383%\uff0c\u4e3a\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5206\u5b50\u8bbe\u8ba1\u4e2d\u751f\u6210\u5316\u5b66\u65e0\u6548\u6216\u4e0d\u5b9e\u7528\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5176\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u81ea\u52a8\u5316\u5b66\u9a8c\u8bc1\u548c\u5fae\u8c03\u9886\u57df\u9002\u5e94\u7684LLM\uff0c\u786e\u4fdd\u751f\u6210\u53ef\u5408\u6210\u4e14\u6027\u80fd\u4f18\u5316\u7684\u5206\u5b50\u3002", "result": "\u751f\u6210\u6709\u6548\u5316\u5b66\u7ed3\u6784\u7684\u6bd4\u4f8b\u663e\u8457\u63d0\u5347\uff083%\u523083%\uff09\uff0c\u5e76\u9884\u6d4b\u76ee\u6807\u4eb2\u548c\u529b\u63d0\u5347\u9ad8\u8fbe17\u500d\u3002", "conclusion": "VALID-Mol\u4e3a\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a8\u5e7f\u7684LLM\u5e94\u7528\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.22979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22979", "abs": "https://arxiv.org/abs/2506.22979", "authors": ["Jie Liu", "Jiayi Shen", "Pan Zhou", "Jan-Jakob Sonke", "Efstratios Gavves"], "title": "Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation", "comment": "ICCV2025 Proceeding", "summary": "Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a\nsegmentation model to novel classes with only a few annotated examples while\nmaintaining performance on base classes. Recently, pretrained vision-language\nmodels (VLMs) such as CLIP have been leveraged in GFSS to improve\ngeneralization on novel classes through multi-modal prototypes learning.\nHowever, existing prototype-based methods are inherently deterministic,\nlimiting the adaptability of learned prototypes to diverse samples,\nparticularly for novel classes with scarce annotations. To address this, we\npropose FewCLIP, a probabilistic prototype calibration framework over\nmulti-modal prototypes from the pretrained CLIP, thus providing more adaptive\nprototype learning for GFSS. Specifically, FewCLIP first introduces a prototype\ncalibration mechanism, which refines frozen textual prototypes with learnable\nvisual calibration prototypes, leading to a more discriminative and adaptive\nrepresentation. Furthermore, unlike deterministic prototype learning\ntechniques, FewCLIP introduces distribution regularization over these\ncalibration prototypes. This probabilistic formulation ensures structured and\nuncertainty-aware prototype learning, effectively mitigating overfitting to\nlimited novel class data while enhancing generalization. Extensive experimental\nresults on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed\nFewCLIP significantly outperforms state-of-the-art approaches across both GFSS\nand class-incremental setting. The code is available at\nhttps://github.com/jliu4ai/FewCLIP.", "AI": {"tldr": "FewCLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u539f\u578b\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u539f\u578b\u5b66\u4e60\u6539\u8fdb\u5e7f\u4e49\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\uff08GFSS\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7684\u65b9\u6cd5\u5177\u6709\u786e\u5b9a\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u6837\u672c\uff0c\u5c24\u5176\u662f\u6807\u6ce8\u7a00\u7f3a\u7684\u65b0\u7c7b\u522b\u3002", "method": "FewCLIP\u5f15\u5165\u539f\u578b\u6821\u51c6\u673a\u5236\u548c\u5206\u5e03\u6b63\u5219\u5316\uff0c\u4f18\u5316\u591a\u6a21\u6001\u539f\u578b\u5b66\u4e60\u3002", "result": "\u5728PASCAL-5$^i$\u548cCOCO-20$^i$\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FewCLIP\u901a\u8fc7\u6982\u7387\u539f\u578b\u6821\u51c6\u63d0\u5347\u4e86GFSS\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23349", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23349", "abs": "https://arxiv.org/abs/2506.23349", "authors": ["Keziah Naggita", "Julienne LaChance"], "title": "A case for data valuation transparency via DValCards", "comment": null, "summary": "Following the rise in popularity of data-centric machine learning (ML),\nvarious data valuation methods have been proposed to quantify the contribution\nof each datapoint to desired ML model performance metrics (e.g., accuracy).\nBeyond the technical applications of data valuation methods (e.g., data\ncleaning, data acquisition, etc.), it has been suggested that within the\ncontext of data markets, data buyers might utilize such methods to fairly\ncompensate data owners. Here we demonstrate that data valuation metrics are\ninherently biased and unstable under simple algorithmic design choices,\nresulting in both technical and ethical implications. By analyzing 9 tabular\nclassification datasets and 6 data valuation methods, we illustrate how (1)\ncommon and inexpensive data pre-processing techniques can drastically alter\nestimated data values; (2) subsampling via data valuation metrics may increase\nclass imbalance; and (3) data valuation metrics may undervalue underrepresented\ngroup data. Consequently, we argue in favor of increased transparency\nassociated with data valuation in-the-wild and introduce the novel Data\nValuation Cards (DValCards) framework towards this aim. The proliferation of\nDValCards will reduce misuse of data valuation metrics, including in data\npricing, and build trust in responsible ML systems.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u900f\u660e\u5316\u6846\u67b6DValCards\u4ee5\u51cf\u5c11\u8bef\u7528\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4e3a\u4e2d\u5fc3\u673a\u5668\u5b66\u4e60\u7684\u5174\u8d77\uff0c\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u88ab\u7528\u4e8e\u91cf\u5316\u6570\u636e\u70b9\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\uff0c\u4f46\u5176\u5728\u6570\u636e\u5e02\u573a\u4e2d\u7684\u516c\u5e73\u8865\u507f\u5e94\u7528\u5b58\u5728\u6280\u672f\u53ca\u4f26\u7406\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u67909\u4e2a\u8868\u683c\u5206\u7c7b\u6570\u636e\u96c6\u548c6\u79cd\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u7814\u7a76\u6570\u636e\u9884\u5904\u7406\u3001\u5b50\u91c7\u6837\u548c\u5c11\u6570\u7fa4\u4f53\u6570\u636e\u4f30\u503c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u6613\u53d7\u9884\u5904\u7406\u6280\u672f\u5f71\u54cd\uff0c\u53ef\u80fd\u52a0\u5267\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u4f4e\u4f30\u5c11\u6570\u7fa4\u4f53\u6570\u636e\u7684\u4ef7\u503c\u3002", "conclusion": "\u63d0\u51faDValCards\u6846\u67b6\u4ee5\u63d0\u9ad8\u6570\u636e\u4f30\u503c\u7684\u900f\u660e\u5ea6\uff0c\u51cf\u5c11\u8bef\u7528\u5e76\u589e\u5f3a\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2506.22982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22982", "abs": "https://arxiv.org/abs/2506.22982", "authors": ["Atharv Mittal", "Agam Pandey", "Amritanshu Tiwari", "Sukrit Jindal", "Swadesh Swain"], "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models", "comment": "Accepted to MLRC 2025", "summary": "Large Vision-Language Models (VLMs) have revolutionized computer vision,\nenabling tasks such as image classification, captioning, and visual question\nanswering. However, they remain highly vulnerable to adversarial attacks,\nparticularly in scenarios where both visual and textual modalities can be\nmanipulated. In this study, we conduct a comprehensive reproducibility study of\n\"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on\nVision-Language Models\" validating the Cross-Prompt Attack (CroPA) and\nconfirming its superior cross-prompt transferability compared to existing\nbaselines. Beyond replication we propose several key improvements: (1) A novel\ninitialization strategy that significantly improves Attack Success Rate (ASR).\n(2) Investigate cross-image transferability by learning universal\nperturbations. (3) A novel loss function targeting vision encoder attention\nmechanisms to improve generalization. Our evaluation across prominent VLMs --\nincluding Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on\nLLaVA validates the original results and demonstrates that our improvements\nconsistently boost adversarial effectiveness. Our work reinforces the\nimportance of studying adversarial vulnerabilities in VLMs and provides a more\nrobust framework for generating transferable adversarial examples, with\nsignificant implications for understanding the security of VLMs in real-world\napplications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u9a8c\u8bc1\u4e86CroPA\u65b9\u6cd5\u7684\u8de8\u63d0\u793a\u8fc1\u79fb\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7b56\u7565\u4ee5\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u5747\u53ef\u88ab\u64cd\u7eb5\u7684\u573a\u666f\u4e2d\u3002", "method": "\u7814\u7a76\u9a8c\u8bc1\u4e86CroPA\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e09\u9879\u6539\u8fdb\uff1a\u65b0\u9896\u7684\u521d\u59cb\u5316\u7b56\u7565\u3001\u8de8\u56fe\u50cf\u8fc1\u79fb\u6027\u7814\u7a76\u3001\u9488\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u6ce8\u610f\u529b\u673a\u5236\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u5728\u591a\u4e2aVLMs\uff08\u5982Flamingo\u3001BLIP-2\u7b49\uff09\u4e0a\u9a8c\u8bc1\u4e86\u539f\u59cb\u7ed3\u679c\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7814\u7a76VLMs\u5bf9\u6297\u6f0f\u6d1e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u751f\u6210\u53ef\u8fc1\u79fb\u5bf9\u6297\u6837\u672c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2506.23358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23358", "abs": "https://arxiv.org/abs/2506.23358", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "comment": "conference paper", "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "AI": {"tldr": "FTS\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff08\u5982\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff09\u8bad\u7ec3\u751f\u6210\u57fa\u7840\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u6027\u548c\u591a\u4efb\u52a1\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u533b\u7597\u6570\u636e\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u9690\u79c1\u548c\u534f\u4f5c\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u591a\u6837\u5316\u7684\u533b\u7597\u9884\u6d4b\u548c\u6a21\u62df\u4efb\u52a1\u3002", "method": "\u5c06\u60a3\u8005\u5386\u53f2\u8868\u793a\u4e3a\u8bed\u8a00\u65e0\u5173\u7684PHT\u5e8f\u5217\uff0c\u672c\u5730\u8bad\u7ec3\u81ea\u56de\u5f52\u53d8\u538b\u5668\uff0c\u4ec5\u5171\u4eab\u6a21\u578b\u6743\u91cd\uff0c\u670d\u52a1\u5668\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5168\u5c40\u751f\u6210\u5668\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5f53\u3002", "conclusion": "FTS\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u6027\u548c\u591a\u4efb\u52a1\u5e94\u7528\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u533b\u7597\u9886\u57df\u3002"}}
{"id": "2506.23004", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23004", "abs": "https://arxiv.org/abs/2506.23004", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "comment": null, "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9c81\u68d2\u7684CNN\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c4f\u5e55\u5230\u76f8\u673a\uff08S2C\uff09\u53ef\u89c1\u5149\u901a\u4fe1\uff08VLC\uff09\u7cfb\u7edf\u4e2d\u7684\u5e27\u8bc6\u522b\u4e0e\u540c\u6b65\uff0c\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7387\u8fbe98.74%\u3002", "motivation": "\u63d0\u5347\u77ed\u8ddd\u79bb\u901a\u4fe1\u6027\u80fd\uff0c\u89e3\u51b3S2C\u901a\u4fe1\u4e2d\u7684\u5b9e\u65f6\u6311\u6218\uff08\u5982\u6a21\u7cca\u3001\u88c1\u526a\u548c\u65cb\u8f6c\u56fe\u50cf\uff09\u3002", "method": "\u4f7f\u7528Python\u548cTensorFlow Keras\u6846\u67b6\u5f00\u53d1CNN\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u5b9e\u65f6\u5b9e\u9a8c\u8bad\u7ec3\uff0c\u6570\u636e\u96c6\u9488\u5bf9S2C\u901a\u4fe1\u95ee\u9898\u8bbe\u8ba1\u3002", "result": "\u6a21\u578b\u5728\u5e27\u8bc6\u522b\u548c\u540c\u6b65\u4e2d\u8fbe\u523098.74%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684CNN\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86S2C VLC\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23374", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23374", "abs": "https://arxiv.org/abs/2506.23374", "authors": ["Dominik Meier", "Sujai Hiremath", "Promit Ghosal", "Kyra Gan"], "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery", "comment": null, "summary": "Distinguishing cause and effect from bivariate observational data is a\nfoundational problem in many disciplines, but challenging without additional\nassumptions. Additive noise models (ANMs) are widely used to enable\nsample-efficient bivariate causal discovery. However, conventional ANM-based\nmethods fail when unobserved mediators corrupt the causal relationship between\nvariables. This paper makes three key contributions: first, we rigorously\ncharacterize why standard ANM approaches break down in the presence of\nunmeasured mediators. Second, we demonstrate that prior solutions for hidden\nmediation are brittle in finite sample settings, limiting their practical\nutility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)\nfor causal discovery, a method designed to handle latent noise introduced by\nunmeasured mediators. Unlike prior methods that infer directionality through\nmean squared error loss comparisons, our approach introduces a novel\nindependence test statistic: during the noising and denoising processes for\neach variable, we condition on the other variable as input and evaluate the\nindependence of the predicted noise relative to this input. We prove asymptotic\nconsistency of BiDD under the ANM, and conjecture that it performs well under\nhidden mediation. Experiments on synthetic and real-world data demonstrate\nconsistent performance, outperforming existing methods in mediator-corrupted\nsettings while maintaining strong performance in mediator-free settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5BiDD\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u52a0\u6027\u566a\u58f0\u6a21\u578b\uff08ANM\uff09\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u65f6\u7684\u5931\u6548\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edfANM\u65b9\u6cd5\u5728\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u5b58\u5728\u65f6\u5931\u6548\uff0c\u4e14\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u5c0f\u6837\u672c\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBiDD\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u5f15\u5165\u65b0\u7684\u72ec\u7acb\u6027\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u8bc4\u4f30\u566a\u58f0\u4e0e\u8f93\u5165\u53d8\u91cf\u7684\u72ec\u7acb\u6027\u3002", "result": "BiDD\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b58\u5728\u4e2d\u4ecb\u53d8\u91cf\u7684\u573a\u666f\u4e0b\u3002", "conclusion": "BiDD\u662f\u4e00\u79cd\u6709\u6548\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u672a\u89c2\u6d4b\u4e2d\u4ecb\u53d8\u91cf\u5b58\u5728\u7684\u60c5\u51b5\uff0c\u4e14\u5728\u5c0f\u6837\u672c\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.23009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23009", "abs": "https://arxiv.org/abs/2506.23009", "authors": ["Jian Chen", "Wenye Ma", "Penghang Liu", "Wei Wang", "Tengwei Song", "Ming Li", "Chenguang Wang", "Ruiyi Zhang", "Changyou Chen"], "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.", "AI": {"tldr": "MusiXQA\u6570\u636e\u96c6\u586b\u8865\u4e86MLLMs\u5728\u4e50\u8c31\u7406\u89e3\u9886\u57df\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u8bc4\u4f30\u548c\u6539\u8fdbMLLMs\u5728\u4e50\u8c31\u89e3\u6790\u4e0a\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86Phi-3-MusiX\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u4e50\u8c31\u7406\u89e3\u9886\u57df\u7684\u7814\u7a76\u4e0d\u8db3\uff0cMusiXQA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8MLLMs\u5728\u97f3\u4e50\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528MusiXTeX\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4e50\u8c31\uff0c\u5e76\u6807\u6ce8\u97f3\u7b26\u3001\u548c\u5f26\u3001\u8c31\u53f7\u7b49\u4fe1\u606f\uff0c\u6784\u5efaMusiXQA\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5fae\u8c03Phi-3-MusiX\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709MLLMs\u5728\u4e50\u8c31\u7406\u89e3\u4e0a\u8868\u73b0\u6709\u9650\uff0cPhi-3-MusiX\u663e\u8457\u4f18\u4e8eGPT\u7c7b\u65b9\u6cd5\u3002", "conclusion": "MusiXQA\u548cPhi-3-MusiX\u4e3aMLLMs\u5728\u4e50\u8c31\u7406\u89e3\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23408", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.23408", "abs": "https://arxiv.org/abs/2506.23408", "authors": ["Claudionor Coelho Jr", "Yanen Li", "Philip Tee"], "title": "Do LLMs Dream of Discrete Algorithms?", "comment": null, "summary": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u903b\u8f91\u63a8\u7406\u6a21\u5757\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u903b\u8f91\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "LLMs\u5728\u4e25\u683c\u903b\u8f91\u63a8\u7406\u548c\u79bb\u6563\u51b3\u7b56\u9886\u57df\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u589e\u5f3a\u5176\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u4e00\u9636\u903b\u8f91\u548c\u663e\u5f0f\u89c4\u5219\u7cfb\u7edf\uff0c\u7ed3\u5408Prolog\u8c13\u8bcd\u548c\u53ef\u7ec4\u5408\u5de5\u5177\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6df7\u5408\u67b6\u6784\u3002", "result": "\u5728DABStep\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3001\u8986\u76d6\u7387\u548c\u7cfb\u7edf\u6587\u6863\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408LLMs\u4e0e\u6a21\u5757\u5316\u903b\u8f91\u63a8\u7406\u53ef\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u4e3a\u590d\u6742\u9886\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u4fe1\u8d56\u7684AI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23030", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23030", "abs": "https://arxiv.org/abs/2506.23030", "authors": ["Alejandro Romero Amezcua", "Mariano Jos\u00e9 Juan Rivera Meraz"], "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "AI": {"tldr": "VisionScores\u662f\u9996\u4e2a\u7cfb\u7edf\u5206\u5272\u7684\u56fe\u50cf\u4e50\u8c31\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u53cc\u94a2\u7434\u66f2\u76ee\uff0c\u63d0\u4f9b\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u7684\u7ed3\u6784\u5316\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u673a\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002", "motivation": "\u65e8\u5728\u4e3a\u673a\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u7ed3\u6784\u4e30\u5bcc\u3001\u4fe1\u606f\u5bc6\u5ea6\u9ad8\u7684\u56fe\u50cf\u4e50\u8c31\u6570\u636e\uff0c\u540c\u65f6\u8003\u8651\u56fe\u5f62\u76f8\u4f3c\u6027\u548c\u4f5c\u66f2\u6a21\u5f0f\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b24.8k\u6837\u672c\uff0c\u5206\u4e3a\u4e24\u79cd\u573a\u666f\uff1a\u540c\u4e00\u4f5c\u66f2\u7c7b\u578b\u4e0d\u540c\u4f5c\u8005\uff0814k\u6837\u672c\uff09\u548c\u540c\u4e00\u4f5c\u8005\u4e0d\u540c\u4f5c\u66f2\u7c7b\u578b\uff0810.8k\u6837\u672c\uff09\u3002\u6837\u672c\u4e3a128\u00d7512\u50cf\u7d20\u7684\u7070\u5ea6\u56fe\u50cf\uff0c\u5e76\u63d0\u4f9b\u5143\u6570\u636e\u548c\u672a\u5206\u5272\u7684\u539f\u59cb\u4e50\u8c31\u3002", "result": "\u63d0\u4f9b\u4e8624.8k\u4e2a\u683c\u5f0f\u5316\u6837\u672c\uff0c\u4ee5\u53ca\u7cfb\u7edf\u987a\u5e8f\u3001\u5143\u6570\u636e\u548c\u539f\u59cb\u4e50\u8c31\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u5206\u6790\u3002", "conclusion": "VisionScores\u4e3a\u4e50\u8c31\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u673a\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u3002"}}
{"id": "2506.23419", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "pdf": "https://arxiv.org/pdf/2506.23419", "abs": "https://arxiv.org/abs/2506.23419", "authors": ["Amanda S Barnard"], "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aBenchMake\u7684\u65b0\u5de5\u5177\uff0c\u53ef\u5c06\u79d1\u5b66\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u901a\u8fc7\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u8bc6\u522b\u8fb9\u7f18\u6848\u4f8b\uff0c\u5e76\u751f\u6210\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u7684\u6d4b\u8bd5\u96c6\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u79d1\u5b66\u4e2d\u57fa\u51c6\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u65b0\u65b9\u6cd5\u7684\u8bc4\u4f30\u548c\u521b\u65b0\u3002", "method": "\u4f7f\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u8bc6\u522b\u8fb9\u7f18\u6848\u4f8b\uff0c\u5e76\u5728\u51f8\u5305\u4e0a\u5206\u533a\u6570\u636e\u96c6\uff0c\u751f\u6210\u6d4b\u8bd5\u96c6\u3002", "result": "\u572810\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86BenchMake\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u968f\u673a\u5206\u5272\u3002", "conclusion": "BenchMake\u4e3a\u79d1\u5b66\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u3002"}}
{"id": "2506.23038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23038", "abs": "https://arxiv.org/abs/2506.23038", "authors": ["Xinrong Hu", "Yiyu Shi"], "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation", "comment": null, "summary": "Collecting pixel-level labels for medical datasets can be a laborious and\nexpensive process, and enhancing segmentation performance with a scarcity of\nlabeled data is a crucial challenge. This work introduces AugPaint, a data\naugmentation framework that utilizes inpainting to generate image-label pairs\nfrom limited labeled data. AugPaint leverages latent diffusion models, known\nfor their ability to generate high-quality in-domain images with low overhead,\nand adapts the sampling process for the inpainting task without need for\nretraining. Specifically, given a pair of image and label mask, we crop the\narea labeled with the foreground and condition on it during reversed denoising\nprocess for every noise level. Masked background area would gradually be filled\nin, and all generated images are paired with the label mask. This approach\nensures the accuracy of match between synthetic images and label masks, setting\nit apart from existing dataset generation methods. The generated images serve\nas valuable supervision for training downstream segmentation models,\neffectively addressing the challenge of limited annotations. We conducted\nextensive evaluations of our data augmentation method on four public medical\nimage segmentation datasets, including CT, MRI, and skin imaging. Results\nacross all datasets demonstrate that AugPaint outperforms state-of-the-art\nlabel-efficient methodologies, significantly improving segmentation\nperformance.", "AI": {"tldr": "AugPaint\u662f\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf-\u6807\u7b7e\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u96c6\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5982\u4f55\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5206\u5272\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "AugPaint\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u4fee\u590d\uff0c\u751f\u6210\u4e0e\u6807\u7b7e\u63a9\u6a21\u5339\u914d\u7684\u5408\u6210\u56fe\u50cf\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cAugPaint\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "conclusion": "AugPaint\u4e3a\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23424", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23424", "abs": "https://arxiv.org/abs/2506.23424", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "AI": {"tldr": "PETSA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u4ec5\u66f4\u65b0\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5c0f\u578b\u6821\u51c6\u6a21\u5757\uff0c\u9002\u5e94\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u901a\u5e38\u5177\u6709\u975e\u5e73\u7a33\u6027\uff0c\u8fd9\u4f1a\u964d\u4f4e\u9884\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u66f4\u65b0\u6574\u4e2a\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "PETSA\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u548c\u52a8\u6001\u95e8\u63a7\u8c03\u6574\u8868\u793a\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u9879\u3001\u9891\u57df\u9879\u548c\u5757\u72b6\u7ed3\u6784\u9879\u7684\u7279\u6b8a\u635f\u5931\u51fd\u6570\uff0c\u4fdd\u6301\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPETSA\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53c2\u6570\u9700\u6c42\u5c11\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PETSA\u901a\u8fc7\u9ad8\u6548\u7684\u53c2\u6570\u66f4\u65b0\u548c\u4e13\u7528\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23042", "abs": "https://arxiv.org/abs/2506.23042", "authors": ["Hung Nguyen", "An Le", "Runfa Li", "Truong Nguyen"], "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting", "comment": "Accepted to ICCV Workshop", "summary": "3D Gaussian Splatting has emerged as a powerful approach in novel view\nsynthesis, delivering rapid training and rendering but at the cost of an\never-growing set of Gaussian primitives that strains memory and bandwidth. We\nintroduce AutoOpti3DGS, a training-time framework that automatically restrains\nGaussian proliferation without sacrificing visual fidelity. The key idea is to\nfeed the input images to a sequence of learnable Forward and Inverse Discrete\nWavelet Transforms, where low-pass filters are kept fixed, high-pass filters\nare learnable and initialized to zero, and an auxiliary orthogonality loss\ngradually activates fine frequencies. This wavelet-driven, coarse-to-fine\nprocess delays the formation of redundant fine Gaussians, allowing 3DGS to\ncapture global structure first and refine detail only when necessary. Through\nextensive experiments, AutoOpti3DGS requires just a single filter learning-rate\nhyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,\nand consistently produces sparser scene representations more compatible with\nmemory or storage-constrained hardware.", "AI": {"tldr": "AutoOpti3DGS\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u9650\u5236\u9ad8\u65af\u589e\u6b96\uff0c\u63d0\u53473D\u9ad8\u65af\u6e85\u5c04\u7684\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u4e2d\u9ad8\u65af\u57fa\u5143\u6570\u91cf\u589e\u957f\u5bfc\u81f4\u7684\u5185\u5b58\u548c\u5e26\u5bbd\u538b\u529b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff0c\u56fa\u5b9a\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u5b66\u4e60\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u5e76\u901a\u8fc7\u6b63\u4ea4\u6027\u635f\u5931\u9010\u6b65\u6fc0\u6d3b\u9ad8\u9891\u7ec6\u8282\u3002", "result": "AutoOpti3DGS\u751f\u6210\u66f4\u7a00\u758f\u7684\u573a\u666f\u8868\u793a\uff0c\u517c\u5bb9\u5185\u5b58\u53d7\u9650\u786c\u4ef6\uff0c\u4e14\u4ec5\u9700\u4e00\u4e2a\u8d85\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u89c6\u89c9\u8d28\u91cf\u4e0e\u5185\u5b58\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2506.23446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23446", "abs": "https://arxiv.org/abs/2506.23446", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders", "comment": null, "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u7684Transformer\u65b9\u6cd5\uff08UBS-Transformer\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u5185\u90e8\u5a01\u80c1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c06\u7528\u6237\u6d3b\u52a8\u89c6\u4e3a\u5b64\u7acb\u4e8b\u4ef6\uff0c\u5ffd\u7565\u4e86\u884c\u4e3a\u5e8f\u5217\u7684\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06CERT\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u4f7f\u7528Transformer Encoder\u5efa\u6a21\u6b63\u5e38\u884c\u4e3a\uff0c\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u4f5c\u4e3a\u5f02\u5e38\u5206\u6570\uff0c\u5e76\u7ed3\u5408\u4e09\u79cd\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff08OCSVM\u3001LOF\u3001iForest\uff09\u3002", "result": "\u5728\u56db\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a96.61%\u51c6\u786e\u7387\u300199.43%\u53ec\u56de\u7387\u300196.38% F1\u5206\u6570\u300195.00% AUROC\uff0c\u4e14\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u6781\u4f4e\u3002", "conclusion": "\u5e8f\u5217\u5efa\u6a21\u548c\u9ad8\u7ea7\u5f02\u5e38\u68c0\u6d4b\u5728\u5185\u90e8\u5a01\u80c1\u9886\u57df\u6548\u679c\u663e\u8457\uff0cUBS-Transformer\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.23044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23044", "abs": "https://arxiv.org/abs/2506.23044", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "title": "Ovis-U1 Technical Report", "comment": "A unified model for multimodal understanding, text-to-image\n  generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.", "AI": {"tldr": "Ovis-U1\u662f\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u7edf\u4e00\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u529f\u80fd\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\uff08\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\uff09\u7684\u6027\u80fd\uff0c\u7a81\u7834\u73b0\u6709\u6a21\u578b\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u89e3\u7801\u5668\u548c\u53cc\u5411\u4ee4\u724c\u7ec6\u5316\u5668\uff0c\u4ece\u8bed\u8a00\u6a21\u578b\u5f00\u59cb\u8fdb\u884c\u7edf\u4e00\u8bad\u7ec3\u3002", "result": "\u5728OpenCompass\u7b49\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8aRistretto-3B\u7b49\u6a21\u578b\u3002", "conclusion": "Ovis-U1\u4f5c\u4e3a\u7cfb\u5217\u9996\u7248\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23462", "abs": "https://arxiv.org/abs/2506.23462", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "AI": {"tldr": "DisasterNet-LLM\u662f\u4e00\u79cd\u4e13\u4e3a\u707e\u5bb3\u5206\u6790\u8bbe\u8ba1\u7684LLM\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u5148\u8fdb\u6280\u672f\uff0c\u5728\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u5929\u6c14\u8bb0\u5f55\u548c\u6587\u672c\u62a5\u544a\uff09\uff0c\u9650\u5236\u4e86\u707e\u5bb3\u7ba1\u7406\u7684\u65f6\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u9002\u5e94Transformer\u6280\u672f\uff0c\u6784\u5efaDisasterNet-LLM\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u51c6\u786e\u7387\u8fbe89.5%\uff0cF1\u5206\u657088.0%\uff0cAUC 0.92\uff0cBERTScore 0.88\u3002", "conclusion": "DisasterNet-LLM\u5728\u591a\u6a21\u6001\u707e\u5bb3\u5206\u6790\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u707e\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23061", "abs": "https://arxiv.org/abs/2506.23061", "authors": ["Jiazhen Liu", "Yuchuan Deng", "Long Chen"], "title": "Empowering Small VLMs to Think with Dynamic Memorization and Exploration", "comment": null, "summary": "Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking\ncapabilities remains fundamentally challenging due to their limited parameter\ncapacity and weak instruction-following abilities. Existing training paradigms,\nincluding Supervised Fine-Tuning (SFT) and Reinforcement Learning with\nVerifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding\nthe capabilities of SVLMs. Consequently, directly applying these paradigms to\nSVLMs often suffers from severe pseudo thinking traces and advantage collapse,\nultimately undermining both thinking reliability and task performance. A\nnatural solution is to combine SFT and RLVR, leveraging their complementarity\nto reduce the dependence on model capacity. However, the widely adopted\ntwo-stage training paradigm still performs poorly on SVLMs, as their tendency\ntoward sub-optimal convergence hinders the trade-off and limits the benefits of\nthe combination. To address this, we propose DyME, a novel training paradigm\nthat Dynamically selects between Memorization (via SFT) and Exploration (via\nRLVR) modes at each optimization step, ensuring that every update contributes\nto the trade-off. Extensive experiments across diverse domains demonstrate that\nDyME consistently achieves this balance, and thus delivers substantial\nperformance improvements. These results establish DyME as a practical and\neffective solution for empowering SVLMs with reliable thinking capabilities.\nGitHub: https://github.com/HKUST-LongGroup/DyME", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDyME\u8bad\u7ec3\u8303\u5f0f\uff0c\u52a8\u6001\u9009\u62e9\u8bb0\u5fc6\uff08SFT\uff09\u548c\u63a2\u7d22\uff08RLVR\uff09\u6a21\u5f0f\uff0c\u89e3\u51b3\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SVLMs\uff09\u7684\u4f2a\u601d\u8003\u75d5\u8ff9\u548c\u4f18\u52bf\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eSVLMs\u53c2\u6570\u5bb9\u91cf\u6709\u9650\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5f31\uff0c\u73b0\u6709\u8bad\u7ec3\u8303\u5f0f\uff08\u5982SFT\u548cRLVR\uff09\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\uff0c\u5bfc\u81f4\u4f2a\u601d\u8003\u75d5\u8ff9\u548c\u4f18\u52bf\u5d29\u6e83\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u548c\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u63d0\u51faDyME\u8bad\u7ec3\u8303\u5f0f\uff0c\u52a8\u6001\u9009\u62e9\u8bb0\u5fc6\uff08SFT\uff09\u548c\u63a2\u7d22\uff08RLVR\uff09\u6a21\u5f0f\uff0c\u786e\u4fdd\u6bcf\u6b21\u4f18\u5316\u66f4\u65b0\u90fd\u80fd\u5e73\u8861\u4e24\u8005\uff0c\u907f\u514d\u6b21\u4f18\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDyME\u80fd\u6709\u6548\u5e73\u8861\u8bb0\u5fc6\u4e0e\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347SVLMs\u7684\u53ef\u9760\u601d\u8003\u80fd\u529b\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "DyME\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u589e\u5f3aSVLMs\u7684\u53ef\u9760\u601d\u8003\u80fd\u529b\u3002"}}
{"id": "2506.23469", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.23469", "abs": "https://arxiv.org/abs/2506.23469", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "AI": {"tldr": "TripleAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e92\u84b8\u998f\u7684\u4e09\u901a\u9053\u56fe\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u5206\u522b\u68c0\u6d4b\u5c5e\u6027\u3001\u7ed3\u6784\u548c\u6df7\u5408\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u5f02\u5e38\u65f6\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u540c\u65f6\u68c0\u6d4b\u5c5e\u6027\u4e0e\u7ed3\u6784\u5f02\u5e38\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0cTripleAD\u65e8\u5728\u901a\u8fc7\u591a\u901a\u9053\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "TripleAD\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u591a\u5c3a\u5ea6\u5c5e\u6027\u4f30\u8ba1\u6a21\u5757\u3001\u94fe\u63a5\u589e\u5f3a\u7ed3\u6784\u4f30\u8ba1\u6a21\u5757\u548c\u57fa\u4e8e\u5c5e\u6027\u6df7\u5408\u66f2\u7387\u7684\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u4e92\u84b8\u998f\u7b56\u7565\u4fc3\u8fdb\u6a21\u5757\u95f4\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTripleAD\u5728\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u5f02\u5e38\u65f6\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TripleAD\u901a\u8fc7\u591a\u901a\u9053\u8bbe\u8ba1\u548c\u4e92\u84b8\u998f\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23066", "categories": ["cs.CV", "cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.23066", "abs": "https://arxiv.org/abs/2506.23066", "authors": ["Jiale Meng", "Yiming Li", "Zheming Lu", "Zewei He", "Hao Luo", "Tianwei Zhang"], "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique", "comment": "10 pages, 16 figures", "summary": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCORE\u7684\u65b0\u6587\u672c\u6c34\u5370\u5d4c\u5165\u8303\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86CoreMark\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u3001\u901a\u7528\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6c34\u5370\u65b9\u6848\u5728\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\u3001\u901a\u7528\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "CoreMark\u6846\u67b6\u52a8\u6001\u63d0\u53d6\u5b57\u7b26\u4e2d\u7684CORE\uff08\u8fde\u7eed\u5bf9\u9f50\u7684\u9ed1\u8272\u50cf\u7d20\u6bb5\uff09\uff0c\u6839\u636eCORE\u957f\u5ea6\u9009\u62e9\u9c81\u68d2\u6027\u5f3a\u7684\u5b57\u7b26\uff0c\u5e76\u901a\u8fc7\u8c03\u6574CORE\u539a\u5ea6\u5d4c\u5165\u6570\u636e\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5d4c\u5165\u5f3a\u5ea6\u8c03\u8282\u5668\uff0c\u6839\u636e\u5b57\u4f53\u5927\u5c0f\u81ea\u9002\u5e94\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoreMark\u5728\u591a\u8bed\u8a00\u548c\u5b57\u4f53\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6297\u622a\u56fe\u3001\u6253\u5370\u626b\u63cf\u548c\u6253\u5370\u76f8\u673a\u653b\u51fb\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "CoreMark\u4e3a\u6587\u672c\u6c34\u5370\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23492", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23492", "abs": "https://arxiv.org/abs/2506.23492", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "comment": null, "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "AI": {"tldr": "SMART\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8elogit\u95f4\u9699\u7684\u7cbe\u786e\u8c03\u6574\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u5728\u9ad8\u504f\u5dee\u6216\u9ad8\u65b9\u5dee\u4e4b\u95f4\u7684\u56f0\u5883\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u98ce\u9669\u663e\u8457\u3002\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u8981\u4e48\u5f15\u5165\u9ad8\u504f\u5dee\uff0c\u8981\u4e48\u56e0\u9ad8\u7ef4\u566a\u58f0\u5bfc\u81f4\u9ad8\u65b9\u5dee\u3002", "method": "SMART\u5229\u7528logit\u95f4\u9699\uff08\u524d\u4e24\u4e2alogit\u7684\u5dee\u503c\uff09\u4f5c\u4e3a\u53bb\u566a\u6807\u91cf\u4fe1\u53f7\uff0c\u7ed3\u5408\u8f6f\u5206\u7bb1\u7684ECE\u76ee\u6807\uff08SoftECE\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSMART\u5728\u53c2\u6570\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6821\u51c6\u6027\u80fd\u3002", "conclusion": "SMART\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23072", "abs": "https://arxiv.org/abs/2506.23072", "authors": ["Jing Gao"], "title": "Unsupervised 3D Braided Hair Reconstruction from a Single-View Image", "comment": "6 pages, 3 figures, accepted to the 2025 International Conference on\n  Machine Vision Applications (MVA 2025)", "summary": "Reconstructing 3D braided hairstyles from single-view images remains a\nchallenging task due to the intricate interwoven structure and complex\ntopologies of braids. Existing strand-based hair reconstruction methods\ntypically focus on loose hairstyles and often struggle to capture the\nfine-grained geometry of braided hair. In this paper, we propose a novel\nunsupervised pipeline for efficiently reconstructing 3D braided hair from\nsingle-view RGB images. Leveraging a synthetic braid model inspired by braid\ntheory, our approach effectively captures the complex intertwined structures of\nbraids. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches, providing superior accuracy, realism, and\nefficiency in reconstructing 3D braided hairstyles, supporting expressive\nhairstyle modeling in digital humans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4ece\u5355\u89c6\u89d2RGB\u56fe\u50cf\u9ad8\u6548\u91cd\u5efa3D\u7f16\u7ec7\u53d1\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7f16\u7ec7\u53d1\u578b\u7cbe\u7ec6\u51e0\u4f55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53d1\u4e1d\u7684\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u677e\u6563\u53d1\u578b\uff0c\u96be\u4ee5\u5904\u7406\u7f16\u7ec7\u53d1\u578b\u7684\u590d\u6742\u4ea4\u7ec7\u7ed3\u6784\u3002", "method": "\u5229\u7528\u53d7\u7f16\u7ec7\u7406\u8bba\u542f\u53d1\u7684\u5408\u6210\u7f16\u7ec7\u6a21\u578b\uff0c\u6355\u6349\u7f16\u7ec7\u53d1\u578b\u7684\u590d\u6742\u4ea4\u7ec7\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u771f\u5b9e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u6570\u5b57\u4eba\u7c7b\u4e2d\u66f4\u5177\u8868\u73b0\u529b\u7684\u53d1\u578b\u5efa\u6a21\u3002"}}
{"id": "2506.23516", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23516", "abs": "https://arxiv.org/abs/2506.23516", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "comment": null, "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "AI": {"tldr": "FedWSQ\u6846\u67b6\u901a\u8fc7\u6743\u91cd\u6807\u51c6\u5316\u548c\u975e\u5747\u5300\u91cf\u5316\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u548c\u901a\u4fe1\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5e38\u56e0\u6570\u636e\u5f02\u6784\u548c\u901a\u4fe1\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6743\u91cd\u6807\u51c6\u5316\uff08WS\uff09\u548c\u5206\u5e03\u611f\u77e5\u975e\u5747\u5300\u91cf\u5316\uff08DANUQ\uff09\uff0cWS\u8fc7\u6ee4\u672c\u5730\u66f4\u65b0\u4e2d\u7684\u504f\u5dee\uff0cDANUQ\u57fa\u4e8e\u7edf\u8ba1\u7279\u6027\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u3002", "result": "FedWSQ\u5728\u591a\u79cd\u6311\u6218\u6027\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "FedWSQ\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.23074", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23074", "abs": "https://arxiv.org/abs/2506.23074", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u4e8b\u5b9e\u89e3\u8026\u6ce8\u610f\u529b\u5b66\u4e60\uff08CDAL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u6a21\u578b\u5f52\u5c5e\uff0c\u901a\u8fc7\u5efa\u6a21\u6ce8\u610f\u529b\u89c6\u89c9\u75d5\u8ff9\u4e0e\u6e90\u6a21\u578b\u5f52\u5c5e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u533a\u57df\u5212\u5206\u6216\u7279\u5f81\u7a7a\u95f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\uff0c\u6613\u53d7\u865a\u5047\u7edf\u8ba1\u76f8\u5173\u6027\u5e72\u6270\uff0c\u96be\u4ee5\u5e94\u5bf9\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u65b0\u578b\u653b\u51fb\u3002", "method": "CDAL\u663e\u5f0f\u5efa\u6a21\u6ce8\u610f\u529b\u89c6\u89c9\u75d5\u8ff9\u4e0e\u6e90\u6a21\u578b\u5f52\u5c5e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53cd\u4e8b\u5b9e\u89e3\u8026\u5224\u522b\u6027\u6a21\u578b\u7279\u5b9a\u4f2a\u5f71\u4e0e\u6df7\u6dc6\u6e90\u504f\u5dee\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u6a21\u578b\u5f52\u5c5e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDAL\u663e\u8457\u63d0\u5347\u73b0\u6709\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u672a\u89c1\u7684\u65b0\u578b\u653b\u51fb\u6548\u679c\u663e\u8457\u3002", "conclusion": "CDAL\u901a\u8fc7\u6700\u5927\u5316\u56e0\u679c\u6548\u5e94\uff0c\u9f13\u52b1\u7f51\u7edc\u6355\u6349\u6cdb\u5316\u5230\u672a\u89c1\u6e90\u6a21\u578b\u7684\u751f\u6210\u6a21\u5f0f\uff0c\u63d0\u5347\u6a21\u578b\u5f52\u5c5e\u6027\u80fd\u3002"}}
{"id": "2506.23544", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23544", "abs": "https://arxiv.org/abs/2506.23544", "authors": ["Kento Imaizumi", "Hideaki Iiduka"], "title": "Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size", "comment": null, "summary": "Momentum methods were originally introduced for their superiority to\nstochastic gradient descent (SGD) in deterministic settings with convex\nobjective functions. However, despite their widespread application to deep\nneural networks -- a representative case of stochastic nonconvex optimization\n-- the theoretical justification for their effectiveness in such settings\nremains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that\ngeneralizes various momentum methods and has been studied to better understand\nthe class of momentum-based algorithms as a whole. In this paper, we provide\nboth asymptotic and non-asymptotic convergence results for mini-batch QHM with\nan increasing batch size. We show that achieving asymptotic convergence\nrequires either a decaying learning rate or an increasing batch size. Since a\ndecaying learning rate adversely affects non-asymptotic convergence, we\ndemonstrate that using mini-batch QHM with an increasing batch size -- without\ndecaying the learning rate -- can be a more effective strategy. Our experiments\nshow that even a finite increase in batch size can provide benefits for\ntraining neural networks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u91cf\u65b9\u6cd5\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u52a8\u91cf\u65b9\u6cd5QHM\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6279\u91cf\u5927\u5c0f\u800c\u975e\u964d\u4f4e\u5b66\u4e60\u7387\u662f\u66f4\u6709\u6548\u7684\u7b56\u7565\u3002", "motivation": "\u52a8\u91cf\u65b9\u6cd5\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u968f\u673a\u975e\u51f8\u4f18\u5316\uff09\u4e2d\u7684\u7406\u8bba\u652f\u6301\u4e0d\u8db3\uff0cQHM\u4f5c\u4e3a\u5e7f\u4e49\u52a8\u91cf\u65b9\u6cd5\uff0c\u65e8\u5728\u66f4\u597d\u5730\u7406\u89e3\u6b64\u7c7b\u7b97\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u5c0f\u6279\u91cfQHM\u5728\u6279\u91cf\u5927\u5c0f\u9012\u589e\u65f6\u7684\u6536\u655b\u6027\uff0c\u5206\u6790\u4e86\u5b66\u4e60\u7387\u8870\u51cf\u4e0e\u6279\u91cf\u5927\u5c0f\u589e\u52a0\u7684\u5f71\u54cd\u3002", "result": "\u8bc1\u660e\u6e10\u8fdb\u6536\u655b\u9700\u8981\u5b66\u4e60\u7387\u8870\u51cf\u6216\u6279\u91cf\u589e\u52a0\uff1b\u5b9e\u9a8c\u663e\u793a\u6709\u9650\u6279\u91cf\u589e\u52a0\u5bf9\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6709\u76ca\u3002", "conclusion": "\u589e\u52a0\u6279\u91cf\u5927\u5c0f\u800c\u975e\u964d\u4f4e\u5b66\u4e60\u7387\u662f\u66f4\u6709\u6548\u7684\u7b56\u7565\uff0c\u4e3a\u52a8\u91cf\u65b9\u6cd5\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.23077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23077", "abs": "https://arxiv.org/abs/2506.23077", "authors": ["Suofei Zhang", "Xinxin Wang", "Xiaofu Wu", "Quan Zhou", "Haifeng Hu"], "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization", "comment": null, "summary": "Existing deep learning-based cross-view geo-localization methods primarily\nfocus on improving the accuracy of cross-domain image matching, rather than\nenabling models to comprehensively capture contextual information around the\ntarget and minimize the cost of localization errors. To support systematic\nresearch into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,\nwe construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs\nmulti-view imagery with precise distance annotations across three spatial\nresolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical\nretrieval problem across different domains. Our study further reveals that, due\nto the inherent complexity of spatial relationships among buildings, this\nproblem can only be addressed via a contrastive learning paradigm, rather than\nconventional metric learning. To tackle this challenge, we propose Dynamic\nContrastive Learning (DyCL), a novel framework that progressively aligns\nfeature representations according to hierarchical spatial margins. Extensive\nexperiments demonstrate that DyCL is highly complementary to existing\nmulti-scale metric learning methods and yields substantial improvements in both\nhierarchical retrieval performance and overall cross-view geo-localization\naccuracy. Our code and benchmark are publicly available at\nhttps://github.com/anocodetest1/DyCL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08DyCL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8ddd\u79bb\u611f\u77e5\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u95ee\u9898\uff08DACVGL\uff09\uff0c\u5e76\u901a\u8fc7\u6784\u5efaDA-Campus\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8de8\u57df\u56fe\u50cf\u5339\u914d\u7684\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5168\u9762\u6355\u6349\u76ee\u6807\u5468\u56f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ca\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u6210\u672c\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efaDA-Campus\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06DACVGL\u95ee\u9898\u5b9a\u4e49\u4e3a\u8de8\u57df\u5206\u5c42\u68c0\u7d22\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08DyCL\uff09\u9010\u6b65\u5bf9\u9f50\u7279\u5f81\u8868\u793a\u3002", "result": "DyCL\u5728\u5206\u5c42\u68c0\u7d22\u6027\u80fd\u548c\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u4e0e\u73b0\u6709\u591a\u5c3a\u5ea6\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u9ad8\u5ea6\u4e92\u8865\u3002", "conclusion": "DyCL\u901a\u8fc7\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86DACVGL\u95ee\u9898\uff0c\u4e3a\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23551", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23551", "abs": "https://arxiv.org/abs/2506.23551", "authors": ["Jingpu Cheng", "Qianxiao Li", "Ting Lin", "Zuowei Shen"], "title": "A unified framework on the universal approximation of transformer-type architectures", "comment": null, "summary": "We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Transformer\u7c7b\u67b6\u6784\u7684\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff08UAP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u6b8b\u5dee\u7f51\u7edc\u7684\u7ed3\u679c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5e7f\u6cdb\u67b6\u6784\u7684\u5145\u5206\u6761\u4ef6\u3002", "motivation": "\u63a2\u7d22Transformer\u7c7b\u67b6\u6784\u7684\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff0c\u4e3a\u8bbe\u8ba1\u5177\u6709UAP\u4fdd\u8bc1\u7684\u65b0\u578b\u67b6\u6784\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5c42\u7684\u53ef\u533a\u5206\u6027\uff0c\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5229\u7528\u89e3\u6790\u6027\u5047\u8bbe\u7b80\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u57fa\u4e8e\u6838\u548c\u7a00\u758f\u6ce8\u610f\u529b\uff09\u7684Transformer\u5177\u6709UAP\uff0c\u5e76\u6269\u5c55\u4e86\u5148\u524d\u7684\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u5177\u6709UAP\u4fdd\u8bc1\u7684\u65b0\u578bTransformer\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5177\u4f53\u793a\u4f8b\u3002"}}
{"id": "2506.23086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23086", "abs": "https://arxiv.org/abs/2506.23086", "authors": ["Jian Shi", "Tianqi You", "Pingping Zhang", "Hongli Zhang", "Rui Xu", "Haojie Li"], "title": "Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation", "comment": "Accepted by MICCAI2025. More modifications my be performed", "summary": "Automated and accurate segmentation of individual vertebra in 3D CT and MRI\nimages is essential for various clinical applications. Due to the limitations\nof current imaging techniques and the complexity of spinal structures, existing\nmethods still struggle with reducing the impact of image blurring and\ndistinguishing similar vertebrae. To alleviate these issues, we introduce a\nFrequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the\naccuracy of vertebrae segmentation. Specifically, we first apply wavelet\ntransform for lossless downsampling to reduce the feature distortion in blurred\nimages. The decomposed high and low-frequency components are then processed\nseparately. For the high-frequency components, we apply a High-frequency\nFeature Refinement (HFR) to amplify the prominence of key features and filter\nout noises, restoring fine-grained details in blurred images. For the\nlow-frequency components, we use a Multi-granularity State Space Model (MG-SSM)\nto aggregate feature representations with different receptive fields,\nextracting spatially-varying contexts while capturing long-range dependencies\nwith linear complexity. The utilization of multi-granularity contexts is\nessential for distinguishing similar vertebrae and improving segmentation\naccuracy. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.\nThe source code is publicly available at https://github.com/anaanaa/FMCNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u589e\u5f3a\u7684\u591a\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7f51\u7edc\uff08FMC-Net\uff09\uff0c\u7528\u4e8e\u63d0\u9ad83D CT\u548cMRI\u56fe\u50cf\u4e2d\u810a\u690e\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u548c\u9ad8\u4f4e\u9891\u5206\u91cf\u5904\u7406\u89e3\u51b3\u56fe\u50cf\u6a21\u7cca\u548c\u76f8\u4f3c\u810a\u690e\u533a\u5206\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6210\u50cf\u6280\u672f\u548c\u810a\u67f1\u7ed3\u6784\u7684\u590d\u6742\u6027\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u56fe\u50cf\u6a21\u7cca\u5f71\u54cd\u548c\u533a\u5206\u76f8\u4f3c\u810a\u690e\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u8fdb\u884c\u65e0\u635f\u4e0b\u91c7\u6837\uff0c\u5206\u522b\u5904\u7406\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\uff1a\u9ad8\u9891\u5206\u91cf\u901a\u8fc7\u9ad8\u9891\u7279\u5f81\u7ec6\u5316\uff08HFR\uff09\u589e\u5f3a\u5173\u952e\u7279\u5f81\u5e76\u8fc7\u6ee4\u566a\u58f0\uff1b\u4f4e\u9891\u5206\u91cf\u901a\u8fc7\u591a\u7c92\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08MG-SSM\uff09\u805a\u5408\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728CT\u548cMRI\u810a\u690e\u5206\u5272\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "FMC-Net\u901a\u8fc7\u591a\u7c92\u5ea6\u4e0a\u4e0b\u6587\u548c\u9891\u7387\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u810a\u690e\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23589", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23589", "abs": "https://arxiv.org/abs/2506.23589", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "title": "Transition Matching: Scalable and Flexible Generative Modeling", "comment": null, "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransition Matching (TM)\u7684\u65b0\u751f\u6210\u8303\u5f0f\uff0c\u7edf\u4e00\u4e86\u6269\u6563/\u6d41\u6a21\u578b\u548c\u8fde\u7eed\u81ea\u56de\u5f52\u751f\u6210\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u4e3a\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5df2\u8f83\u4e3a\u6210\u719f\uff0c\u6539\u8fdb\u7a7a\u95f4\u6709\u9650\uff1b\u540c\u65f6\uff0c\u8fde\u7eed\u81ea\u56de\u5f52\u6a21\u578b\u5728\u7edf\u4e00\u6587\u672c\u548c\u5a92\u4f53\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "method": "TM\u5c06\u590d\u6742\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u7b80\u5355\u7684\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\uff0c\u652f\u6301\u975e\u786e\u5b9a\u6027\u6982\u7387\u8f6c\u79fb\u6838\u548c\u4efb\u610f\u975e\u8fde\u7eed\u76d1\u7763\u8fc7\u7a0b\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u53d8\u4f53\uff1aDTM\u3001ARTM\u548cFHTM\u3002", "result": "DTM\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230SOTA\uff0cARTM\u548cFHTM\u5728\u8fde\u7eed\u81ea\u56de\u5f52\u751f\u6210\u4e2d\u4e0e\u975e\u56e0\u679c\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0cFHTM\u9996\u6b21\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8d85\u8d8a\u6d41\u65b9\u6cd5\u3002", "conclusion": "TM\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7075\u6d3b\u8bbe\u8ba1\u65b9\u5411\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.23088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23088", "abs": "https://arxiv.org/abs/2506.23088", "authors": ["Yuchen Zhou", "Jiayu Tang", "Xiaoyan Xiao", "Yueyao Lin", "Linkai Liu", "Zipeng Guo", "Hao Fei", "Xiaobo Xia", "Chao Gou"], "title": "Where, What, Why: Towards Explainable Driver Attention Prediction", "comment": "Accepted by ICCV 2025", "summary": "Modeling task-driven attention in driving is a fundamental challenge for both\nautonomous vehicles and cognitive science. Existing methods primarily predict\nwhere drivers look by generating spatial heatmaps, but fail to capture the\ncognitive motivations behind attention allocation in specific contexts, which\nlimits deeper understanding of attention mechanisms. To bridge this gap, we\nintroduce Explainable Driver Attention Prediction, a novel task paradigm that\njointly predicts spatial attention regions (where), parses attended semantics\n(what), and provides cognitive reasoning for attention allocation (why). To\nsupport this, we present W3DA, the first large-scale explainable driver\nattention dataset. It enriches existing benchmarks with detailed semantic and\ncausal annotations across diverse driving scenarios, including normal\nconditions, safety-critical situations, and traffic accidents. We further\npropose LLada, a Large Language model-driven framework for driver attention\nprediction, which unifies pixel modeling, semantic parsing, and cognitive\nreasoning within an end-to-end architecture. Extensive experiments demonstrate\nthe effectiveness of LLada, exhibiting robust generalization across datasets\nand driving conditions. This work serves as a key step toward a deeper\nunderstanding of driver attention mechanisms, with significant implications for\nautonomous driving, intelligent driver training, and human-computer\ninteraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u9884\u6d4b\u4efb\u52a1\u8303\u5f0f\uff08W3DA\uff09\uff0c\u7ed3\u5408\u9884\u6d4b\u7a7a\u95f4\u6ce8\u610f\u529b\u533a\u57df\uff08where\uff09\u3001\u89e3\u6790\u8bed\u4e49\uff08what\uff09\u548c\u63d0\u4f9b\u8ba4\u77e5\u63a8\u7406\uff08why\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6LLada\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u751f\u6210\u7a7a\u95f4\u70ed\u56fe\u9884\u6d4b\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\uff0c\u4f46\u672a\u80fd\u6355\u6349\u7279\u5b9a\u60c5\u5883\u4e0b\u6ce8\u610f\u529b\u5206\u914d\u7684\u8ba4\u77e5\u52a8\u673a\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86W3DA\u6570\u636e\u96c6\uff0c\u5305\u542b\u8be6\u7ec6\u8bed\u4e49\u548c\u56e0\u679c\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1\u4e86LLada\u6846\u67b6\uff0c\u7edf\u4e00\u50cf\u7d20\u5efa\u6a21\u3001\u8bed\u4e49\u89e3\u6790\u548c\u8ba4\u77e5\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLLada\u5728\u6570\u636e\u96c6\u548c\u9a7e\u9a76\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5165\u7406\u89e3\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u5173\u952e\u8fdb\u5c55\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u9a7e\u9a76\u57f9\u8bad\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.23596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23596", "abs": "https://arxiv.org/abs/2506.23596", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA2P\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u4e8b\u4ef6\u7684\u65f6\u95f4\u70b9\uff0c\u7ed3\u5408\u4e86\u5f02\u5e38\u611f\u77e5\u9884\u6d4b\u548c\u5408\u6210\u5f02\u5e38\u63d0\u793a\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u4e8b\u4ef6\u7684\u65f6\u95f4\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "A2P\u6846\u67b6\u5305\u62ec\u5f02\u5e38\u611f\u77e5\u9884\u6d4b\uff08AAF\uff09\u548c\u5408\u6210\u5f02\u5e38\u63d0\u793a\uff08SAP\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u6a21\u62df\u5f02\u5e38\u5173\u7cfb\u6765\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cA2P\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u3002", "conclusion": "A2P\u4e3a\u89e3\u51b3\u5f02\u5e38\u9884\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23104", "abs": "https://arxiv.org/abs/2506.23104", "authors": ["Jihun Kim", "Hoyong Kwon", "Hyeokjun Kweon", "Wooseong Jeong", "Kuk-Jin Yoon"], "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation", "comment": null, "summary": "Interactive segmentation (IS) allows users to iteratively refine object\nboundaries with minimal cues, such as positive and negative clicks. While the\nSegment Anything Model (SAM) has garnered attention in the IS community for its\npromptable segmentation capabilities, it often struggles in specialized domains\nor when handling complex scenarios (e.g., camouflaged or multi-part objects).\nTo overcome these challenges, we propose DC-TTA, a novel test-time adaptation\n(TTA) framework that adapts SAM on a per-sample basis by leveraging user\ninteractions as supervision. Instead of forcing a single model to incorporate\nall user clicks at once, DC-TTA partitions the clicks into more coherent\nsubsets, each processed independently via TTA with a separated model. This\nDivide-and-Conquer strategy reduces conflicts among diverse cues and enables\nmore localized updates. Finally, we merge the adapted models to form a unified\npredictor that integrates the specialized knowledge from each subset.\nExperimental results across various benchmarks demonstrate that DC-TTA\nsignificantly outperforms SAM's zero-shot results and conventional TTA methods,\neffectively handling complex tasks such as camouflaged object segmentation with\nfewer interactions and improved accuracy.", "AI": {"tldr": "DC-TTA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u76d1\u7763\u4f18\u5316SAM\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SAM\u5728\u4e13\u4e1a\u9886\u57df\u6216\u590d\u6742\u573a\u666f\uff08\u5982\u4f2a\u88c5\u6216\u591a\u90e8\u5206\u5bf9\u8c61\uff09\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u5c06\u7528\u6237\u4ea4\u4e92\u5212\u5206\u4e3a\u66f4\u4e00\u81f4\u7684\u5b50\u96c6\uff0c\u6bcf\u4e2a\u5b50\u96c6\u901a\u8fc7\u72ec\u7acb\u7684TTA\u6a21\u578b\u5904\u7406\uff0c\u6700\u540e\u5408\u5e76\u4e3a\u7edf\u4e00\u9884\u6d4b\u5668\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDC-TTA\u663e\u8457\u4f18\u4e8eSAM\u7684\u96f6\u6837\u672c\u7ed3\u679c\u548c\u4f20\u7edfTTA\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u4ea4\u4e92\u6b21\u6570\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "DC-TTA\u901a\u8fc7\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86SAM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23629", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.23629", "abs": "https://arxiv.org/abs/2506.23629", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "comment": "7 pages, 2 figures, conference", "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027\u4f4e\u79e9\u8868\u793a\u6a21\u578b\uff08NLR\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u586b\u8865\u6c34\u8d28\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u6c34\u8d28\u6570\u636e\u7684\u5b8c\u6574\u6027\u5bf9\u79d1\u5b66\u51b3\u7b56\u548c\u751f\u6001\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u6df1\u5c42\u7279\u5f81\u3002", "method": "\u5229\u7528CNN\u878d\u5408\u65f6\u95f4\u7279\u5f81\u4ee5\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u53d6\u975e\u7ebf\u6027\u4ea4\u4e92\u548c\u5c40\u90e8\u6a21\u5f0f\u4ee5\u5b9e\u73b0\u591a\u7ef4\u4fe1\u606f\u7684\u6df1\u5ea6\u878d\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6c34\u8d28\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6570\u636e\u586b\u8865\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6c34\u8d28\u76d1\u6d4b\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5904\u7406\u9014\u5f84\u3002"}}
{"id": "2506.23106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23106", "abs": "https://arxiv.org/abs/2506.23106", "authors": ["Ryo Ishiyama", "Shinnosuke Matsuo", "Seiichi Uchida"], "title": "Computer-Aided Multi-Stroke Character Simplification by Stroke Removal", "comment": "ICDAR2025 (Oral)", "summary": "Multi-stroke characters in scripts such as Chinese and Japanese can be highly\ncomplex, posing significant challenges for both native speakers and,\nespecially, non-native learners. If these characters can be simplified without\ndegrading their legibility, it could reduce learning barriers for non-native\nspeakers, facilitate simpler and legible font designs, and contribute to\nefficient character-based communication systems. In this paper, we propose a\nframework to systematically simplify multi-stroke characters by selectively\nremoving strokes while preserving their overall legibility. More specifically,\nwe use a highly accurate character recognition model to assess legibility and\nremove those strokes that minimally impact it. Experimental results on 1,256\ncharacter classes with 5, 10, 15, and 20 strokes reveal several key findings,\nincluding the observation that even after removing multiple strokes, many\ncharacters remain distinguishable. These findings suggest the potential for\nmore formalized simplification strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u9009\u62e9\u6027\u53bb\u9664\u7b14\u753b\u6765\u7b80\u5316\u591a\u7b14\u753b\u6c49\u5b57\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u964d\u4f4e\u5b66\u4e60\u96be\u5ea6\u5e76\u63d0\u9ad8\u5b57\u4f53\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u591a\u7b14\u753b\u6c49\u5b57\u5bf9\u975e\u6bcd\u8bed\u5b66\u4e60\u8005\u96be\u5ea6\u5927\uff0c\u7b80\u5316\u540e\u80fd\u964d\u4f4e\u5b66\u4e60\u95e8\u69db\u3001\u4f18\u5316\u5b57\u4f53\u8bbe\u8ba1\u5e76\u63d0\u5347\u5b57\u7b26\u901a\u4fe1\u6548\u7387\u3002", "method": "\u5229\u7528\u9ad8\u7cbe\u5ea6\u5b57\u7b26\u8bc6\u522b\u6a21\u578b\u8bc4\u4f30\u53ef\u8bfb\u6027\uff0c\u9009\u62e9\u6027\u53bb\u9664\u5bf9\u53ef\u8bfb\u6027\u5f71\u54cd\u6700\u5c0f\u7684\u7b14\u753b\u3002", "result": "\u57281,256\u4e2a\u5b57\u7b26\u7c7b\u522b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u53bb\u9664\u591a\u7b14\u753b\uff0c\u8bb8\u591a\u5b57\u7b26\u4ecd\u53ef\u533a\u5206\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6c49\u5b57\u7b80\u5316\u63d0\u4f9b\u4e86\u6f5c\u5728\u7b56\u7565\uff0c\u652f\u6301\u66f4\u7cfb\u7edf\u5316\u7684\u7b80\u5316\u65b9\u6cd5\u3002"}}
{"id": "2506.23679", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.23679", "abs": "https://arxiv.org/abs/2506.23679", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "title": "Learning Modular Exponentiation with Transformers", "comment": null, "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u6a21\u5e42\u8fd0\u7b97\u4e2d\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u901a\u8fc7\u7279\u5b9a\u8ba1\u7b97\u7535\u8def\u5b66\u4e60\u6a21\u7b97\u672f\u3002", "motivation": "\u6a21\u5e42\u8fd0\u7b97\u5728\u6570\u8bba\u548c\u5bc6\u7801\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bad\u7ec34\u5c42\u7f16\u7801\u5668-\u89e3\u7801\u5668Transformer\u6a21\u578b\uff0c\u91c7\u7528PCA\u5d4c\u5165\u5206\u6790\u548c\u6fc0\u6d3b\u4fee\u8865\u7b49\u6280\u672f\u7814\u7a76\u6570\u503c\u63a8\u7406\u7684\u6d8c\u73b0\u3002", "result": "\u53d1\u73b0\u4e92\u64cd\u4f5c\u6570\u8bad\u7ec3\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6a21\u578b\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u5b50\u56fe\u5b9e\u73b0\u6a21\u5e42\u8fd0\u7b97\u3002", "conclusion": "Transformer\u6a21\u578b\u901a\u8fc7\u4e13\u7528\u8ba1\u7b97\u7535\u8def\u5b66\u4e60\u6a21\u7b97\u672f\uff0c\u4e3a\u66f4\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2506.23108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23108", "abs": "https://arxiv.org/abs/2506.23108", "authors": ["Zhiyuan Zhu", "Jian Wang", "Yong Jiang", "Tong Han", "Yuhao Huang", "Ang Zhang", "Kaiwen Yang", "Mingyuan Luo", "Zhe Liu", "Yaofei Duan", "Dong Ni", "Tianhong Tang", "Xin Yang"], "title": "Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound", "comment": "Accepted at MICCAI 2025", "summary": "Accurate carotid plaque grading (CPG) is vital to assess the risk of\ncardiovascular and cerebrovascular diseases. Due to the small size and high\nintra-class variability of plaque, CPG is commonly evaluated using a\ncombination of transverse and longitudinal ultrasound views in clinical\npractice. However, most existing deep learning-based multi-view classification\nmethods focus on feature fusion across different views, neglecting the\nimportance of representation learning and the difference in class features. To\naddress these issues, we propose a novel Corpus-View-Category Refinement\nFramework (CVC-RF) that processes information from Corpus-, View-, and\nCategory-levels, enhancing model performance. Our contribution is four-fold.\nFirst, to the best of our knowledge, we are the foremost deep learning-based\nmethod for CPG according to the latest Carotid Plaque-RADS guidelines. Second,\nwe propose a novel center-memory contrastive loss, which enhances the network's\nglobal modeling capability by comparing with representative cluster centers and\ndiverse negative samples at the Corpus level. Third, we design a cascaded\ndown-sampling attention module to fuse multi-scale information and achieve\nimplicit feature interaction at the View level. Finally, a parameter-free\nmixture-of-experts weighting strategy is introduced to leverage class\nclustering knowledge to weight different experts, enabling feature decoupling\nat the Category level. Experimental results indicate that CVC-RF effectively\nmodels global features via multi-level refinement, achieving state-of-the-art\nperformance in the challenging CPG task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCVC-RF\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u7ec6\u5316\uff08Corpus\u3001View\u3001Category\uff09\u63d0\u5347\u9888\u52a8\u8109\u6591\u5757\u5206\u7ea7\uff08CPG\uff09\u7684\u6027\u80fd\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u3001\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u52a0\u6743\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u9888\u52a8\u8109\u6591\u5757\u5206\u7ea7\uff08CPG\uff09\u5bf9\u8bc4\u4f30\u5fc3\u8111\u8840\u7ba1\u75be\u75c5\u98ce\u9669\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8868\u793a\u5b66\u4e60\u548c\u7c7b\u522b\u7279\u5f81\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51faCVC-RF\u6846\u67b6\uff0c\u5305\u62ec\u4e2d\u5fc3\u8bb0\u5fc6\u5bf9\u6bd4\u635f\u5931\uff08Corpus\u7ea7\uff09\u3001\u7ea7\u8054\u4e0b\u91c7\u6837\u6ce8\u610f\u529b\u6a21\u5757\uff08View\u7ea7\uff09\u548c\u65e0\u53c2\u6570\u4e13\u5bb6\u52a0\u6743\u7b56\u7565\uff08Category\u7ea7\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCVC-RF\u901a\u8fc7\u591a\u7ea7\u7ec6\u5316\u6709\u6548\u5efa\u6a21\u5168\u5c40\u7279\u5f81\uff0c\u5728CPG\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CVC-RF\u901a\u8fc7\u591a\u7ea7\u7ec6\u5316\u663e\u8457\u63d0\u5347\u4e86CPG\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2506.23719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23719", "abs": "https://arxiv.org/abs/2506.23719", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "comment": "13 pages, 5 figures", "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "AI": {"tldr": "DABstep\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u73b0\u5b9e\u591a\u6b65\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b450\u591a\u4e2a\u771f\u5b9e\u6311\u6218\uff0c\u6d4b\u8bd5\u6570\u636e\u64cd\u4f5c\u3001\u591a\u6e90\u4ea4\u53c9\u5f15\u7528\u548c\u7ed3\u679c\u62a5\u544a\u80fd\u529b\u3002\u6700\u4f73\u4ee3\u7406\u5728\u6700\u96be\u4efb\u52a1\u4e0a\u4ec514.55%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u5728\u591a\u6b65\u6570\u636e\u5206\u6790\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u4e00\u4e2a\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u57fa\u4e8e\u91d1\u878d\u5206\u6790\u5e73\u53f0\u7684450\u591a\u4e2a\u771f\u5b9e\u4efb\u52a1\uff0c\u7ed3\u5408\u4ee3\u7801\u5904\u7406\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u91c7\u7528\u81ea\u52a8\u8bc4\u5206\u673a\u5236\u3002", "result": "\u6700\u4f73\u4ee3\u7406\u5728\u6700\u5177\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec514.55%\uff0c\u663e\u793a\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u3002", "conclusion": "DABstep\u4e3a\u81ea\u4e3b\u6570\u636e\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u57fa\u51c6\u548c\u5de5\u5177\u5305\uff0c\u52a0\u901f\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2506.23115", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23115", "abs": "https://arxiv.org/abs/2506.23115", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "comment": "Homepage: https://haon-chen.github.io/MoCa/", "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "AI": {"tldr": "MoCa\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u9884\u8bad\u7ec3\u7684\u56e0\u679c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u6ce8\u610f\u529b\u673a\u5236\u3001\u6570\u636e\u4f9d\u8d56\u6027\u548c\u8bad\u7ec3\u76ee\u6807\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u9002\u7528\u4e8e\u5d4c\u5165\u4efb\u52a1\u3001\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "MoCa\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u6a21\u6001\u611f\u77e5\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u8054\u5408\u91cd\u5efa\u76ee\u6807\u589e\u5f3a\u53cc\u5411\u4e0a\u4e0b\u6587\u63a8\u7406\uff1b2\uff09\u5f02\u6784\u5bf9\u6bd4\u5fae\u8c03\uff0c\u5229\u7528\u591a\u6837\u5316\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u6cdb\u5316\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "result": "MoCa\u5728MMEB\u548cViDoRe-v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u65b0SOTA\u7ed3\u679c\uff0c\u5e76\u5728\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u4e0a\u5c55\u793a\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MoCa\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u3001\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u548c\u591a\u6837\u5316\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.23726", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23726", "abs": "https://arxiv.org/abs/2506.23726", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "title": "System-Embedded Diffusion Bridge Models", "comment": "Preprint", "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u6865\u63a5\u65b9\u6cd5SDBs\uff0c\u901a\u8fc7\u5c06\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\u5d4c\u5165\u77e9\u9635\u503cSDE\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u9006\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9006\u95ee\u9898\uff08\u4ece\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u6d4b\u91cf\u4e2d\u6062\u590d\u4fe1\u53f7\uff09\u662f\u79d1\u5b66\u548c\u5de5\u7a0b\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u6d4b\u91cf\u6a21\u578b\u4fe1\u606f\uff0c\u8981\u4e48\u5047\u8bbe\u5176\u5df2\u77e5\u3002", "method": "\u5f15\u5165\u7cfb\u7edf\u5d4c\u5165\u6269\u6563\u6865\u6a21\u578b\uff08SDBs\uff09\uff0c\u5c06\u5df2\u77e5\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\u5d4c\u5165\u77e9\u9635\u503cSDE\u7684\u7cfb\u6570\u4e2d\u3002", "result": "SDBs\u5728\u591a\u79cd\u7ebf\u6027\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u7cfb\u7edf\u8bef\u914d\u60c5\u51b5\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "SDBs\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23120", "abs": "https://arxiv.org/abs/2506.23120", "authors": ["Zhenhua Ning", "Zhuotao Tian", "Shaoshuai Shi", "Guangming Lu", "Daojing He", "Wenjie Pei", "Li Jiang"], "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation", "comment": null, "summary": "Recent advances in point cloud perception have demonstrated remarkable\nprogress in scene understanding through vision-language alignment leveraging\nlarge language models (LLMs). However, existing methods may still encounter\nchallenges in handling complex instructions that require accurate spatial\nreasoning, even if the 3D point cloud data provides detailed spatial cues such\nas size and position for identifying the targets. To tackle this issue, we\npropose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based\nsegmentation framework. The framework emulates human cognitive processes by\ndecomposing spatial reasoning into two sequential stages: first identifying\nrelevant elements, then processing instructions guided by their associated\nvisual priors. Furthermore, acknowledging the inadequacy of existing datasets\nin complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based\nsegmentation dataset comprising 25,185 training samples and 3,966 validation\nsamples with precise annotations. Both quantitative and qualitative experiments\ndemonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud\nperception with stronger spatial reasoning capabilities, and we hope that they\ncan serve as a new baseline and benchmark for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u5206\u5272\u6846\u67b6R\u00b2S\u548c\u6570\u636e\u96c63D ReasonSeg\uff0c\u4ee5\u589e\u5f3a3D\u70b9\u4e91\u611f\u77e5\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u63a8\u7406\u7684\u590d\u6742\u6307\u4ee4\u65f6\u5b58\u5728\u6311\u6218\uff0c\u5c3d\u7ba13D\u70b9\u4e91\u6570\u636e\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u7a7a\u95f4\u7ebf\u7d22\u3002", "method": "R\u00b2S\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5c06\u7a7a\u95f4\u63a8\u7406\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u5148\u8bc6\u522b\u76f8\u5173\u5143\u7d20\uff0c\u518d\u6839\u636e\u89c6\u89c9\u5148\u9a8c\u5904\u7406\u6307\u4ee4\u3002", "result": "R\u00b2S\u548c3D ReasonSeg\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u611f\u77e5\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "R\u00b2S\u548c3D ReasonSeg\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.23731", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubi\u0144ski", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "comment": null, "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u548c\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\uff08IARs\uff09\u4e2d\u6c34\u5370\u7684\u653e\u5c04\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728DMs\u4e2d\u5931\u6548\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9IARs\u7684\u65b0\u578b\u653e\u5c04\u6027\u6c34\u5370\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4f46\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3002\u4e3a\u9632\u6b62\u751f\u6210\u56fe\u50cf\u88ab\u672a\u7ecf\u6388\u6743\u7528\u4e8e\u8bad\u7ec3\u65b0\u6a21\u578b\uff0c\u9700\u8981\u6c34\u5370\u5177\u6709\u653e\u5c04\u6027\uff08\u5373\u80fd\u5728\u65b0\u6a21\u578b\u4e2d\u4fdd\u7559\uff09\u3002", "method": "\u5206\u6790\u4e86DMs\u548cIARs\u4e2d\u6c34\u5370\u7684\u653e\u5c04\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u8303\u5f0f\uff08\u7c7b\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\u3002", "result": "\u73b0\u6709DMs\u6c34\u5370\u65b9\u6cd5\u56e0\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u6216\u566a\u58f0\u5904\u7406\u800c\u5931\u6548\uff0c\u800c\u63d0\u51fa\u7684IARs\u6c34\u5370\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u653e\u5c04\u6027\u3002", "conclusion": "\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\u5728IARs\u4e2d\u6210\u529f\u5b9e\u73b0\u653e\u5c04\u6027\uff0c\u4e3a\u56fe\u50cf\u6765\u6e90\u8ffd\u8e2a\u548c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23132", "abs": "https://arxiv.org/abs/2506.23132", "authors": ["Sophie Zhou", "Shu Kong"], "title": "Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval", "comment": "to appear at AVSS'25", "summary": "Art plagiarism detection plays a crucial role in protecting artists'\ncopyrights and intellectual property, yet it remains a challenging problem in\nforensic analysis. In this paper, we address the task of recognizing\nplagiarized paintings and explaining the detected plagarisms by retrieving\nvisually similar authentic artworks. To support this study, we construct a\ndataset by collecting painting photos and synthesizing plagiarized versions\nusing generative AI, tailored to specific artists' styles. We first establish a\nbaseline approach using off-the-shelf features from the visual foundation model\nDINOv2 to retrieve the most similar images in the database and classify\nplagiarism based on a similarity threshold. Surprisingly, this non-learned\nmethod achieves a high recognition accuracy of 97.2\\% but suffers from low\nretrieval precision 29.0\\% average precision (AP). To improve retrieval\nquality, we finetune DINOv2 with a metric learning loss using positive and\nnegative sample pairs sampled in the database. The finetuned model greatly\nimproves retrieval performance by 12\\% AP over the baseline, though it\nunexpectedly results in a lower recognition accuracy (92.7\\%). We conclude with\ninsightful discussions and outline directions for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578bDINOv2\u7684\u827a\u672f\u4f5c\u54c1\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3c\u7684\u771f\u5b9e\u753b\u4f5c\u6765\u8bc6\u522b\u548c\u89e3\u91ca\u6284\u88ad\u884c\u4e3a\u3002\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u9ad8\u4f46\u68c0\u7d22\u7cbe\u5ea6\u4f4e\uff0c\u5fae\u8c03\u540e\u68c0\u7d22\u6027\u80fd\u63d0\u5347\u4f46\u51c6\u786e\u7387\u4e0b\u964d\u3002", "motivation": "\u4fdd\u62a4\u827a\u672f\u5bb6\u7248\u6743\u548c\u77e5\u8bc6\u4ea7\u6743\uff0c\u89e3\u51b3\u827a\u672f\u6284\u88ad\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u6570\u636e\u96c6\u5e76\u4f7f\u7528DINOv2\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u9608\u503c\u5206\u7c7b\u6284\u88ad\uff1b\u5fae\u8c03\u6a21\u578b\u4ee5\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u738797.2%\uff0c\u68c0\u7d22\u7cbe\u5ea629.0%\uff1b\u5fae\u8c03\u540e\u68c0\u7d22\u6027\u80fd\u63d0\u534712%\uff0c\u4f46\u51c6\u786e\u7387\u964d\u81f392.7%\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u4f46\u9700\u6743\u8861\u68c0\u7d22\u6027\u80fd\u4e0e\u51c6\u786e\u7387\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2506.23757", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23757", "abs": "https://arxiv.org/abs/2506.23757", "authors": ["Dan Yao", "Steve McLaughlin", "Yoann Altmann"], "title": "Training of Spiking Neural Networks with Expectation-Propagation", "comment": "10 pages", "summary": "In this paper, we propose a unifying message-passing framework for training\nspiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free\nmethod is capable of learning the marginal distributions of network parameters\nand simultaneously marginalizes nuisance parameters, such as the outputs of\nhidden layers. This framework allows for the first time, training of discrete\nand continuous weights, for deterministic and stochastic spiking networks,\nusing batches of training samples. Although its convergence is not ensured, the\nalgorithm converges in practice faster than gradient-based methods, without\nrequiring a large number of passes through the training data. The\nclassification and regression results presented pave the way for new efficient\ntraining methods for deep Bayesian networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u4f20\u64ad\u7684\u7edf\u4e00\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\uff0c\u652f\u6301\u79bb\u6563\u548c\u8fde\u7eed\u6743\u91cd\u8bad\u7ec3\uff0c\u4e14\u65e0\u9700\u68af\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\u5728\u8bad\u7ec3SNNs\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u671f\u671b\u4f20\u64ad\u7684\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u68af\u5ea6\u65e0\u5173\uff0c\u53ef\u540c\u65f6\u5b66\u4e60\u53c2\u6570\u5206\u5e03\u548c\u8fb9\u7f18\u5316\u5e72\u6270\u53c2\u6570\u3002", "result": "\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u6536\u655b\u901f\u5ea6\u5feb\u4e8e\u68af\u5ea6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u3002", "conclusion": "\u4e3a\u6df1\u5ea6\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u9ad8\u6548\u8bad\u7ec3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.23776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23776", "abs": "https://arxiv.org/abs/2506.23776", "authors": ["Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Model-driven Stochastic Trace Clustering", "comment": null, "summary": "Process discovery algorithms automatically extract process models from event\nlogs, but high variability often results in complex and hard-to-understand\nmodels. To mitigate this issue, trace clustering techniques group process\nexecutions into clusters, each represented by a simpler and more understandable\nprocess model. Model-driven trace clustering improves on this by assigning\ntraces to clusters based on their conformity to cluster-specific process\nmodels. However, most existing clustering techniques rely on either no process\nmodel discovery, or non-stochastic models, neglecting the frequency or\nprobability of activities and transitions, thereby limiting their capability to\ncapture real-world execution dynamics. We propose a novel model-driven trace\nclustering method that optimizes stochastic process models within each cluster.\nOur approach uses entropic relevance, a stochastic conformance metric based on\ndirectly-follows probabilities, to guide trace assignment. This allows\nclustering decisions to consider both structural alignment with a cluster's\nprocess model and the likelihood that a trace originates from a given\nstochastic process model. The method is computationally efficient, scales\nlinearly with input size, and improves model interpretability by producing\nclusters with clearer control-flow patterns. Extensive experiments on public\nreal-life datasets show that our method outperforms existing alternatives in\nrepresenting process behavior and reveals how clustering performance rankings\ncan shift when stochasticity is considered.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u7684\u8f68\u8ff9\u805a\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u71b5\u76f8\u5173\u6027\u5ea6\u91cf\u4f18\u5316\u805a\u7c7b\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u9ad8\u53d8\u5f02\u6027\u5bfc\u81f4\u8fc7\u7a0b\u6a21\u578b\u590d\u6742\u96be\u61c2\uff0c\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u5ffd\u7565\u6d3b\u52a8\u9891\u7387\u548c\u8f6c\u79fb\u6982\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u76f4\u63a5\u8ddf\u968f\u6982\u7387\u7684\u71b5\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u4f18\u5316\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7ed3\u6784\u6e05\u6670\u7684\u805a\u7c7b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8003\u8651\u968f\u673a\u6027\u65f6\u805a\u7c7b\u6027\u80fd\u6392\u540d\u7684\u53d8\u5316\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u548c\u71b5\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u7a0b\u884c\u4e3a\u7684\u8868\u793a\u80fd\u529b\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.23138", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23138", "abs": "https://arxiv.org/abs/2506.23138", "authors": ["Shiyu Wu", "Mingzhen Sun", "Weining Wang", "Yequan Wang", "Jing Liu"], "title": "VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "comment": "12 pages, 5 figures", "summary": "Since there exists a notable gap between user-provided and model-preferred\nprompts, generating high-quality and satisfactory images using diffusion models\noften requires prompt engineering to optimize user inputs. Current studies on\ntext-to-image prompt engineering can effectively enhance the style and\naesthetics of generated images. However, they often neglect the semantic\nalignment between generated images and user descriptions, resulting in visually\nappealing but content-wise unsatisfying outputs. In this work, we propose\nVisualPrompter, a novel training-free prompt engineering framework that refines\nuser inputs to model-preferred sentences. In particular, VisualPrompter\nutilizes an automatic self-reflection module to identify the missing concepts\nin generated images and a target-specific prompt optimization mechanism to\nrevise the prompts in a fine-grained manner. Extensive experiments demonstrate\nthe effectiveness of our VisualPrompter, which achieves new state-of-the-art\nperformance on multiple benchmarks for text-image alignment evaluation.\nAdditionally, our framework features a plug-and-play design, making it highly\nadaptable to various generative models.", "AI": {"tldr": "VisualPrompter\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u53cd\u601d\u6a21\u5757\u548c\u7ec6\u7c92\u5ea6\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u4e0e\u7528\u6237\u63cf\u8ff0\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u56fe\u50cf\u98ce\u683c\u548c\u7f8e\u5b66\uff0c\u4f46\u5e38\u5ffd\u89c6\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u5185\u5bb9\u4e0d\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u63d0\u51faVisualPrompter\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u53cd\u601d\u6a21\u5757\u8bc6\u522b\u7f3a\u5931\u6982\u5ff5\uff0c\u4ee5\u53ca\u76ee\u6807\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u673a\u5236\u7ec6\u7c92\u5ea6\u4fee\u8ba2\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5177\u6709\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u3002", "conclusion": "VisualPrompter\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2506.23782", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23782", "abs": "https://arxiv.org/abs/2506.23782", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5c0f\u6ce2\u7684\u8282\u70b9\u7279\u5b9a\u6e29\u5ea6\u7f29\u653e\u65b9\u6cd5\uff08WATS\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6821\u51c6\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4e0e\u5b9e\u9645\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7edf\u8ba1\u6216\u6f5c\u5728\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u56fe\u62d3\u6251\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51faWATS\u6846\u67b6\uff0c\u5229\u7528\u53ef\u8c03\u70ed\u6838\u56fe\u5c0f\u6ce2\u7279\u5f81\u4e3a\u8282\u70b9\u5206\u914d\u7279\u5b9a\u6e29\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6216\u8bbf\u95ee\u90bb\u57df\u4fe1\u606f\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cWATS\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u9ad8\u63d0\u534742.3%\uff0c\u5e73\u5747\u51cf\u5c11\u6821\u51c6\u65b9\u5dee17.24%\u3002", "conclusion": "WATS\u901a\u8fc7\u56fe\u5c0f\u6ce2\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6821\u51c6\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u5bc6\u5ea6\u7684\u56fe\u3002"}}
{"id": "2506.23150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23150", "abs": "https://arxiv.org/abs/2506.23150", "authors": ["Xinyue Liang", "Zhiyuan Ma", "Lingchen Sun", "Yanjun Guo", "Lei Zhang"], "title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation", "comment": null, "summary": "Single-image-to-3D models typically follow a sequential generation and\nreconstruction workflow. However, intermediate multi-view images synthesized by\npre-trained generation models often lack cross-view consistency (CVC),\nsignificantly degrading 3D reconstruction performance. While recent methods\nattempt to refine CVC by feeding reconstruction results back into the\nmulti-view generator, these approaches struggle with noisy and unstable\nreconstruction outputs that limit effective CVC improvement. We introduce\nAlignCVC, a novel framework that fundamentally re-frames single-image-to-3D\ngeneration through distribution alignment rather than relying on strict\nregression losses. Our key insight is to align both generated and reconstructed\nmulti-view distributions toward the ground-truth multi-view distribution,\nestablishing a principled foundation for improved CVC. Observing that generated\nimages exhibit weak CVC while reconstructed images display strong CVC due to\nexplicit rendering, we propose a soft-hard alignment strategy with distinct\nobjectives for generation and reconstruction models. This approach not only\nenhances generation quality but also dramatically accelerates inference to as\nfew as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,\nseamlessly integrates various multi-view generation models with 3D\nreconstruction models. Extensive experiments demonstrate the effectiveness and\nefficiency of AlignCVC for single-image-to-3D generation.", "AI": {"tldr": "AlignCVC\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u56fe\u50cf\u52303D\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u800c\u975e\u4e25\u683c\u56de\u5f52\u635f\u5931\u6765\u63d0\u5347\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\uff08CVC\uff09\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\uff0c\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u7f3a\u4e4f\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u5bfc\u81f43D\u91cd\u5efa\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u53cd\u9988\u91cd\u5efa\u7ed3\u679c\u6539\u8fdbCVC\uff0c\u4f46\u53d7\u9650\u4e8e\u566a\u58f0\u548c\u4e0d\u7a33\u5b9a\u7684\u8f93\u51fa\u3002", "method": "AlignCVC\u901a\u8fc7\u8f6f\u786c\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u751f\u6210\u548c\u91cd\u5efa\u7684\u591a\u89c6\u56fe\u5206\u5e03\u4e0e\u771f\u5b9e\u591a\u89c6\u56fe\u5206\u5e03\u5bf9\u9f50\uff0c\u5206\u522b\u4f18\u5316\u751f\u6210\u548c\u91cd\u5efa\u6a21\u578b\u7684\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAlignCVC\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\uff08\u4ec5\u97004\u6b65\uff09\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u591a\u79cd\u591a\u89c6\u56fe\u751f\u6210\u548c3D\u91cd\u5efa\u6a21\u578b\u3002", "conclusion": "AlignCVC\u4e3a\u5355\u56fe\u50cf\u52303D\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u663e\u8457\u6539\u5584\u4e86\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.23799", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23799", "abs": "https://arxiv.org/abs/2506.23799", "authors": ["Jiongli Zhu", "Parjanya Prajakta Prashant", "Alex Cloninger", "Babak Salimi"], "title": "KAIROS: Scalable Model-Agnostic Data Valuation", "comment": "19 pages, 9 figures", "summary": "Training data increasingly shapes not only model accuracy but also regulatory\ncompliance and market valuation of AI assets. Yet existing valuation methods\nremain inadequate: model-based techniques depend on a single fitted model and\ninherit its biases, while algorithm-based approaches such as Data Shapley\nrequire costly retrainings at web scale. Recent Wasserstein-based\nmodel-agnostic methods rely on approximations that misrank examples relative to\ntheir true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,\nmodel-agnostic valuation framework that assigns each example a distributional\ninfluence score: its contribution to the Maximum Mean Discrepancy (MMD) between\nthe empirical training distribution and a clean reference set. Unlike\nWasserstein surrogates, our MMD-based influence admits a closed-form solution\nthat faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,\nrequires no retraining, and naturally extends to conditional kernels for\nunified label- and feature-error detection. Moreover, KAIROS supports efficient\nonline updates: when a new batch of size m arrives, all scores can be updated\nin $O(mN)$ time, delivering up to 50x speedup without compromising ranking\nquality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks\nshow that KAIROS consistently outperforms state-of-the-art model-, Shapley-,\nand Wasserstein-based baselines in both accuracy and runtime. We provide\nrigorous theoretical guarantees, including symmetry for reproducible rankings\nand density-separation for interpretable thresholds.", "AI": {"tldr": "KAIROS\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6a21\u578b\u65e0\u5173\u4f30\u503c\u6846\u67b6\uff0c\u901a\u8fc7MMD\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u5206\u6570\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5b58\u5728\u504f\u5dee\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "KAIROS\u57fa\u4e8eMMD\u8ba1\u7b97\u6bcf\u4e2a\u6837\u672c\u5bf9\u8bad\u7ec3\u5206\u5e03\u7684\u5f71\u54cd\u5206\u6570\uff0c\u652f\u6301\u5728\u7ebf\u66f4\u65b0\u548c\u6761\u4ef6\u6838\u6269\u5c55\u3002", "result": "KAIROS\u5728\u566a\u58f0\u3001\u9519\u8bef\u6807\u6ce8\u548c\u6c61\u67d3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u534750\u500d\u3002", "conclusion": "KAIROS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2506.23151", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.23151", "abs": "https://arxiv.org/abs/2506.23151", "authors": ["Vladislav Bargatin", "Egor Chistov", "Alexander Yakovenko", "Dmitriy Vatolin"], "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation", "comment": "Accepted at ICCV 2025", "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.", "AI": {"tldr": "MEMFOF\u662f\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u591a\u5e27\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\uff081080p\uff09\u8f93\u5165\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u6d88\u8017\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5149\u6d41\u4f30\u8ba1\u4e2d\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8GPU\u5185\u5b58\u6d88\u8017\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u4e0b\u3002", "method": "\u901a\u8fc7\u4f18\u5316RAFT-like\u67b6\u6784\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5305\u62ec\u51cf\u5c11\u76f8\u5173\u6027\u4f53\u79ef\u548c\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\uff0c\u7ed3\u5408\u591a\u5e27\u4f30\u8ba1\uff0c\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u7684\u5149\u6d41\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982Spring\u3001Sintel\u548cKITTI-2015\uff0c\u540c\u65f6\u8fd0\u884c\u65f6\u4ec5\u97002.09GB GPU\u5185\u5b58\uff08\u8bad\u7ec3\u65f628.5GB\uff09\u3002", "conclusion": "MEMFOF\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u5149\u6d41\u4f30\u8ba1\u3002"}}
{"id": "2506.23800", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23800", "abs": "https://arxiv.org/abs/2506.23800", "authors": ["Chang Qi", "Matteo Forasassi", "Thomas Lukasiewicz", "Tommaso Salvatori"], "title": "Towards the Training of Deeper Predictive Coding Neural Networks", "comment": "18 Pages, 7 figures", "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u5728\u6df1\u5c42\u67b6\u6784\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u4f18\u5316\u6f5c\u5728\u53d8\u91cf\u548c\u6743\u91cd\u66f4\u65b0\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5c42\u7f51\u7edc\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u5728\u8d85\u8fc7\u4e94\u5230\u4e03\u5c42\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u6df1\u5c42\u7f51\u7edc\u8bad\u7ec3\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u4f18\u5316\u6f5c\u5728\u53d8\u91cf\u7684\u65b9\u6cd5\uff08\u4f7f\u7528\u7cbe\u5ea6\u52a0\u6743\uff09\u548c\u65b0\u7684\u6743\u91cd\u66f4\u65b0\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u6df1\u5c42\u7f51\u7edc\u7684\u8bef\u5dee\u79ef\u7d2f\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\uff0c\u6df1\u5c42\u7f51\u7edc\uff08\u8d85\u8fc7\u4e03\u5c42\uff09\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u63a5\u8fd1\u53cd\u5411\u4f20\u64ad\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u66f4\u597d\u5730\u7406\u89e3\u677e\u5f1b\u9636\u6bb5\u5bf9\u8bad\u7ec3\u6df1\u5c42\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.23153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23153", "abs": "https://arxiv.org/abs/2506.23153", "authors": ["Huiqiang Sun", "Xingyi Li", "Juewen Peng", "Liao Shen", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "title": "Dynamic View Synthesis from Small Camera Motion Videos", "comment": "Accepted by TVCG", "summary": "Novel view synthesis for dynamic $3$D scenes poses a significant challenge.\nMany notable efforts use NeRF-based approaches to address this task and yield\nimpressive results. However, these methods rely heavily on sufficient motion\nparallax in the input images or videos. When the camera motion range becomes\nlimited or even stationary (i.e., small camera motion), existing methods\nencounter two primary challenges: incorrect representation of scene geometry\nand inaccurate estimation of camera parameters. These challenges make prior\nmethods struggle to produce satisfactory results or even become invalid. To\naddress the first challenge, we propose a novel Distribution-based Depth\nRegularization (DDR) that ensures the rendering weight distribution to align\nwith the true distribution. Specifically, unlike previous methods that use\ndepth loss to calculate the error of the expectation, we calculate the\nexpectation of the error by using Gumbel-softmax to differentiably sample\npoints from discrete rendering weight distribution. Additionally, we introduce\nconstraints that enforce the volume density of spatial points before the object\nboundary along the ray to be near zero, ensuring that our model learns the\ncorrect geometry of the scene. To demystify the DDR, we further propose a\nvisualization tool that enables observing the scene geometry representation at\nthe rendering weight level. For the second challenge, we incorporate camera\nparameter learning during training to enhance the robustness of our model to\ncamera parameters. We conduct extensive experiments to demonstrate the\neffectiveness of our approach in representing scenes with small camera motion\ninput, and our results compare favorably to state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u6df1\u5ea6\u6b63\u5219\u5316\uff08DDR\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u52a8\u60013D\u573a\u666f\u4e2d\u65b0\u89c6\u89d2\u5408\u6210\u5728\u76f8\u673a\u8fd0\u52a8\u53d7\u9650\u65f6\u7684\u51e0\u4f55\u8868\u793a\u548c\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u52a8\u60013D\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u5728\u76f8\u673a\u8fd0\u52a8\u8303\u56f4\u53d7\u9650\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u8868\u793a\u573a\u666f\u51e0\u4f55\u548c\u4f30\u8ba1\u76f8\u673a\u53c2\u6570\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faDDR\u65b9\u6cd5\uff0c\u901a\u8fc7Gumbel-softmax\u91c7\u6837\u70b9\u8ba1\u7b97\u8bef\u5dee\u671f\u671b\uff0c\u5e76\u5f15\u5165\u7ea6\u675f\u786e\u4fdd\u573a\u666f\u51e0\u4f55\u6b63\u786e\uff1b\u540c\u65f6\u7ed3\u5408\u76f8\u673a\u53c2\u6570\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u76f8\u673a\u8fd0\u52a8\u8f93\u5165\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DDR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u673a\u8fd0\u52a8\u53d7\u9650\u65f6\u7684\u573a\u666f\u8868\u793a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23802", "abs": "https://arxiv.org/abs/2506.23802", "authors": ["Konstantinos Bourazas", "Savvas Papaioannou", "Panayiotis Kolios"], "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations", "comment": "23rd European Control Conference (ECC 2025), Thessaloniki, Greece,\n  24-27 June 2025", "summary": "In this work we introduce a novel adaptive anomaly detection framework\nspecifically designed for monitoring sequential random finite set (RFS)\nobservations. Our approach effectively distinguishes between In-Control data\n(normal) and Out-Of-Control data (anomalies) by detecting deviations from the\nexpected statistical behavior of the process. The primary contributions of this\nstudy include the development of an innovative RFS-based framework that not\nonly learns the normal behavior of the data-generating process online but also\ndynamically adapts to behavioral shifts to accurately identify abnormal point\npatterns. To achieve this, we introduce a new class of RFS-based posterior\ndistributions, named Power Discounting Posteriors (PD), which facilitate\nadaptation to systematic changes in data while enabling anomaly detection of\npoint pattern data through a novel predictive posterior density function. The\neffectiveness of the proposed approach is demonstrated by extensive qualitative\nand quantitative simulation experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5e8f\u5217\u968f\u673a\u6709\u9650\u96c6\u89c2\u6d4b\u7684\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u7edf\u8ba1\u884c\u4e3a\u504f\u5dee\u6765\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38\u6570\u636e\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6b63\u5e38\u884c\u4e3a\u5e76\u52a8\u6001\u9002\u5e94\u884c\u4e3a\u53d8\u5316\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u51c6\u786e\u8bc6\u522b\u5f02\u5e38\u70b9\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u6709\u9650\u96c6\u540e\u9a8c\u5206\u5e03\u7c7b\u522b\uff08Power Discounting Posteriors, PD\uff09\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9884\u6d4b\u540e\u9a8c\u5bc6\u5ea6\u51fd\u6570\u5b9e\u73b0\u6570\u636e\u7cfb\u7edf\u6027\u53d8\u5316\u7684\u9002\u5e94\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u9002\u5e94\u6570\u636e\u884c\u4e3a\u53d8\u5316\u5e76\u51c6\u786e\u68c0\u6d4b\u5f02\u5e38\u70b9\u6a21\u5f0f\u3002"}}
{"id": "2506.23156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23156", "abs": "https://arxiv.org/abs/2506.23156", "authors": ["Jiale Chen"], "title": "Self-Supervised Contrastive Learning for Multi-Label Images", "comment": null, "summary": "Self-supervised learning (SSL) has demonstrated its effectiveness in learning\nrepresentations through comparison methods that align with human intuition.\nHowever, mainstream SSL methods heavily rely on high body datasets with single\nlabel, such as ImageNet, resulting in intolerable pre-training overhead.\nBesides, more general multi-label images are frequently overlooked in SSL,\ndespite their potential for richer semantic information and broader\napplicability in downstream scenarios. Therefore, we tailor the mainstream SSL\napproach to guarantee excellent representation learning capabilities using\nfewer multi-label images. Firstly, we propose a block-wise augmentation module\naimed at extracting additional potential positive view pairs from multi-label\nimages. Subsequently, an image-aware contrastive loss is devised to establish\nconnections between these views, thereby facilitating the extraction of\nsemantically consistent representations. Comprehensive linear fine-tuning and\ntransfer learning validate the competitiveness of our approach despite\nchallenging sample quality and quantity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6807\u7b7e\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7ea7\u589e\u5f3a\u548c\u56fe\u50cf\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u51cf\u5c11\u4e86\u9884\u8bad\u7ec3\u5f00\u9500\u5e76\u63d0\u5347\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3b\u6d41\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5355\u6807\u7b7e\u9ad8\u4f53\u91cf\u6570\u636e\u96c6\uff08\u5982ImageNet\uff09\uff0c\u5ffd\u7565\u4e86\u591a\u6807\u7b7e\u56fe\u50cf\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u9884\u8bad\u7ec3\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa\u5757\u7ea7\u589e\u5f3a\u6a21\u5757\u63d0\u53d6\u591a\u6807\u7b7e\u56fe\u50cf\u7684\u6f5c\u5728\u6b63\u89c6\u56fe\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u56fe\u50cf\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\u4ee5\u5efa\u7acb\u89c6\u56fe\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u5728\u7ebf\u6027\u5fae\u8c03\u548c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\uff0c\u5c3d\u7ba1\u6837\u672c\u8d28\u91cf\u548c\u6570\u91cf\u6709\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6807\u7b7e\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u9884\u8bad\u7ec3\u5f00\u9500\u5e76\u63d0\u5347\u4e86\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2506.23803", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23803", "abs": "https://arxiv.org/abs/2506.23803", "authors": ["Dmitry Kovalev"], "title": "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration", "comment": null, "summary": "In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type\npreconditioning. Our contributions are twofold. First, we develop a unified\nconvergence analysis of SGD with adaptive preconditioning under anisotropic or\nmatrix smoothness and noise assumptions. This allows us to recover\nstate-of-the-art convergence results for several popular adaptive gradient\nmethods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In\naddition, we establish the fundamental connection between two recently proposed\nalgorithms, Scion and DASGO, and provide the first theoretical guarantees for\nthe latter. Second, we show that the convergence of methods like AdaGrad and\nDASGO can be provably accelerated beyond the best-known rates using Nesterov\nmomentum. Consequently, we obtain the first theoretical justification that\nAdaGrad-type algorithms can simultaneously benefit from both diagonal\npreconditioning and momentum, which may provide an ultimate explanation for the\npractical efficiency of Adam.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5e26\u6709AdaGrad\u578b\u9884\u5904\u7406\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6536\u655b\u5206\u6790\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86AdaGrad\u548cDASGO\u7b49\u65b9\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u53ef\u4ee5\u901a\u8fc7Nesterov\u52a8\u91cf\u52a0\u901f\u3002", "motivation": "\u7814\u7a76SGD\u4e0e\u81ea\u9002\u5e94\u9884\u5904\u7406\u65b9\u6cd5\u7684\u6536\u655b\u6027\uff0c\u7279\u522b\u662fAdaGrad\u578b\u7b97\u6cd5\uff0c\u4ee5\u89e3\u91ca\u5176\u5b9e\u9645\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u6536\u655b\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u5411\u5f02\u6027\u6216\u77e9\u9635\u5e73\u6ed1\u548c\u566a\u58f0\u5047\u8bbe\uff0c\u5e76\u5206\u6790\u4e86AdaGrad-Norm\u3001AdaGrad\u3001ASGO/One-sided Shampoo\u7b49\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86AdaGrad\u548cDASGO\u7684\u6536\u655b\u901f\u5ea6\u53ef\u4ee5\u901a\u8fc7Nesterov\u52a8\u91cf\u52a0\u901f\uff0c\u5e76\u9996\u6b21\u4e3aDASGO\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "AdaGrad\u578b\u7b97\u6cd5\u53ef\u4ee5\u540c\u65f6\u53d7\u76ca\u4e8e\u5bf9\u89d2\u9884\u5904\u7406\u548c\u52a8\u91cf\uff0c\u8fd9\u53ef\u80fd\u662fAdam\u5b9e\u9645\u9ad8\u6548\u7684\u6839\u672c\u539f\u56e0\u3002"}}
{"id": "2506.23157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23157", "abs": "https://arxiv.org/abs/2506.23157", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "comment": null, "summary": "High-dynamic scene reconstruction aims to represent static background with\nrigid spatial features and dynamic objects with deformed continuous\nspatiotemporal features. Typically, existing methods adopt unified\nrepresentation model (e.g., Gaussian) to directly match the spatiotemporal\nfeatures of dynamic scene from frame camera. However, this unified paradigm\nfails in the potential discontinuous temporal features of objects due to frame\nimaging and the heterogeneous spatial features between background and objects.\nTo address this issue, we disentangle the spatiotemporal features into various\nlatent representations to alleviate the spatiotemporal mismatching between\nbackground and objects. In this work, we introduce event camera to compensate\nfor frame camera, and propose a spatiotemporal-disentangled Gaussian splatting\nframework for high-dynamic scene reconstruction. As for dynamic scene, we\nfigure out that background and objects have appearance discrepancy in\nframe-based spatial features and motion discrepancy in event-based temporal\nfeatures, which motivates us to distinguish the spatiotemporal features between\nbackground and objects via clustering. As for dynamic object, we discover that\nGaussian representations and event data share the consistent spatiotemporal\ncharacteristic, which could serve as a prior to guide the spatiotemporal\ndisentanglement of object Gaussians. Within Gaussian splatting framework, the\ncumulative scene-object disentanglement can improve the spatiotemporal\ndiscrimination between background and objects to render the time-continuous\ndynamic scene. Extensive experiments have been performed to verify the\nsuperiority of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u8865\u507f\u5e27\u76f8\u673a\uff0c\u89e3\u51b3\u4e86\u80cc\u666f\u4e0e\u7269\u4f53\u65f6\u7a7a\u7279\u5f81\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u8868\u793a\u6a21\u578b\uff08\u5982\u9ad8\u65af\uff09\u76f4\u63a5\u5339\u914d\u52a8\u6001\u573a\u666f\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4f46\u7531\u4e8e\u5e27\u6210\u50cf\u548c\u80cc\u666f\u4e0e\u7269\u4f53\u7a7a\u95f4\u7279\u5f81\u7684\u5f02\u8d28\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u6f5c\u5728\u7684\u65f6\u7a7a\u4e0d\u8fde\u7eed\u6027\u3002", "method": "\u5f15\u5165\u4e8b\u4ef6\u76f8\u673a\u8865\u507f\u5e27\u76f8\u673a\uff0c\u63d0\u51fa\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u533a\u5206\u80cc\u666f\u4e0e\u7269\u4f53\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5229\u7528\u9ad8\u65af\u8868\u793a\u4e0e\u4e8b\u4ef6\u6570\u636e\u7684\u4e00\u81f4\u6027\u6307\u5bfc\u7269\u4f53\u9ad8\u65af\u65f6\u7a7a\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u63d0\u9ad8\u80cc\u666f\u4e0e\u7269\u4f53\u65f6\u7a7a\u533a\u5206\u80fd\u529b\uff0c\u5b9e\u73b0\u65f6\u95f4\u8fde\u7eed\u7684\u52a8\u6001\u573a\u666f\u6e32\u67d3\u3002", "conclusion": "\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u548c\u4e8b\u4ef6\u76f8\u673a\u8865\u507f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u65f6\u7a7a\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2506.23824", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23824", "abs": "https://arxiv.org/abs/2506.23824", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "comment": null, "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\u663e\u5f0f\u7ed3\u5408\u805a\u7c7b\u5047\u8bbe\uff0c\u7b80\u5316\u8bad\u7ec3\u7b56\u7565\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5b66\u4e60\u591a\u4f9d\u8d56\u590d\u6742\u7684\u6b63\u5219\u5316\u6216\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u805a\u7c7b\u5047\u8bbe\u7b80\u5316\u6a21\u578b\u5e76\u63d0\u5347\u6548\u679c\u3002", "method": "\u6269\u5c55\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u5229\u7528\u6807\u6ce8\u6570\u636e\u5f15\u5bfc\u805a\u7c7b\u4e2d\u5fc3\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u4ec5\u76d1\u7763\u57fa\u7ebf\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u534a\u76d1\u7763\u65b9\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6269\u5c55\u6027\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2506.23189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23189", "abs": "https://arxiv.org/abs/2506.23189", "authors": ["Mustafa Hakan Kara", "Aysegul Dundar", "U\u011fur G\u00fcd\u00fckbay"], "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning", "comment": "11 pages, 3 figures, and 7 tables", "summary": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrident\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5143\u7ec4\u5b66\u4e60\u548cSiamese\u7f51\u7edc\u67b6\u6784\u63d0\u5347\u5bf9\u4e0d\u540c\u4f2a\u9020\u65b9\u6cd5\u7684\u9002\u5e94\u6027\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u7684\u4eba\u8138\u4f2a\u9020\u6280\u672f\u65e5\u76ca\u590d\u6742\uff0c\u68c0\u6d4b\u6570\u5b57\u5a92\u4f53\u4e2d\u7684\u4eba\u8138\u7be1\u6539\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u7ef4\u62a4\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u548c\u6253\u51fb\u89c6\u89c9\u865a\u5047\u4fe1\u606f\u7684\u91cd\u8981\u6027\u51f8\u663e\u3002\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u4f2a\u9020\u6280\u672f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Trident\u6846\u67b6\u91c7\u7528\u4e09\u5143\u7ec4\u5b66\u4e60\u548cSiamese\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e09\u5143\u7ec4\u8bad\u7ec3\u6570\u636e\u6355\u6349\u4f2a\u9020\u6837\u672c\u7684\u7ec6\u5fae\u5dee\u5f02\u3002\u540c\u65f6\u7ed3\u5408\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u548c\u4f2a\u9020\u5224\u522b\u5668\uff0c\u751f\u6210\u5bf9\u4f2a\u9020\u65b9\u6cd5\u4e0d\u654f\u611f\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u963b\u6b62\u68af\u5ea6\u56de\u4f20\u907f\u514d\u8fc7\u62df\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6d88\u878d\u5b9e\u9a8c\u4e2d\uff0cTrident\u6846\u67b6\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Trident\u6846\u67b6\u901a\u8fc7\u4e09\u5143\u7ec4\u5b66\u4e60\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5e94\u5bf9\u590d\u6742\u4f2a\u9020\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23843", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23843", "abs": "https://arxiv.org/abs/2506.23843", "authors": ["Joris Bekkers"], "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment", "comment": null, "summary": "Understanding team formations and player positioning is crucial for tactical\nanalysis in football (soccer). This paper presents a flexible method for\nformation recognition and player position assignment in football using\npredefined static formation templates and cost minimization from spatiotemporal\ntracking data, called EFPI. Our approach employs linear sum assignment to\noptimally match players to positions within a set of template formations by\nminimizing the total distance between actual player locations and template\npositions, subsequently selecting the formation with the lowest assignment\ncost. To improve accuracy, we scale actual player positions to match the\ndimensions of these formation templates in both width and length. While the\nmethod functions effectively on individual frames, it extends naturally to\nlarger game segments such as complete periods, possession sequences or specific\nintervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we\nincorporate an optional stability parameter that prevents unnecessary formation\nchanges when assignment costs differ only marginally between time segments.\nEFPI is available as open-source code through the unravelsports Python package.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u9635\u578b\u6a21\u677f\u548c\u6210\u672c\u6700\u5c0f\u5316\u7684\u8db3\u7403\u9635\u578b\u8bc6\u522b\u65b9\u6cd5EFPI\uff0c\u901a\u8fc7\u7ebf\u6027\u5206\u914d\u4f18\u5316\u7403\u5458\u4f4d\u7f6e\u5339\u914d\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u7a33\u5b9a\u6027\u5206\u6790\u3002", "motivation": "\u8db3\u7403\u6218\u672f\u5206\u6790\u9700\u8981\u51c6\u786e\u8bc6\u522b\u7403\u961f\u9635\u578b\u548c\u7403\u5458\u4f4d\u7f6e\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u9884\u5b9a\u4e49\u9759\u6001\u9635\u578b\u6a21\u677f\uff0c\u901a\u8fc7\u7ebf\u6027\u603b\u548c\u5206\u914d\u4f18\u5316\u7403\u5458\u4f4d\u7f6e\u5339\u914d\uff0c\u5e76\u5f15\u5165\u7a33\u5b9a\u6027\u53c2\u6570\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u9635\u578b\u53d8\u5316\u3002", "result": "EFPI\u80fd\u6709\u6548\u8bc6\u522b\u9635\u578b\u5e76\u5206\u914d\u7403\u5458\u4f4d\u7f6e\uff0c\u652f\u6301\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u5206\u6790\uff0c\u4e14\u5f00\u6e90\u5b9e\u73b0\u53ef\u7528\u3002", "conclusion": "EFPI\u4e3a\u8db3\u7403\u6218\u672f\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7a33\u5b9a\u7684\u9635\u578b\u8bc6\u522b\u5de5\u5177\u3002"}}
{"id": "2506.23196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23196", "abs": "https://arxiv.org/abs/2506.23196", "authors": ["Mona Ahmadian", "Amir Shirian", "Frank Guerin", "Andrew Gilbert"], "title": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding", "comment": null, "summary": "Real-world videos often contain overlapping events and complex temporal\ndependencies, making multimodal interaction modeling particularly challenging.\nWe introduce DEL, a framework for dense semantic action localization, aiming to\naccurately detect and classify multiple actions at fine-grained temporal\nresolutions in long untrimmed videos. DEL consists of two key modules: the\nalignment of audio and visual features that leverage masked self-attention to\nenhance intra-mode consistency and a multimodal interaction refinement module\nthat models cross-modal dependencies across multiple scales, enabling\nhigh-level semantics and fine-grained details. Our method achieves\nstate-of-the-art performance on multiple real-world Temporal Action\nLocalization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and\nEPIC-Kitchens-100, surpassing previous approaches with notable average mAP\ngains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.", "AI": {"tldr": "DEL\u6846\u67b6\u901a\u8fc7\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u53ca\u591a\u6a21\u6001\u4ea4\u4e92\u7ec6\u5316\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u957f\u672a\u526a\u8f91\u89c6\u9891\u4e2d\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u548c\u5206\u7c7b\u591a\u52a8\u4f5c\uff0c\u5e76\u5728\u591a\u4e2aTAL\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u7684\u91cd\u53e0\u4e8b\u4ef6\u548c\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u4f7f\u5f97\u591a\u6a21\u6001\u4ea4\u4e92\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u68c0\u6d4b\u548c\u5206\u7c7b\u591a\u52a8\u4f5c\u7684\u65b9\u6cd5\u3002", "method": "DEL\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5229\u7528\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u6001\u5185\u4e00\u81f4\u6027\u7684\u97f3\u9891\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u53ca\u5efa\u6a21\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728UnAV-100\u3001THUMOS14\u3001ActivityNet 1.3\u548cEPIC-Kitchens-100\u6570\u636e\u96c6\u4e0a\uff0cDEL\u7684\u5e73\u5747mAP\u5206\u522b\u63d0\u5347\u4e86+3.3%\u3001+2.6%\u3001+1.2%\u3001+1.7%\uff08\u52a8\u8bcd\uff09\u548c+1.4%\uff08\u540d\u8bcd\uff09\u3002", "conclusion": "DEL\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u4ea4\u4e92\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u8bed\u4e49\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u89c6\u9891\u573a\u666f\u3002"}}
{"id": "2506.23845", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23845", "abs": "https://arxiv.org/abs/2506.23845", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "comment": null, "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "AI": {"tldr": "SAEs\u5728\u5df2\u77e5\u6982\u5ff5\u4e0a\u6548\u679c\u6709\u9650\uff0c\u4f46\u5728\u53d1\u73b0\u672a\u77e5\u6982\u5ff5\u4e0a\u8868\u73b0\u5f3a\u5927\uff0c\u9002\u7528\u4e8eML\u53ef\u89e3\u91ca\u6027\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u5173\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u6709\u6548\u6027\u7684\u4e89\u8bae\uff0c\u660e\u786e\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u533a\u5206\uff0c\u5c06SAEs\u5728\u5df2\u77e5\u548c\u672a\u77e5\u6982\u5ff5\u4e0a\u7684\u8868\u73b0\u5206\u5f00\u8ba8\u8bba\u3002", "result": "SAEs\u5728\u53d1\u73b0\u672a\u77e5\u6982\u5ff5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u4e2a\u5e94\u7528\u9886\u57df\u3002", "conclusion": "SAEs\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u53d1\u73b0\u65b0\u6982\u5ff5\u548c\u8de8\u5b66\u79d1\u5e94\u7528\u4e2d\u3002"}}
{"id": "2506.23202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23202", "abs": "https://arxiv.org/abs/2506.23202", "authors": ["Qilin Shu", "Qixian Zhang", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing", "comment": null, "summary": "The person search task aims to locate a target person within a set of scene\nimages. In recent years, transformer-based models in this field have made some\nprogress. However, they still face three primary challenges: 1) the\nself-attention mechanism tends to suppress high-frequency components in the\nfeatures, which severely impacts model performance; 2) the computational cost\nof transformers is relatively high. To address these issues, we propose a novel\nHigh-frequency Augmentation and Multi-Wave mixing (HAMW) method for person\nsearch. HAMW is designed to enhance the discriminative feature extraction\ncapabilities of transformers while reducing computational overhead and\nimproving efficiency. Specifically, we develop a three-stage framework that\nprogressively optimizes both detection and re-identification performance. Our\nmodel enhances the perception of high-frequency features by learning from\naugmented inputs containing additional high-frequency components. Furthermore,\nwe replace the self-attention layers in the transformer with a strategy based\non multi-level Haar wavelet fusion to capture multi-scale features. This not\nonly lowers the computational complexity but also alleviates the suppression of\nhigh-frequency features and enhances the ability to exploit multi-scale\ninformation. Extensive experiments demonstrate that HAMW achieves\nstate-of-the-art performance on both the CUHK-SYSU and PRW datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAMW\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u9891\u589e\u5f3a\u548c\u591a\u6ce2\u6df7\u5408\u6280\u672f\u6539\u8fdb\u57fa\u4e8etransformer\u7684\u4eba\u7269\u641c\u7d22\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6291\u5236\u9ad8\u9891\u7279\u5f81\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8etransformer\u7684\u4eba\u7269\u641c\u7d22\u6a21\u578b\u5b58\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6291\u5236\u9ad8\u9891\u7279\u5f81\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faHAMW\u65b9\u6cd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\u4f18\u5316\u68c0\u6d4b\u548c\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u901a\u8fc7\u9ad8\u9891\u589e\u5f3a\u8f93\u5165\u548c\u591a\u7ea7Haar\u5c0f\u6ce2\u878d\u5408\u7b56\u7565\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u5c42\u3002", "result": "\u5728CUHK-SYSU\u548cPRW\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HAMW\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u7269\u641c\u7d22\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.23872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23872", "abs": "https://arxiv.org/abs/2506.23872", "authors": ["Eduard Buss", "Till Aust", "Heiko Hamann"], "title": "When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems", "comment": "Submitted and Accepted at the 14th international conference on\n  biomimetic and biohybrid systems (Living Machines)", "summary": "Living plants, while contributing to ecological balance and climate\nregulation, also function as natural sensors capable of transmitting\ninformation about their internal physiological states and surrounding\nconditions. This rich source of data provides potential for applications in\nenvironmental monitoring and precision agriculture. With integration into\nbiohybrid systems, we establish novel channels of physiological signal flow\nbetween living plants and artificial devices. We equipped *Hedera helix* with a\nplant-wearable device called PhytoNode to continuously record the plant's\nelectrophysiological activity. We deployed plants in an uncontrolled outdoor\nenvironment to map electrophysiological patterns to environmental conditions.\nOver five months, we collected data that we analyzed using state-of-the-art and\nautomated machine learning (AutoML). Our classification models achieve high\nperformance, reaching macro F1 scores of up to 95 percent in binary tasks.\nAutoML approaches outperformed manual tuning, and selecting subsets of\nstatistical features further improved accuracy. Our biohybrid living system\nmonitors the electrophysiology of plants in harsh, real-world conditions. This\nwork advances scalable, self-sustaining, and plant-integrated living biohybrid\nsystems for sustainable environmental monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u690d\u7269\u53ef\u7a7f\u6234\u8bbe\u5907PhytoNode\uff0c\u7528\u4e8e\u8fde\u7eed\u8bb0\u5f55\u690d\u7269\u7535\u751f\u7406\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u6790\u73af\u5883\u6761\u4ef6\u4e0e\u7535\u751f\u7406\u6a21\u5f0f\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u5206\u7c7b\u6a21\u578b\u3002", "motivation": "\u5229\u7528\u690d\u7269\u4f5c\u4e3a\u81ea\u7136\u4f20\u611f\u5668\uff0c\u76d1\u6d4b\u5176\u751f\u7406\u72b6\u6001\u548c\u5468\u56f4\u73af\u5883\u6761\u4ef6\uff0c\u4e3a\u73af\u5883\u76d1\u6d4b\u548c\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "method": "\u5728\u6237\u5916\u73af\u5883\u4e2d\u90e8\u7f72\u914d\u5907PhytoNode\u7684\u5e38\u6625\u85e4\uff0c\u8fde\u7eed\u8bb0\u5f55\u7535\u751f\u7406\u6d3b\u52a8\uff0c\u5e76\u4f7f\u7528\u81ea\u52a8\u673a\u5668\u5b66\u4e60\uff08AutoML\uff09\u5206\u6790\u6570\u636e\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u5728\u4e8c\u5143\u4efb\u52a1\u4e2d\u8fbe\u523095%\u7684\u5b8fF1\u5206\u6570\uff0cAutoML\u65b9\u6cd5\u4f18\u4e8e\u624b\u52a8\u8c03\u53c2\uff0c\u7edf\u8ba1\u7279\u5f81\u5b50\u96c6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u6301\u7684\u690d\u7269\u96c6\u6210\u751f\u7269\u6df7\u5408\u7cfb\u7edf\uff0c\u4e3a\u53ef\u6301\u7eed\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.23205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23205", "abs": "https://arxiv.org/abs/2506.23205", "authors": ["Dequan Kong", "Zhe Zhu", "Honghua Chen", "Mingqiang Wei"], "title": "BridgeShape: Latent Diffusion Schr\u00f6dinger Bridge for 3D Shape Completion", "comment": null, "summary": "Existing diffusion-based 3D shape completion methods typically use a\nconditional paradigm, injecting incomplete shape information into the denoising\nnetwork via deep feature interactions (e.g., concatenation, cross-attention) to\nguide sampling toward complete shapes, often represented by voxel-based\ndistance functions. However, these approaches fail to explicitly model the\noptimal global transport path, leading to suboptimal completions. Moreover,\nperforming diffusion directly in voxel space imposes resolution constraints,\nlimiting the generation of fine-grained geometric details. To address these\nchallenges, we propose BridgeShape, a novel framework for 3D shape completion\nvia latent diffusion Schr\\\"odinger bridge. The key innovations lie in two\naspects: (i) BridgeShape formulates shape completion as an optimal transport\nproblem, explicitly modeling the transition between incomplete and complete\nshapes to ensure a globally coherent transformation. (ii) We introduce a\nDepth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D\nshapes into a compact latent space, leveraging self-projected multi-view depth\ninformation enriched with strong DINOv2 features to enhance geometric\nstructural perception. By operating in a compact yet structurally informative\nlatent space, BridgeShape effectively mitigates resolution constraints and\nenables more efficient and high-fidelity 3D shape completion. BridgeShape\nachieves state-of-the-art performance on large-scale 3D shape completion\nbenchmarks, demonstrating superior fidelity at higher resolutions and for\nunseen object classes.", "AI": {"tldr": "BridgeShape\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563Schr\u00f6dinger\u6865\u76843D\u5f62\u72b6\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6700\u4f18\u5168\u5c40\u4f20\u8f93\u8def\u5f84\u548c\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c\u51e0\u4f55\u7ec6\u8282\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u76843D\u5f62\u72b6\u8865\u5168\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u6700\u4f18\u5168\u5c40\u4f20\u8f93\u8def\u5f84\uff0c\u4e14\u53d7\u9650\u4e8e\u4f53\u7d20\u7a7a\u95f4\u7684\u5206\u8fa8\u7387\u7ea6\u675f\uff0c\u5bfc\u81f4\u8865\u5168\u6548\u679c\u4e0d\u4f73\u3002", "method": "BridgeShape\u5c06\u5f62\u72b6\u8865\u5168\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6df1\u5ea6\u589e\u5f3a\u7684VQ-VAE\u7f16\u78013D\u5f62\u72b6\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6df1\u5ea6\u4fe1\u606f\u548cDINOv2\u7279\u5f81\u589e\u5f3a\u51e0\u4f55\u611f\u77e5\u3002", "result": "BridgeShape\u5728\u5927\u89c4\u6a213D\u5f62\u72b6\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u7684\u8865\u5168\u3002", "conclusion": "BridgeShape\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u8def\u5f84\u548c\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5f62\u72b6\u8865\u5168\u7684\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2506.23875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23875", "abs": "https://arxiv.org/abs/2506.23875", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "comment": "14 pages, 10 figures", "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u6392\u5e8f\u89e3\u7801\u5668\u8f93\u5165\u4ee4\u724c\u7684\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316Transformer\u5728\u7b97\u672f\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u987a\u5e8f\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u5c42\u65b9\u6cd5\uff0c\u4ece\u6570\u5341\u4ebf\u5019\u9009\u987a\u5e8f\u4e2d\u7b5b\u9009\u51fa\u5b66\u4e60\u53cb\u597d\u987a\u5e8f\u3002", "motivation": "\u63a2\u7d22\u4e2d\u95f4\u6b65\u9aa4\u7684\u987a\u5e8f\u5982\u4f55\u5f71\u54cdTransformer\u7684\u63a8\u7406\u96be\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u987a\u5e8f\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u8bad\u7ec3Transformer\u5728\u4e0d\u540c\u987a\u5e8f\u7684\u76ee\u6807\u5e8f\u5217\u4e0a\uff0c\u901a\u8fc7\u65e9\u671f\u635f\u5931\u4e0b\u964d\u901f\u5ea6\u7b5b\u9009\u826f\u6027\u987a\u5e8f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5c42\u65b9\u6cd5\u8fdb\u884c\u5757\u95f4\u548c\u5757\u5185\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5728\u56db\u4e2a\u987a\u5e8f\u654f\u611f\u7684\u7b97\u672f\u4efb\u52a1\u4e2d\u6210\u529f\u7b5b\u9009\u51fa\u5b66\u4e60\u53cb\u597d\u987a\u5e8f\uff0c\u5e76\u5728\u4e58\u6cd5\u4efb\u52a1\u4e2d\u590d\u73b0\u4e86\u5148\u524d\u7814\u7a76\u4e2d\u62a5\u544a\u7684\u53cd\u5411\u6570\u5b57\u987a\u5e8f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4f18\u5316\u5b66\u4e60\u987a\u5e8f\uff0c\u4e3aTransformer\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23207", "abs": "https://arxiv.org/abs/2506.23207", "authors": ["Zhen Tan", "Xieyuanli Chen", "Lei Feng", "Yangbing Ge", "Shuaifeng Zhi", "Jiaxiong Liu", "Dewen Hu"], "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM\nsystems to achieve high-fidelity scene representation. However, the heavy\nreliance of existing systems on photometric rendering loss for camera tracking\nundermines their robustness, especially in unbounded outdoor environments with\nsevere viewpoint and illumination changes. To address these challenges, we\npropose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel\ntri-view geometry paradigm to ensure consistent tracking and high-quality\nmapping. We introduce a dense tri-view matching module that aggregates reliable\npairwise correspondences into consistent tri-view matches, forming robust\ngeometric constraints across frames. For tracking, we propose Hybrid Geometric\nConstraints, which leverage tri-view matches to construct complementary\ngeometric cues alongside photometric loss, ensuring accurate and stable pose\nestimation even under drastic viewpoint shifts and lighting variations. For\nmapping, we propose a new probabilistic initialization strategy that encodes\ngeometric uncertainty from tri-view correspondences into newly initialized\nGaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust\nmechanism to mitigate tracking drift caused by mapping latency. Experiments on\nmultiple public outdoor datasets show that our TVG-SLAM outperforms prior\nRGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our\nmethod improves tracking robustness, reducing the average Absolute Trajectory\nError (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The\nimplementation of our method will be released as open-source.", "AI": {"tldr": "TVG-SLAM\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684RGB-only SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u89c6\u56fe\u51e0\u4f55\u8303\u5f0f\u63d0\u5347\u8ddf\u8e2a\u548c\u5efa\u56fe\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6237\u5916\u73af\u5883\u3002", "motivation": "\u73b0\u6709RGB-only SLAM\u7cfb\u7edf\u4f9d\u8d56\u5149\u5ea6\u6e32\u67d3\u635f\u5931\uff0c\u5728\u6237\u5916\u65e0\u8fb9\u754c\u73af\u5883\u4e2d\u56e0\u89c6\u89d2\u548c\u5149\u7167\u53d8\u5316\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e09\u89c6\u56fe\u5339\u914d\u6a21\u5757\u548c\u6df7\u5408\u51e0\u4f55\u7ea6\u675f\uff0c\u7ed3\u5408\u5149\u5ea6\u635f\u5931\uff1b\u5f15\u5165\u6982\u7387\u521d\u59cb\u5316\u7b56\u7565\u548c\u52a8\u6001\u6e32\u67d3\u4fe1\u4efb\u8870\u51cf\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6237\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u8f68\u8ff9\u8bef\u5dee\uff08ATE\u51cf\u5c1169.0%\uff09\uff0c\u6e32\u67d3\u8d28\u91cf\u8fbe\u5230SOTA\u3002", "conclusion": "TVG-SLAM\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u52a8\u6001\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86RGB-only SLAM\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23923", "abs": "https://arxiv.org/abs/2506.23923", "authors": ["Miguel Camacho-S\u00e1nchez", "Fernando Garc\u00eda-Torres", "Jesper John Lisegaard", "Roc\u00edo del Amor", "Sankhya Mohanty", "Valery Naranjo"], "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u63a7\u5236\u6811\u8102\u6ce8\u5165\u8fc7\u7a0b\u4e2d\u7684\u6d41\u52a8\u52a8\u6001\uff0c\u4ee5\u63d0\u9ad8\u590d\u5408\u6750\u6599\u5236\u9020\u7684\u5747\u5300\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u6811\u8102\u6ce8\u5165\u8fc7\u7a0b\u4e2d\u7684\u6d41\u52a8\u52a8\u6001\u63a7\u5236\u5bf9\u786e\u4fdd\u7ea4\u7ef4\u589e\u5f3a\u6750\u6599\u7684\u5747\u5300\u6d78\u6e0d\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u907f\u514d\u5b54\u9699\u548c\u5e72\u6591\u5f71\u54cd\u6700\u7ec8\u4ea7\u54c1\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "method": "\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\uff0c\u540c\u6b65\u4e24\u4e2a\u6811\u8102\u5165\u53e3\u548c\u4e00\u4e2a\u51fa\u53e3\u7684\u6d41\u52a8\u524d\u6cbf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u6d41\u52a8\u524d\u6cbf\u7684\u7cbe\u786e\u6536\u655b\uff0c\u63d0\u5347\u5de5\u827a\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u590d\u5408\u6750\u6599\u5236\u9020\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f18\u5316\u5de5\u827a\u63a7\u5236\u5e76\u63d0\u9ad8\u4ea7\u54c1\u6027\u80fd\u3002"}}
{"id": "2506.23209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23209", "abs": "https://arxiv.org/abs/2506.23209", "authors": ["Chia-Wen Huang", "Haw Hwai", "Chien-Chang Lee", "Pei-Yuan Wu"], "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans", "comment": "8 pages, 1 figure, 3 tables. Published in IEEE ISBI 2025. This\n  version corrects citation numbering errors", "summary": "Timely and accurate diagnosis of appendicitis is critical in clinical\nsettings to prevent serious complications. While CT imaging remains the\nstandard diagnostic tool, the growing number of cases can overwhelm\nradiologists, potentially causing delays. In this paper, we propose a deep\nlearning model that leverages 3D CT scans for appendicitis classification,\nincorporating Slice Attention mechanisms guided by external 2D datasets to\nenhance small lesion detection. Additionally, we introduce a hierarchical\nclassification framework using pre-trained 2D models to differentiate between\nsimple and complicated appendicitis. Our approach improves AUC by 3% for\nappendicitis and 5.9% for complicated appendicitis, offering a more efficient\nand reliable diagnostic solution compared to previous work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D CT\u626b\u63cf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5207\u7247\u6ce8\u610f\u529b\u548c\u9884\u8bad\u7ec32D\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9611\u5c3e\u708e\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9611\u5c3e\u708e\u7684\u53ca\u65f6\u51c6\u786e\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46CT\u5f71\u50cf\u6570\u91cf\u589e\u52a0\u53ef\u80fd\u5bfc\u81f4\u653e\u5c04\u79d1\u533b\u751f\u8d1f\u62c5\u8fc7\u91cd\uff0c\u5f15\u53d1\u5ef6\u8bef\u3002", "method": "\u5229\u75283D CT\u626b\u63cf\u548c\u5207\u7247\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5916\u90e82D\u6570\u636e\u96c6\u589e\u5f3a\u5c0f\u75c5\u53d8\u68c0\u6d4b\uff1b\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u6846\u67b6\u533a\u5206\u7b80\u5355\u548c\u590d\u6742\u9611\u5c3e\u708e\u3002", "result": "\u9611\u5c3e\u708e\u5206\u7c7b\u7684AUC\u63d0\u9ad8\u4e863%\uff0c\u590d\u6742\u9611\u5c3e\u708e\u63d0\u9ad8\u4e865.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u8bca\u65ad\u65b9\u6848\uff0c\u4f18\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u3002"}}
{"id": "2506.23958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23958", "abs": "https://arxiv.org/abs/2506.23958", "authors": ["Ikechukwu Ogbonna", "Lesley Davidson", "Soumya Banerjee", "Abhishek Dasgupta", "Laurence Kenney", "Vikranth Harthikote Nagaraja"], "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages", "comment": "5 pages, 0 figures, 0 tables", "summary": "Millions of people in African countries face barriers to accessing healthcare\ndue to language and literacy gaps. This research tackles this challenge by\ntransforming complex medical documents -- in this case, prosthetic device user\nmanuals -- into accessible formats for underserved populations. This case study\nin cross-cultural translation is particularly pertinent/relevant for\ncommunities that receive donated prosthetic devices but may not receive the\naccompanying user documentation. Or, if available online, may only be available\nin formats (e.g., language and readability) that are inaccessible to local\npopulations (e.g., English-language, high resource settings/cultural context).\nThe approach is demonstrated using the widely spoken Pidgin dialect, but our\nopen-source framework has been designed to enable rapid and easy extension to\nother languages/dialects. This work presents an AI-powered framework designed\nto process and translate complex medical documents, e.g., user manuals for\nprosthetic devices, into marginalised languages. The system enables users --\nsuch as healthcare workers or patients -- to upload English-language medical\nequipment manuals, pose questions in their native language, and receive\naccurate, localised answers in real time. Technically, the system integrates a\nRetrieval-Augmented Generation (RAG) pipeline for processing and semantic\nunderstanding of the uploaded manuals. It then employs advanced Natural\nLanguage Processing (NLP) models for generative question-answering and\nmultilingual translation. Beyond simple translation, it ensures accessibility\nto device instructions, treatment protocols, and safety information, empowering\npatients and clinicians to make informed healthcare decisions.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u533b\u7597\u6587\u6863\uff08\u5982\u5047\u80a2\u8bbe\u5907\u624b\u518c\uff09\u7ffb\u8bd1\u6210\u8fb9\u7f18\u5316\u8bed\u8a00\uff0c\u4ee5\u89e3\u51b3\u975e\u6d32\u56fd\u5bb6\u56e0\u8bed\u8a00\u548c\u8bc6\u5b57\u969c\u788d\u5bfc\u81f4\u7684\u533b\u7597\u83b7\u53d6\u95ee\u9898\u3002", "motivation": "\u975e\u6d32\u56fd\u5bb6\u8bb8\u591a\u4eba\u56e0\u8bed\u8a00\u548c\u8bc6\u5b57\u969c\u788d\u65e0\u6cd5\u83b7\u53d6\u533b\u7597\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u6350\u8d60\u7684\u5047\u80a2\u8bbe\u5907\u7f3a\u4e4f\u672c\u5730\u5316\u6587\u6863\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u5904\u7406\u82f1\u6587\u624b\u518c\uff0c\u7ed3\u5408NLP\u6a21\u578b\u5b9e\u73b0\u591a\u8bed\u8a00\u7ffb\u8bd1\u548c\u751f\u6210\u5f0f\u95ee\u7b54\u3002", "result": "\u7cfb\u7edf\u80fd\u5b9e\u65f6\u7ffb\u8bd1\u548c\u56de\u7b54\u672c\u5730\u8bed\u8a00\u95ee\u9898\uff0c\u63d0\u5347\u533b\u7597\u4fe1\u606f\u7684\u53ef\u53ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18\u5316\u793e\u533a\u63d0\u4f9b\u4e86\u533b\u7597\u4fe1\u606f\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8bed\u8a00\u3002"}}
{"id": "2506.23219", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23219", "abs": "https://arxiv.org/abs/2506.23219", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "comment": "Accepted by ICCV 2025", "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bUrbanLLaVA\uff0c\u7528\u4e8e\u7edf\u4e00\u5904\u7406\u57ce\u5e02\u7814\u7a76\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57ce\u5e02\u7814\u7a76\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u6570\u636e\u7c7b\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u7684\u57ce\u5e02\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u591a\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5206\u79bb\u7a7a\u95f4\u63a8\u7406\u589e\u5f3a\u4e0e\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUrbanLLaVA\u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5f00\u6e90\u548c\u4e13\u6709MLLMs\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u57ce\u5e02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "UrbanLLaVA\u4e3a\u57ce\u5e02\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5904\u7406\u6846\u67b6\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.23960", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23960", "abs": "https://arxiv.org/abs/2506.23960", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "comment": null, "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADReFT\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u4fee\u590d\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u56e0\u8bbe\u8ba1\u548c\u6027\u80fd\u7684\u56fa\u6709\u5c40\u9650\u6027\u9762\u4e34\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u5728\u7ebf\u4fee\u590d\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u4fee\u590d\u6548\u679c\u4e0d\u4f73\u3002", "method": "ADReFT\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u53cc\u5934\u6a21\u578b\uff08\u72b6\u6001\u76d1\u63a7\u5668\u548c\u51b3\u7b56\u9002\u914d\u5668\uff09\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5206\u9636\u6bb5\u8bad\u7ec3\uff0c\u751f\u6210\u81ea\u9002\u5e94\u4fee\u590d\u52a8\u4f5c\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cADReFT\u5728\u4fee\u590d\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADReFT\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.23227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23227", "abs": "https://arxiv.org/abs/2506.23227", "authors": ["Lunhao Duan", "Shanshan Zhao", "Xingxing Weng", "Jing Zhang", "Gui-Song Xia"], "title": "High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation", "comment": "Accepted by TPAMI. Code: https://github.com/LHDuan/WSegPC", "summary": "This paper investigates indoor point cloud semantic segmentation under\nscene-level annotation, which is less explored compared to methods relying on\nsparse point-level labels. In the absence of precise point-level labels,\ncurrent methods first generate point-level pseudo-labels, which are then used\nto train segmentation models. However, generating accurate pseudo-labels for\neach point solely based on scene-level annotations poses a considerable\nchallenge, substantially affecting segmentation performance. Consequently, to\nenhance accuracy, this paper proposes a high-quality pseudo-label generation\nframework by exploring contemporary multi-modal information and region-point\nsemantic consistency. Specifically, with a cross-modal feature guidance module,\nour method utilizes 2D-3D correspondences to align point cloud features with\ncorresponding 2D image pixels, thereby assisting point cloud feature learning.\nTo further alleviate the challenge presented by the scene-level annotation, we\nintroduce a region-point semantic consistency module. It produces regional\nsemantics through a region-voting strategy derived from point-level semantics,\nwhich are subsequently employed to guide the point-level semantic predictions.\nLeveraging the aforementioned modules, our method can rectify inaccurate\npoint-level semantic predictions during training and obtain high-quality\npseudo-labels. Significant improvements over previous works on ScanNet v2 and\nS3DIS datasets under scene-level annotation can demonstrate the effectiveness.\nAdditionally, comprehensive ablation studies validate the contributions of our\napproach's individual components. The code is available at\nhttps://github.com/LHDuan/WSegPC .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u7ea7\u6807\u6ce8\u7684\u5ba4\u5185\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u548c\u533a\u57df-\u70b9\u8bed\u4e49\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u70b9\u7ea7\u6807\u7b7e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4f46\u4ec5\u57fa\u4e8e\u573a\u666f\u7ea7\u6807\u6ce8\u751f\u6210\u51c6\u786e\u4f2a\u6807\u7b7e\u5177\u6709\u6311\u6218\u6027\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u7279\u5f81\u5f15\u5bfc\u6a21\u5757\u548c\u533a\u57df-\u70b9\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u5229\u75282D-3D\u5bf9\u5e94\u5173\u7cfb\u548c\u533a\u57df\u6295\u7968\u7b56\u7565\u4f18\u5316\u4f2a\u6807\u7b7e\u751f\u6210\u3002", "result": "\u5728ScanNet v2\u548cS3DIS\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u63d0\u5347\u573a\u666f\u7ea7\u6807\u6ce8\u4e0b\u7684\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2506.23971", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23971", "abs": "https://arxiv.org/abs/2506.23971", "authors": ["Brandon M. Wood", "Misko Dzamba", "Xiang Fu", "Meng Gao", "Muhammed Shuaibi", "Luis Barroso-Luque", "Kareem Abdelmaqsoud", "Vahe Gharakhanyan", "John R. Kitchin", "Daniel S. Levine", "Kyle Michel", "Anuroop Sriram", "Taco Cohen", "Abhishek Das", "Ammar Rizvi", "Sushree Jagriti Sahoo", "Zachary W. Ulissi", "C. Lawrence Zitnick"], "title": "UMA: A Family of Universal Models for Atoms", "comment": "29 pages, 5 figures", "summary": "The ability to quickly and accurately compute properties from atomic\nsimulations is critical for advancing a large number of applications in\nchemistry and materials science including drug discovery, energy storage, and\nsemiconductor manufacturing. To address this need, Meta FAIR presents a family\nof Universal Models for Atoms (UMA), designed to push the frontier of speed,\naccuracy, and generalization. UMA models are trained on half a billion unique\n3D atomic structures (the largest training runs to date) by compiling data\nacross multiple chemical domains, e.g. molecules, materials, and catalysts. We\ndevelop empirical scaling laws to help understand how to increase model\ncapacity alongside dataset size to achieve the best accuracy. The UMA small and\nmedium models utilize a novel architectural design we refer to as mixture of\nlinear experts that enables increasing model capacity without sacrificing\nspeed. For example, UMA-medium has 1.4B parameters but only ~50M active\nparameters per atomic structure. We evaluate UMA models on a diverse set of\napplications across multiple domains and find that, remarkably, a single model\nwithout any fine-tuning can perform similarly or better than specialized\nmodels. We are releasing the UMA code, weights, and associated data to\naccelerate computational workflows and enable the community to continue to\nbuild increasingly capable AI models.", "AI": {"tldr": "Meta FAIR\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u539f\u5b50\u6a21\u578b\uff08UMA\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u65b0\u578b\u67b6\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u5316\u5b66\u9886\u57df\u7684\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u5feb\u901f\u51c6\u786e\u8ba1\u7b97\u539f\u5b50\u6a21\u62df\u6027\u8d28\u5bf9\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5982\u836f\u7269\u53d1\u73b0\u548c\u80fd\u6e90\u5b58\u50a8\u3002", "method": "UMA\u6a21\u578b\u57fa\u4e8e5\u4ebf\u4e2a3D\u539f\u5b50\u7ed3\u6784\u8bad\u7ec3\uff0c\u91c7\u7528\u6df7\u5408\u7ebf\u6027\u4e13\u5bb6\u67b6\u6784\uff0c\u652f\u6301\u6a21\u578b\u5bb9\u91cf\u6269\u5c55\u800c\u4e0d\u727a\u7272\u901f\u5ea6\u3002", "result": "UMA\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "UMA\u7684\u4ee3\u7801\u548c\u6743\u91cd\u5df2\u516c\u5f00\uff0c\u65e8\u5728\u63a8\u52a8\u8ba1\u7b97\u5de5\u4f5c\u6d41\u548cAI\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2506.23236", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23236", "abs": "https://arxiv.org/abs/2506.23236", "authors": ["Marko Mihajlovic", "Siwei Zhang", "Gen Li", "Kaifeng Zhao", "Lea M\u00fcller", "Siyu Tang"], "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions", "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL", "summary": "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", "AI": {"tldr": "VolumetricSMPL\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6df7\u5408\u6743\u91cd\uff08NBW\uff09\u7684\u795e\u7ecf\u4f53\u79ef\u4eba\u4f53\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u4f53\u6a21\u578b\u4f7f\u7528\u8868\u9762\u7f51\u683c\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u4e0e\u5176\u4ed6\u51e0\u4f55\u5b9e\u4f53\u7684\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u7684\u4f53\u79ef\u795e\u7ecf\u9690\u5f0f\u6a21\u578b\u5728\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u6216\u8ba1\u7b97\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faVolumetricSMPL\uff0c\u5229\u7528NBW\u52a8\u6001\u6df7\u5408\u5c11\u91cf\u5b66\u4e60\u6743\u91cd\u77e9\u9635\uff0c\u751f\u6210\u7d27\u51d1\u9ad8\u6548\u7684MLP\u89e3\u7801\u5668\uff0c\u652f\u6301SDF\u5efa\u6a21\u3002", "result": "\u76f8\u6bd4COAP\u6a21\u578b\uff0cVolumetricSMPL\u63a8\u7406\u901f\u5ea6\u5feb10\u500d\uff0cGPU\u5185\u5b58\u5360\u7528\u4f4e6\u500d\uff0c\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "VolumetricSMPL\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4eba\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.23977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23977", "abs": "https://arxiv.org/abs/2506.23977", "authors": ["Zain ul Abdeen", "Vassilis Kekatos", "Ming Jin"], "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks", "comment": null, "summary": "Certified robustness is a critical property for deploying neural networks\n(NN) in safety-critical applications. A principle approach to achieving such\nguarantees is to constrain the global Lipschitz constant of the network.\nHowever, accurate methods for Lipschitz-constrained training often suffer from\nnon-convex formulations and poor scalability due to reliance on global\nsemidefinite programs (SDPs). In this letter, we propose a convex training\nframework that enforces global Lipschitz constraints via semidefinite\nrelaxation. By reparameterizing the NN using loop transformation, we derive a\nconvex admissibility condition that enables tractable and certifiable training.\nWhile the resulting formulation guarantees robustness, its scalability is\nlimited by the size of global SDP. To overcome this, we develop a randomized\nsubspace linear matrix inequalities (RS-LMI) approach that decomposes the\nglobal constraints into sketched layerwise constraints projected onto\nlow-dimensional subspaces, yielding a smooth and memory-efficient training\nobjective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that\nthe proposed framework achieves competitive accuracy with significantly\nimproved Lipschitz bounds and runtime performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u5168\u5c40Lipschitz\u7ea6\u675f\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u5b9a\u677e\u5f1b\u548c\u968f\u673a\u5b50\u7a7a\u95f4\u65b9\u6cd5\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u8ba4\u8bc1\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u975e\u51f8\u6027\u548c\u5168\u5c40\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u73af\u8def\u53d8\u6362\u91cd\u65b0\u53c2\u6570\u5316\u7f51\u7edc\uff0c\u63d0\u51fa\u51f8\u53ef\u63a5\u53d7\u6761\u4ef6\uff0c\u5e76\u7ed3\u5408\u968f\u673a\u5b50\u7a7a\u95f4\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08RS-LMI\uff09\u5206\u89e3\u5168\u5c40\u7ea6\u675f\u4e3a\u5c42\u95f4\u7ea6\u675f\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548cImageNet\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86Lipschitz\u8fb9\u754c\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8ba4\u8bc1\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23247", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23247", "abs": "https://arxiv.org/abs/2506.23247", "authors": ["James Hinns", "David Martens"], "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "comment": null, "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSegment Attribution Tables (SATs)\uff0c\u901a\u8fc7\u603b\u7ed3\u5c40\u90e8\u663e\u8457\u6027\u89e3\u91ca\u63d0\u4f9b\u534a\u5168\u5c40\u89c1\u89e3\uff0c\u586b\u8865\u4e86\u8fc7\u4e8e\u7b80\u5316\u7684\u5168\u5c40\u65b9\u6cd5\u4e0e\u8fc7\u4e8e\u8be6\u7ec6\u7684\u5c40\u90e8\u89e3\u91ca\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u6216\u8fc7\u4e8e\u5c40\u90e8\uff08\u5982\u663e\u8457\u6027\u56fe\uff09\uff0c\u6216\u8fc7\u4e8e\u5168\u5c40\u800c\u5ffd\u7565\u91cd\u8981\u5c40\u90e8\u884c\u4e3a\u3002", "method": "\u63d0\u51faSATs\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u7247\u6bb5\uff08\u5982\u201c\u773c\u775b\u201d\uff09\u548c\u663e\u8457\u6027\u56fe\u91cf\u5316\u5176\u5f71\u54cd\uff0c\u63ed\u793a\u6a21\u578b\u4f9d\u8d56\u7684\u6982\u5ff5\u548c\u865a\u5047\u76f8\u5173\u6027\u3002", "result": "SATs\u80fd\u591f\u89e3\u91ca\u4efb\u4f55\u53ef\u751f\u6210\u663e\u8457\u6027\u56fe\u7684\u5206\u7c7b\u5668\uff0c\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u5206\u6790\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "conclusion": "SATs\u586b\u8865\u4e86\u5168\u5c40\u4e0e\u5c40\u90e8\u89e3\u91ca\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5206\u6790\u548c\u8c03\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2506.23978", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "pdf": "https://arxiv.org/pdf/2506.23978", "abs": "https://arxiv.org/abs/2506.23978", "authors": ["Samuele Marro", "Philip Torr"], "title": "LLM Agents Are the Antidote to Walled Gardens", "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "AI": {"tldr": "LLM-based agents enable universal interoperability, disrupting proprietary platforms by making data exchange cheaper and unavoidable, though it introduces new risks.", "motivation": "The dominance of closed, proprietary platforms in the application layer limits data exchange and interoperability, reinforcing monopolistic behaviors.", "method": "Proposes using LLM-based agents to automatically translate data formats and interact with human-designed interfaces, enabling seamless data exchange.", "result": "Universal interoperability undermines monopolistic behaviors and promotes data portability, but introduces security risks and technical debt.", "conclusion": "The ML community should embrace universal interoperability while developing frameworks to mitigate risks, leveraging AI to restore user freedom and competitive markets."}}
{"id": "2506.23252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23252", "abs": "https://arxiv.org/abs/2506.23252", "authors": ["Kunwei Lv", "Ping Lan"], "title": "DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection", "comment": "8 pages, 5 figures", "summary": "The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted\nthe importance of robust and efficient object detection in diverse aerial\nscenarios. Detecting small objects under complex conditions, however, remains a\nsignificant challenge. Existing approaches often prioritize inference speed,\nleading to degraded performance when handling multi-modal inputs. To address\nthis, we present DGE-YOLO, an enhanced YOLO-based detection framework designed\nto effectively fuse multi-modal information. Specifically, we introduce a\ndual-branch architecture for modality-specific feature extraction, enabling the\nmodel to process both infrared and visible images. To further enrich semantic\nrepresentation, we propose an Efficient Multi-scale Attention (EMA) mechanism\nthat enhances feature learning across spatial scales. Additionally, we replace\nthe conventional neck with a Gather-and-Distribute module to mitigate\ninformation loss during feature aggregation. Extensive experiments on the Drone\nVehicle dataset demonstrate that DGE-YOLO achieves superior performance over\nstate-of-the-art methods, validating its effectiveness in multi-modal UAV\nobject detection tasks.", "AI": {"tldr": "DGE-YOLO\u662f\u4e00\u79cd\u6539\u8fdb\u7684YOLO\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548cEMA\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u5c0f\u76ee\u6807\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u662f\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u67b6\u6784\u5904\u7406\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u56fe\u50cf\uff0c\u5f15\u5165EMA\u673a\u5236\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528Gather-and-Distribute\u6a21\u5757\u66ff\u4ee3\u4f20\u7edfneck\u3002", "result": "\u5728Drone Vehicle\u6570\u636e\u96c6\u4e0a\uff0cDGE-YOLO\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DGE-YOLO\u5728\u591a\u6a21\u6001\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.23996", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23996", "abs": "https://arxiv.org/abs/2506.23996", "authors": ["Juan Maro\u00f1as"], "title": "The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)", "comment": null, "summary": "This document shows how to obtain the Jacobian and Hessian matrices of the\nKullback-Leibler divergence between two multivariate Gaussian distributions,\nusing the first and second-order differentials. The presented derivations are\nbased on the theory presented by \\cite{magnus99}. I've also got great\ninspiration from some of the derivations in \\cite{minka}.\n  Since I pretend to be at most didactic, the document is split into a summary\nof results and detailed derivations on each of the elements involved, with\nspecific references to the tricks used in the derivations, and to many of the\nunderlying concepts.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u5fae\u5206\u83b7\u5f97\u4e24\u4e2a\u591a\u5143\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4Kullback-Leibler\u6563\u5ea6\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6d77\u68ee\u77e9\u9635\u3002", "motivation": "\u65e8\u5728\u4ee5\u6559\u5b66\u4e3a\u76ee\u7684\uff0c\u8be6\u7ec6\u63a8\u5bfcKullback-Leibler\u6563\u5ea6\u7684\u76f8\u5173\u77e9\u9635\uff0c\u5e76\u53c2\u8003\u4e86\u524d\u4eba\u7406\u8bba\u3002", "method": "\u57fa\u4e8e\u4e00\u9636\u548c\u4e8c\u9636\u5fae\u5206\uff0c\u7ed3\u5408Magnus\u548cMinka\u7684\u7406\u8bba\u8fdb\u884c\u63a8\u5bfc\u3002", "result": "\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u63a8\u5bfc\u8fc7\u7a0b\u548c\u7ed3\u679c\uff0c\u5305\u62ec\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6d77\u68ee\u77e9\u9635\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5206\u6b65\u63a8\u5bfc\u548c\u6559\u5b66\u5f0f\u5c55\u793a\uff0c\u4e3a\u7406\u89e3Kullback-Leibler\u6563\u5ea6\u7684\u77e9\u9635\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6e05\u6670\u6307\u5bfc\u3002"}}
{"id": "2506.23254", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23254", "abs": "https://arxiv.org/abs/2506.23254", "authors": ["Aradhana Mishra", "Bumshik Lee"], "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "comment": null, "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "AI": {"tldr": "PixelBoost\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5e03\u6717\u8fd0\u52a8\u7684\u968f\u673a\u6027\u63d0\u5347\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u5728\u7eb9\u7406\u548c\u8fb9\u7f18\u5b9a\u4e49\u4e0a\u5b9e\u73b0\u9ad8\u5ea6\u771f\u5b9e\u611f\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u771f\u5b9e\u611f\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u65f6\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\u548c\u4e0d\u771f\u5b9e\u7684\u60c5\u51b5\u3002", "method": "\u5f15\u5165\u5e03\u6717\u8fd0\u52a8\u7684\u968f\u673a\u6027\u5230\u8bad\u7ec3\u4e2d\uff0c\u907f\u514d\u5c40\u90e8\u6700\u4f18\uff0c\u91c7\u7528sigmoidal\u566a\u58f0\u5e8f\u5217\u65b9\u6cd5\u7b80\u5316\u8bad\u7ec3\u5e76\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728LPIPS\u3001LOE\u3001PSNR\u3001SSIM\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fb9\u7f18\u91cd\u5efa\u80fd\u529b\u66f4\u5f3a\uff0c\u89c6\u89c9\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "PixelBoost\u901a\u8fc7\u63a7\u5236\u968f\u673a\u6027\u548c\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u771f\u5b9e\u611f\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.24000", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24000", "abs": "https://arxiv.org/abs/2506.24000", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "AI": {"tldr": "TTA-VLM\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524dTTA\u7814\u7a76\u5b58\u5728\u7ed3\u679c\u91cd\u590d\u3001\u8bc4\u4f30\u6307\u6807\u6709\u9650\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u548c\u4e0d\u8db3\u5206\u6790\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "TTA-VLM\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e868\u79cd\u60c5\u666fTTA\u548c7\u79cd\u5728\u7ebfTTA\u65b9\u6cd5\uff0c\u5e76\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6269\u5c55\u4e86\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709TTA\u65b9\u6cd5\u6539\u8fdb\u6709\u9650\uff0c\u4e0e\u8bad\u7ec3\u65f6\u5fae\u8c03\u65b9\u6cd5\u534f\u4f5c\u5dee\uff0c\u4e14\u51c6\u786e\u6027\u63d0\u5347\u5e38\u4ee5\u6a21\u578b\u53ef\u4fe1\u5ea6\u964d\u4f4e\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "TTA-VLM\u65e8\u5728\u4fc3\u8fdb\u66f4\u53ef\u9760\u548c\u901a\u7528\u7684TTA\u7b56\u7565\u53d1\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u5168\u9762\u8bc4\u4f30\u7684\u5e73\u53f0\u3002"}}
{"id": "2506.23257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23257", "abs": "https://arxiv.org/abs/2506.23257", "authors": ["Chongke Bi", "Xin Gao", "Baofeng Fu", "Yuheng Zhao", "Siming Chen", "Ying Zhao", "Yunhai Wang"], "title": "PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation", "comment": null, "summary": "Large-scale simulations on supercomputers have become important tools for\nusers. However, their scalability remains a problem due to the huge\ncommunication cost among parallel processes. Most of the existing communication\nlatency analysis methods rely on the physical link layer information, which is\nonly available to administrators. In this paper, a framework called PCLVis is\nproposed to help general users analyze process communication latency (PCL)\nevents. Instead of the physical link layer information, the PCLVis uses the MPI\nprocess communication data for the analysis. First, a spatial PCL event\nlocating method is developed. All processes with high correlation are\nclassified into a single cluster by constructing a process-correlation tree.\nSecond, the propagation path of PCL events is analyzed by constructing a\ncommunication-dependency-based directed acyclic graph (DAG), which can help\nusers interactively explore a PCL event from the temporal evolution of a\nlocated PCL event cluster. In this graph, a sliding window algorithm is\ndesigned to generate the PCL events abstraction. Meanwhile, a new glyph called\nthe communication state glyph (CS-Glyph) is designed for each process to show\nits communication states, including its in/out messages and load balance. Each\nleaf node can be further unfolded to view additional information. Third, a PCL\nevent attribution strategy is formulated to help users optimize their\nsimulations. The effectiveness of the PCLVis framework is demonstrated by\nanalyzing the PCL events of several simulations running on the TH-1A\nsupercomputer. By using the proposed framework, users can greatly improve the\nefficiency of their simulations.", "AI": {"tldr": "PCLVis\u6846\u67b6\u901a\u8fc7MPI\u901a\u4fe1\u6570\u636e\u5206\u6790\u901a\u4fe1\u5ef6\u8fdf\u4e8b\u4ef6\uff0c\u5e2e\u52a9\u7528\u6237\u4f18\u5316\u5927\u89c4\u6a21\u6a21\u62df\u7684\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u62df\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7ba1\u7406\u5458\u624d\u80fd\u83b7\u53d6\u7684\u7269\u7406\u94fe\u8def\u5c42\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u666e\u901a\u7528\u6237\u7684\u5206\u6790\u80fd\u529b\u3002", "method": "PCLVis\u5229\u7528MPI\u901a\u4fe1\u6570\u636e\uff0c\u901a\u8fc7\u6784\u5efa\u8fdb\u7a0b\u76f8\u5173\u6811\u548c\u901a\u4fe1\u4f9d\u8d56DAG\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u7b97\u6cd5\u548cCS-Glyph\u53ef\u89c6\u5316\u901a\u4fe1\u72b6\u6001\u3002", "result": "\u5728TH-1A\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCLVis\u80fd\u6709\u6548\u5206\u6790\u901a\u4fe1\u5ef6\u8fdf\u4e8b\u4ef6\u5e76\u4f18\u5316\u6a21\u62df\u6548\u7387\u3002", "conclusion": "PCLVis\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7269\u7406\u94fe\u8def\u5c42\u4fe1\u606f\u7684\u901a\u4fe1\u5ef6\u8fdf\u5206\u6790\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u6548\u7387\u3002"}}
{"id": "2506.24005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.24005", "abs": "https://arxiv.org/abs/2506.24005", "authors": ["He Wang", "Xingyu Xu", "Yuejie Chi"], "title": "Provably Efficient and Agile Randomized Q-Learning", "comment": null, "summary": "While Bayesian-based exploration often demonstrates superior empirical\nperformance compared to bonus-based methods in model-based reinforcement\nlearning (RL), its theoretical understanding remains limited for model-free\nsettings. Existing provable algorithms either suffer from computational\nintractability or rely on stage-wise policy updates which reduce responsiveness\nand slow down the learning process. In this paper, we propose a novel variant\nof Q-learning algorithm, refereed to as RandomizedQ, which integrates\nsampling-based exploration with agile, step-wise, policy updates, for episodic\ntabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where\n$S$ is the number of states, $A$ is the number of actions, $H$ is the episode\nlength, and $T$ is the total number of episodes. In addition, we present a\nlogarithmic regret bound under a mild positive sub-optimality condition on the\noptimal Q-function. Empirically, RandomizedQ exhibits outstanding performance\ncompared to existing Q-learning variants with both bonus-based and\nBayesian-based exploration on standard benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRandomizedQ\u7684\u65b0\u578bQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u91c7\u6837\u7684\u63a2\u7d22\u548c\u654f\u6377\u7684\u9010\u6b65\u7b56\u7565\u66f4\u65b0\uff0c\u5728\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u7406\u8bba\u4e0a\u7684\u4f4e\u9057\u61be\u754c\u9650\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u63a2\u7d22\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u65e0\u6a21\u578b\u73af\u5883\u4e2d\u7684\u7406\u8bba\u7406\u89e3\u6709\u9650\uff0c\u73b0\u6709\u7b97\u6cd5\u8981\u4e48\u8ba1\u7b97\u590d\u6742\uff0c\u8981\u4e48\u4f9d\u8d56\u9636\u6bb5\u6027\u7b56\u7565\u66f4\u65b0\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51faRandomizedQ\u7b97\u6cd5\uff0c\u7ed3\u5408\u91c7\u6837\u63a2\u7d22\u548c\u9010\u6b65\u7b56\u7565\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u7b97\u6cd5\u5177\u6709$\\widetilde{O}(\\sqrt{H^5SAT})$\u7684\u9057\u61be\u754c\u9650\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709Q\u5b66\u4e60\u53d8\u4f53\u3002", "conclusion": "RandomizedQ\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u63a2\u7d22\u65b9\u6cd5\u3002"}}
{"id": "2506.23263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23263", "abs": "https://arxiv.org/abs/2506.23263", "authors": ["Lei-lei Li", "Jianwu Fang", "Junbin Xiao", "Shanmin Pang", "Hongkai Yu", "Chen Lv", "Jianru Xue", "Tat-Seng Chua"], "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis", "comment": "Accepted by ICCV2025", "summary": "Egocentricly comprehending the causes and effects of car accidents is crucial\nfor the safety of self-driving cars, and synthesizing causal-entity reflected\naccident videos can facilitate the capability test to respond to unaffordable\naccidents in reality. However, incorporating causal relations as seen in\nreal-world videos into synthetic videos remains challenging. This work argues\nthat precisely identifying the accident participants and capturing their\nrelated behaviors are of critical importance. In this regard, we propose a\nnovel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic\naccident videos. To enable causal entity grounding in video diffusion,\nCausal-VidSyn leverages the cause descriptions and driver fixations to identify\nthe accident participants and behaviors, facilitated by accident reason\nanswering and gaze-conditioned selection modules. To support Causal-VidSyn, we\nfurther construct Drive-Gaze, the largest driver gaze dataset (with 1.54M\nframes of fixations) in driving accident scenarios. Extensive experiments show\nthat Causal-VidSyn surpasses state-of-the-art video diffusion models in terms\nof frame quality and causal sensitivity in various tasks, including accident\nvideo editing, normal-to-accident video diffusion, and text-to-video\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCausal-VidSyn\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\uff0c\u901a\u8fc7\u7ed3\u5408\u539f\u56e0\u63cf\u8ff0\u548c\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\u6765\u8bc6\u522b\u4e8b\u6545\u53c2\u4e0e\u8005\u548c\u884c\u4e3a\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5bf9\u4ea4\u901a\u4e8b\u6545\u56e0\u679c\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u89c6\u9891\u6d4b\u8bd5\u5176\u5bf9\u73b0\u5b9e\u4e2d\u96be\u4ee5\u627f\u53d7\u7684\u4e8b\u6545\u7684\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578bCausal-VidSyn\uff0c\u7ed3\u5408\u4e8b\u6545\u539f\u56e0\u56de\u7b54\u6a21\u5757\u548c\u6ce8\u89c6\u6761\u4ef6\u9009\u62e9\u6a21\u5757\uff0c\u901a\u8fc7\u539f\u56e0\u63cf\u8ff0\u548c\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\u8bc6\u522b\u4e8b\u6545\u53c2\u4e0e\u8005\u548c\u884c\u4e3a\u3002", "result": "Causal-VidSyn\u5728\u89c6\u9891\u5e27\u8d28\u91cf\u548c\u56e0\u679c\u654f\u611f\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u5982\u4e8b\u6545\u89c6\u9891\u7f16\u8f91\u3001\u6b63\u5e38\u5230\u4e8b\u6545\u89c6\u9891\u6269\u6563\u548c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u3002", "conclusion": "Causal-VidSyn\u901a\u8fc7\u7cbe\u786e\u8bc6\u522b\u4e8b\u6545\u53c2\u4e0e\u8005\u548c\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u89c6\u9891\u7684\u56e0\u679c\u654f\u611f\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.24018", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24018", "abs": "https://arxiv.org/abs/2506.24018", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u63a2\u8ba8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u94fe\u63a5\u8868\u793a\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6$k_\\phi$-$k_\\rho$-$m$\uff0c\u5e76\u5efa\u7acb\u4e86\u65b9\u6cd5\u5c42\u6b21\u7ed3\u6784\u3002\u901a\u8fc7\u5408\u6210\u8bc4\u4f30\u534f\u8bae\u548c\u5bf9\u79f0\u6027\u6307\u6807\uff0c\u7814\u7a76\u53d1\u73b0\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5728\u5bf9\u79f0\u6027\u9ad8\u7684\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u5bf9GNN\u8868\u8fbe\u80fd\u529b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u56fe\u7ea7\u8868\u793a\uff0c\u800c\u5ffd\u7565\u4e86\u94fe\u63a5\u8868\u793a\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u5bf9GNN\u5728\u94fe\u63a5\u4efb\u52a1\u4e2d\u8868\u8fbe\u80fd\u529b\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u63d0\u51fa$k_\\phi$-$k_\\rho$-$m$\u6846\u67b6\uff0c\u7edf\u4e00\u73b0\u6709\u6d88\u606f\u4f20\u9012\u94fe\u63a5\u6a21\u578b\uff0c\u5e76\u5efa\u7acb\u65b9\u6cd5\u5c42\u6b21\u7ed3\u6784\u3002\u8bbe\u8ba1\u4e86\u5408\u6210\u8bc4\u4f30\u534f\u8bae\u548c\u5bf9\u79f0\u6027\u6307\u6807\u6765\u9a8c\u8bc1\u7406\u8bba\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5728\u5bf9\u79f0\u6027\u9ad8\u7684\u6570\u636e\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u6a21\u578b\uff0c\u4f46\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u611f\u77e5\u7684\u6a21\u578b\u9009\u62e9\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u94fe\u63a5\u8868\u793a\u67b6\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2506.23270", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23270", "abs": "https://arxiv.org/abs/2506.23270", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "title": "Token Activation Map to Visually Explain Multimodal LLMs", "comment": "ICCV2025 Accepted", "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToken Activation Map (TAM)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u56e0\u679c\u63a8\u7406\u548c\u79e9\u9ad8\u65af\u6ee4\u6ce2\u5668\u6765\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u89e3\u91ca\u4e2d\u7684\u5197\u4f59\u6fc0\u6d3b\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89c6\u5316\u8d28\u91cf\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u89e3\u91ca\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5197\u4f59\u7684\u4e0a\u4e0b\u6587\u6fc0\u6d3b\u5e72\u6270\u4e86\u540e\u7eed\u4ee4\u724c\u7684\u89e3\u91ca\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89c6\u5316\u6548\u679c\u3002", "method": "\u63d0\u51faTAM\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f30\u8ba1\u56e0\u679c\u63a8\u7406\u548c\u79e9\u9ad8\u65af\u6ee4\u6ce2\u5668\uff0c\u51cf\u5c11\u5197\u4f59\u6fc0\u6d3b\u5e72\u6270\uff0c\u4f18\u5316MLLMs\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "result": "TAM\u5728\u591a\u79cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5982\u76ee\u6807\u5b9a\u4f4d\u3001\u5931\u8d25\u6848\u4f8b\u5206\u6790\u3001\u89c6\u9891\u53ef\u89c6\u5316\u7b49\u3002", "conclusion": "TAM\u4e3aMLLMs\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u7406\u89e3\u6df1\u5ea6\u3002"}}
{"id": "2506.24042", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.24042", "abs": "https://arxiv.org/abs/2506.24042", "authors": ["Gen Li", "Yuchen Zhou", "Yuting Wei", "Yuxin Chen"], "title": "Faster Diffusion Models via Higher-Order Approximation", "comment": null, "summary": "In this paper, we explore provable acceleration of diffusion models without\nany additional retraining. Focusing on the task of approximating a target data\ndistribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation\ndistance, we propose a principled, training-free sampling algorithm that\nrequires only the order of\n  $$ d^{1+2/K} \\varepsilon^{-1/K} $$\n  score function evaluations (up to log factor) in the presence of accurate\nscores, where $K$ is an arbitrarily large fixed integer. This result applies to\na broad class of target data distributions, without the need for assumptions\nsuch as smoothness or log-concavity. Our theory is robust vis-a-vis inexact\nscore estimation, degrading gracefully as the score estimation error increases\n-- without demanding higher-order smoothness on the score estimates as assumed\nin previous work. The proposed algorithm draws insight from high-order ODE\nsolvers, leveraging high-order Lagrange interpolation and successive refinement\nto approximate the integral derived from the probability flow ODE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u9636ODE\u6c42\u89e3\u5668\u51cf\u5c11\u8bc4\u5206\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u903c\u8fd1\u76ee\u6807\u6570\u636e\u5206\u5e03\u3002", "method": "\u57fa\u4e8e\u9ad8\u9636ODE\u6c42\u89e3\u5668\uff0c\u5229\u7528\u62c9\u683c\u6717\u65e5\u63d2\u503c\u548c\u9010\u6b65\u7ec6\u5316\u8fd1\u4f3c\u6982\u7387\u6d41ODE\u7684\u79ef\u5206\u3002", "result": "\u5728\u51c6\u786e\u8bc4\u5206\u4e0b\uff0c\u4ec5\u9700$d^{1+2/K} \\varepsilon^{-1/K}$\u6b21\u8bc4\u5206\u51fd\u6570\u8bc4\u4f30\u5373\u53ef\u903c\u8fd1\u76ee\u6807\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u6570\u636e\u5206\u5e03\uff0c\u5bf9\u8bc4\u5206\u4f30\u8ba1\u8bef\u5dee\u5177\u6709\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u5e73\u6ed1\u5047\u8bbe\u3002"}}
{"id": "2506.23271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23271", "abs": "https://arxiv.org/abs/2506.23271", "authors": ["Jinxing Zhou", "Zhihui Li", "Yongqiang Yu", "Yanghao Zhou", "Ruohao Guo", "Guangyao Li", "Yuxin Mao", "Mingfei Han", "Xiaojun Chang", "Meng Wang"], "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation", "comment": "Technical Report", "summary": "We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple\nand memory-efficient method for adapting large-scale pretrained transformer\nmodels to downstream audio-visual tasks. Instead of sequentially modifying the\noutput feature distribution of the transformer backbone, Mettle utilizes a\nlightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in\nparallel the intact audio or visual features embedded by each transformer layer\ninto compact meta-tokens. This distillation process considers both pretrained\nknowledge preservation and task-specific adaptation. The obtained meta-tokens\ncan be directly applied to classification tasks, such as audio-visual event\nlocalization and audio-visual video parsing. To further support fine-grained\nsegmentation tasks, such as audio-visual segmentation, we introduce a\n\\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual\nmeta-tokens distilled from the top transformer layer to guide feature\nadaptation in earlier layers. Extensive experiments on multiple audiovisual\nbenchmarks demonstrate that our method significantly reduces memory usage and\ntraining time while maintaining parameter efficiency and competitive accuracy.", "AI": {"tldr": "Mettle\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u84b8\u998f\u97f3\u9891\u6216\u89c6\u89c9\u7279\u5f81\u4e3a\u5143\u6807\u8bb0\uff0c\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u51cf\u5c11\u5185\u5b58\u548c\u65f6\u95f4\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u97f3\u9891-\u89c6\u89c9\u4efb\u52a1\u4e2d\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Layer-Centric Distillation\u6a21\u5757\u5e76\u884c\u84b8\u998f\u7279\u5f81\u4e3a\u5143\u6807\u8bb0\uff0c\u5e76\u5f15\u5165Meta-Token Injection\u6a21\u5757\u652f\u6301\u7ec6\u7c92\u5ea6\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "Mettle\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u97f3\u9891-\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2506.24093", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "pdf": "https://arxiv.org/pdf/2506.24093", "abs": "https://arxiv.org/abs/2506.24093", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Sch\u00f6ning"], "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "comment": "21pages, 14 figures, 2 tables", "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df7\u5408\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u67b6\u6784\u4e2d\u7684\u901a\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u5408\u6210\u6570\u636e\u867d\u6210\u672c\u4f4e\uff0c\u4f46\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u4e24\u79cd\u6df7\u5408\u7b56\u7565\uff0c\u5728\u4e09\u79cd\u67b6\u6784\u548c\u4e09\u79cd\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8c03\u6574\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u6bd4\u4f8b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u4e2d\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u7f29\u5c0f\u9886\u57df\u5dee\u8ddd\uff0c\u4f46\u9700\u6839\u636e\u4efb\u52a1\u548c\u67b6\u6784\u8c03\u6574\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u6bd4\u4f8b\u3002"}}
{"id": "2506.23275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23275", "abs": "https://arxiv.org/abs/2506.23275", "authors": ["Chengyou Jia", "Xin Shen", "Zhuohang Dang", "Zhuohang Dang", "Changliang Xia", "Weijia Wu", "Xinyu Zhang", "Hangwei Qian", "Ivor W. Tsang", "Minnan Luo"], "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "comment": null, "summary": "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u590d\u6742\u7684\u6587\u672c\u5230\u56fe\u50cf\u96c6\uff08T2IS\uff09\u751f\u6210\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86T2IS-Bench\u548cT2IS-Eval\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\uff0c\u540c\u65f6\u63d0\u51fa\u4e86AutoT2IS\u6846\u67b6\u4ee5\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u5177\u6709\u591a\u6837\u4e00\u81f4\u6027\u8981\u6c42\u7684\u56fe\u50cf\u96c6\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86T2IS-Bench\u6570\u636e\u96c6\u548cT2IS-Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86AutoT2IS\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u80fd\u529b\u3002", "result": "AutoT2IS\u5728T2IS-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "AutoT2IS\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u96c6\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.24120", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.24120", "abs": "https://arxiv.org/abs/2506.24120", "authors": ["Yuqing Wang", "Shangding Gu"], "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "comment": null, "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u5747\u5300\u5206\u5e03\u9009\u62e9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6570\u636e\u70b9\u95f4\u7684\u6700\u5c0f\u8ddd\u79bb\uff08$h_{\\min}$\uff09\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u9009\u62e9\u4e2d\u662f\u5426\u5b58\u5728\u901a\u7528\u7684\u5b9a\u91cf\u539f\u5219\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u548c\u6709\u9650\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6570\u636e\u5747\u5300\u5206\u5e03\uff08$h_{\\min}$\u8f83\u5927\uff09\u80fd\u52a0\u901f\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u6536\u655b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5305\u62ecTransformer\u5728\u5185\u7684\u591a\u79cd\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6570\u636e\u70b9\u95f4\u8ddd\u79bb\u9009\u62e9\u6570\u636e\u80fd\u663e\u8457\u52a0\u901f\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u53ef\u6bd4\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6570\u636e\u5747\u5300\u5206\u5e03\u662f\u4e00\u79cd\u6709\u6548\u7684\u901a\u7528\u6570\u636e\u9009\u62e9\u539f\u5219\uff0c\u80fd\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u3002"}}
{"id": "2506.23282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23282", "abs": "https://arxiv.org/abs/2506.23282", "authors": ["Hanwen Zhang", "Congqi Cao", "Qinyi Lv", "Lingtong Min", "Yanning Zhang"], "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector", "comment": null, "summary": "Video anomaly detection (VAD) is an important computer vision problem. Thanks\nto the mode coverage capabilities of generative models, the likelihood-based\nparadigm is catching growing interest, as it can model normal distribution and\ndetect out-of-distribution anomalies. However, these likelihood-based methods\nare blind to the anomalies located in local modes near the learned\ndistribution. To handle these ``unseen\" anomalies, we dive into three gaps\nuniquely existing in VAD regarding scene, motion and appearance. Specifically,\nwe first build a noise-conditioned score transformer for denoising score\nmatching. Then, we introduce a scene-dependent and motion-aware score function\nby embedding the scene condition of input sequences into our model and\nassigning motion weights based on the difference between key frames of input\nsequences. Next, to solve the problem of blindness in principle, we integrate\nunaffected visual information via a novel autoregressive denoising score\nmatching mechanism for inference. Through autoregressively injecting\nintensifying Gaussian noise into the denoised data and estimating the\ncorresponding score function, we compare the denoised data with the original\ndata to get a difference and aggregate it with the score function for an\nenhanced appearance perception and accumulate the abnormal context. With all\nthree gaps considered, we can compute a more comprehensive anomaly indicator.\nExperiments on three popular VAD benchmarks demonstrate the state-of-the-art\nperformance of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3\u573a\u666f\u3001\u8fd0\u52a8\u548c\u5916\u89c2\u4e09\u4e2a\u72ec\u7279\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u5c40\u90e8\u6a21\u5f0f\u4e2d\u7684\u5f02\u5e38\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u573a\u666f\u3001\u8fd0\u52a8\u548c\u5916\u89c2\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u6784\u5efa\u566a\u58f0\u6761\u4ef6\u8bc4\u5206\u53d8\u6362\u5668\uff0c\u5f15\u5165\u573a\u666f\u4f9d\u8d56\u548c\u8fd0\u52a8\u611f\u77e5\u8bc4\u5206\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u81ea\u56de\u5f52\u53bb\u566a\u8bc4\u5206\u5339\u914d\u673a\u5236\u589e\u5f3a\u5916\u89c2\u611f\u77e5\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u7efc\u5408\u8003\u8651\u573a\u666f\u3001\u8fd0\u52a8\u548c\u5916\u89c2\u4e09\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u8ba1\u7b97\u66f4\u5168\u9762\u7684\u5f02\u5e38\u6307\u6807\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2506.24124", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24124", "abs": "https://arxiv.org/abs/2506.24124", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "comment": "Code: https://github.com/Ironieser/TimesCLIP", "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u5316\u4e3a\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u6a21\u6001\u6570\u503c\u8f93\u5165\u96be\u4ee5\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u6a21\u5f0f\uff0c\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u7c7b\u76f4\u89c9\u7684\u89c6\u89c9\u6a21\u5f0f\u7406\u89e3\u3002", "method": "\u6784\u5efa\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u53d8\u91cf\u9009\u62e9\u6a21\u5757\u4f18\u5316\u591a\u53d8\u91cf\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23283", "abs": "https://arxiv.org/abs/2506.23283", "authors": ["Yuhuan Yang", "Chaofan Ma", "Zhenjie Mao", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition", "comment": "ICML 2025 paper", "summary": "Video understanding is a complex challenge that requires effective modeling\nof spatial-temporal dynamics. With the success of image foundation models\n(IFMs) in image understanding, recent approaches have explored\nparameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most\nof these methods tend to process spatial and temporal information separately,\nwhich may fail to capture the full intricacy of video dynamics. In this paper,\nwe propose MoMa, an efficient adapter framework that achieves full\nspatial-temporal modeling by integrating Mamba's selective state space modeling\ninto IFMs. We propose a novel SeqMod operation to inject spatial-temporal\ninformation into pre-trained IFMs, without disrupting their original features.\nBy incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances\nvideo understanding while maintaining computational efficiency. Extensive\nexperiments on multiple video benchmarks demonstrate the effectiveness of MoMa,\nachieving superior performance with reduced computational cost.", "AI": {"tldr": "MoMa\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9002\u914d\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Mamba\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u96c6\u6210\u5230\u56fe\u50cf\u57fa\u7840\u6a21\u578b\uff08IFMs\uff09\u4e2d\uff0c\u5b9e\u73b0\u5168\u65f6\u7a7a\u5efa\u6a21\uff0c\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\u5206\u5f00\u5904\u7406\uff0c\u96be\u4ee5\u6355\u6349\u89c6\u9891\u52a8\u6001\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5168\u65f6\u7a7a\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSeqMod\u64cd\u4f5c\uff0c\u5c06\u65f6\u7a7a\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3\u7684IFMs\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7279\u5f81\u4e0d\u53d8\uff0c\u7ed3\u5408Divide-and-Modulate\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u5347\u4e14\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "MoMa\u901a\u8fc7\u5168\u65f6\u7a7a\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.23285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23285", "abs": "https://arxiv.org/abs/2506.23285", "authors": ["Daqian Shi", "Xiaolei Diao", "Xu Chen", "C\u00e9dric M. John"], "title": "Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification", "comment": "Accepted by ICCV 2025", "summary": "Deep Neural Networks (DNNs) have significantly advanced the field of computer\nvision. To improve DNN training process, knowledge distillation methods\ndemonstrate their effectiveness in accelerating network training by introducing\na fixed learning direction from the teacher network to student networks. In\nthis context, several distillation-based optimization strategies are proposed,\ne.g., deep mutual learning and self-distillation, as an attempt to achieve\ngeneric training performance enhancement through the cooperative training of\nmultiple networks. However, such strategies achieve limited improvements due to\nthe poor understanding of the impact of learning directions among networks\nacross different iterations. In this paper, we propose a novel competitive\ndistillation strategy that allows each network in a group to potentially act as\na teacher based on its performance, enhancing the overall learning performance.\nCompetitive distillation organizes a group of networks to perform a shared task\nand engage in competition, where competitive optimization is proposed to\nimprove the parameter updating process. We further introduce stochastic\nperturbation in competitive distillation, aiming to motivate networks to induce\nmutations to achieve better visual representations and global optimum. The\nexperimental results show that competitive distillation achieves promising\nperformance in diverse tasks and datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ade\u4e89\u84b8\u998f\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8868\u73b0\u6700\u4f73\u7684\u7f51\u7edc\u4f5c\u4e3a\u6559\u5e08\uff0c\u63d0\u5347\u6574\u4f53\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u56e0\u5bf9\u5b66\u4e60\u65b9\u5411\u5f71\u54cd\u7406\u89e3\u4e0d\u8db3\u800c\u6539\u8fdb\u6709\u9650\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u3002", "method": "\u91c7\u7528\u7ade\u4e89\u84b8\u998f\u7b56\u7565\uff0c\u7f51\u7edc\u7ec4\u57fa\u4e8e\u6027\u80fd\u7ade\u4e89\u52a8\u6001\u9009\u62e9\u6559\u5e08\uff0c\u5e76\u5f15\u5165\u968f\u673a\u6270\u52a8\u4f18\u5316\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ade\u4e89\u84b8\u998f\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ade\u4e89\u84b8\u998f\u901a\u8fc7\u52a8\u6001\u7ade\u4e89\u548c\u968f\u673a\u6270\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2506.23292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23292", "abs": "https://arxiv.org/abs/2506.23292", "authors": ["Changtao Miao", "Yi Zhang", "Weize Gao", "Man Luo", "Weiwei Feng", "Zhiya Tan", "Jianshu Li", "Ajian Liu", "Yunfeng Diao", "Qi Chu", "Tao Gong", "Zhe Li", "Weibin Yao", "Joey Tianyi Zhou"], "title": "DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios", "comment": "This paper is a preliminary version, with an extended and\n  comprehensive version currently under development", "summary": "Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability. In\ncritical domains such as law, interpretability is crucial for enhancing the\ncredibility and authority of decisions. Recent studies attempt to improve the\ninterpretability of classification results by providing spatial manipulation\nmasks or temporal forgery segments. However, the practical effectiveness of\nthese methods remains suboptimal due to limitations of the forgery data. Most\ncurrent deepfake datasets predominantly offer binary labels, only a few\ndatasets with localization annotations. However, they suffer from restricted\nforgery scenarios, limited diversity in deepfake types, and insufficient data\nscale, making them inadequate for complex real-world scenarios. To address this\npredicament, we construct a novel large-scale deepfake detection and\nlocalization ($\\textbf{DDL}$) dataset containing over $\\textbf{1.8M}$ forged\nsamples and encompassing up to $\\textbf{75}$ distinct deepfake methods. The DDL\ndesign incorporates four key innovations: (1) $\\textbf{Diverse Forgery\nScenarios}$, (2) $\\textbf{Comprehensive Deepfake Methods}$, (3) $\\textbf{Varied\nManipulation Modes}$, and (4) $\\textbf{Fine-grained Forgery Annotations}$.\nThrough these improvements, our DDL not only provides a more challenging\nbenchmark for complex real-world forgeries, but also offers crucial support for\nbuilding next-generation deepfake detection, localization, and interpretability\nmethods. The DDL dataset project page is on\nhttps://deepfake-workshop-ijcai2025.github.io/main/index.html.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u89c4\u6a21\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6570\u636e\u96c6\uff08DDL\uff09\uff0c\u5305\u542b180\u4e07\u4f2a\u9020\u6837\u672c\u548c75\u79cd\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u3001\u89c4\u6a21\u548c\u6ce8\u91ca\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u4e14\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u548c\u89c4\u6a21\u4e0a\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u73b0\u5b9e\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86DDL\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u4f2a\u9020\u573a\u666f\u3001\u5168\u9762\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u3001\u591a\u79cd\u64cd\u7eb5\u6a21\u5f0f\u548c\u7ec6\u7c92\u5ea6\u4f2a\u9020\u6ce8\u91ca\u3002", "result": "DDL\u6570\u636e\u96c6\u4e3a\u590d\u6742\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5e76\u652f\u6301\u4e0b\u4e00\u4ee3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "conclusion": "DDL\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2506.23295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23295", "abs": "https://arxiv.org/abs/2506.23295", "authors": ["Xiang Xu"], "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On", "comment": null, "summary": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing\na target garment, with broad applications in e-commerce and digital fashion.\nWhile recent advances in latent diffusion models have substantially improved\nvisual quality, existing approaches still struggle with preserving fine-grained\ngarment details, achieving precise garment-body alignment, maintaining\ninference efficiency, and generalizing to diverse poses and clothing styles. To\naddress these challenges, we propose DiffFit, a novel two-stage latent\ndiffusion framework for high-fidelity virtual try-on. DiffFit adopts a\nprogressive generation strategy: the first stage performs geometry-aware\ngarment warping, aligning the garment with the target body through fine-grained\ndeformation and pose adaptation. The second stage refines texture fidelity via\na cross-modal conditional diffusion model that integrates the warped garment,\nthe original garment appearance, and the target person image for high-quality\nrendering. By decoupling geometric alignment and appearance refinement, DiffFit\neffectively reduces task complexity and enhances both generation stability and\nvisual realism. It excels in preserving garment-specific attributes such as\ntextures, wrinkles, and lighting, while ensuring accurate alignment with the\nhuman body. Extensive experiments on large-scale VTON benchmarks demonstrate\nthat DiffFit achieves superior performance over existing state-of-the-art\nmethods in both quantitative metrics and perceptual evaluations.", "AI": {"tldr": "DiffFit\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u865a\u62df\u8bd5\u7a7f\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u670d\u88c5\u53d8\u5f62\u548c\u8de8\u6a21\u6001\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u670d\u88c5\u7ec6\u8282\u4fdd\u7559\u3001\u5bf9\u9f50\u7cbe\u5ea6\u548c\u6548\u7387\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5728\u670d\u88c5\u7ec6\u8282\u4fdd\u7559\u3001\u5bf9\u9f50\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u591a\u6837\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cDiffFit\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffFit\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u51e0\u4f55\u611f\u77e5\u7684\u670d\u88c5\u53d8\u5f62\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8de8\u6a21\u6001\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4f18\u5316\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002", "result": "DiffFit\u5728\u5927\u89c4\u6a21\u865a\u62df\u8bd5\u7a7f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9a\u91cf\u548c\u611f\u77e5\u8bc4\u4f30\u5747\u663e\u793a\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "DiffFit\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u5bf9\u9f50\u548c\u5916\u89c2\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u751f\u6210\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002"}}
{"id": "2506.23308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23308", "abs": "https://arxiv.org/abs/2506.23308", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-Endo-4DGX/", "summary": "Accurate reconstruction of soft tissue is crucial for advancing automation in\nimage-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)\ntechniques and their variants, 4DGS, achieve high-quality renderings of dynamic\nsurgical scenes in real-time. However, 3D-GS-based methods still struggle in\nscenarios with varying illumination, such as low light and over-exposure.\nTraining 3D-GS in such extreme light conditions leads to severe optimization\nproblems and devastating rendering quality. To address these challenges, we\npresent Endo-4DGX, a novel reconstruction method with illumination-adaptive\nGaussian Splatting designed specifically for endoscopic scenes with uneven\nlighting. By incorporating illumination embeddings, our method effectively\nmodels view-dependent brightness variations. We introduce a region-aware\nenhancement module to model the sub-area lightness at the Gaussian level and a\nspatial-aware adjustment module to learn the view-consistent brightness\nadjustment. With the illumination adaptive design, Endo-4DGX achieves superior\nrendering performance under both low-light and over-exposure conditions while\nmaintaining geometric accuracy. Additionally, we employ an exposure control\nloss to restore the appearance from adverse exposure to the normal level for\nillumination-adaptive optimization. Experimental results demonstrate that\nEndo-4DGX significantly outperforms combinations of state-of-the-art\nreconstruction and restoration methods in challenging lighting environments,\nunderscoring its potential to advance robot-assisted surgical applications. Our\ncode is available at https://github.com/lastbasket/Endo-4DGX.", "AI": {"tldr": "Endo-4DGX\u662f\u4e00\u79cd\u65b0\u578b\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u7167\u81ea\u9002\u5e94\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u5149\u7167\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u5728\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u624b\u672f\u4e2d\uff0c\u8f6f\u7ec4\u7ec7\u7684\u7cbe\u786e\u91cd\u5efa\u81f3\u5173\u91cd\u8981\u3002\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "method": "Endo-4DGX\u7ed3\u5408\u5149\u7167\u5d4c\u5165\u3001\u533a\u57df\u611f\u77e5\u589e\u5f3a\u6a21\u5757\u548c\u7a7a\u95f4\u611f\u77e5\u8c03\u6574\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5149\u7167\u81ea\u9002\u5e94\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEndo-4DGX\u5728\u4f4e\u5149\u548c\u8fc7\u66dd\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u3002", "conclusion": "Endo-4DGX\u5728\u6311\u6218\u6027\u5149\u7167\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u5e94\u7528\u3002"}}
{"id": "2506.23323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23323", "abs": "https://arxiv.org/abs/2506.23323", "authors": ["Quang-Huy Che", "Vinh-Tiep Nguyen"], "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method", "comment": null, "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency.", "AI": {"tldr": "FastSeg\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff0c\u7ed3\u5408\u53cc\u63d0\u793a\u673a\u5236\u548c\u5206\u5c42\u6ce8\u610f\u529b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u5168\u5c40\u8868\u793a\u504f\u5dee\u5bfc\u81f4\u7684\u7a7a\u95f4\u7cbe\u5ea6\u635f\u5931\u95ee\u9898\uff0c\u540c\u65f6\u5e73\u8861\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u6b21\u6570\u4e0e\u5206\u5272\u8d28\u91cf\u3002", "method": "\u63d0\u51faFastSeg\u6846\u67b6\uff0c\u4ec5\u9700\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684(1+1)\u6b65\u53cd\u5411\u8fc7\u7a0b\uff0c\u7ed3\u5408\u53cc\u63d0\u793a\u673a\u5236\u3001\u5206\u5c42\u6ce8\u610f\u529b\u7ec6\u5316\u65b9\u6cd5\uff08HARD\uff09\u548c\u6d4b\u8bd5\u65f6\u7ffb\u8f6c\uff08TTF\uff09\u65b9\u6848\u3002", "result": "\u5728PASCAL VOC\u3001PASCAL Context\u548cCOCO Object\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747mIoU\u8fbe\u523043.8%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "FastSeg\u5728\u5206\u5272\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.23329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23329", "abs": "https://arxiv.org/abs/2506.23329", "authors": ["Parker Liu", "Chenxin Li", "Zhengxin Li", "Yipeng Wu", "Wuyang Li", "Zhiqin Yang", "Zhenyuan Zhang", "Yunlong Lin", "Sirui Han", "Brandon Y. Feng"], "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "comment": "Project Page: https://ir3d-bench.github.io/", "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.", "AI": {"tldr": "IR3D-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8981\u6c42\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3b\u52a8\u4f7f\u7528\u5de5\u5177\u91cd\u5efa\u56fe\u50cf\u76843D\u7ed3\u6784\uff0c\u8bc4\u4f30\u5176\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u573a\u666f\u4ecd\u4e0d\u786e\u5b9a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u521b\u5efa\u800c\u975e\u88ab\u52a8\u8bc6\u522b\u6765\u9a8c\u8bc1\u5176\u7406\u89e3\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5206\u6790-\u5408\u6210\u8303\u5f0f\uff0cIR3D-Bench\u8981\u6c42\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff08VLAs\uff09\u4f7f\u7528\u7f16\u7a0b\u548c\u6e32\u67d3\u5de5\u5177\u91cd\u5efa\u8f93\u5165\u56fe\u50cf\u76843D\u7ed3\u6784\uff0c\u5b9e\u73b0\u9006\u5411\u6e32\u67d3\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524dVLMs\u5728\u89c6\u89c9\u7cbe\u5ea6\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u800c\u975e\u57fa\u672c\u5de5\u5177\u4f7f\u7528\u3002", "conclusion": "IR3D-Bench\u4e3a\u7cfb\u7edf\u7814\u7a76\u548c\u5f00\u53d1\u5de5\u5177\u4f7f\u7528\u7684VLAs\u63d0\u4f9b\u4e86\u6570\u636e\u4e0e\u8bc4\u4f30\u534f\u8bae\uff0c\u63a8\u52a8\u901a\u8fc7\u521b\u5efa\u5b9e\u73b0\u771f\u6b63\u7684\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2506.23347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23347", "abs": "https://arxiv.org/abs/2506.23347", "authors": ["Yi Liu", "Shengqian Li", "Zuzeng Lin", "Feng Wang", "Si Liu"], "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation", "comment": null, "summary": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCycleVAR\uff0c\u4e00\u79cd\u57fa\u4e8eSoftmax Relaxed Quantization\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u6982\u7387\u6df7\u5408\u4fdd\u7559\u68af\u5ea6\u4f20\u64ad\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5411\u91cf\u91cf\u5316\u7684\u65b9\u6cd5\u56e0\u79bb\u6563\u91cf\u5316\u963b\u788d\u68af\u5ea6\u4f20\u64ad\uff0c\u9650\u5236\u4e86\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528Softmax Relaxed Quantization\u5b9e\u73b0\u8fde\u7eed\u6982\u7387\u6df7\u5408\uff0c\u63d0\u51faCycleVAR\u6846\u67b6\uff0c\u652f\u6301\u4e32\u884c\u591a\u6b65\u548c\u5e76\u884c\u5355\u6b65\u751f\u6210\u6a21\u5f0f\u3002", "result": "\u5e76\u884c\u5355\u6b65\u751f\u6210\u6a21\u5f0f\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4e32\u884c\u6a21\u5f0f\uff0cCycleVAR\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5982CycleGAN-Turbo\u3002", "conclusion": "CycleVAR\u901a\u8fc7\u6539\u8fdb\u91cf\u5316\u65b9\u6cd5\u548c\u751f\u6210\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u7684\u6548\u679c\u3002"}}
{"id": "2506.23352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23352", "abs": "https://arxiv.org/abs/2506.23352", "authors": ["Shunsuke Yasuki", "Taiki Miyanishi", "Nakamasa Inoue", "Shuhei Kurita", "Koya Sakamoto", "Daichi Azuma", "Masato Taki", "Yutaka Matsuo"], "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields", "comment": "Accepted by ICCV 2025", "summary": "The advancement of 3D language fields has enabled intuitive interactions with\n3D scenes via natural language. However, existing approaches are typically\nlimited to small-scale environments, lacking the scalability and compositional\nreasoning capabilities necessary for large, complex urban settings. To overcome\nthese limitations, we propose GeoProg3D, a visual programming framework that\nenables natural language-driven interactions with city-scale high-fidelity 3D\nscenes. GeoProg3D consists of two key components: (i) a Geography-aware\nCity-scale 3D Language Field (GCLF) that leverages a memory-efficient\nhierarchical 3D model to handle large-scale data, integrated with geographic\ninformation for efficiently filtering vast urban spaces using directional cues,\ndistance measurements, elevation data, and landmark references; and (ii)\nGeographical Vision APIs (GV-APIs), specialized geographic vision tools such as\narea segmentation and object detection. Our framework employs large language\nmodels (LLMs) as reasoning engines to dynamically combine GV-APIs and operate\nGCLF, effectively supporting diverse geographic vision tasks. To assess\nperformance in city-scale reasoning, we introduce GeoEval3D, a comprehensive\nbenchmark dataset containing 952 query-answer pairs across five challenging\ntasks: grounding, spatial reasoning, comparison, counting, and measurement.\nExperiments demonstrate that GeoProg3D significantly outperforms existing 3D\nlanguage fields and vision-language models across multiple tasks. To our\nknowledge, GeoProg3D is the first framework enabling compositional geographic\nreasoning in high-fidelity city-scale 3D environments via natural language. The\ncode is available at https://snskysk.github.io/GeoProg3D/.", "AI": {"tldr": "GeoProg3D\u662f\u4e00\u4e2a\u89c6\u89c9\u7f16\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u57ce\u5e02\u89c4\u6a21\u9ad8\u4fdd\u771f3D\u573a\u666f\u7684\u4ea4\u4e92\uff0c\u7ed3\u5408\u5730\u7406\u611f\u77e5\u548c\u89c6\u89c9API\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u8bed\u8a00\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u73af\u5883\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u7684\u53ef\u6269\u5c55\u6027\u548c\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faGeoProg3D\u6846\u67b6\uff0c\u5305\u542b\u5730\u7406\u611f\u77e5\u57ce\u5e02\u89c4\u6a213D\u8bed\u8a00\u573a\uff08GCLF\uff09\u548c\u5730\u7406\u89c6\u89c9API\uff08GV-APIs\uff09\uff0c\u5229\u7528LLM\u52a8\u6001\u7ec4\u5408\u5de5\u5177\u3002", "result": "\u5728GeoEval3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoProg3D\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoProg3D\u662f\u9996\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u57ce\u5e02\u89c4\u6a213D\u573a\u666f\u7ec4\u5408\u5730\u7406\u63a8\u7406\u7684\u6846\u67b6\u3002"}}
{"id": "2506.23353", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23353", "abs": "https://arxiv.org/abs/2506.23353", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "comment": null, "summary": "Infrared image helps improve the perception capabilities of autonomous\ndriving in complex weather conditions such as fog, rain, and low light.\nHowever, infrared image often suffers from low contrast, especially in\nnon-heat-emitting targets like bicycles, which significantly affects the\nperformance of downstream high-level vision tasks. Furthermore, achieving\ncontrast enhancement without amplifying noise and losing important information\nremains a challenge. To address these challenges, we propose a task-oriented\ninfrared image enhancement method. Our approach consists of two key components:\nlayer decomposition and saliency information extraction. First, we design an\nlayer decomposition method for infrared images, which enhances scene details\nwhile preserving dark region features, providing more features for subsequent\nsaliency information extraction. Then, we propose a morphological\nreconstruction-based saliency extraction method that effectively extracts and\nenhances target information without amplifying noise. Our method improves the\nimage quality for object detection and semantic segmentation tasks. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u7ea2\u5916\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u5206\u89e3\u548c\u663e\u8457\u6027\u4fe1\u606f\u63d0\u53d6\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "motivation": "\u7ea2\u5916\u56fe\u50cf\u5728\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\uff08\u5982\u96fe\u3001\u96e8\u3001\u4f4e\u5149\uff09\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5176\u4f4e\u5bf9\u6bd4\u5ea6\u95ee\u9898\uff08\u5c24\u5176\u662f\u975e\u70ed\u8f90\u5c04\u76ee\u6807\uff09\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u5982\u4f55\u5728\u589e\u5f3a\u5bf9\u6bd4\u5ea6\u7684\u540c\u65f6\u907f\u514d\u566a\u58f0\u653e\u5927\u548c\u4fe1\u606f\u4e22\u5931\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u90e8\u5206\uff1a1\uff09\u7ea2\u5916\u56fe\u50cf\u7684\u5c42\u5206\u89e3\uff0c\u589e\u5f3a\u573a\u666f\u7ec6\u8282\u5e76\u4fdd\u7559\u6697\u533a\u7279\u5f81\uff1b2\uff09\u57fa\u4e8e\u5f62\u6001\u5b66\u91cd\u5efa\u7684\u663e\u8457\u6027\u63d0\u53d6\uff0c\u6709\u6548\u589e\u5f3a\u76ee\u6807\u4fe1\u606f\u4e14\u4e0d\u653e\u5927\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u4f4e\u5bf9\u6bd4\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23361", "abs": "https://arxiv.org/abs/2506.23361", "authors": ["Yuanhao Cai", "He Zhang", "Xi Chen", "Jinbo Xing", "Yiwei Hu", "Yuqian Zhou", "Kai Zhang", "Zhifei Zhang", "Soo Ye Kim", "Tianyu Wang", "Yulun Zhang", "Xiaokang Yang", "Zhe Lin", "Alan Yuille"], "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions", "comment": "A data construction pipeline and a diffusion Transformer framework\n  for controllable subject-driven video customization", "summary": "Existing feedforward subject-driven video customization methods mainly study\nsingle-subject scenarios due to the difficulty of constructing multi-subject\ntraining data pairs. Another challenging problem that how to use the signals\nsuch as depth, mask, camera, and text prompts to control and edit the subject\nin the customized video is still less explored. In this paper, we first propose\na data construction pipeline, VideoCus-Factory, to produce training data pairs\nfor multi-subject customization from raw videos without labels and control\nsignals such as depth-to-video and mask-to-video pairs. Based on our\nconstructed data, we develop an Image-Video Transfer Mixed (IVTM) training with\nimage editing data to enable instructive editing for the subject in the\ncustomized video. Then we propose a diffusion Transformer framework, OmniVCus,\nwith two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned\nEmbedding (TAE). LE enables inference with more subjects by using the training\nsubjects to activate more frame embeddings. TAE encourages the generation\nprocess to extract guidance from temporally aligned control signals by\nassigning the same frame embeddings to the control and noise tokens.\nExperiments demonstrate that our method significantly surpasses\nstate-of-the-art methods in both quantitative and qualitative evaluations.\nVideo demos are at our project page:\nhttps://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released\nat https://github.com/caiyuanhao1998/Open-OmniVCus", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6784\u9020\u7ba1\u9053\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u6269\u6563Transformer\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e3b\u4f53\u573a\u666f\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u591a\u4e3b\u4f53\u8bad\u7ec3\u6570\u636e\u548c\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u6df1\u5ea6\u3001\u906e\u7f69\u7b49\uff09\u7684\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faVideoCus-Factory\u6570\u636e\u6784\u9020\u7ba1\u9053\u751f\u6210\u591a\u4e3b\u4f53\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408IVTM\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86OmniVCus\u6846\u67b6\uff0c\u5305\u542bLottery Embedding\u548cTemporally Aligned Embedding\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u6784\u9020\u548c\u6846\u67b6\u521b\u65b0\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u548c\u63a7\u5236\u4fe1\u53f7\u7684\u6709\u6548\u5229\u7528\u3002"}}
{"id": "2506.23382", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23382", "abs": "https://arxiv.org/abs/2506.23382", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "AI": {"tldr": "SIEDD\u662f\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u5668\u548c\u79bb\u6563\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u52a0\u901f\u4e86\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u89c6\u9891\u7f16\u7801\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u538b\u7f29\u6bd4\u3002", "motivation": "\u89e3\u51b3INR\u89c6\u9891\u7f16\u7801\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u91cd\u5efa\u8d28\u91cf\u6216\u5750\u6807\u7ea7\u63a7\u5236\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7f16\u7801\u5668\u6355\u83b7\u5168\u5c40\u4f4e\u9891\u7279\u5f81\uff0c\u5e76\u884c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u79bb\u6563\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u5750\u6807\u7a7a\u95f4\u91c7\u6837\u52a0\u901f\u3002", "result": "\u5728HD\u548c4K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7f16\u7801\u901f\u5ea6\u63d0\u534720-30\u500d\uff0c\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u538b\u7f29\u6bd4\u3002", "conclusion": "SIEDD\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u4fdd\u771f\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.23414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23414", "abs": "https://arxiv.org/abs/2506.23414", "authors": ["Ming-Zher Poh", "Jonathan Wang", "Jonathan Hsu", "Lawrence Cai", "Eric Teasley", "James A. Taylor", "Jameson K. Rogers", "Anupam Pathak", "Shwetak Patel"], "title": "A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video", "comment": null, "summary": "Smartphone-based heart rate (HR) monitoring apps using finger-over-camera\nphotoplethysmography (PPG) face significant challenges in performance\nevaluation and device compatibility due to device variability and\nfragmentation. Manual testing is impractical, and standardized methods are\nlacking. This paper presents a novel, high-throughput bench-testing platform to\naddress this critical need. We designed a system comprising a test rig capable\nof holding 12 smartphones for parallel testing, a method for generating\nsynthetic PPG test videos with controllable HR and signal quality, and a host\nmachine for coordinating video playback and data logging. The system achieved a\nmean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and\nmeasured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and\nmeasured PPG signals using a clinically-validated smartphone-based HR app.\nBench-testing results of 20 different smartphone models correctly classified\nall the devices as meeting the ANSI/CTA accuracy standards for HR monitors\n(MAPE <10%) when compared to a prospective clinical study with 80 participants,\ndemonstrating high positive predictive value. This platform offers a scalable\nsolution for pre-deployment testing of smartphone HR apps to improve app\nperformance, ensure device compatibility, and advance the field of mobile\nhealth.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9ad8\u901a\u91cf\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u624b\u673a\u5fc3\u7387\u76d1\u6d4b\u5e94\u7528\u7684\u6027\u80fd\u4e0e\u8bbe\u5907\u517c\u5bb9\u6027\uff0c\u89e3\u51b3\u4e86\u8bbe\u5907\u591a\u6837\u6027\u548c\u6807\u51c6\u5316\u6d4b\u8bd5\u7684\u6311\u6218\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u5fc3\u7387\u76d1\u6d4b\u5e94\u7528\u56e0\u8bbe\u5907\u591a\u6837\u6027\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6027\u80fd\u8bc4\u4f30\u548c\u517c\u5bb9\u6027\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b12\u90e8\u667a\u80fd\u624b\u673a\u5e76\u884c\u6d4b\u8bd5\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u751f\u6210\u53ef\u63a7\u5fc3\u7387\u4e0e\u4fe1\u53f7\u8d28\u91cf\u7684\u5408\u6210PPG\u6d4b\u8bd5\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u4e3b\u673a\u534f\u8c03\u89c6\u9891\u64ad\u653e\u4e0e\u6570\u636e\u8bb0\u5f55\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8f93\u5165\u4e0e\u6d4b\u91cf\u5fc3\u7387\u4e4b\u95f4\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a0.11% +/- 0.001%\uff0c\u8f93\u5165\u4e0e\u6d4b\u91cfPPG\u4fe1\u53f7\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\u4e3a0.92 +/- 0.008\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u667a\u80fd\u624b\u673a\u5fc3\u7387\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9884\u90e8\u7f72\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u5e94\u7528\u6027\u80fd\u3001\u8bbe\u5907\u517c\u5bb9\u6027\uff0c\u5e76\u63a8\u52a8\u4e86\u79fb\u52a8\u5065\u5eb7\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23418", "abs": "https://arxiv.org/abs/2506.23418", "authors": ["Parham Rezaei", "Arash Marioriyad", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models", "comment": "12 main pages, 18 figures, and 16 tables", "summary": "Despite the ability of text-to-image models to generate high-quality,\nrealistic, and diverse images, they face challenges in compositional\ngeneration, often struggling to accurately represent details specified in the\ninput prompt. A prevalent issue in compositional generation is the misalignment\nof spatial relationships, as models often fail to faithfully generate images\nthat reflect the spatial configurations specified between objects in the input\nprompts. To address this challenge, we propose a novel probabilistic framework\nfor modeling the relative spatial positioning of objects in a scene, leveraging\nthe concept of Probability of Superiority (PoS). Building on this insight, we\nmake two key contributions. First, we introduce a novel evaluation metric,\nPoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D\nspatial relationships between text and image, with improved adherence to human\njudgment. Second, we propose PoS-based Generation (PSG), an inference-time\nmethod that improves the alignment of 2D and 3D spatial relationships in T2I\nmodels without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based\nreward function that can be utilized in two distinct ways: (1) as a\ngradient-based guidance mechanism applied to the cross-attention maps during\nthe denoising steps, or (2) as a search-based strategy that evaluates a set of\ninitial noise vectors to select the best one. Extensive experiments demonstrate\nthat the PSE metric exhibits stronger alignment with human judgment compared to\ntraditional center-based metrics, providing a more nuanced and reliable measure\nof complex spatial relationship accuracy in text-image alignment. Furthermore,\nPSG significantly enhances the ability of text-to-image models to generate\nimages with specified spatial configurations, outperforming state-of-the-art\nmethods across multiple evaluation metrics and benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u4f18\u52bf\uff08PoS\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5305\u62ec\u65b0\u7684\u8bc4\u4f30\u6307\u6807PSE\u548c\u751f\u6210\u65b9\u6cd5PSG\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u914d\u7f6e\u7684\u51c6\u786e\u6027\u3002", "method": "1. \u63d0\u51faPoS-based Evaluation (PSE)\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff1b2. \u63d0\u51faPoS-based Generation (PSG)\u4f5c\u4e3a\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "PSE\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e00\u81f4\uff0cPSG\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "conclusion": "PoS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u80fd\u529b\uff0cPSE\u548cPSG\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.23426", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23426", "abs": "https://arxiv.org/abs/2506.23426", "authors": ["Menna Taha", "Aya Ahmed", "Mohammed Karmoose", "Yasser Gadallah"], "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u5371\u5bb3\u6027\u800c\u975e\u4f20\u7edf\u5206\u7c7b\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u672a\u77e5\u7269\u4f53\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u672a\u77e5\u7269\u4f53\uff08OOD\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u9690\u60a3\uff0c\u5982\u8bef\u5224\u6216\u6f0f\u68c0\u5371\u9669\u7269\u4f53\u3002", "method": "\u901a\u8fc7\u5bf9\u8c61\u4f4d\u7f6e\u548c\u8f68\u8ff9\u5224\u65ad\u5176\u5371\u5bb3\u6027\uff08\u6709\u5bb3/\u65e0\u5bb3\uff09\uff0c\u800c\u975e\u5177\u4f53\u7c7b\u522b\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4bOOD\u7269\u4f53\u5e76\u8bc4\u4f30\u5176\u5371\u5bb3\u6027\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u5b89\u5168\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2506.23440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23440", "abs": "https://arxiv.org/abs/2506.23440", "authors": ["Mahesh Bhosale", "Abdul Wasi", "Yuanhao Zhai", "Yunjie Tian", "Samuel Border", "Nan Xi", "Pinaki Sarder", "Junsong Yuan", "David Doermann", "Xuan Gong"], "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions", "comment": "Accepted to ICCV 2025", "summary": "Diffusion-based generative models have shown promise in synthesizing\nhistopathology images to address data scarcity caused by privacy constraints.\nDiagnostic text reports provide high-level semantic descriptions, and masks\noffer fine-grained spatial structures essential for representing distinct\nmorphological regions. However, public datasets lack paired text and mask data\nfor the same histopathological images, limiting their joint use in image\ngeneration. This constraint restricts the ability to fully exploit the benefits\nof combining both modalities for enhanced control over semantics and spatial\ndetails. To overcome this, we propose PathDiff, a diffusion framework that\neffectively learns from unpaired mask-text data by integrating both modalities\ninto a unified conditioning space. PathDiff allows precise control over\nstructural and contextual features, generating high-quality, semantically\naccurate images. PathDiff also improves image fidelity, text-image alignment,\nand faithfulness, enhancing data augmentation for downstream tasks like nuclei\nsegmentation and classification. Extensive experiments demonstrate its\nsuperiority over existing methods.", "AI": {"tldr": "PathDiff\u662f\u4e00\u4e2a\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u672a\u914d\u5bf9\u7684\u6587\u672c\u548c\u63a9\u7801\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u75c5\u7406\u56fe\u50cf\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u6587\u672c\u548c\u63a9\u7801\u6570\u636e\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u63a7\u5236\u3002", "method": "\u63d0\u51faPathDiff\u6846\u67b6\uff0c\u5c06\u672a\u914d\u5bf9\u7684\u6587\u672c\u548c\u63a9\u7801\u6570\u636e\u6574\u5408\u5230\u7edf\u4e00\u7684\u6761\u4ef6\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u7cbe\u786e\u63a7\u5236\u3002", "result": "PathDiff\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u9ad8\u3001\u8bed\u4e49\u51c6\u786e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\u3001\u6587\u672c\u5bf9\u9f50\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "PathDiff\u5728\u75c5\u7406\u56fe\u50cf\u751f\u6210\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u6570\u636e\u589e\u5f3a\u548c\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23460", "abs": "https://arxiv.org/abs/2506.23460", "authors": ["Dewen Zeng", "Xinrong Hu", "Yu-Jen Chen", "Yawen Wu", "Xiaowei Xu", "Yiyu Shi"], "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation", "comment": null, "summary": "Weakly supervised semantic segmentation (WSSS) methods using class labels\noften rely on class activation maps (CAMs) to localize objects. However,\ntraditional CAM-based methods struggle with partial activations and imprecise\nobject boundaries due to optimization discrepancies between classification and\nsegmentation. Recently, the conditional diffusion model (CDM) has been used as\nan alternative for generating segmentation masks in WSSS, leveraging its strong\nimage generation capabilities tailored to specific class distributions. By\nmodifying or perturbing the condition during diffusion sampling, the related\nobjects can be highlighted in the generated images. Yet, the saliency maps\ngenerated by CDMs are prone to noise from background alterations during reverse\ndiffusion. To alleviate the problem, we introduce Contrastive Learning with\nDiffusion Features (CLDF), a novel method that uses contrastive learning to\ntrain a pixel decoder to map the diffusion features from a frozen CDM to a\nlow-dimensional embedding space for segmentation. Specifically, we integrate\ngradient maps generated from CDM external classifier with CAMs to identify\nforeground and background pixels with fewer false positives/negatives for\ncontrastive learning, enabling robust pixel embedding learning. Experimental\nresults on four segmentation tasks from two public medical datasets demonstrate\nthat our method significantly outperforms existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff08CLDF\uff09\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08WSSS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u7c7b\u6fc0\u6d3b\u56fe\uff08CAMs\uff09\u6539\u8fdb\u5206\u5272\u6548\u679c\u3002", "motivation": "\u4f20\u7edfCAM\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u90e8\u5206\u6fc0\u6d3b\u548c\u8fb9\u754c\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u800c\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u663e\u8457\u6027\u56fe\u6613\u53d7\u80cc\u666f\u566a\u58f0\u5e72\u6270\u3002", "method": "\u63d0\u51faCLDF\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u50cf\u7d20\u89e3\u7801\u5668\uff0c\u5c06\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u7684\u7279\u5f81\u6620\u5c04\u5230\u4f4e\u7ef4\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u56fe\u548cCAMs\u51cf\u5c11\u8bef\u5224\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u533b\u5b66\u6570\u636e\u96c6\u7684\u56db\u4e2a\u5206\u5272\u4efb\u52a1\u4e2d\uff0cCLDF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CLDF\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23461", "abs": "https://arxiv.org/abs/2506.23461", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "comment": null, "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1TAMP\uff08\u65f6\u95f4\u53d8\u5316\u56fe\u50cf\u4fee\u590d\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u4fee\u590d\u53d7\u635f\u76ee\u6807\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86InDiTE\u6a21\u5757\u548cInDiTE-Diff\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u53c2\u8003\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u53d8\u5316\u56fe\u50cf\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u56fe\u50cf\u5185\u5bb9\u5dee\u5f02\u5927\u4e14\u53ef\u80fd\u53d7\u635f\u3002TAMP\u4efb\u52a1\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u5e38\u89c1\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faInDiTE\u6a21\u5757\u81ea\u9002\u5e94\u8865\u5145\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u6574\u5408\u6269\u6563\u6a21\u578b\uff08InDiTE-Diff\uff09\uff0c\u5728\u91c7\u6837\u65f6\u8fdb\u884c\u6f5c\u5728\u4ea4\u53c9\u53c2\u8003\u3002", "result": "\u5728TAMP-Street\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u8003\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002", "conclusion": "InDiTE-Diff\u4e3a\u65f6\u95f4\u53d8\u5316\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.23465", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23465", "abs": "https://arxiv.org/abs/2506.23465", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models", "comment": null, "summary": "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", "AI": {"tldr": "VLSR\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578bCLIP\u5bf9\u591a\u6807\u7b7e\u5236\u9020\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u6807\u7b7e\u6e05\u6d17\u548c\u4f18\u5316\uff0c\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u3002", "motivation": "\u5de5\u4e1a\u5e94\u7528\u4e2d\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5e38\u56e0\u6807\u7b7e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u800c\u9ad8\u8d28\u91cf\u6807\u7b7e\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\u5c06\u56fe\u50cf\u548c\u6587\u672c\u6807\u7b7e\u5d4c\u5165\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fdb\u884c\u6807\u7b7e\u6e05\u6d17\u548c\u805a\u7c7b\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eVLSR\u80fd\u6709\u6548\u8bc6\u522b\u95ee\u9898\u6807\u7b7e\u5e76\u63d0\u5347\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u6807\u7b7e\u8bcd\u6c47\u91cf\u3002", "conclusion": "VLSR\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u5e94\u7528\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2506.23467", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23467", "abs": "https://arxiv.org/abs/2506.23467", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak Jack Mortazavi", "Tianbao Yang"], "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "comment": "This preprint has been accepted by MICCAI 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.", "AI": {"tldr": "AdFair-CLIP\u901a\u8fc7\u5bf9\u6297\u6027\u7279\u5f81\u5e72\u9884\u51cf\u5c11CLIP\u6a21\u578b\u4e2d\u7684\u654f\u611f\u5c5e\u6027\u504f\u5dee\uff0c\u63d0\u5347\u80f8\u90e8X\u5149\u5206\u7c7b\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u79cd\u65cf\u548c\u6027\u522b\u7b49\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u8bca\u65ad\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faAdFair-CLIP\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u6297\u6027\u7279\u5f81\u5e72\u9884\u6291\u5236\u654f\u611f\u5c5e\u6027\uff0c\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u3002", "result": "\u5728\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\uff0cAdFair-CLIP\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u4fdd\u6301\u7a33\u5065\u3002", "conclusion": "AdFair-CLIP\u4e3a\u57fa\u4e8eCLIP\u7684\u533b\u5b66\u8bca\u65ad\u6a21\u578b\u8bbe\u5b9a\u4e86\u516c\u5e73\u6027\u5b66\u4e60\u7684\u65b0\u6807\u51c6\u3002"}}
{"id": "2506.23468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23468", "abs": "https://arxiv.org/abs/2506.23468", "authors": ["Xuan Yao", "Junyu Gao", "Changsheng Xu"], "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "comment": "Accepted by ICCV 2025", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to execute sequential navigation actions in complex environments guided\nby natural language instructions. Current approaches often struggle with\ngeneralizing to novel environments and adapting to ongoing changes during\nnavigation. Inspired by human cognition, we present NavMorph, a self-evolving\nworld model framework that enhances environmental understanding and\ndecision-making in VLN-CE tasks. NavMorph employs compact latent\nrepresentations to model environmental dynamics, equipping agents with\nforesight for adaptive planning and policy refinement. By integrating a novel\nContextual Evolution Memory, NavMorph leverages scene-contextual information to\nsupport effective navigation while maintaining online adaptability. Extensive\nexperiments demonstrate that our method achieves notable performance\nimprovements on popular VLN-CE benchmarks. Code is available at\n\\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.", "AI": {"tldr": "NavMorph\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u6f14\u5316\u8bb0\u5fc6\u63d0\u5347\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u73af\u5883\u7406\u89e3\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u65b0\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u6301\u7eed\u9002\u5e94\u80fd\u529b\u4e0d\u8db3\uff0cNavMorph\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "NavMorph\u4f7f\u7528\u7d27\u51d1\u6f5c\u5728\u8868\u793a\u5efa\u6a21\u73af\u5883\u52a8\u6001\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u6f14\u5316\u8bb0\u5fc6\uff0c\u652f\u6301\u81ea\u9002\u5e94\u89c4\u5212\u548c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "NavMorph\u901a\u8fc7\u81ea\u6f14\u5316\u7684\u4e16\u754c\u6a21\u578b\u548c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23529", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23529", "abs": "https://arxiv.org/abs/2506.23529", "authors": ["Jisu Han", "Jihee Park", "Dongyoon Han", "Wonjun Hwang"], "title": "When Test-Time Adaptation Meets Self-Supervised Models", "comment": "15 pages, 7 figures", "summary": "Training on test-time data enables deep learning models to adapt to dynamic\nenvironmental changes, enhancing their practical applicability. Online\nadaptation from source to target domains is promising but it remains highly\nreliant on the performance of source pretrained model. In this paper, we\ninvestigate whether test-time adaptation (TTA) methods can continuously improve\nmodels trained via self-supervised learning (SSL) without relying on source\npretraining. We introduce a self-supervised TTA protocol after observing that\nexisting TTA approaches struggle when directly applied to self-supervised\nmodels with low accuracy on the source domain. Furthermore, we propose a\ncollaborative learning framework that integrates SSL and TTA models, leveraging\ncontrastive learning and knowledge distillation for stepwise representation\nrefinement. We validate our method on diverse self-supervised models, including\nDINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the\neffectiveness of our approach in SSL, showing that it achieves competitive\nperformance even without source pretraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u534f\u8bae\uff0c\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548cTTA\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u6e90\u57df\u9884\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u662f\u5426\u80fd\u6301\u7eed\u6539\u8fdb\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u4e0d\u4f9d\u8d56\u6e90\u57df\u9884\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u81ea\u76d1\u7763TTA\u534f\u8bae\u548c\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u9010\u6b65\u4f18\u5316\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982DINO\u3001MoCo\u3001iBOT\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u6e90\u9884\u8bad\u7ec3\u4e5f\u80fd\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\u3002", "conclusion": "\u81ea\u76d1\u7763TTA\u534f\u8bae\u548c\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u6e90\u57df\u9884\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.23470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23470", "abs": "https://arxiv.org/abs/2506.23470", "authors": ["Ngoc-Do Tran", "Minh-Tuan Huynh", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis", "comment": null, "summary": "The rapid advancement of AI and computer vision has significantly increased\nthe demand for high-quality annotated datasets, particularly for semantic\nsegmentation. However, creating such datasets is resource-intensive, requiring\nsubstantial time, labor, and financial investment, and often raises privacy\nconcerns due to the use of real-world data. To mitigate these challenges, we\npresent SynthLab, consisting of a modular platform for visual data synthesis\nand a user-friendly interface. The modular architecture of SynthLab enables\neasy maintenance, scalability with centralized updates, and seamless\nintegration of new features. Each module handles distinct aspects of computer\nvision tasks, enhancing flexibility and adaptability. Meanwhile, its\ninteractive, user-friendly interface allows users to quickly customize their\ndata pipelines through drag-and-drop actions. Extensive user studies involving\na diverse range of users across different ages, professions, and expertise\nlevels, have demonstrated flexible usage, and high accessibility of SynthLab,\nenabling users without deep technical expertise to harness AI for real-world\napplications.", "AI": {"tldr": "SynthLab\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u89c6\u89c9\u6570\u636e\u5408\u6210\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u521b\u5efa\u7684\u8d44\u6e90\u5bc6\u96c6\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "AI\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u521b\u5efa\u8fd9\u4e9b\u6570\u636e\u96c6\u8d44\u6e90\u5bc6\u96c6\u4e14\u6d89\u53ca\u9690\u79c1\u95ee\u9898\u3002", "method": "SynthLab\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\u548c\u7528\u6237\u53cb\u597d\u754c\u9762\uff0c\u652f\u6301\u62d6\u62fd\u64cd\u4f5c\u5b9a\u5236\u6570\u636e\u7ba1\u9053\uff0c\u6613\u4e8e\u7ef4\u62a4\u548c\u6269\u5c55\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cSynthLab\u5177\u6709\u9ad8\u7075\u6d3b\u6027\u548c\u6613\u7528\u6027\uff0c\u9002\u5408\u4e0d\u540c\u80cc\u666f\u7684\u7528\u6237\u3002", "conclusion": "SynthLab\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u5408\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2506.23532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23532", "abs": "https://arxiv.org/abs/2506.23532", "authors": ["Jefferson Hernandez", "Ruozhen He", "Guha Balakrishnan", "Alexander C. Berg", "Vicente Ordonez"], "title": "GViT: Representing Images as Gaussians for Visual Recognition", "comment": null, "summary": "We introduce GVIT, a classification framework that abandons conventional\npixel or patch grid input representations in favor of a compact set of\nlearnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose\npositions, scales, orientations, colors, and opacities are optimized jointly\nwith a ViT classifier trained on top of these representations. We reuse the\nclassifier gradients as constructive guidance, steering the Gaussians toward\nclass-salient regions while a differentiable renderer optimizes an image\nreconstruction loss. We demonstrate that by 2D Gaussian input representations\ncoupled with our GVIT guidance, using a relatively standard ViT architecture,\nclosely matches the performance of a traditional patch-based ViT, reaching a\n76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.", "AI": {"tldr": "GVIT\u662f\u4e00\u79cd\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u53ef\u5b66\u4e60\u76842D\u9ad8\u65af\u8868\u793a\u66ff\u4ee3\u4f20\u7edf\u50cf\u7d20\u6216\u8865\u4e01\u7f51\u683c\u8f93\u5165\uff0c\u7ed3\u5408ViT\u5206\u7c7b\u5668\uff0c\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfViT\u3002", "motivation": "\u4f20\u7edf\u50cf\u7d20\u6216\u8865\u4e01\u7f51\u683c\u8f93\u5165\u8868\u793a\u53ef\u80fd\u6548\u7387\u4e0d\u9ad8\uff0cGVIT\u65e8\u5728\u901a\u8fc7\u7d27\u51d1\u7684\u9ad8\u65af\u8868\u793a\u63d0\u5347\u5206\u7c7b\u6548\u7387\u3002", "method": "\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u51e0\u767e\u4e2a\u53ef\u5b66\u4e60\u7684\u9ad8\u65af\u53c2\u6570\uff0c\u8054\u5408\u4f18\u5316\u5176\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\u7b49\uff0c\u5e76\u5229\u7528ViT\u5206\u7c7b\u5668\u68af\u5ea6\u6307\u5bfc\u9ad8\u65af\u805a\u7126\u4e8e\u7c7b\u522b\u663e\u8457\u533a\u57df\u3002", "result": "GVIT\u5728Imagenet-1k\u4e0a\u8fbe\u523076.9%\u7684top-1\u51c6\u786e\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfViT\u3002", "conclusion": "GVIT\u901a\u8fc7\u9ad8\u65af\u8868\u793a\u548c\u68af\u5ea6\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u63a5\u8fd1\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u7c7b\u3002"}}
{"id": "2506.23478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23478", "abs": "https://arxiv.org/abs/2506.23478", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance", "comment": null, "summary": "Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning\ndue to its simplicity and efficiency. However, it suffers from a fundamental\nlimitation: it relies solely on Euclidean distances, which often fail to\ncapture the intrinsic geometry of 3D shapes. To address this limitation, we\npropose GeoCD, a topology-aware and fully differentiable approximation of\ngeodesic distance designed to serve as a metric for 3D point cloud learning.\nOur experiments show that GeoCD consistently improves reconstruction quality\nover standard CD across various architectures and datasets. We demonstrate this\nby fine-tuning several models, initially trained with standard CD, using GeoCD.\nRemarkably, fine-tuning for a single epoch with GeoCD yields significant gains\nacross multiple evaluation metrics.", "AI": {"tldr": "GeoCD\u662f\u4e00\u79cd\u57fa\u4e8e\u6d4b\u5730\u7ebf\u8ddd\u79bb\u76843D\u70b9\u4e91\u5b66\u4e60\u5ea6\u91cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684Chamfer Distance\uff08CD\uff09\u3002", "motivation": "Chamfer Distance\uff08CD\uff09\u4ec5\u4f9d\u8d56\u6b27\u6c0f\u8ddd\u79bb\uff0c\u65e0\u6cd5\u6355\u63493D\u5f62\u72b6\u7684\u5185\u5728\u51e0\u4f55\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGeoCD\uff0c\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u4e14\u5b8c\u5168\u53ef\u5fae\u7684\u6d4b\u5730\u7ebf\u8ddd\u79bb\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u70b9\u4e91\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeoCD\u5728\u5404\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u4ec5\u9700\u5355\u8f6e\u5fae\u8c03\u5373\u53ef\u5728\u591a\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "GeoCD\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3CD\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u63493D\u51e0\u4f55\u7279\u6027\u3002"}}
{"id": "2506.23538", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23538", "abs": "https://arxiv.org/abs/2506.23538", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u7cfb\u7edf\uff0c\u7528\u4e8e\u540c\u65f6\u5b9e\u73b0\u81ea\u52a8\u5316\u5e73\u9762\u5b9a\u4f4d\u548c\u5148\u5929\u6027\u5b50\u5bab\u5f02\u5e38\uff08CUA\uff09\u8bca\u65ad\uff0c\u7ed3\u5408\u53bb\u566a\u6269\u6563\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u679c\u3002", "motivation": "\u5148\u5929\u6027\u5b50\u5bab\u5f02\u5e38\uff08CUAs\uff09\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b55\u3001\u6d41\u4ea7\u548c\u598a\u5a20\u5e76\u53d1\u75c7\uff0c\u4f20\u7edf2D\u8d85\u58f0\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\uff0c\u800c3D\u8d85\u58f0\u80fd\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u5b50\u5bab\u5f62\u6001\u53ef\u89c6\u5316\u3002", "method": "1) \u5f00\u53d1\u4e86\u7ed3\u5408\u5c40\u90e8\uff08\u5e73\u9762\uff09\u548c\u5168\u5c40\uff08\u4f53\u79ef/\u6587\u672c\uff09\u5f15\u5bfc\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff1b2) \u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u63d0\u53d6\u5173\u952e\u5207\u7247\uff1b3) \u63d0\u4f9b\u6587\u672c\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4ee5\u4f18\u5316\u5206\u7c7b\u6982\u7387\u3002", "result": "\u5728\u5927\u89c4\u6a213D\u5b50\u5bab\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u9762\u5b9a\u4f4d\u548cCUA\u8bca\u65ad\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684CUA\u8bca\u65ad\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2506.23479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23479", "abs": "https://arxiv.org/abs/2506.23479", "authors": ["Zhaojie Zeng", "Yuesong Wang", "Chao Yang", "Tao Guan", "Lili Ju"], "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting", "comment": null, "summary": "Implicit Neural Representation (INR) has demonstrated remarkable advances in\nthe field of image representation but demands substantial GPU resources.\nGaussianImage recently pioneered the use of Gaussian Splatting to mitigate this\ncost, however, the slow training process limits its practicality, and the fixed\nnumber of Gaussians per image limits its adaptability to varying information\nentropy. To address these issues, we propose in this paper a generalizable and\nself-adaptive image representation framework based on 2D Gaussian Splatting.\nOur method employs a network to quickly generate a coarse Gaussian\nrepresentation, followed by minimal fine-tuning steps, achieving comparable\nrendering quality of GaussianImage while significantly reducing training time.\nMoreover, our approach dynamically adjusts the number of Gaussian points based\non image complexity to further enhance flexibility and efficiency in practice.\nExperiments on DIV2K and Kodak datasets show that our method matches or exceeds\nGaussianImage's rendering performance with far fewer iterations and shorter\ntraining times. Specifically, our method reduces the training time by up to one\norder of magnitude while achieving superior rendering performance with the same\nnumber of Gaussians.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u81ea\u9002\u5e94\u56fe\u50cf\u8868\u793a\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u5347\u7075\u6d3b\u6027\u3002", "motivation": "\u89e3\u51b3Implicit Neural Representation (INR)\u7684\u9ad8GPU\u8d44\u6e90\u9700\u6c42\u548cGaussianImage\u8bad\u7ec3\u6162\u3001\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u5feb\u901f\u751f\u6210\u7c97\u7565\u9ad8\u65af\u8868\u793a\uff0c\u518d\u901a\u8fc7\u5c11\u91cf\u5fae\u8c03\u6b65\u9aa4\uff0c\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u70b9\u6570\u91cf\u4ee5\u9002\u5e94\u56fe\u50cf\u590d\u6742\u5ea6\u3002", "result": "\u5728DIV2K\u548cKodak\u6570\u636e\u96c6\u4e0a\uff0c\u6e32\u67d3\u6027\u80fd\u5339\u914d\u6216\u4f18\u4e8eGaussianImage\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.23566", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23566", "abs": "https://arxiv.org/abs/2506.23566", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "summary": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.", "AI": {"tldr": "MWT-Diff\u662f\u4e00\u4e2a\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5c0f\u6ce2\u53d8\u6362\u7684\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7MWT-Encoder\u751f\u6210\u5d4c\u5165\u7279\u5f81\uff0c\u9010\u6b65\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u83b7\u53d6\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u65f6\u7a7a\u9650\u5236\u548c\u9ad8\u6210\u672c\uff0c\u5f71\u54cd\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u3002", "method": "\u63d0\u51faMWT-Diff\u6846\u67b6\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5229\u7528MWT-Encoder\u751f\u6210\u5d4c\u5165\u7279\u5f81\u6307\u5bfc\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7FID\u548cLPIPS\u7b49\u6307\u6807\u9a8c\u8bc1\u3002", "conclusion": "MWT-Diff\u80fd\u6709\u6548\u89e3\u51b3\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u4fdd\u7559\u5173\u952e\u7a7a\u95f4\u7279\u5f81\u3002"}}
{"id": "2506.23481", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23481", "abs": "https://arxiv.org/abs/2506.23481", "authors": ["Xian Zhang", "Xiang Cheng"], "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks", "comment": null, "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f15\u53d1\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\u3002\u7814\u7a76\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\uff0c\u53d1\u73b0\u6a21\u578b\u57281\u516c\u91cc\u534a\u5f84\u5185\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe49%\uff0c\u5e76\u63a2\u8ba8\u4e86\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u968f\u7740MLLMs\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\u5e26\u6765\u4e86\u9690\u79c1\u5165\u4fb5\uff08\u5982\u4eba\u8089\u641c\u7d22\u3001\u76d1\u63a7\uff09\u7684\u98ce\u9669\uff0c\u4e9f\u9700\u7814\u7a76\u5176\u5f71\u54cd\u53ca\u5e94\u5bf9\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8eMLLMs\u7684\u5730\u7406\u5b9a\u4f4d\u6280\u672f\u6587\u732e\uff0c\u5e76\u8bc4\u4f30\u4e86\u5148\u8fdb\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u5728\u8857\u666f\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u5927\u6a21\u578b\u57281\u516c\u91cc\u534a\u5f84\u5185\u5b9a\u4f4d\u8857\u666f\u56fe\u50cf\u7684\u51c6\u786e\u7387\u9ad8\u8fbe49%\uff0c\u8868\u660e\u5176\u80fd\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u63d0\u53d6\u7cbe\u7ec6\u5730\u7406\u7ebf\u7d22\u3002", "conclusion": "\u7814\u7a76\u603b\u7ed3\u4e86\u5f71\u54cd\u5b9a\u4f4d\u6210\u529f\u7684\u5173\u952e\u89c6\u89c9\u5143\u7d20\uff08\u5982\u6587\u672c\u3001\u5efa\u7b51\u98ce\u683c\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u548c\u653f\u7b56\u5c42\u9762\u7684\u5e94\u5bf9\u63aa\u65bd\u4ee5\u964d\u4f4e\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2506.23482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23482", "abs": "https://arxiv.org/abs/2506.23482", "authors": ["Jun Huang", "Ting Liu", "Yihang Wu", "Xiaochao Qu", "Luoqi Liu", "Xiaolin Hu"], "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting", "comment": "CVPR 2025", "summary": "Advancements in generative models have enabled image inpainting models to\ngenerate content within specific regions of an image based on provided prompts\nand masks. However, existing inpainting methods often suffer from problems such\nas semantic misalignment, structural distortion, and style inconsistency. In\nthis work, we present MTADiffusion, a Mask-Text Alignment diffusion model\ndesigned for object inpainting. To enhance the semantic capabilities of the\ninpainting model, we introduce MTAPipeline, an automatic solution for\nannotating masks with detailed descriptions. Based on the MTAPipeline, we\nconstruct a new MTADataset comprising 5 million images and 25 million mask-text\npairs. Furthermore, we propose a multi-task training strategy that integrates\nboth inpainting and edge prediction tasks to improve structural stability. To\npromote style consistency, we present a novel inpainting style-consistency loss\nusing a pre-trained VGG network and the Gram matrix. Comprehensive evaluations\non BrushBench and EditBench demonstrate that MTADiffusion achieves\nstate-of-the-art performance compared to other methods.", "AI": {"tldr": "MTADiffusion\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u8c61\u4fee\u590d\u7684Mask-Text Alignment\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7MTAPipeline\u81ea\u52a8\u6807\u6ce8\u63a9\u7801\u548c\u63cf\u8ff0\uff0c\u6784\u5efaMTADataset\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u548c\u98ce\u683c\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3001\u7ed3\u6784\u626d\u66f2\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0cMTADiffusion\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMTAPipeline\u81ea\u52a8\u6807\u6ce8\u63a9\u7801\u548c\u63cf\u8ff0\uff0c\u6784\u5efaMTADataset\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff08\u4fee\u590d\u548c\u8fb9\u7f18\u9884\u6d4b\uff09\uff1b\u5f15\u5165\u98ce\u683c\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728BrushBench\u548cEditBench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MTADiffusion\u901a\u8fc7\u6539\u8fdb\u8bed\u4e49\u5bf9\u9f50\u3001\u7ed3\u6784\u7a33\u5b9a\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u4fee\u590d\u7684\u6548\u679c\u3002"}}
{"id": "2506.23581", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23581", "abs": "https://arxiv.org/abs/2506.23581", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "comment": "Accepted by ICCV 2025", "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5PBCAT\uff0c\u7528\u4e8e\u9632\u5fa1\u7269\u4f53\u68c0\u6d4b\u5668\u4e2d\u7684\u591a\u79cd\u7269\u7406\u53ef\u5b9e\u73b0\u653b\u51fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3002", "motivation": "\u7269\u4f53\u68c0\u6d4b\u5728\u5b89\u5168\u654f\u611f\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6613\u53d7\u7269\u7406\u53ef\u5b9e\u73b0\u653b\u51fb\uff08\u5982\u5bf9\u6297\u6027\u8865\u4e01\u548c\u7eb9\u7406\uff09\u7684\u5a01\u80c1\u3002\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u6a21\u578b\uff0c\u5bf9\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u9632\u5fa1\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPBCAT\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c0f\u533a\u57df\u68af\u5ea6\u5f15\u5bfc\u5bf9\u6297\u8865\u4e01\u548c\u5168\u5c40\u4e0d\u53ef\u5bdf\u89c9\u6270\u52a8\uff0c\u4f18\u5316\u6a21\u578b\u4ee5\u9632\u5fa1\u591a\u79cd\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPBCAT\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u591a\u79cd\u7269\u7406\u53ef\u5b9e\u73b0\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad829.7%\u3002", "conclusion": "PBCAT\u4e3a\u7269\u4f53\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u9632\u5fa1\u591a\u79cd\u7269\u7406\u53ef\u5b9e\u73b0\u653b\u51fb\u3002"}}
{"id": "2506.23491", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23491", "abs": "https://arxiv.org/abs/2506.23491", "authors": ["ZongHan Hsieh", "Tzer-Jen Wei"], "title": "Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding", "comment": null, "summary": "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", "AI": {"tldr": "Qwen-GUI-3B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4efb\u52a1\u8bbe\u8ba1\uff0c\u6027\u80fd\u5ab2\u7f8e\u66f4\u5927\u6a21\u578b\uff0c\u4e14\u53ef\u5728\u5355GPU\u4e0a\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3001\u4e0d\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7\u786c\u4ef6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347GUI\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u8de8\u5e73\u53f0\u591a\u5206\u8fa8\u7387\u6570\u636e\u96c6\u3001\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u53ca\u6570\u636e\u53bb\u5197\u4f59\u6280\u672f\uff0c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe84.9%\u548c86.4%\u3002", "conclusion": "Qwen-GUI-3B\u901a\u8fc7\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e14\u8d44\u6e90\u53cb\u597d\u7684GUI\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23627", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23627", "abs": "https://arxiv.org/abs/2506.23627", "authors": ["Roham Maiti", "Debasmita Bhoumik"], "title": "Brain Tumor Detection through Thermal Imaging and MobileNET", "comment": null, "summary": "Brain plays a crucial role in regulating body functions and cognitive\nprocesses, with brain tumors posing significant risks to human health. Precise\nand prompt detection is a key factor in proper treatment and better patient\noutcomes. Traditional methods for detecting brain tumors, that include\nbiopsies, MRI, and CT scans often face challenges due to their high costs and\nthe need for specialized medical expertise. Recent developments in machine\nlearning (ML) and deep learning (DL) has exhibited strong capabilities in\nautomating the identification and categorization of brain tumors from medical\nimages, especially MRI scans. However, these classical ML models have\nlimitations, such as high computational demands, the need for large datasets,\nand long training times, which hinder their accessibility and efficiency. Our\nresearch uses MobileNET model for efficient detection of these tumors. The\nnovelty of this project lies in building an accurate tumor detection model\nwhich use less computing re-sources and runs in less time followed by efficient\ndecision making through the use of image processing technique for accurate\nresults. The suggested method attained an average accuracy of 98.5%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMobileNET\u6a21\u578b\u7684\u9ad8\u6548\u8111\u80bf\u7624\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfML/DL\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u957f\u8bad\u7ec3\u65f6\u95f4\u95ee\u9898\uff0c\u51c6\u786e\u7387\u8fbe98.5%\u3002", "motivation": "\u4f20\u7edf\u8111\u80bf\u7624\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982\u6d3b\u68c0\u3001MRI\u3001CT\u626b\u63cf\uff09\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u4e13\u4e1a\u533b\u7597\u77e5\u8bc6\uff0c\u800c\u73b0\u6709ML/DL\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528MobileNET\u6a21\u578b\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3001\u8fd0\u884c\u65f6\u95f4\u77ed\u7684\u8111\u80bf\u7624\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523098.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8111\u80bf\u7624\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u6f5c\u5728\u652f\u6301\u3002"}}
{"id": "2506.23502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23502", "abs": "https://arxiv.org/abs/2506.23502", "authors": ["Mengxiao Tian", "Xinxiao Wu", "Shuo Yang"], "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching", "comment": "accepted by ICCV 2025", "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165LLM\u589e\u5f3a\u7684\u52a8\u4f5c\u611f\u77e5\u591a\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u8d4b\u4e88CLIP\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7ea7\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u5bf9\u8c61\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "CLIP\u5728\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff08\u5982\u5bf9\u8c61\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u52a8\u4f5c\u611f\u77e5\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u52a8\u4f5c\u4e09\u5143\u7ec4\u63d0\u793a\u548c\u52a8\u4f5c\u72b6\u6001\u63d0\u793a\uff0c\u5229\u7528LLM\u751f\u6210\u7684\u52a8\u4f5c\u76f8\u5173\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u4ea4\u4e92\u6a21\u5757\u4ee5\u805a\u5408\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165LLM\u589e\u5f3a\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u5728\u52a8\u4f5c\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.23663", "categories": ["cs.CV", "cs.LG", "I.4"], "pdf": "https://arxiv.org/pdf/2506.23663", "abs": "https://arxiv.org/abs/2506.23663", "authors": ["Mario Koddenbrock", "Rudolf Hoffmann", "David Brodmann", "Erik Rodner"], "title": "On the Domain Robustness of Contrastive Vision-Language Models", "comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench", "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.", "AI": {"tldr": "Deepbench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9886\u57df\u7279\u5b9a\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u56fe\u50cf\u635f\u574f\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u53d8\u5316\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7f3a\u4e4f\u900f\u660e\u6027\u3002", "method": "Deepbench\u5229\u7528LLM\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u56fe\u50cf\u635f\u574f\uff0c\u8bc4\u4f30\u591a\u79cd\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u67b6\u6784\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u9886\u57df\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\u9c81\u68d2\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u8981\u9488\u5bf9\u6027\u8bc4\u4f30\u3002", "conclusion": "Deepbench\u5f00\u6e90\u4ee5\u652f\u6301\u9886\u57df\u611f\u77e5\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.23505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23505", "abs": "https://arxiv.org/abs/2506.23505", "authors": ["Tinh Nguyen"], "title": "Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation", "comment": null, "summary": "Underwater object detection is crucial for autonomous navigation,\nenvironmental monitoring, and marine exploration, but it is severely hampered\nby light attenuation, turbidity, and occlusion. Current methods balance\naccuracy and computational efficiency, but they have trouble deploying in\nreal-time under low visibility conditions. Through the integration of\nphysics-informed augmentation techniques with the YOLOv12 architecture, this\nstudy advances underwater detection. With Residual ELAN blocks to preserve\nstructural features in turbid waters and Area Attention to maintain large\nreceptive fields for occluded objects while reducing computational complexity.\nUnderwater optical properties are addressed by domain-specific augmentations\nsuch as turbulence adaptive blurring, biologically grounded occlusion\nsimulation, and spectral HSV transformations for color distortion. Extensive\ntests on four difficult datasets show state-of-the-art performance, with\nBrackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion\nrobustness by 18.9%, small-object recall by 22.4%, and detection precision by\nup to 7.94% compared to previous models. The crucial role of augmentation\nstrategy is validated by ablation studies. This work offers a precise and\neffective solution for conservation and underwater robotics applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u6280\u672f\u548cYOLOv12\u67b6\u6784\u7684\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u5728\u81ea\u4e3b\u5bfc\u822a\u3001\u73af\u5883\u76d1\u6d4b\u548c\u6d77\u6d0b\u63a2\u7d22\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u5149\u7ebf\u8870\u51cf\u3001\u6d51\u6d4a\u548c\u906e\u6321\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u6280\u672f\uff08\u5982\u6e4d\u6d41\u81ea\u9002\u5e94\u6a21\u7cca\u3001\u751f\u7269\u906e\u6321\u6a21\u62df\u548c\u5149\u8c31HSV\u53d8\u6362\uff09\u4e0eYOLOv12\u67b6\u6784\uff0c\u7ed3\u5408Residual ELAN\u5757\u548c\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8698.30%\u7684mAP\u548c142 FPS\u7684\u6027\u80fd\uff0c\u906e\u6321\u9c81\u68d2\u6027\u63d0\u534718.9%\uff0c\u5c0f\u76ee\u6807\u53ec\u56de\u7387\u63d0\u9ad822.4%\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u53477.94%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u548c\u4fdd\u62a4\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u589e\u5f3a\u7b56\u7565\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2506.23783", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23783", "abs": "https://arxiv.org/abs/2506.23783", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u590d\u6742\u5ea6Vision Mamba\u7f51\u7edc\u7684\u9ad8\u6548RGB-Event\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6Mamba-FETrack V2\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7Prompt Generator\u548cFEMamba\u4e3b\u5e72\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0e\u878d\u5408\u3002", "motivation": "\u73b0\u6709RGB-Event\u8ddf\u8e2a\u7b97\u6cd5\u4f9d\u8d56\u9ad8\u590d\u6742\u5ea6Vision Transformer\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u8de8\u6a21\u6001\u4ea4\u4e92\u6548\u679c\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7Prompt Generator\u751f\u6210\u6a21\u6001\u7279\u5b9a\u63d0\u793a\u5411\u91cf\uff0c\u7ed3\u5408Vision Mamba\u7684FEMamba\u4e3b\u5e72\u5b9e\u73b0\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2aRGB-Event\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6548\u7387\u9ad8\u3002", "conclusion": "Mamba-FETrack V2\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.23513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23513", "abs": "https://arxiv.org/abs/2506.23513", "authors": ["Zixun Fang", "Kai Zhu", "Zhiheng Liu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models", "comment": "https://becauseimbatman0.github.io/ViewPoint", "summary": "Panoramic video generation aims to synthesize 360-degree immersive videos,\nholding significant importance in the fields of VR, world models, and spatial\nintelligence. Existing works fail to synthesize high-quality panoramic videos\ndue to the inherent modality gap between panoramic data and perspective data,\nwhich constitutes the majority of the training data for modern diffusion\nmodels. In this paper, we propose a novel framework utilizing pretrained\nperspective video models for generating panoramic videos. Specifically, we\ndesign a novel panorama representation named ViewPoint map, which possesses\nglobal spatial continuity and fine-grained visual details simultaneously. With\nour proposed Pano-Perspective attention mechanism, the model benefits from\npretrained perspective priors and captures the panoramic spatial correlations\nof the ViewPoint map effectively. Extensive experiments demonstrate that our\nmethod can synthesize highly dynamic and spatially consistent panoramic videos,\nachieving state-of-the-art performance and surpassing previous methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89d2\u89c6\u9891\u6a21\u578b\u751f\u6210\u5168\u666f\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7ViewPoint map\u548cPano-Perspective\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5168\u666f\u6570\u636e\u4e0e\u89c6\u89d2\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u5168\u666f\u6570\u636e\u4e0e\u89c6\u89d2\u6570\u636e\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u89c6\u9891\uff0c\u5f71\u54cd\u4e86VR\u3001\u4e16\u754c\u6a21\u578b\u548c\u7a7a\u95f4\u667a\u80fd\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aViewPoint map\u7684\u5168\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408Pano-Perspective\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89d2\u6a21\u578b\u751f\u6210\u5168\u666f\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u52a8\u6001\u6027\u5f3a\u3001\u7a7a\u95f4\u4e00\u81f4\u7684\u5168\u666f\u89c6\u9891\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u5168\u666f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23881", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23881", "abs": "https://arxiv.org/abs/2506.23881", "authors": ["Reihaneh Zohrabi", "Hosein Hasani", "Mahdieh Soleymani Baghshah", "Anna Rohrbach", "Marcus Rohrbach", "Mohammad Hossein Rohban"], "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%\nover the second best.", "AI": {"tldr": "SPROD\u662f\u4e00\u79cd\u65b0\u578b\u539f\u578bOOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7c7b\u539f\u578b\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u8c03\u53c2\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u6613\u53d7\u865a\u5047\u76f8\u5173\u6027\u8bef\u5bfc\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u77e5\u6570\u636e\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSPROD\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u5904\u7406\u4f18\u5316\u7c7b\u539f\u578b\uff0c\u51cf\u5c11\u865a\u5047\u7279\u5f81\u7684\u504f\u5dee\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u548cOOD\u68c0\u6d4b\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cSPROD\u5e73\u5747AUROC\u63d0\u53474.7%\uff0cFPR@95\u964d\u4f4e9.3%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SPROD\u6709\u6548\u89e3\u51b3\u4e86\u865a\u5047\u76f8\u5173\u6027\u5bf9OOD\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2506.23518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23518", "abs": "https://arxiv.org/abs/2506.23518", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "comment": null, "summary": "Generating high-quality novel views of a scene from a single image requires\nmaintaining structural coherence across different views, referred to as view\nconsistency. While diffusion models have driven advancements in novel view\nsynthesis, they still struggle to preserve spatial continuity across views.\nDiffusion models have been combined with 3D models to address the issue, but\nsuch approaches lack efficiency due to their complex multi-step pipelines. This\npaper proposes a novel view-consistent image generation method which utilizes\ndiffusion models without additional modules. Our key idea is to enhance\ndiffusion models with a training-free method that enables adaptive attention\nmanipulation and noise reinitialization by leveraging view-guided warping to\nensure view consistency. Through our comprehensive metric framework suitable\nfor novel-view datasets, we show that our method improves view consistency\nacross various diffusion models, demonstrating its broader applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6a21\u5757\u7684\u89c6\u56fe\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u64cd\u4f5c\u548c\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u591a\u89c6\u56fe\u751f\u6210\u4e2d\u96be\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u7eed\u6027\u7684\u95ee\u9898\uff0c\u907f\u514d\u590d\u6742\u591a\u6b65\u6d41\u7a0b\u7684\u4f4e\u6548\u6027\u3002", "method": "\u5229\u7528\u89c6\u56fe\u5f15\u5bfc\u7684\u53d8\u5f62\u6280\u672f\uff0c\u5b9e\u73b0\u8bad\u7ec3\u81ea\u7531\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u64cd\u4f5c\u548c\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u3002", "result": "\u901a\u8fc7\u9002\u7528\u4e8e\u65b0\u89c6\u56fe\u6570\u636e\u96c6\u7684\u7efc\u5408\u6307\u6807\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5404\u79cd\u6269\u6563\u6a21\u578b\u4e2d\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.23519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23519", "abs": "https://arxiv.org/abs/2506.23519", "authors": ["Qi Qin", "Runmin Cong", "Gen Zhan", "Yiting Liao", "Sam Kwong"], "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection", "comment": "15 Pages, 9 Figures", "summary": "The eye-tracking video saliency prediction (VSP) task and video salient\nobject detection (VSOD) task both focus on the most attractive objects in video\nand show the result in the form of predictive heatmaps and pixel-level saliency\nmasks, respectively. In practical applications, eye tracker annotations are\nmore readily obtainable and align closely with the authentic visual patterns of\nhuman eyes. Therefore, this paper aims to introduce fixation information to\nassist the detection of video salient objects under weak supervision. On the\none hand, we ponder how to better explore and utilize the information provided\nby fixation, and then propose a Position and Semantic Embedding (PSE) module to\nprovide location and semantic guidance during the feature learning process. On\nthe other hand, we achieve spatiotemporal feature modeling under weak\nsupervision from the aspects of feature selection and feature contrast. A\nSemantics and Locality Query (SLQ) Competitor with semantic and locality\nconstraints is designed to effectively select the most matching and accurate\nobject query for spatiotemporal modeling. In addition, an Intra-Inter Mixed\nContrastive (IIMC) model improves the spatiotemporal modeling capabilities\nunder weak supervision by forming an intra-video and inter-video contrastive\nlearning paradigm. Experimental results on five popular VSOD benchmarks\nindicate that our model outperforms other competitors on various evaluation\nmetrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6ce8\u89c6\u4fe1\u606f\u8f85\u52a9\u89c6\u9891\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4d\u7f6e\u548c\u8bed\u4e49\u5d4c\u5165\u6a21\u5757\u53ca\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u63d0\u5347\u5f31\u76d1\u7763\u4e0b\u7684\u65f6\u7a7a\u7279\u5f81\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u6ce8\u89c6\u4fe1\u606f\u66f4\u6613\u83b7\u53d6\u4e14\u7b26\u5408\u4eba\u773c\u771f\u5b9e\u89c6\u89c9\u6a21\u5f0f\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5229\u7528\u6ce8\u89c6\u4fe1\u606f\u8f85\u52a9\u89c6\u9891\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u3002", "method": "\u63d0\u51faPSE\u6a21\u5757\u63d0\u4f9b\u4f4d\u7f6e\u548c\u8bed\u4e49\u6307\u5bfc\uff0c\u8bbe\u8ba1SLQ Competitor\u9009\u62e9\u6700\u4f18\u5bf9\u8c61\u67e5\u8be2\uff0c\u5e76\u91c7\u7528IIMC\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u6d41\u884cVSOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u591a\u9879\u8bc4\u4ef7\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6ce8\u89c6\u4fe1\u606f\u548c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u663e\u8457\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23523", "abs": "https://arxiv.org/abs/2506.23523", "authors": ["Tuong Do", "Binh X. Nguyen", "Quang D. Tran", "Erman Tjiputra", "Te-Chuan Chiu", "Anh Nguyen"], "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving", "comment": "Accepted in IROS 2025", "summary": "Traditional vision-based autonomous driving systems often face difficulties\nin navigating complex environments when relying solely on single-image inputs.\nTo overcome this limitation, incorporating temporal data such as past image\nframes or steering sequences, has proven effective in enhancing robustness and\nadaptability in challenging scenarios. While previous high-performance methods\nexist, they often rely on resource-intensive fusion networks, making them\nimpractical for training and unsuitable for federated learning. To address\nthese challenges, we propose lightweight temporal transformer decomposition, a\nmethod that processes sequential image frames and temporal steering data by\nbreaking down large attention maps into smaller matrices. This approach reduces\nmodel complexity, enabling efficient weight updates for convergence and\nreal-time predictions while leveraging temporal information to enhance\nautonomous driving performance. Intensive experiments on three datasets\ndemonstrate that our method outperforms recent approaches by a clear margin\nwhile achieving real-time performance. Additionally, real robot experiments\nfurther confirm the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65f6\u5e8fTransformer\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5927\u6ce8\u610f\u529b\u56fe\u4e3a\u5c0f\u77e9\u9635\uff0c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f9d\u8d56\u5355\u56fe\u50cf\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u9ad8\u6027\u80fd\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u878d\u5408\u7f51\u7edc\uff0c\u4e0d\u9002\u5408\u8054\u90a6\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u65f6\u5e8fTransformer\u5206\u89e3\uff0c\u5904\u7406\u65f6\u5e8f\u56fe\u50cf\u5e27\u548c\u8f6c\u5411\u6570\u636e\uff0c\u5206\u89e3\u5927\u6ce8\u610f\u529b\u56fe\u4e3a\u5c0f\u77e9\u9635\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002\u673a\u5668\u4eba\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.23542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23542", "abs": "https://arxiv.org/abs/2506.23542", "authors": ["Weida Wang", "Changyong He", "Jin Zeng", "Di Qiu"], "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention", "comment": "This paper has been accepted for publication at the International\n  Conference on Computer Vision (ICCV) 2025", "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\n\\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u4e0d\u53d8\u56fe\u878d\u5408\u7684ToF\u6df1\u5ea6\u53bb\u566a\u7f51\u7edc\uff0c\u901a\u8fc7\u8de8\u5e27\u51e0\u4f55\u6ce8\u610f\u529b\u589e\u5f3a\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u7a7a\u95f4\u6e05\u6670\u5ea6\u3002", "motivation": "ToF\u4f20\u611f\u5668\u6355\u83b7\u7684\u6df1\u5ea6\u56fe\u50cf\u566a\u58f0\u8f83\u591a\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u591a\u5e27\u95f4\u7684\u6df1\u5ea6\u53d8\u5316\uff0c\u5bfc\u81f4\u65f6\u95f4\u4e0d\u4e00\u81f4\u548c\u7a7a\u95f4\u6a21\u7cca\u3002", "method": "\u5229\u7528\u56fe\u7684\u65f6\u5e8f\u81ea\u76f8\u4f3c\u6027\u8fdb\u884c\u8de8\u5e27\u51e0\u4f55\u6ce8\u610f\u529b\u878d\u5408\uff0c\u7ed3\u5408\u56fe\u50cf\u5e73\u6ed1\u5148\u9a8c\u548cToF\u566a\u58f0\u5206\u5e03\uff0c\u6784\u5efa\u6700\u5927\u540e\u9a8c\u95ee\u9898\u5e76\u5c55\u5f00\u4e3a\u53ef\u5b66\u4e60\u8fed\u4ee3\u6ee4\u6ce2\u5668\u3002", "result": "\u5728\u5408\u6210DVToF\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9eKinectv2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728ToF\u6df1\u5ea6\u53bb\u566a\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u4e14\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u5f3a\u3002"}}
{"id": "2506.23543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23543", "abs": "https://arxiv.org/abs/2506.23543", "authors": ["Hui Li", "Baoyou Chen", "Liwei Zhang", "Jiaye Li", "Jingdong Wang", "Siyu Zhu"], "title": "Pyramidal Patchification Flow for Visual Generation", "comment": "10 pages, 9figures", "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations\nto token representations through linear projections, to adjust the number of\ntokens input to DiT blocks and thus the computation cost. Instead of a single\npatch size for all the timesteps, we introduce a Pyramidal Patchification Flow\n(PPFlow) approach: Large patch sizes are used for high noise timesteps and\nsmall patch sizes for low noise timesteps; Linear projections are learned for\neach patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,\nour approach operates over full latent representations other than pyramid\nrepresentations, and adopts the normal denoising process without requiring the\nrenoising trick. We demonstrate the effectiveness of our approach through two\ntraining manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$)\ninference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with\nslightly lower training FLOPs and similar image generation performance.\nTraining from pretrained normal DiTs achieves even better performance with\nsmall training time. The code and checkpoint are at\nhttps://github.com/fudan-generative-vision/PPFlow.", "AI": {"tldr": "PPFlow\u901a\u8fc7\u52a8\u6001\u8c03\u6574patch\u5927\u5c0f\u548c\u6295\u5f71\uff0c\u4f18\u5316\u4e86DiTs\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u76f8\u4f3c\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfDiTs\u4f7f\u7528\u56fa\u5b9apatch\u5927\u5c0f\uff0c\u65e0\u6cd5\u7075\u6d3b\u8c03\u6574\u8ba1\u7b97\u6210\u672c\u3002PPFlow\u65e8\u5728\u901a\u8fc7\u52a8\u6001patch\u5316\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faPPFlow\u65b9\u6cd5\uff0c\u6839\u636e\u566a\u58f0\u6c34\u5e73\u52a8\u6001\u8c03\u6574patch\u5927\u5c0f\uff0c\u5b66\u4e60\u5bf9\u5e94\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u5e76\u4fee\u6539Unpatchify\u3002", "result": "\u8bad\u7ec3\u7ed3\u679c\u663e\u793a\uff0cPPFlow\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u53471.6-2.0\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "PPFlow\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684DiTs\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4ece\u5934\u8bad\u7ec3\u6216\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2506.23547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23547", "abs": "https://arxiv.org/abs/2506.23547", "authors": ["Jiwon Kim", "Soohyun Hwang", "Dong-O Kim", "Changsu Han", "Min Kyu Park", "Chang-Su Kim"], "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions", "comment": null, "summary": "The first algorithm, called Oneta, for a novel task of multi-style image\nenhancement is proposed in this work. Oneta uses two point operators\nsequentially: intensity enhancement with a transformation function (TF) and\ncolor correction with a color correction matrix (CCM). This two-step\nenhancement model, though simple, achieves a high performance upper bound.\nAlso, we introduce eigentransformation function (eigenTF) to represent TF\ncompactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and\nCCM parameters, respectively. To support $K$ styles, Oneta employs $K$\nlearnable tokens. During training, each style token is learned using image\npairs from the corresponding dataset. In testing, Oneta selects one of the $K$\nstyle tokens to enhance an image accordingly. Extensive experiments show that\nthe single Oneta network can effectively undertake six enhancement tasks --\nretouching, image signal processing, low-light image enhancement, dehazing,\nunderwater image enhancement, and white balancing -- across 30 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOneta\u7684\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u64cd\u4f5c\uff08\u5f3a\u5ea6\u589e\u5f3a\u548c\u8272\u5f69\u6821\u6b63\uff09\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u652f\u6301\u591a\u79cd\u98ce\u683c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\uff0c\u901a\u8fc7\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u4e24\u6b65\u6a21\u578b\u5b9e\u73b0\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528Y-Net\u548cC-Net\u9884\u6d4b\u53c2\u6570\uff0c\u5f15\u5165eigenTF\u7d27\u51d1\u8868\u793a\uff0c\u901a\u8fc7K\u4e2a\u53ef\u5b66\u4e60\u4ee4\u724c\u652f\u6301\u591a\u98ce\u683c\u3002", "result": "\u572830\u4e2a\u6570\u636e\u96c6\u4e0a\u6210\u529f\u5b8c\u6210\u516d\u79cd\u589e\u5f3a\u4efb\u52a1\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Oneta\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2506.23552", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23552", "abs": "https://arxiv.org/abs/2506.23552", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "AI": {"tldr": "JAM-Flow\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u548c\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiT\uff09\u540c\u65f6\u5408\u6210\u9762\u90e8\u8fd0\u52a8\u548c\u8bed\u97f3\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u9762\u90e8\u8fd0\u52a8\u5408\u6210\u548c\u8bed\u97f3\u5408\u6210\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u4e8c\u8005\u7684\u5185\u5728\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u548cMM-DiT\u67b6\u6784\uff0c\u7ed3\u5408Motion-DiT\u548cAudio-DiT\u6a21\u5757\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8054\u5408\u6ce8\u610f\u529b\u5c42\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "JAM-Flow\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff08\u5982\u6587\u672c\u9a71\u52a8\u7684\u540c\u6b65\u5934\u90e8\u751f\u6210\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u751f\u6210\u5efa\u6a21\u3002", "conclusion": "JAM-Flow\u4e3a\u97f3\u9891-\u89c6\u89c9\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6574\u4f53\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23555", "abs": "https://arxiv.org/abs/2506.23555", "authors": ["Fan Xie", "Pan Cao"], "title": "LH2Face: Loss function for Hard High-quality Face", "comment": null, "summary": "In current practical face authentication systems, most face recognition (FR)\nalgorithms are based on cosine similarity with softmax classification. Despite\nits reliable classification performance, this method struggles with hard\nsamples. A popular strategy to improve FR performance is incorporating angular\nor cosine margins. However, it does not take face quality or recognition\nhardness into account, simply increasing the margin value and thus causing an\noverly uniform training strategy. To address this problem, a novel loss\nfunction is proposed, named Loss function for Hard High-quality Face (LH2Face).\nFirstly, a similarity measure based on the von Mises-Fisher (vMF) distribution\nis stated, specifically focusing on the logarithm of the Probability Density\nFunction (PDF), which represents the distance between a probability\ndistribution and a vector. Then, an adaptive margin-based multi-classification\nmethod using softmax, called the Uncertainty-Aware Margin Function, is\nimplemented in the article. Furthermore, proxy-based loss functions are used to\napply extra constraints between the proxy and sample to optimize their\nrepresentation space distribution. Finally, a renderer is constructed that\noptimizes FR through face reconstruction and vice versa. Our LH2Face is\nsuperior to similiar schemes on hard high-quality face datasets, achieving\n49.39% accuracy on the IJB-B dataset, which surpasses the second-place method\nby 2.37%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLH2Face\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7ed3\u5408vMF\u5206\u5e03\u548c\u81ea\u9002\u5e94\u8fb9\u8ddd\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u9ad8\u8d28\u91cf\u56f0\u96be\u6837\u672c\u7684\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548csoftmax\u5206\u7c7b\u7684\u4eba\u8138\u8bc6\u522b\u65b9\u6cd5\u5728\u5904\u7406\u56f0\u96be\u6837\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u672a\u8003\u8651\u4eba\u8138\u8d28\u91cf\u6216\u8bc6\u522b\u96be\u5ea6\u3002", "method": "\u63d0\u51favMF\u5206\u5e03\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u8fb9\u8ddd\u7684softmax\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4ee3\u7406\u635f\u5931\u51fd\u6570\u548c\u6e32\u67d3\u5668\u4f18\u5316\u8868\u793a\u7a7a\u95f4\u5206\u5e03\u3002", "result": "\u5728IJB-B\u6570\u636e\u96c6\u4e0a\u8fbe\u523049.39%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7b2c\u4e8c\u540d2.37%\u3002", "conclusion": "LH2Face\u901a\u8fc7\u81ea\u9002\u5e94\u7b56\u7565\u548c\u8868\u793a\u7a7a\u95f4\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u8d28\u91cf\u56f0\u96be\u6837\u672c\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2506.23565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23565", "abs": "https://arxiv.org/abs/2506.23565", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving", "comment": "Accepted by ICCV2025", "summary": "Current multi-view 3D object detection methods typically transfer 2D features\ninto 3D space using depth estimation or 3D position encoder, but in a fully\ndata-driven and implicit manner, which limits the detection performance.\nInspired by the success of radiance fields on 3D reconstruction, we assume they\ncan be used to enhance the detector's ability of 3D geometry estimation.\nHowever, we observe a decline in detection performance, when we directly use\nthem for 3D rendering as an auxiliary task. From our analysis, we find the\nperformance drop is caused by the strong responses on the background when\nrendering the whole scene. To address this problem, we propose object-centric\nradiance fields, focusing on modeling foreground objects while discarding\nbackground noises. Specifically, we employ Object-centric Radiance Fields\n(OcRF) to enhance 3D voxel features via an auxiliary task of rendering\nforeground objects. We further use opacity - the side-product of rendering- to\nenhance the 2D foreground BEV features via Height-aware Opacity-based Attention\n(HOA), where attention maps at different height levels are generated separately\nvia multiple networks in parallel. Extensive experiments on the nuScenes\nvalidation and test datasets demonstrate that our OcRFDet achieves superior\nperformance, outperforming previous state-of-the-art methods with 57.2$\\%$ mAP\nand 64.8$\\%$ NDS on the nuScenes test benchmark. Code will be available at\nhttps://github.com/Mingqj/OcRFDet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u4f53\u4e2d\u5fc3\u8f90\u5c04\u573a\uff08OcRF\uff09\u7684\u591a\u89c6\u89d23D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a3D\u4f53\u7d20\u7279\u5f81\u548c2D BEV\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u4f30\u8ba1\u62163D\u4f4d\u7f6e\u7f16\u7801\u5c062D\u7279\u5f81\u9690\u5f0f\u8f6c\u6362\u52303D\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u3002\u4f5c\u8005\u8ba4\u4e3a\u8f90\u5c04\u573a\u53ef\u4ee5\u589e\u5f3a3D\u51e0\u4f55\u4f30\u8ba1\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u7269\u4f53\u4e2d\u5fc3\u8f90\u5c04\u573a\uff08OcRF\uff09\uff0c\u4e13\u6ce8\u4e8e\u524d\u666f\u7269\u4f53\u5efa\u6a21\uff0c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\uff1b\u5e76\u5229\u7528\u6e32\u67d3\u7684\u526f\u4ea7\u54c1\u4e0d\u900f\u660e\u5ea6\uff0c\u901a\u8fc7\u9ad8\u5ea6\u611f\u77e5\u4e0d\u900f\u660e\u5ea6\u6ce8\u610f\u529b\uff08HOA\uff09\u589e\u5f3a2D BEV\u7279\u5f81\u3002", "result": "\u5728nuScenes\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523057.2% mAP\u548c64.8% NDS\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OcRFDet\u901a\u8fc7\u7ed3\u5408OcRF\u548cHOA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d23D\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23575", "abs": "https://arxiv.org/abs/2506.23575", "authors": ["Nuo Chen", "Chao Xiao", "Yimian Dai", "Shiman He", "Miao Li", "Wei An"], "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline", "comment": null, "summary": "Small object detection (SOD) in anti-UAV task is a challenging problem due to\nthe small size of UAVs and complex backgrounds. Traditional frame-based cameras\nstruggle to detect small objects in complex environments due to their low frame\nrates, limited dynamic range, and data redundancy. Event cameras, with\nmicrosecond temporal resolution and high dynamic range, provide a more\neffective solution for SOD. However, existing event-based object detection\ndatasets are limited in scale, feature large targets size, and lack diverse\nbackgrounds, making them unsuitable for SOD benchmarks. In this paper, we\nintroduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),\nthe first large-scale, highly diverse benchmark for anti-UAV tasks. It includes\n147 sequences with over 2.3 million event-level annotations, featuring\nextremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse\nscenarios such as urban clutter and extreme lighting conditions. Furthermore,\nbased on the observation that small moving targets form continuous curves in\nspatiotemporal event point clouds, we propose Event based Sparse Segmentation\nNetwork (EV-SpSegNet), a novel baseline for event segmentation in point cloud\nspace, along with a Spatiotemporal Correlation (STC) loss that leverages motion\ncontinuity to guide the network in retaining target events. Extensive\nexperiments on the EV-UAV dataset demonstrate the superiority of our method and\nprovide a benchmark for future research in EVSOD. The dataset and code are at\nhttps://github.com/ChenYichen9527/Ev-UAV.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6EV-UAV\uff0c\u5e76\u63d0\u51faEV-SpSegNet\u65b9\u6cd5\u548cSTC\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u5728\u590d\u6742\u80cc\u666f\u4e0b\u68c0\u6d4b\u5c0f\u578b\u65e0\u4eba\u673a\uff08UAV\uff09\u5b58\u5728\u56f0\u96be\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5fae\u79d2\u7ea7\u5206\u8fa8\u7387\u66f4\u9002\u5408\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u76ee\u6807\u5927\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faEV-UAV\u6570\u636e\u96c6\uff0c\u5305\u542b147\u4e2a\u5e8f\u5217\u548c230\u4e07\u4e8b\u4ef6\u7ea7\u6807\u6ce8\uff1b\u8bbe\u8ba1EV-SpSegNet\u7f51\u7edc\u548cSTC\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u65f6\u7a7a\u8fde\u7eed\u6027\u4f18\u5316\u4e8b\u4ef6\u70b9\u4e91\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEV-SpSegNet\u5728EV-UAV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5c0f\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "EV-UAV\u6570\u636e\u96c6\u548cEV-SpSegNet\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u57fa\u51c6\u3002"}}
{"id": "2506.23577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23577", "abs": "https://arxiv.org/abs/2506.23577", "authors": ["Yanning Hou", "Yanran Ruan", "Junfa Li", "Shanshan Wang", "Jianfeng Qiu", "Ke Xu"], "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection", "comment": null, "summary": "Enhancing the alignment between text and image features in the CLIP model is\na critical challenge in zero-shot industrial anomaly detection tasks. Recent\nstudies predominantly utilize specific category prompts during pretraining,\nwhich can cause overfitting to the training categories and limit model\ngeneralization. To address this, we propose a method that transforms category\nnames through multicategory name stacking to create stacked prompts, forming\nthe basis of our StackCLIP model. Our approach introduces two key components.\nThe Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts\nby stacking semantically analogous categories, while utilizing multi-object\ntextual feature fusion to amplify discriminative anomalies among similar\nobjects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific\nlinear layers tailored for each stack cluster and adaptively integrates them\nbased on the attributes of test categories. These modules work together to\ndeliver superior training speed, stability, and convergence, significantly\nboosting anomaly segmentation performance. Additionally, our stacked prompt\nframework offers robust generalization across classification tasks. To further\nimprove performance, we introduce the Regulating Prompt Learning (RPL) module,\nwhich leverages the generalization power of stacked prompts to refine prompt\nlearning, elevating results in anomaly detection classification tasks.\nExtensive testing on seven industrial anomaly detection datasets demonstrates\nthat our method achieves state-of-the-art performance in both zero-shot anomaly\ndetection and segmentation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStackCLIP\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7c7b\u522b\u540d\u79f0\u5806\u53e0\u751f\u6210\u5806\u53e0\u63d0\u793a\uff0c\u7ed3\u5408CSP\u548cEFA\u6a21\u5757\u63d0\u5347\u96f6\u6837\u672c\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3CLIP\u6a21\u578b\u4e2d\u6587\u672c\u4e0e\u56fe\u50cf\u7279\u5f81\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u56e0\u7279\u5b9a\u7c7b\u522b\u63d0\u793a\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u95ee\u9898\u3002", "method": "\u63d0\u51faStackCLIP\u6a21\u578b\uff0c\u5305\u542bCSP\u6a21\u5757\uff08\u901a\u8fc7\u5806\u53e0\u8bed\u4e49\u76f8\u4f3c\u7c7b\u522b\u751f\u6210\u901a\u7528\u63d0\u793a\uff09\u548cEFA\u6a21\u5757\uff08\u8bad\u7ec3\u77e5\u8bc6\u7279\u5b9a\u7ebf\u6027\u5c42\u5e76\u81ea\u9002\u5e94\u96c6\u6210\uff09\u3002\u6b64\u5916\u5f15\u5165RPL\u6a21\u5757\u4f18\u5316\u63d0\u793a\u5b66\u4e60\u3002", "result": "\u5728\u4e03\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "StackCLIP\u901a\u8fc7\u5806\u53e0\u63d0\u793a\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2506.23580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23580", "abs": "https://arxiv.org/abs/2506.23580", "authors": ["Yawen Zou", "Guang Li", "Duo Su", "Zi Wang", "Jun Yu", "Chao Zhang"], "title": "Dataset Distillation via Vision-Language Category Prototype", "comment": "accepted by ICCV2025", "summary": "Dataset distillation (DD) condenses large datasets into compact yet\ninformative substitutes, preserving performance comparable to the original\ndataset while reducing storage, transmission costs, and computational\nconsumption. However, previous DD methods mainly focus on distilling\ninformation from images, often overlooking the semantic information inherent in\nthe data. The disregard for context hinders the model's generalization ability,\nparticularly in tasks involving complex datasets, which may result in illogical\noutputs or the omission of critical objects. In this study, we integrate\nvision-language methods into DD by introducing text prototypes to distill\nlanguage information and collaboratively synthesize data with image prototypes,\nthereby enhancing dataset distillation performance. Notably, the text\nprototypes utilized in this study are derived from descriptive text information\ngenerated by an open-source large language model. This framework demonstrates\nbroad applicability across datasets without pre-existing text descriptions,\nexpanding the potential of dataset distillation beyond traditional image-based\napproaches. Compared to other methods, the proposed approach generates\nlogically coherent images containing target objects, achieving state-of-the-art\nvalidation performance and demonstrating robust generalization. Source code and\ngenerated data are available in\nhttps://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6587\u672c\u539f\u578b\u6765\u589e\u5f3a\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u8bed\u4e49\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u65b9\u6cd5\u63d0\u5347\u84b8\u998f\u6548\u679c\u3002", "method": "\u5f15\u5165\u6587\u672c\u539f\u578b\uff08\u7531\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff09\u4e0e\u56fe\u50cf\u539f\u578b\u534f\u540c\u5408\u6210\u6570\u636e\uff0c\u589e\u5f3a\u6570\u636e\u96c6\u84b8\u998f\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u903b\u8f91\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5305\u542b\u76ee\u6807\u5bf9\u8c61\uff0c\u9a8c\u8bc1\u6027\u80fd\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23590", "abs": "https://arxiv.org/abs/2506.23590", "authors": ["Qiming Li", "Zekai Ye", "Xiaocheng Feng", "Weihong Zhong", "Libo Qin", "Ruihan Chen", "Baohang Li", "Kui Jiang", "Yaowei Wang", "Ting Liu", "Bing Qin"], "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "comment": null, "summary": "Although Large Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in interpreting visual information, they frequently produce\ncontent that deviates from visual information, leading to object hallucination.\nTo tackle this, recent works mostly depend on expensive manual annotations and\ntraining cost, or significantly increase inference time. In this work, we\nobserve that LVLMs' attention to visual information is significantly stronger\nwhen answering caption queries compared to non-caption queries. Inspired by\nthis phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a\ntraining-free, plug-and-play hallucination mitigation method that leverages the\nattention activation pattern in response to caption queries to enhance LVLMs'\nvisual perception capability. Extensive experimental results across four\nbenchmarks covering both discriminative and generative tasks, demonstrate that\nCAI achieves state-of-the-art (SOTA) hallucination mitigating performance only\nwith minimal additional inference cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5CAI\uff0c\u901a\u8fc7\u5229\u7528LVLMs\u5bf9caption queries\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e0d\u7b26\u7684\u5e7b\u89c9\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u548c\u8bad\u7ec3\u6210\u672c\uff0c\u6216\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002", "method": "\u63d0\u51faCaption-sensitive Attention Intervention (CAI)\uff0c\u5229\u7528LVLMs\u5bf9caption queries\u7684\u6ce8\u610f\u529b\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u589e\u5f3a\u5176\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u5224\u522b\u548c\u751f\u6210\u4efb\u52a1\uff09\u4e2d\uff0cCAI\u4ee5\u6700\u5c0f\u7684\u989d\u5916\u63a8\u7406\u6210\u672c\u5b9e\u73b0\u4e86SOTA\u7684\u5e7b\u89c9\u7f13\u89e3\u6027\u80fd\u3002", "conclusion": "CAI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LVLMs\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.23605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23605", "abs": "https://arxiv.org/abs/2506.23605", "authors": ["Suyash Maniyar", "Vishvesh Trivedi", "Ajoy Mondal", "Anand Mishra", "C. V. Jawahar"], "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval", "comment": "40 pages including supplementary, accepted at ICDAR 2025", "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5408\u6210\u5e7b\u706f\u7247\u751f\u6210\u65b9\u6cd5SynLecSlideGen\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u5927\u91cf\u5e7b\u706f\u7247\u6570\u636e\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5e7b\u706f\u7247\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u5408\u6210\u5e7b\u706f\u7247\u751f\u6210\u7ba1\u9053SynLecSlideGen\uff0c\u5e76\u521b\u5efa\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u5e7b\u706f\u7247\u57fa\u51c6RealSlide\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u901a\u8fc7\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6709\u6548\u5f25\u8865\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.23606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23606", "abs": "https://arxiv.org/abs/2506.23606", "authors": ["Zhengkang Xiang", "Zizhao Li", "Amir Khodabandeh", "Kourosh Khoshelham"], "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion", "comment": null, "summary": "Lidar point cloud synthesis based on generative models offers a promising\nsolution to augment deep learning pipelines, particularly when real-world data\nis scarce or lacks diversity. By enabling flexible object manipulation, this\nsynthesis approach can significantly enrich training datasets and enhance\ndiscriminative models. However, existing methods focus on unconditional lidar\npoint cloud generation, overlooking their potential for real-world\napplications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar\nDiffusion Model that employs latent alignment to enable robust\nsemantic-to-lidar synthesis. By directly operating in the native lidar space\nand leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art\nperformance in generating high-fidelity lidar point clouds guided by semantic\nlabels. Moreover, we propose the first diffusion-based lidar translation\nframework based on SG-LDM, which enables cross-domain translation as a domain\nadaptation strategy to enhance downstream perception performance. Systematic\nexperiments demonstrate that SG-LDM significantly outperforms existing lidar\ndiffusion models and the proposed lidar translation framework further improves\ndata augmentation performance in the downstream lidar segmentation task.", "AI": {"tldr": "SG-LDM\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u6fc0\u5149\u96f7\u8fbe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u5bf9\u9f50\u5b9e\u73b0\u8bed\u4e49\u5230\u6fc0\u5149\u96f7\u8fbe\u7684\u5408\u6210\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u7684\u751f\u6210\u65b9\u6cd5\u4ee5\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faSG-LDM\uff0c\u5229\u7528\u6f5c\u5728\u5bf9\u9f50\u548c\u663e\u5f0f\u8bed\u4e49\u6761\u4ef6\uff0c\u5728\u539f\u751f\u6fc0\u5149\u96f7\u8fbe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u4fdd\u771f\u70b9\u4e91\u751f\u6210\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u7684\u6fc0\u5149\u96f7\u8fbe\u7ffb\u8bd1\u6846\u67b6\u3002", "result": "SG-LDM\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u7ffb\u8bd1\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6570\u636e\u589e\u5f3a\u5728\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "SG-LDM\u4e3a\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u548c\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23607", "abs": "https://arxiv.org/abs/2506.23607", "authors": ["Shiqi Zhang", "Sha Zhang", "Jiajun Deng", "Yedong Shen", "Mingxiao MA", "Yanyong Zhang"], "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum", "comment": null, "summary": "Existing open-vocabulary 3D semantic segmentation methods typically supervise\n3D segmentation models by merging text-aligned features (e.g., CLIP) extracted\nfrom multi-view images onto 3D points. However, such approaches treat\nmulti-view images merely as intermediaries for transferring open-vocabulary\ninformation, overlooking their rich semantic content and cross-view\ncorrespondences, which limits model effectiveness. To address this, we propose\nPGOV3D, a novel framework that introduces a Partial-to-Global curriculum for\nimproving open-vocabulary 3D semantic segmentation. The key innovation lies in\na two-stage training strategy. In the first stage, we pre-train the model on\npartial scenes that provide dense semantic information but relatively simple\ngeometry. These partial point clouds are derived from multi-view RGB-D inputs\nvia pixel-wise depth projection. To enable open-vocabulary learning, we\nleverage a multi-modal large language model (MLLM) and a 2D segmentation\nfoundation model to generate open-vocabulary labels for each viewpoint,\noffering rich and aligned supervision. An auxiliary inter-frame consistency\nmodule is introduced to enforce feature consistency across varying viewpoints\nand enhance spatial understanding. In the second stage, we fine-tune the model\non complete scene-level point clouds, which are sparser and structurally more\ncomplex. We aggregate the partial vocabularies associated with each scene and\ngenerate pseudo labels using the pre-trained model, effectively bridging the\nsemantic gap between dense partial observations and large-scale 3D\nenvironments. Extensive experiments on ScanNet, ScanNet200, and S3DIS\nbenchmarks demonstrate that PGOV3D achieves competitive performance in\nopen-vocabulary 3D semantic segmentation.", "AI": {"tldr": "PGOV3D\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u90e8\u5206\u5230\u5168\u5c40\u7684\u8bfe\u7a0b\u5b66\u4e60\u6539\u8fdb\u5f00\u653e\u8bcd\u6c473D\u8bed\u4e49\u5206\u5272\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c2D\u5206\u5272\u57fa\u7840\u6a21\u578b\u751f\u6210\u5f00\u653e\u8bcd\u6c47\u6807\u7b7e\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u89c6\u56fe\u56fe\u50cf\u4ec5\u89c6\u4e3a\u4f20\u9012\u5f00\u653e\u8bcd\u6c47\u4fe1\u606f\u7684\u4e2d\u4ecb\uff0c\u5ffd\u7565\u4e86\u5176\u4e30\u5bcc\u7684\u8bed\u4e49\u5185\u5bb9\u548c\u8de8\u89c6\u56fe\u5bf9\u5e94\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6548\u679c\u3002", "method": "PGOV3D\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u90e8\u5206\u573a\u666f\u9884\u8bad\u7ec3\uff0c\u5229\u7528\u591a\u89c6\u56feRGB-D\u8f93\u5165\u751f\u6210\u5bc6\u96c6\u8bed\u4e49\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u5b8c\u6574\u573a\u666f\u5fae\u8c03\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u6865\u63a5\u8bed\u4e49\u5dee\u8ddd\u3002", "result": "\u5728ScanNet\u3001ScanNet200\u548cS3DIS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPGOV3D\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PGOV3D\u901a\u8fc7\u90e8\u5206\u5230\u5168\u5c40\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u591a\u6a21\u6001\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c473D\u8bed\u4e49\u5206\u5272\u7684\u6548\u679c\u3002"}}
{"id": "2506.23611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23611", "abs": "https://arxiv.org/abs/2506.23611", "authors": ["Ziao Liu", "Zhenjia Li", "Yifeng Shi", "Xiangang Li"], "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance\nFields (NeRF), excelling in complex scene reconstruction and efficient\nrendering. However, it relies on high-quality point clouds from\nStructure-from-Motion (SfM), limiting its applicability. SfM also fails in\ntexture-deficient or constrained-view scenarios, causing severe degradation in\n3DGS reconstruction. To address this limitation, we propose AttentionGS, a\nnovel framework that eliminates the dependency on high-quality initial point\nclouds by leveraging structural attention for direct 3D reconstruction from\nrandomly initialization. In the early training stage, we introduce geometric\nattention to rapidly recover the global scene structure. As training\nprogresses, we incorporate texture attention to refine fine-grained details and\nenhance rendering quality. Furthermore, we employ opacity-weighted gradients to\nguide Gaussian densification, leading to improved surface reconstruction.\nExtensive experiments on multiple benchmark datasets demonstrate that\nAttentionGS significantly outperforms state-of-the-art methods, particularly in\nscenarios where point cloud initialization is unreliable. Our approach paves\nthe way for more robust and flexible 3D Gaussian Splatting in real-world\napplications.", "AI": {"tldr": "AttentionGS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6ce8\u610f\u529b\u76f4\u63a5\u4ece\u968f\u673a\u521d\u59cb\u5316\u8fdb\u884c3D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5bf9\u9ad8\u8d28\u91cf\u70b9\u4e91\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "3DGS\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u70b9\u4e91\u521d\u59cb\u5316\uff0c\u800c\u5728\u7eb9\u7406\u4e0d\u8db3\u6216\u89c6\u89d2\u53d7\u9650\u7684\u573a\u666f\u4e2d\uff0cSfM\u65b9\u6cd5\u4f1a\u5931\u8d25\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "AttentionGS\u7ed3\u5408\u51e0\u4f55\u6ce8\u610f\u529b\u548c\u7eb9\u7406\u6ce8\u610f\u529b\uff0c\u65e9\u671f\u6062\u590d\u5168\u5c40\u7ed3\u6784\uff0c\u540e\u671f\u4f18\u5316\u7ec6\u8282\uff0c\u5e76\u4f7f\u7528\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u68af\u5ea6\u6307\u5bfc\u9ad8\u65af\u5bc6\u5ea6\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAttentionGS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u70b9\u4e91\u521d\u59cb\u5316\u4e0d\u53ef\u9760\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "AttentionGS\u4e3a3DGS\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.23618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23618", "abs": "https://arxiv.org/abs/2506.23618", "authors": ["Zhongdao Wang", "Guodongfang Zhao", "Jingjing Ren", "Bailan Feng", "Shifeng Zhang", "Wenbo Li"], "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them", "comment": "ICCV, 2025", "summary": "Diffusion-based generative models have demonstrated exceptional promise in\nthe video super-resolution (VSR) task, achieving a substantial advancement in\ndetail generation relative to prior methods. However, these approaches face\nsignificant computational efficiency challenges. For instance, current\ntechniques may require tens of minutes to super-resolve a mere 2-second, 1080p\nvideo. In this paper, we present TurboVSR, an ultra-efficient diffusion-based\nvideo super-resolution model. Our core design comprises three key aspects: (1)\nWe employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8\nto reduce the number of tokens. (2) Highly compressed latents pose substantial\nchallenges for training. We introduce factorized conditioning to mitigate the\nlearning complexity: we first learn to super-resolve the initial frame;\nsubsequently, we condition the super-resolution of the remaining frames on the\nhigh-resolution initial frame and the low-resolution subsequent frames. (3) We\nconvert the pre-trained diffusion model to a shortcut model to enable fewer\nsampling steps, further accelerating inference. As a result, TurboVSR performs\non par with state-of-the-art VSR methods, while being 100+ times faster, taking\nonly 7 seconds to process a 2-second long 1080p video. TurboVSR also supports\nimage resolution by considering image as a one-frame video. Our efficient\ndesign makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image\nSR show surprising fine details.", "AI": {"tldr": "TurboVSR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u9ad8\u6548\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u538b\u7f29\u6bd4\u81ea\u52a8\u7f16\u7801\u5668\u3001\u5206\u89e3\u6761\u4ef6\u548c\u6377\u5f84\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u7ec6\u8282\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5b9e\u7528\u5316\u3002", "method": "1. \u4f7f\u7528\u9ad8\u538b\u7f29\u6bd4\u81ea\u52a8\u7f16\u7801\u5668\u51cf\u5c11token\u6570\u91cf\uff1b2. \u5f15\u5165\u5206\u89e3\u6761\u4ef6\u964d\u4f4e\u5b66\u4e60\u590d\u6742\u5ea6\uff1b3. \u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u6377\u5f84\u6a21\u578b\u4ee5\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "TurboVSR\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u5feb100\u500d\u4ee5\u4e0a\uff0c\u5904\u74062\u79d21080p\u89c6\u9891\u4ec5\u97007\u79d2\uff0c\u5e76\u652f\u63014K\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "TurboVSR\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23623", "abs": "https://arxiv.org/abs/2506.23623", "authors": ["Shaofei Huang", "Rui Ling", "Tianrui Hui", "Hongyu Li", "Xu Zhou", "Shifeng Zhang", "Si Liu", "Richang Hong", "Meng Wang"], "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer", "comment": "Accepted by CVPR 2025; Code: https://github.com/spyflying/VCT_AVS;\n  Models: https://huggingface.co/nowherespyfly/VCT_AVS", "summary": "Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in\nvideo frames based on the associated audio signal. Prevailing AVS methods\ntypically adopt an audio-centric Transformer architecture, where object queries\nare derived from audio features. However, audio-centric Transformers suffer\nfrom two limitations: perception ambiguity caused by the mixed nature of audio,\nand weakened dense prediction ability due to visual detail loss. To address\nthese limitations, we propose a new Vision-Centric Transformer (VCT) framework\nthat leverages vision-derived queries to iteratively fetch corresponding audio\nand visual information, enabling queries to better distinguish between\ndifferent sounding objects from mixed audio and accurately delineate their\ncontours. Additionally, we also introduce a Prototype Prompted Query Generation\n(PPQG) module within our VCT framework to generate vision-derived queries that\nare both semantically aware and visually rich through audio prototype prompting\nand pixel context grouping, facilitating audio-visual information aggregation.\nExtensive experiments demonstrate that our VCT framework achieves new\nstate-of-the-art performances on three subsets of the AVSBench dataset. The\ncode is available at https://github.com/spyflying/VCT_AVS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u4e2d\u5fc3\u7684Transformer\u6846\u67b6\uff08VCT\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u9a71\u52a8\u7684\u67e5\u8be2\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4e2d\u7684\u611f\u77e5\u6a21\u7cca\u548c\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u4e2d\u5fc3Transformer\u65b9\u6cd5\u5b58\u5728\u611f\u77e5\u6a21\u7cca\u548c\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u4e2d\u5fc3Transformer\u6846\u67b6\uff0c\u7ed3\u5408\u539f\u578b\u63d0\u793a\u67e5\u8be2\u751f\u6210\u6a21\u5757\uff08PPQG\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u67e5\u8be2\u8fed\u4ee3\u83b7\u53d6\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728AVSBench\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u5b50\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "VCT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.23630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23630", "abs": "https://arxiv.org/abs/2506.23630", "authors": ["Lorenzo Olearo", "Giorgio Longari", "Alessandro Raganato", "Rafael Pe\u00f1aloza", "Simone Melzi"], "title": "Blending Concepts with Text-to-Image Diffusion Models", "comment": "Currently under review", "summary": "Diffusion models have dramatically advanced text-to-image generation in\nrecent years, translating abstract concepts into high-fidelity images with\nremarkable ease. In this work, we examine whether they can also blend distinct\nconcepts, ranging from concrete objects to intangible ideas, into coherent new\nvisual entities under a zero-shot framework. Specifically, concept blending\nmerges the key attributes of multiple concepts (expressed as textual prompts)\ninto a single, novel image that captures the essence of each concept. We\ninvestigate four blending methods, each exploiting different aspects of the\ndiffusion pipeline (e.g., prompt scheduling, embedding interpolation, or\nlayer-wise conditioning). Through systematic experimentation across diverse\nconcept categories, such as merging concrete concepts, synthesizing compound\nwords, transferring artistic styles, and blending architectural landmarks, we\nshow that modern diffusion models indeed exhibit creative blending capabilities\nwithout further training or fine-tuning. Our extensive user study, involving\n100 participants, reveals that no single approach dominates in all scenarios:\neach blending technique excels under certain conditions, with factors like\nprompt ordering, conceptual distance, and random seed affecting the outcome.\nThese findings highlight the remarkable compositional potential of diffusion\nmodels while exposing their sensitivity to seemingly minor input variations.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u96f6\u6837\u672c\u6846\u67b6\u4e0b\u80fd\u591f\u5c06\u4e0d\u540c\u6982\u5ff5\u878d\u5408\u4e3a\u65b0\u7684\u89c6\u89c9\u5b9e\u4f53\uff0c\u7814\u7a76\u4e86\u56db\u79cd\u878d\u5408\u65b9\u6cd5\uff0c\u53d1\u73b0\u6bcf\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u662f\u5426\u80fd\u591f\u5c06\u5177\u4f53\u6216\u62bd\u8c61\u7684\u6982\u5ff5\u878d\u5408\u4e3a\u8fde\u8d2f\u7684\u65b0\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u878d\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u63d0\u793a\u8c03\u5ea6\u3001\u5d4c\u5165\u63d2\u503c\u548c\u5206\u5c42\u6761\u4ef6\u7b49\u3002", "result": "\u6269\u6563\u6a21\u578b\u5c55\u73b0\u51fa\u521b\u9020\u6027\u878d\u5408\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5177\u6709\u663e\u8457\u7684\u7ec4\u5408\u6f5c\u529b\uff0c\u4f46\u5bf9\u8f93\u5165\u53d8\u5316\u654f\u611f\u3002"}}
{"id": "2506.23639", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23639", "abs": "https://arxiv.org/abs/2506.23639", "authors": ["Wanpeng Zhang", "Yicheng Feng", "Hao Luo", "Yijiang Li", "Zihao Yue", "Sipeng Zheng", "Zongqing Lu"], "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u8282\u5bf9\u7f16\u7801\u7684\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u5f15\u5bfc\u7684\u7f16\u7801\u65b9\u6848\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5173\u7cfb\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e0d\u540c\u6a21\u6001\u7684\u6709\u6548\u5bf9\u9f50\u4ecd\u662f\u6311\u6218\u3002", "method": "\u91c7\u7528\u5b57\u8282\u5bf9\u7f16\u7801\u89c6\u89c9\u6807\u8bb0\uff0c\u5f15\u5165\u4f18\u5148\u7ea7\u5f15\u5bfc\u7684\u7f16\u7801\u65b9\u6848\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7f29\u5c0f\u89c6\u89c9\u4e0e\u6587\u672c\u8868\u793a\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u4e86\u66f4\u9ad8\u6548\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2506.23641", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23641", "abs": "https://arxiv.org/abs/2506.23641", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "comment": null, "summary": "As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "AI": {"tldr": "VAP-Diffusion\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5916\u90e8\u77e5\u8bc6\uff0c\u901a\u8fc7\u89c6\u89c9\u5c5e\u6027\u63d0\u793a\u751f\u6210\u66f4\u771f\u5b9e\u591a\u6837\u7684\u533b\u5b66\u56fe\u50cf\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u7684\u751f\u6210\u9700\u8981\u4e30\u5bcc\u7684\u5c5e\u6027\u4fe1\u606f\uff0c\u4f46\u8be6\u7ec6\u63cf\u8ff0\u5f80\u5f80\u4e0d\u53ef\u5f97\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eChain-of-Thoughts\u7684\u63d0\u793a\u4eceMLLMs\u83b7\u53d6\u63cf\u8ff0\uff0c\u63d0\u51fa\u539f\u578b\u6761\u4ef6\u673a\u5236\u589e\u5f3a\u751f\u6210\u5668\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u79cd\u6570\u636e\u96c6\u7684\u4e09\u79cd\u533b\u5b66\u56fe\u50cf\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86VAP-Diffusion\u7684\u6709\u6548\u6027\u3002", "conclusion": "VAP-Diffusion\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u548c\u539f\u578b\u6761\u4ef6\u673a\u5236\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2506.23648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23648", "abs": "https://arxiv.org/abs/2506.23648", "authors": ["Zhe Liu", "Yuhao Huang", "Lian Liu", "Chengrui Zhang", "Haotian Lin", "Tong Han", "Zhiyuan Zhu", "Yanlin Chen", "Yuerui Chen", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "title": "MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis", "comment": "10 pages, 5 figures, accepted by MICCAI 2025", "summary": "Color Doppler echocardiography is a crucial tool for diagnosing mitral\nregurgitation (MR). Recent studies have explored intelligent methods for MR\ndiagnosis to minimize user dependence and improve accuracy. However, these\napproaches often fail to align with clinical workflow and may lead to\nsuboptimal accuracy and interpretability. In this study, we introduce an\nautomated MR diagnosis model (MReg) developed on the 4-chamber cardiac color\nDoppler echocardiography video (A4C-CDV). It follows comprehensive feature\nmining strategies to detect MR and assess its severity, considering clinical\nrealities. Our contribution is threefold. First, we formulate the MR diagnosis\nas a regression task to capture the continuity and ordinal relationships\nbetween categories. Second, we design a feature selection and amplification\nmechanism to imitate the sonographer's diagnostic logic for accurate MR\ngrading. Third, inspired by the Mixture-of-Experts concept, we introduce a\nfeature summary module to extract the category-level features, enhancing the\nrepresentational capacity for more accurate grading. We trained and evaluated\nour proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases\nwith three graded regurgitation labels. Compared to other weakly supervised\nvideo anomaly detection and supervised classification methods, MReg\ndemonstrated superior performance in MR diagnosis. Our code is available at:\nhttps://github.com/cskdstz/MReg.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u8154\u5fc3\u5f69\u8272\u591a\u666e\u52d2\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff08A4C-CDV\uff09\u7684\u81ea\u52a8\u5316\u4e8c\u5c16\u74e3\u53cd\u6d41\uff08MR\uff09\u8bca\u65ad\u6a21\u578bMReg\uff0c\u901a\u8fc7\u56de\u5f52\u4efb\u52a1\u548c\u7279\u5f81\u9009\u62e9\u673a\u5236\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u65b9\u6cd5\u5728MR\u8bca\u65ad\u4e2d\u4f9d\u8d56\u6027\u5f3a\u4e14\u4e0e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e0d\u7b26\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002", "method": "MReg\u91c7\u7528\u56de\u5f52\u4efb\u52a1\u3001\u7279\u5f81\u9009\u62e9\u4e0e\u653e\u5927\u673a\u5236\uff0c\u4ee5\u53ca\u57fa\u4e8eMixture-of-Experts\u7684\u7279\u5f81\u603b\u7ed3\u6a21\u5757\u3002", "result": "\u57281868\u4f8bA4C-CDV\u6570\u636e\u96c6\u4e0a\uff0cMReg\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f31\u76d1\u7763\u548c\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "MReg\u901a\u8fc7\u6a21\u4eff\u8d85\u58f0\u533b\u5e08\u903b\u8f91\u548c\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86MR\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002"}}
{"id": "2506.23657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23657", "abs": "https://arxiv.org/abs/2506.23657", "authors": ["Connor Daly", "Elettra Marconi", "Marco Riva", "Jinendra Ekanayake", "Daniel S. Elson", "Ferdinando Rodriguez y Baena"], "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue", "comment": "Preprint of paper, submitted", "summary": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is\na promising method with high translational potential. Unlike bone-mounted\ntracking devices, markerless tracking can reduce operating time and complexity.\nHowever, its use has been limited to cadaveric studies. This paper introduces\nthe first real-world clinical RGB-D dataset for spine surgery and develops\nSpineAlign, a system for capturing deformation between preoperative and\nintraoperative spine states. We also present an intraoperative segmentation\nnetwork trained on this data and introduce CorrespondNet, a multi-task\nframework for predicting key regions for registration in both intraoperative\nand preoperative scenes.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u7528\u4e8e\u810a\u67f1\u624b\u672f\u7684\u4e34\u5e8aRGB-D\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86SpineAlign\u7cfb\u7edf\u7528\u4e8e\u6355\u6349\u672f\u524d\u548c\u672f\u4e2d\u810a\u67f1\u72b6\u6001\u7684\u53d8\u5f62\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u672f\u4e2d\u5206\u5272\u7f51\u7edc\u548cCorrespondNet\u591a\u4efb\u52a1\u6846\u67b6\u3002", "motivation": "\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u548c\u590d\u6742\u6027\uff0c\u901a\u8fc7\u65e0\u6807\u8bb0\u8ddf\u8e2a\u6280\u672f\u66ff\u4ee3\u4f20\u7edf\u7684\u9aa8\u56fa\u5b9a\u8ddf\u8e2a\u8bbe\u5907\u3002", "method": "\u5229\u7528RGB-D\u6210\u50cf\u6280\u672f\uff0c\u5f00\u53d1SpineAlign\u7cfb\u7edf\u6355\u6349\u810a\u67f1\u53d8\u5f62\uff0c\u8bad\u7ec3\u672f\u4e2d\u5206\u5272\u7f51\u7edc\uff0c\u5e76\u63d0\u51faCorrespondNet\u591a\u4efb\u52a1\u6846\u67b6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2a\u4e34\u5e8aRGB-D\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86SpineAlign\u7cfb\u7edf\u548cCorrespondNet\u6846\u67b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u810a\u67f1\u624b\u672f\u4e2d\u65e0\u6807\u8bb0\u8ddf\u8e2a\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.23674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23674", "abs": "https://arxiv.org/abs/2506.23674", "authors": ["Dongyue Wu", "Zilin Guo", "Jialong Zuo", "Nong Sang", "Changxin Gao"], "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration", "comment": "Accepted by ICCV2025", "summary": "The ever-growing size of training datasets enhances the generalization\ncapability of modern machine learning models but also incurs exorbitant\ncomputational costs. Existing data pruning approaches aim to accelerate\ntraining by removing those less important samples. However, they often rely on\ngradients or proxy models, leading to prohibitive additional costs of gradient\nback-propagation and proxy model training. In this paper, we propose Partial\nForward Blocking (PFB), a novel framework for lossless training acceleration.\nThe efficiency of PFB stems from its unique adaptive pruning pipeline: sample\nimportance is assessed based on features extracted from the shallow layers of\nthe target model. Less important samples are then pruned, allowing only the\nretained ones to proceed with the subsequent forward pass and loss\nback-propagation. This mechanism significantly reduces the computational\noverhead of deep-layer forward passes and back-propagation for pruned samples,\nwhile also eliminating the need for auxiliary backward computations and proxy\nmodel training. Moreover, PFB introduces probability density as an indicator of\nsample importance. Combined with an adaptive distribution estimation module,\nour method dynamically prioritizes relatively rare samples, aligning with the\nconstantly evolving training state. Extensive experiments demonstrate the\nsignificant superiority of PFB in performance and speed. On ImageNet, PFB\nachieves a 0.5% accuracy improvement and 33% training time reduction with 40%\ndata pruned.", "AI": {"tldr": "PFB\u6846\u67b6\u901a\u8fc7\u6d45\u5c42\u7279\u5f81\u8bc4\u4f30\u6837\u672c\u91cd\u8981\u6027\uff0c\u52a8\u6001\u526a\u679d\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u65e0\u9700\u4ee3\u7406\u6a21\u578b\u6216\u989d\u5916\u53cd\u5411\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u526a\u679d\u65b9\u6cd5\u56e0\u4f9d\u8d56\u68af\u5ea6\u6216\u4ee3\u7406\u6a21\u578b\u800c\u4ea7\u751f\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faPartial Forward Blocking (PFB)\uff0c\u57fa\u4e8e\u6d45\u5c42\u7279\u5f81\u8bc4\u4f30\u6837\u672c\u91cd\u8981\u6027\u5e76\u52a8\u6001\u526a\u679d\uff0c\u51cf\u5c11\u6df1\u5c42\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728ImageNet\u4e0a\uff0cPFB\u526a\u679d40%\u6570\u636e\u65f6\uff0c\u51c6\u786e\u7387\u63d0\u53470.5%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1133%\u3002", "conclusion": "PFB\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u635f\u8bad\u7ec3\u52a0\u901f\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.23675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23675", "abs": "https://arxiv.org/abs/2506.23675", "authors": ["Patrick Glandorf", "Bodo Rosenhahn"], "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation", "comment": "ICCV'25 Workshops", "summary": "Vision Transformer have set new benchmarks in several tasks, but these models\ncome with the lack of high computational costs which makes them impractical for\nresource limited hardware. Network pruning reduces the computational complexity\nby removing less important operations while maintaining performance. However,\npruning a model on an unseen data domain, leads to a misevaluation of weight\nsignificance, resulting in suboptimal resource assignment. In this work, we\nfind that task-sensitive layers initially fail to improve the feature\nrepresentation on downstream tasks, leading to performance loss for early\npruning decisions. To address this problem, we introduce Pruning by Block\nBenefit (P3B), a pruning method that utilizes the relative contribution on\nblock level to globally assign parameter resources. P3B identifies low-impact\ncomponents to reduce parameter allocation while preserving critical ones.\nClassical pruning mask optimization struggles to reactivate zero-mask-elements.\nIn contrast, P3B sets a layerwise keep ratio based on global performance\nmetrics, ensuring the reactivation of late-converging blocks. We show in\nextensive experiments that P3B is a state of the art pruning method with most\nnoticeable gains in transfer learning tasks. Notably, P3B is able to conserve\nhigh performance, even in high sparsity regimes of 70% parameter reduction\nwhile only losing 0.64% in accuracy.", "AI": {"tldr": "P3B\u662f\u4e00\u79cd\u57fa\u4e8e\u5757\u7ea7\u8d21\u732e\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u5728\u9ad8\u7a00\u758f\u6027\u4e0b\u4fdd\u6301\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u3002", "motivation": "Vision Transformer\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5728\u672a\u89c1\u6570\u636e\u57df\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\u3002", "method": "\u63d0\u51faP3B\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7ea7\u76f8\u5bf9\u8d21\u732e\u5168\u5c40\u5206\u914d\u53c2\u6570\u8d44\u6e90\uff0c\u4fdd\u7559\u5173\u952e\u7ec4\u4ef6\u3002", "result": "P3B\u5728\u9ad8\u7a00\u758f\u6027\uff0870%\u53c2\u6570\u51cf\u5c11\uff09\u4e0b\u4ec5\u635f\u59310.64%\u51c6\u786e\u7387\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "P3B\u662f\u5148\u8fdb\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.23676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23676", "abs": "https://arxiv.org/abs/2506.23676", "authors": ["Gaozheng Pei", "Ke Ma", "Dongpeng Zhang", "Chengzhi Sun", "Qianqian Xu", "Qingming Huang"], "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement", "comment": null, "summary": "Due to their powerful image generation capabilities, diffusion-based\nadversarial example generation methods through image editing are rapidly\ngaining popularity. However, due to reliance on the discriminative capability\nof the diffusion model, these diffusion-based methods often struggle to\ngeneralize beyond conventional image classification tasks, such as in Deepfake\ndetection. Moreover, traditional strategies for enhancing adversarial example\ntransferability are challenging to adapt to these methods. To address these\nchallenges, we propose a unified framework that seamlessly incorporates\ntraditional transferability enhancement strategies into diffusion model-based\nadversarial example generation via image editing, enabling their application\nacross a wider range of downstream tasks. Our method won first place in the\n\"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of\nAI-Generated Media\" competition at ACM MM25, which validates the effectiveness\nof our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u878d\u5165\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u5bf9\u6297\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5728\u5bf9\u6297\u6837\u672c\u751f\u6210\u4e2d\u96be\u4ee5\u6cdb\u5316\u5230\u4f20\u7edf\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e4b\u5916\uff08\u5982Deepfake\u68c0\u6d4b\uff09\uff0c\u4e14\u4f20\u7edf\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u96be\u4ee5\u9002\u5e94\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u8f91\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728ACM MM25\u7684\u5bf9\u6297\u653b\u51fb\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u5bf9\u6297\u6837\u672c\u751f\u6210\u4e2d\u7684\u6cdb\u5316\u548c\u8fc1\u79fb\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.23690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23690", "abs": "https://arxiv.org/abs/2506.23690", "authors": ["Shuai Tan", "Biao Gong", "Yujie Wei", "Shiwei Zhang", "Zhuoxin Liu", "Dandan Zheng", "Jingdong Chen", "Yan Wang", "Hao Ouyang", "Kecheng Zheng", "Yujun Shen"], "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation", "comment": "Project page: https://lucaria-academy.github.io/SynMotion/", "summary": "Diffusion-based video motion customization facilitates the acquisition of\nhuman motion representations from a few video samples, while achieving\narbitrary subjects transfer through precise textual conditioning. Existing\napproaches often rely on semantic-level alignment, expecting the model to learn\nnew motion concepts and combine them with other entities (e.g., ''cats'' or\n''dogs'') to produce visually appealing results. However, video data involve\ncomplex spatio-temporal patterns, and focusing solely on semantics cause the\nmodel to overlook the visual complexity of motion. Conversely, tuning only the\nvisual representation leads to semantic confusion in representing the intended\naction. To address these limitations, we propose SynMotion, a new\nmotion-customized video generation model that jointly leverages semantic\nguidance and visual adaptation. At the semantic level, we introduce the\ndual-embedding semantic comprehension mechanism which disentangles subject and\nmotion representations, allowing the model to learn customized motion features\nwhile preserving its generative capabilities for diverse subjects. At the\nvisual level, we integrate parameter-efficient motion adapters into a\npre-trained video generation model to enhance motion fidelity and temporal\ncoherence. Furthermore, we introduce a new embedding-specific training strategy\nwhich \\textbf{alternately optimizes} subject and motion embeddings, supported\nby the manually constructed Subject Prior Video (SPV) training dataset. This\nstrategy promotes motion specificity while preserving generalization across\ndiverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark\nwith diverse motion patterns. Experimental results across both T2V and I2V\nsettings demonstrate that \\method outperforms existing baselines. Project page:\nhttps://lucaria-academy.github.io/SynMotion/", "AI": {"tldr": "SynMotion\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u6307\u5bfc\u548c\u89c6\u89c9\u9002\u5e94\u7684\u89c6\u9891\u8fd0\u52a8\u5b9a\u5236\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5d4c\u5165\u8bed\u4e49\u7406\u89e3\u673a\u5236\u548c\u53c2\u6570\u9ad8\u6548\u7684\u8fd0\u52a8\u9002\u914d\u5668\uff0c\u63d0\u5347\u4e86\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8bed\u4e49\u5bf9\u9f50\u6216\u89c6\u89c9\u8868\u793a\uff0c\u5bfc\u81f4\u8fd0\u52a8\u590d\u6742\u6027\u88ab\u5ffd\u89c6\u6216\u8bed\u4e49\u6df7\u6dc6\u3002", "method": "\u5f15\u5165\u53cc\u5d4c\u5165\u8bed\u4e49\u7406\u89e3\u673a\u5236\u5206\u79bb\u4e3b\u4f53\u548c\u8fd0\u52a8\u8868\u793a\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u8fd0\u52a8\u9002\u914d\u5668\uff0c\u5e76\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7684\u5d4c\u5165\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728T2V\u548cI2V\u8bbe\u7f6e\u4e0b\uff0cSynMotion\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "SynMotion\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bed\u4e49\u548c\u89c6\u89c9\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u5b9a\u5236\u4e2d\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2506.23705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23705", "abs": "https://arxiv.org/abs/2506.23705", "authors": ["Smriti Joshi", "Richard Osuala", "Lidia Garrucho", "Kaisar Kushibar", "Dimitri Kessler", "Oliver Diaz", "Karim Lekadir"], "title": "Single Image Test-Time Adaptation via Multi-View Co-Training", "comment": "MICCAI 2025", "summary": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8865\u4e01\u7684\u591a\u89c6\u56fe\u534f\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u56fe\u50cf\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u5b9e\u65f6\u9002\u5e94\u548c\u4f53\u79ef\u6570\u636e\u5229\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u4e14\u5ffd\u7565\u533b\u5b66\u5f71\u50cf\u7684\u4f53\u79ef\u7279\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u5b9e\u73b0\u7279\u5f81\u548c\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u4ec5\u9700\u5355\u5f20\u6d4b\u8bd5\u56fe\u50cf\u5373\u53ef\u5b8c\u6210\u4f53\u79ef\u5206\u5272\u3002", "result": "\u5728\u4e09\u4e2a\u4e73\u817a\u764cMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u63a5\u8fd1\u76d1\u7763\u57fa\u51c6\uff0c\u5e73\u5747Dice\u7cfb\u6570\u63d0\u53473.75%\u3002", "conclusion": "\u65b9\u6cd5\u9ad8\u6548\u4e14\u6613\u96c6\u6210\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u65f6\u9700\u6c42\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23711", "abs": "https://arxiv.org/abs/2506.23711", "authors": ["Haoyang Chen", "Dongfang Sun", "Caoyuan Ma", "Shiqin Wang", "Kewei Zhang", "Zheng Wang", "Zhixiang Wang"], "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion", "comment": null, "summary": "We propose Subjective Camera, a human-as-imaging-device paradigm that\nreconstructs real-world scenes from mental impressions through synergistic use\nof verbal descriptions and progressive rough sketches. This approach overcomes\ndual limitations of language ambiguity and sketch abstraction by treating the\nuser's drawing sequence as priors, effectively translating subjective\nperceptual expectations into photorealistic images.\n  Existing approaches face three fundamental barriers: (1) user-specific\nsubjective input biases, (2) huge modality gap between planar sketch and 3D\npriors in diffusion, and (3) sketch quality-sensitive performance degradation.\nCurrent solutions either demand resource-intensive model adaptation or impose\nimpractical requirements on sketch precision.\n  Our framework addresses these challenges through concept-sequential\ngeneration. (1) We establish robust appearance priors through text-reward\noptimization, and then implement sequence-aware disentangled generation that\nprocesses concepts in sketching order; these steps accommodate user-specific\nsubjective expectation in a train-free way. (2) We employ latent optimization\nthat effectively bridges the modality gap between planar sketches and 3D priors\nin diffusion. (3) Our hierarchical reward-guided framework enables the use of\nrough sketches without demanding artistic expertise. Comprehensive evaluation\nacross diverse datasets demonstrates that our approach achieves\nstate-of-the-art performance in maintaining both semantic and spatial\ncoherence.", "AI": {"tldr": "Subjective Camera\u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u63cf\u8ff0\u548c\u6e10\u8fdb\u8349\u56fe\u91cd\u5efa\u771f\u5b9e\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u7cca\u6027\u548c\u8349\u56fe\u62bd\u8c61\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7528\u6237\u4e3b\u89c2\u8f93\u5165\u504f\u5dee\u3001\u8349\u56fe\u4e0e3D\u5148\u9a8c\u7684\u6a21\u6001\u5dee\u8ddd\u5927\u3001\u4ee5\u53ca\u8349\u56fe\u8d28\u91cf\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8d44\u6e90\u5bc6\u96c6\u578b\u8c03\u6574\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u8349\u56fe\u7cbe\u5ea6\u8981\u6c42\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u987a\u5e8f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5956\u52b1\u4f18\u5316\u5efa\u7acb\u5916\u89c2\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u5e8f\u5217\u611f\u77e5\u89e3\u8026\u751f\u6210\u548c\u6f5c\u5728\u4f18\u5316\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u7528\u6237\u4e3b\u89c2\u671f\u671b\u3002", "result": "\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Subjective Camera\u901a\u8fc7\u521b\u65b0\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2506.23714", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23714", "abs": "https://arxiv.org/abs/2506.23714", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkil\u00e4", "Mourad Oussalah"], "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u884c\u4e3a\u611f\u77e5\u7684\u591a\u6a21\u6001\u89c6\u9891\u6458\u8981\u6846\u67b6\uff0c\u6574\u5408\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u7ebf\u7d22\u751f\u6210\u65f6\u95f4\u6233\u5bf9\u9f50\u7684\u6458\u8981\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u5185\u5bb9\u5728\u6559\u80b2\u3001\u804c\u4e1a\u548c\u793e\u4ea4\u9886\u57df\u7684\u589e\u52a0\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u6709\u6548\u6458\u8981\u6280\u672f\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u97f5\u5f8b\u7279\u5f81\u3001\u6587\u672c\u7ebf\u7d22\u548c\u89c6\u89c9\u6307\u6807\uff0c\u8bc6\u522b\u8bed\u4e49\u548c\u60c5\u611f\u91cd\u8981\u65f6\u523b\uff0c\u5e76\u5229\u7528\u8de8\u6a21\u6001\u5f3a\u8c03\u7684\u5956\u52b1\u8bcd\u63d0\u5347\u6458\u8981\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cROUGE-1\u4ece0.4769\u63d0\u5347\u81f30.7929\uff0cBERTScore\u4ece0.9152\u63d0\u5347\u81f30.9536\uff0c\u89c6\u9891\u8bc4\u4f30F1-Score\u63d0\u5347\u8fd123%\u3002", "conclusion": "\u591a\u6a21\u6001\u6574\u5408\u5728\u751f\u6210\u5168\u9762\u4e14\u884c\u4e3a\u611f\u77e5\u7684\u89c6\u9891\u6458\u8981\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.23724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23724", "abs": "https://arxiv.org/abs/2506.23724", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "comment": "15 pages, 5 figures", "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCOCA\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u534f\u540c\u5b66\u4e60\u63d0\u5347\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u6027\u80fd\uff0c\u5229\u7528\u4e0d\u540c\u6a21\u578b\u7684\u4e92\u8865\u77e5\u8bc6\u51cf\u5c11\u504f\u5dee\u5e76\u589e\u5f3a\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u8de8\u6a21\u578b\u77e5\u8bc6\u5982\u4f55\u5f71\u54cdTTA\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u65e0\u76d1\u7763\u5728\u7ebf\u5b66\u4e60\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u4e92\u8865\u77e5\u8bc6\u3002", "method": "\u63d0\u51faCOCA\u6846\u67b6\uff0c\u5305\u542b\u534f\u540c\u9002\u5e94\uff08\u6574\u5408\u4e92\u8865\u77e5\u8bc6\uff09\u548c\u81ea\u9002\u5e94\uff08\u589e\u5f3a\u6a21\u578b\u72ec\u7279\u6027\uff09\u4e24\u79cd\u7b56\u7565\u3002", "result": "COCA\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5c06ViT-Base\u5728ImageNet-C\u4e0a\u7684\u51c6\u786e\u7387\u4ece51.7%\u63d0\u5347\u81f364.5%\u3002", "conclusion": "\u8de8\u6a21\u578b\u534f\u540c\u5b66\u4e60\u662f\u63d0\u5347TTA\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0cCOCA\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.23729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23729", "abs": "https://arxiv.org/abs/2506.23729", "authors": ["Guiyu Zhang", "Chen Shi", "Zijian Jiang", "Xunzhi Xiang", "Jingjing Qian", "Shaoshuai Shi", "Li Jiang"], "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization", "comment": "Preprint. Work in progress", "summary": "Video identity customization seeks to synthesize realistic, temporally\ncoherent videos of a specific subject, given a single reference image and a\ntext prompt. This task presents two core challenges: (1) maintaining identity\nconsistency while aligning with the described appearance and actions, and (2)\ngenerating natural, fluid motion without unrealistic stiffness. To address\nthese challenges, we introduce Proteus-ID, a novel diffusion-based framework\nfor identity-consistent and motion-coherent video customization. First, we\npropose a Multimodal Identity Fusion (MIF) module that unifies visual and\ntextual cues into a joint identity representation using a Q-Former, providing\ncoherent guidance to the diffusion model and eliminating modality imbalance.\nSecond, we present a Time-Aware Identity Injection (TAII) mechanism that\ndynamically modulates identity conditioning across denoising steps, improving\nfine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a\nself-supervised strategy that reweights the training loss based on\noptical-flow-derived motion heatmaps, enhancing motion realism without\nrequiring additional inputs. To support this task, we construct Proteus-Bench,\na high-quality dataset comprising 200K curated clips for training and 150\nindividuals from diverse professions and ethnicities for evaluation. Extensive\nexperiments demonstrate that Proteus-ID outperforms prior methods in identity\npreservation, text alignment, and motion quality, establishing a new benchmark\nfor video identity customization. Codes and data are publicly available at\nhttps://grenoble-zhang.github.io/Proteus-ID/.", "AI": {"tldr": "Proteus-ID\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8eab\u4efd\u4e00\u81f4\u548c\u8fd0\u52a8\u8fde\u8d2f\u7684\u89c6\u9891\u5b9a\u5236\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u81ea\u7136\u6027\u7684\u6311\u6218\u3002", "motivation": "\u89c6\u9891\u8eab\u4efd\u5b9a\u5236\u4efb\u52a1\u9762\u4e34\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u81ea\u7136\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u70b9\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u8eab\u4efd\u878d\u5408\u6a21\u5757\uff08MIF\uff09\u3001\u65f6\u95f4\u611f\u77e5\u8eab\u4efd\u6ce8\u5165\u673a\u5236\uff08TAII\uff09\u548c\u81ea\u9002\u5e94\u8fd0\u52a8\u5b66\u4e60\u7b56\u7565\uff08AML\uff09\u3002", "result": "Proteus-ID\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u6587\u672c\u5bf9\u9f50\u548c\u8fd0\u52a8\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6Proteus-Bench\u3002", "conclusion": "Proteus-ID\u4e3a\u89c6\u9891\u8eab\u4efd\u5b9a\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.23751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23751", "abs": "https://arxiv.org/abs/2506.23751", "authors": ["Annika M\u00fctze", "Sadia Ilyas", "Christian D\u00f6rpelkus", "Matthias Rottmann"], "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?", "comment": null, "summary": "Open-vocabulary object detectors such as Grounding DINO are trained on vast\nand diverse data, achieving remarkable performance on challenging datasets. Due\nto that, it is unclear where to find their limitations, which is of major\nconcern when using in safety-critical applications. Real-world data does not\nprovide sufficient control, required for a rigorous evaluation of model\ngeneralization. In contrast, synthetically generated data allows to\nsystematically explore the boundaries of model competence/generalization. In\nthis work, we address two research questions: 1) Can we challenge\nopen-vocabulary object detectors with generated image content? 2) Can we find\nsystematic failure modes of those models? To address these questions, we design\ntwo automated pipelines using stable diffusion to inpaint unusual objects with\nhigh diversity in semantics, by sampling multiple substantives from WordNet and\nChatGPT. On the synthetically generated data, we evaluate and compare multiple\nopen-vocabulary object detectors as well as a classical object detector. The\nsynthetic data is derived from two real-world datasets, namely LostAndFound, a\nchallenging out-of-distribution (OOD) detection benchmark, and the NuImages\ndataset. Our results indicate that inpainting can challenge open-vocabulary\nobject detectors in terms of overlooking objects. Additionally, we find a\nstrong dependence of open-vocabulary models on object location, rather than on\nobject semantics. This provides a systematic approach to challenge\nopen-vocabulary models and gives valuable insights on how data could be\nacquired to effectively improve these models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5408\u6210\u6570\u636e\u6311\u6218\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u53d1\u73b0\u5176\u4f9d\u8d56\u5bf9\u8c61\u4f4d\u7f6e\u800c\u975e\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528Stable Diffusion\u751f\u6210\u591a\u6837\u5316\u8bed\u4e49\u7684\u5408\u6210\u6570\u636e\uff0c\u8bc4\u4f30\u591a\u79cd\u68c0\u6d4b\u5668\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u6311\u6218\u68c0\u6d4b\u5668\uff0c\u53d1\u73b0\u5176\u66f4\u4f9d\u8d56\u5bf9\u8c61\u4f4d\u7f6e\u800c\u975e\u8bed\u4e49\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e3a\u6311\u6218\u548c\u6539\u8fdb\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.23785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23785", "abs": "https://arxiv.org/abs/2506.23785", "authors": ["Yongjian Wu", "Yang Zhou", "Jiya Saiyin", "Bingzheng Wei", "Yan Xu"], "title": "Visual Textualization for Image Prompted Object Detection", "comment": "Accepted by ICCV 2025", "summary": "We propose VisTex-OVLM, a novel image prompted object detection method that\nintroduces visual textualization -- a process that projects a few visual\nexemplars into the text feature space to enhance Object-level Vision-Language\nModels' (OVLMs) capability in detecting rare categories that are difficult to\ndescribe textually and nearly absent from their pre-training data, while\npreserving their pre-trained object-text alignment. Specifically, VisTex-OVLM\nleverages multi-scale textualizing blocks and a multi-stage fusion strategy to\nintegrate visual information from visual exemplars, generating textualized\nvisual tokens that effectively guide OVLMs alongside text prompts. Unlike\nprevious methods, our method maintains the original architecture of OVLM,\nmaintaining its generalization capabilities while enhancing performance in\nfew-shot settings. VisTex-OVLM demonstrates superior performance across\nopen-set datasets which have minimal overlap with OVLM's pre-training data and\nachieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.\nThe code will be released at https://github.com/WitGotFlg/VisTex-OVLM.", "AI": {"tldr": "VisTex-OVLM\u662f\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u6587\u672c\u5316\u589e\u5f3a\u5bf9\u8c61\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08OVLM\uff09\u68c0\u6d4b\u7f55\u89c1\u7c7b\u522b\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7f55\u89c1\u7c7b\u522b\u56e0\u96be\u4ee5\u6587\u672c\u63cf\u8ff0\u6216\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u5931\u800c\u5bfc\u81f4\u7684\u68c0\u6d4b\u56f0\u96be\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u5c3a\u5ea6\u6587\u672c\u5316\u5757\u548c\u591a\u9636\u6bb5\u878d\u5408\u7b56\u7565\uff0c\u5c06\u89c6\u89c9\u793a\u4f8b\u4fe1\u606f\u8f6c\u5316\u4e3a\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u89c6\u89c9\u6807\u8bb0\u3002", "result": "\u5728\u5f00\u653e\u96c6\u6570\u636e\u96c6\u548c\u5c11\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\uff08PASCAL VOC\u548cMSCOCO\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "VisTex-OVLM\u5728\u4fdd\u6301OVLM\u539f\u6709\u67b6\u6784\u548c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23801", "abs": "https://arxiv.org/abs/2506.23801", "authors": ["Ce Wang", "Wanjie Sun"], "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors", "comment": null, "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation.", "AI": {"tldr": "CRefDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u53ef\u63a7\u53c2\u8003\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u878d\u5408\u673a\u5236\u548cBetter Start\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RefSR\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RefSR\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u573a\u666f\uff08\u5982\u8de8\u4f20\u611f\u5668\u5206\u8fa8\u7387\u5dee\u5f02\u548c\u5730\u8868\u8986\u76d6\u53d8\u5316\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u8db3\u6216\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684Stable Diffusion\u6a21\u578b\uff0c\u5f15\u5165\u53cc\u5206\u652f\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u6574\u5408\u53c2\u8003\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u63d0\u51faBetter Start\u7b56\u7565\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728Real-RefRSSRD\u6570\u636e\u96c6\u4e0a\uff0cCRefDiff\u5728\u591a\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u63d0\u5347\u4e86\u573a\u666f\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "CRefDiff\u901a\u8fc7\u53ef\u63a7\u53c2\u8003\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\u548c\u5e94\u7528\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.23808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23808", "abs": "https://arxiv.org/abs/2506.23808", "authors": ["Carl Olsson", "Amanda Nilsson"], "title": "Towards Initialization-free Calibrated Bundle Adjustment", "comment": null, "summary": "A recent series of works has shown that initialization-free BA can be\nachieved using pseudo Object Space Error (pOSE) as a surrogate objective. The\ninitial reconstruction-step optimizes an objective where all terms are\nprojectively invariant and it cannot incorporate knowledge of the camera\ncalibration. As a result, the solution is only determined up to a projective\ntransformation of the scene and the process requires more data for successful\nreconstruction.\n  In contrast, we present a method that is able to use the known camera\ncalibration thereby producing near metric solutions, that is, reconstructions\nthat are accurate up to a similarity transformation. To achieve this we\nintroduce pairwise relative rotation estimates that carry information about\ncamera calibration. These are only invariant to similarity transformations,\nthus encouraging solutions that preserve metric features of the real scene. Our\nmethod can be seen as integrating rotation averaging into the pOSE framework\nstriving towards initialization-free calibrated SfM.\n  Our experimental evaluation shows that we are able to reliably optimize our\nobjective, achieving convergence to the global minimum with high probability\nfrom random starting solutions, resulting in accurate near metric\nreconstructions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5df2\u77e5\u76f8\u673a\u6807\u5b9a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6210\u5bf9\u76f8\u5bf9\u65cb\u8f6c\u4f30\u8ba1\uff0c\u5b9e\u73b0\u8fd1\u5ea6\u91cf\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u81ea\u7531BA\u4e2d\u4ec5\u80fd\u751f\u6210\u6295\u5f71\u53d8\u6362\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u521d\u59cb\u5316\u81ea\u7531BA\u65b9\u6cd5\u4f7f\u7528\u4f2a\u7269\u4f53\u7a7a\u95f4\u8bef\u5dee\uff08pOSE\uff09\u4f5c\u4e3a\u66ff\u4ee3\u76ee\u6807\uff0c\u4f46\u5176\u65e0\u6cd5\u5229\u7528\u76f8\u673a\u6807\u5b9a\u4fe1\u606f\uff0c\u5bfc\u81f4\u89e3\u4ec5\u80fd\u786e\u5b9a\u5230\u6295\u5f71\u53d8\u6362\uff0c\u4e14\u9700\u8981\u66f4\u591a\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u76f8\u673a\u6807\u5b9a\u4fe1\u606f\uff0c\u5b9e\u73b0\u8fd1\u5ea6\u91cf\u91cd\u5efa\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6210\u5bf9\u76f8\u5bf9\u65cb\u8f6c\u4f30\u8ba1\uff08\u4ec5\u5bf9\u76f8\u4f3c\u53d8\u6362\u4e0d\u53d8\uff09\uff0c\u5c06\u65cb\u8f6c\u5e73\u5747\u96c6\u6210\u5230pOSE\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u521d\u59cb\u5316\u81ea\u7531\u6807\u5b9aSfM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u53ef\u9760\u4f18\u5316\u76ee\u6807\uff0c\u4ece\u968f\u673a\u521d\u59cb\u89e3\u9ad8\u6982\u7387\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u751f\u6210\u51c6\u786e\u7684\u8fd1\u5ea6\u91cf\u91cd\u5efa\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6210\u529f\u5229\u7528\u76f8\u673a\u6807\u5b9a\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u8fd1\u5ea6\u91cf\u91cd\u5efa\uff0c\u4e3a\u521d\u59cb\u5316\u81ea\u7531\u6807\u5b9aSfM\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23810", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23810", "abs": "https://arxiv.org/abs/2506.23810", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP", "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  submitted version). MICCAI proceedings DOI will appear here", "summary": "An innovative few-shot anomaly detection approach is presented, leveraging\nthe pre-trained CLIP model for medical data, and adapting it for both\nimage-level anomaly classification (AC) and pixel-level anomaly segmentation\n(AS). A dual-branch design is proposed to separately capture normal and\nabnormal features through learnable adapters in the CLIP vision encoder. To\nimprove semantic alignment, learnable text prompts are employed to link visual\nfeatures. Furthermore, SigLIP loss is applied to effectively handle the\nmany-to-one relationship between images and unpaired text prompts, showcasing\nits adaptation in the medical field for the first time. Our approach is\nvalidated on multiple modalities, demonstrating superior performance over\nexisting methods for AC and AS, in both same-dataset and cross-dataset\nevaluations. Unlike prior work, it does not rely on synthetic data or memory\nbanks, and an ablation study confirms the contribution of each component. The\ncode is available at https://github.com/mahshid1998/MadCLIP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u6570\u636e\u7684\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u8bbe\u8ba1\u548c\u53ef\u5b66\u4e60\u6587\u672c\u63d0\u793a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u6570\u636e\u4e2d\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u907f\u514d\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u8bb0\u5fc6\u5e93\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\u6355\u83b7\u6b63\u5e38\u548c\u5f02\u5e38\u7279\u5f81\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u6587\u672c\u63d0\u793a\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u5e94\u7528SigLIP\u635f\u5931\u5904\u7406\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u7684\u591a\u5bf9\u4e00\u5173\u7cfb\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u540c\u6570\u636e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5408\u6210\u6570\u636e\u6216\u8bb0\u5fc6\u5e93\uff0c\u5404\u7ec4\u4ef6\u5747\u5bf9\u6027\u80fd\u6709\u8d21\u732e\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23822", "abs": "https://arxiv.org/abs/2506.23822", "authors": ["Shiming Chen", "Bowen Duan", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model", "comment": "Accepted to ICCV'25", "summary": "Large-scale vision-language models (VLMs), such as CLIP, have achieved\nremarkable success in zero-shot learning (ZSL) by leveraging large-scale\nvisual-text pair datasets. However, these methods often lack interpretability,\nas they compute the similarity between an entire query image and the embedded\ncategory words, making it difficult to explain their predictions. One approach\nto address this issue is to develop interpretable models by integrating\nlanguage, where classifiers are built using discrete attributes, similar to\nhuman perception. This introduces a new challenge: how to effectively align\nlocal visual features with corresponding attributes based on pre-trained VLMs.\nTo tackle this, we propose LaZSL, a locally-aligned vision-language model for\ninterpretable ZSL. LaZSL employs local visual-semantic alignment via optimal\ntransport to perform interaction between visual regions and their associated\nattributes, facilitating effective alignment and providing interpretable\nsimilarity without the need for additional training. Extensive experiments\ndemonstrate that our method offers several advantages, including enhanced\ninterpretability, improved accuracy, and strong domain generalization. Codes\navailable at: https://github.com/shiming-chen/LaZSL.", "AI": {"tldr": "LaZSL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u89c6\u89c9-\u8bed\u4e49\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u89c6\u89c9\u533a\u57df\u4e0e\u5c5e\u6027\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5bf9\u9f50\u5c40\u90e8\u89c6\u89c9\u7279\u5f81\u4e0e\u79bb\u6563\u5c5e\u6027\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "method": "LaZSL\u5229\u7528\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u5c40\u90e8\u89c6\u89c9-\u8bed\u4e49\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b8c\u6210\u89c6\u89c9\u533a\u57df\u4e0e\u5c5e\u6027\u7684\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLaZSL\u5728\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u9886\u57df\u6cdb\u5316\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LaZSL\u4e3a\u53ef\u89e3\u91ca\u96f6\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u517c\u5177\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.23825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23825", "abs": "https://arxiv.org/abs/2506.23825", "authors": ["Haoji Zhang", "Yiqin Wang", "Yansong Tang", "Yong Liu", "Jiashi Feng", "Xiaojie Jin"], "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", "comment": "Accepted by ICCV 2025", "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.", "AI": {"tldr": "Flash-VStream\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u957f\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1Flash Memory\u6a21\u5757\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5904\u7406\u957f\u89c6\u9891\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u6cdb\u5316\uff0cFlash-VStream\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFlash Memory\u6a21\u5757\uff0c\u5305\u542b\u4f4e\u5bb9\u91cf\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u9ad8\u5bb9\u91cf\u589e\u5f3a\u8bb0\u5fc6\uff0c\u5206\u522b\u805a\u5408\u957f\u65f6\u4fe1\u606f\u5e76\u68c0\u7d22\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982EgoSchema\u3001MLVU\u7b49\uff09\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "Flash-VStream\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.23827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23827", "abs": "https://arxiv.org/abs/2506.23827", "authors": ["Mingcheng Qu", "Yuncong Wu", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning", "comment": "Our paper has been accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) provides crucial insights into tissue\nmicro-environments, but is limited to its high cost and complexity. As an\nalternative, predicting gene expression from pathology whole slide images (WSI)\nis gaining increasing attention. However, existing methods typically rely on\nsingle patches or a single pathology modality, neglecting the complex spatial\nand molecular interactions between target and neighboring information (e.g.,\ngene co-expression). This leads to a failure in establishing connections among\nadjacent regions and capturing intricate cross-modal relationships. To address\nthese issues, we propose NH2ST, a framework that integrates spatial context and\nboth pathology and gene modalities for gene expression prediction. Our model\ncomprises a query branch and a neighbor branch to process paired target patch\nand gene data and their neighboring regions, where cross-attention and\ncontrastive learning are employed to capture intrinsic associations and ensure\nalignments between pathology and gene expression. Extensive experiments on six\ndatasets demonstrate that our model consistently outperforms existing methods,\nachieving over 20% in PCC metrics. Codes are available at\nhttps://github.com/MCPathology/NH2ST", "AI": {"tldr": "NH2ST\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u75c5\u7406\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u5ffd\u7565\u7a7a\u95f4\u548c\u5206\u5b50\u4ea4\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u53d7\u9650\uff0cNH2ST\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u67e5\u8be2\u5206\u652f\u548c\u90bb\u5c45\u5206\u652f\u5904\u7406\u76ee\u6807\u533a\u57df\u53ca\u5176\u90bb\u8fd1\u6570\u636e\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPCC\u6307\u6807\u63d0\u5347\u8d85\u8fc720%\u3002", "conclusion": "NH2ST\u901a\u8fc7\u591a\u6a21\u6001\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.23832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23832", "abs": "https://arxiv.org/abs/2506.23832", "authors": ["Ronit D. Gross", "Tal Halevi", "Ella Koresh", "Yarden Tzach", "Ido Kanter"], "title": "Low-latency vision transformers via large-scale multi-head attention", "comment": "23 pages, 4 figures, 7 tables", "summary": "The emergence of spontaneous symmetry breaking among a few heads of\nmulti-head attention (MHA) across transformer blocks in classification tasks\nwas recently demonstrated through the quantification of single-nodal\nperformance (SNP). This finding indicates that each head focuses its attention\non a subset of labels through cooperation among its SNPs. This underlying\nlearning mechanism is generalized to large-scale MHA (LS-MHA) using a single\nmatrix value representing single-head performance (SHP), analogous to\nsingle-filter performance in convolutional neural networks (CNNs). The results\nindicate that each SHP matrix comprises multiple unit clusters such that each\nlabel being explicitly recognized by a few heads with negligible noise. This\nleads to an increased signal-to-noise ratio (SNR) along the transformer blocks,\nthereby improving classification accuracy. These features give rise to several\ndistinct vision transformer (ViT) architectures that achieve the same accuracy\nbut differ in their LS-MHA structures. As a result, their soft committee yields\nsuperior accuracy, an outcome not typically observed in CNNs which rely on\nhundreds of filters. In addition, a significant reduction in latency is\nachieved without affecting the accuracy by replacing the initial transformer\nblocks with convolutional layers. This substitution accelerates early-stage\nlearning, which is then improved by subsequent transformer layers. The\nextension of this learning mechanism to natural language processing tasks,\nbased on quantitative differences between CNNs and ViT architectures, has the\npotential to yield new insights in deep learning. The findings are demonstrated\nusing compact convolutional transformer architectures trained on the CIFAR-100\ndataset.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u81ea\u53d1\u5bf9\u79f0\u6027\u65ad\u88c2\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5934\u6027\u80fd\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u89c6\u89c9Transformer\u67b6\u6784\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u4e0d\u540c\u5934\u4e4b\u95f4\u7684\u534f\u4f5c\u65b9\u5f0f\u53ca\u5176\u5bf9\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5e7f\u5230\u5927\u89c4\u6a21\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u5355\u5934\u6027\u80fd\uff08SHP\uff09\u77e9\u9635\uff0c\u5206\u6790\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5934\u7684\u534f\u4f5c\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u89c6\u89c9Transformer\u67b6\u6784\u3002", "result": "\u53d1\u73b0SHP\u77e9\u9635\u7531\u591a\u4e2a\u5355\u5143\u7c07\u7ec4\u6210\uff0c\u63d0\u9ad8\u4e86\u4fe1\u53f7\u566a\u58f0\u6bd4\uff08SNR\uff09\uff0c\u4ece\u800c\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5ef6\u8fdf\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eCNN\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9Transformer\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2506.23833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23833", "abs": "https://arxiv.org/abs/2506.23833", "authors": ["Oscar Ovanger", "Ragnar Hauge", "Jacob Skauvold", "Michael J. Pyrcz", "Jo Eidsvik"], "title": "PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric", "comment": "13 pages, 20 figures", "summary": "This paper presents PointSSIM, a novel low-dimensional image-to-image\ncomparison metric that is resolution invariant. Drawing inspiration from the\nstructural similarity index measure and mathematical morphology, PointSSIM\nenables robust comparison across binary images of varying resolutions by\ntransforming them into marked point pattern representations. The key features\nof the image, referred to as anchor points, are extracted from binary images by\nidentifying locally adaptive maxima from the minimal distance transform. Image\ncomparisons are then performed using a summary vector, capturing intensity,\nconnectivity, complexity, and structural attributes. Results show that this\napproach provides an efficient and reliable method for image comparison,\nparticularly suited to applications requiring structural analysis across\ndifferent resolutions.", "AI": {"tldr": "PointSSIM\u662f\u4e00\u79cd\u4f4e\u7ef4\u56fe\u50cf\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5177\u6709\u5206\u8fa8\u7387\u4e0d\u53d8\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u4e8c\u503c\u56fe\u50cf\u6bd4\u8f83\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5206\u8fa8\u7387\u4e8c\u503c\u56fe\u50cf\u7684\u7ed3\u6784\u6bd4\u8f83\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6807\u8bb0\u70b9\u6a21\u5f0f\u8868\u793a\u56fe\u50cf\uff0c\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff08\u951a\u70b9\uff09\uff0c\u4f7f\u7528\u603b\u7ed3\u5411\u91cf\u6bd4\u8f83\u5f3a\u5ea6\u3001\u8fde\u901a\u6027\u3001\u590d\u6742\u6027\u548c\u7ed3\u6784\u5c5e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u53ef\u9760\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8de8\u5206\u8fa8\u7387\u7684\u7ed3\u6784\u5206\u6790\u3002", "conclusion": "PointSSIM\u4e3a\u56fe\u50cf\u6bd4\u8f83\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.23835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23835", "abs": "https://arxiv.org/abs/2506.23835", "authors": ["Ziwei Chen", "Ziling Liu", "Zitong Huang", "Mingqi Gao", "Feng Zheng"], "title": "Refine Any Object in Any Scene", "comment": "9 pages with 6 figures", "summary": "Viewpoint missing of objects is common in scene reconstruction, as camera\npaths typically prioritize capturing the overall scene structure rather than\nindividual objects. This makes it highly challenging to achieve high-fidelity\nobject-level modeling while maintaining accurate scene-level representation.\nAddressing this issue is critical for advancing downstream tasks requiring\ndetailed object understanding and appearance modeling. In this paper, we\nintroduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement\nframework that leverages 3D generative priors to recover fine-grained object\ngeometry and appearance under missing views. Starting from substituting\ndegraded objects with proxies, via a 3D generative model with strong 3D\nunderstanding, RAISE progressively refines geometry and texture by aligning\neach proxy to its degraded counterpart in 7-DOF pose, followed by correcting\nspatial and appearance inconsistencies via registration-constrained\nenhancement. This two-stage refinement ensures the high-fidelity geometry and\nappearance of the original object in unseen views while maintaining consistency\nin spatial positioning, observed geometry, and appearance. Extensive\nexperiments on challenging benchmarks show that RAISE significantly outperforms\nstate-of-the-art methods in both novel view synthesis and geometry completion\ntasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.", "AI": {"tldr": "RAISE\u662f\u4e00\u79cd\u65b0\u578b3D\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u75283D\u751f\u6210\u5148\u9a8c\u6062\u590d\u7f3a\u5931\u89c6\u89d2\u4e0b\u7684\u7269\u4f53\u51e0\u4f55\u548c\u5916\u89c2\u3002", "motivation": "\u89e3\u51b3\u573a\u666f\u91cd\u5efa\u4e2d\u7269\u4f53\u89c6\u89d2\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u5bf9\u7269\u4f53\u7684\u8be6\u7ec6\u7406\u89e3\u548c\u5916\u89c2\u5efa\u6a21\u3002", "method": "\u901a\u8fc73D\u751f\u6210\u6a21\u578b\u66ff\u6362\u9000\u5316\u7269\u4f53\uff0c\u9010\u6b65\u4f18\u5316\u51e0\u4f55\u548c\u7eb9\u7406\uff0c\u5206\u4e24\u9636\u6bb5\u5bf9\u9f50\u548c\u4fee\u6b63\u4e0d\u4e00\u81f4\u3002", "result": "\u5728\u65b0\u578b\u89c6\u56fe\u5408\u6210\u548c\u51e0\u4f55\u5b8c\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAISE\u80fd\u9ad8\u6548\u6062\u590d\u7f3a\u5931\u89c6\u89d2\u4e0b\u7684\u7269\u4f53\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.23852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23852", "abs": "https://arxiv.org/abs/2506.23852", "authors": ["Jianing Jin", "Jiangyong Ying", "Huiyu Duan", "Liu Yang", "Sijing Wu", "Yunhao Li", "Yushuo Zheng", "Xiongkuo Min", "Guangtao Zhai"], "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment", "comment": null, "summary": "As camera-equipped robotic platforms become increasingly integrated into\ndaily life, robotic-generated videos have begun to appear on streaming media\nplatforms, enabling us to envision a future where humans and robots coexist. We\ninnovatively propose the concept of Robotic-Generated Content (RGC) to term\nthese videos generated from egocentric perspective of robots. The perceptual\nquality of RGC videos is critical in human-robot interaction scenarios, and RGC\nvideos exhibit unique distortions and visual requirements that differ markedly\nfrom those of professionally-generated content (PGC) videos and user-generated\ncontent (UGC) videos. However, dedicated research on quality assessment of RGC\nvideos is still lacking. To address this gap and to support broader robotic\napplications, we establish the first Robotic-Generated Content Database (RGCD),\nwhich contains a total of 2,100 videos drawn from three robot categories and\nsourced from diverse platforms. A subjective VQA experiment is conducted\nsubsequently to assess human visual perception of robotic-generated videos.\nFinally, we conduct a benchmark experiment to evaluate the performance of 11\nstate-of-the-art VQA models on our database. Experimental results reveal\nsignificant limitations in existing VQA models when applied to complex,\nrobotic-generated content, highlighting a critical need for RGC-specific VQA\nmodels. Our RGCD is publicly available at:\nhttps://github.com/IntMeGroup/RGC-VQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u751f\u6210\u5185\u5bb9\uff08RGC\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2aRGC\u89c6\u9891\u6570\u636e\u5e93\uff08RGCD\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30RGC\u89c6\u9891\u7684\u611f\u77e5\u8d28\u91cf\u3002\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6a21\u578b\u5728RGC\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u547c\u5401\u5f00\u53d1RGC\u4e13\u7528VQA\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u89c6\u9891\u5728\u6d41\u5a92\u4f53\u5e73\u53f0\u7684\u51fa\u73b0\uff0cRGC\u89c6\u9891\u7684\u611f\u77e5\u8d28\u91cf\u5bf9\u4eba\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7684\u7814\u7a76\u3002", "method": "\u5efa\u7acb\u5305\u542b2,100\u4e2aRGC\u89c6\u9891\u7684\u6570\u636e\u5e93\uff08RGCD\uff09\uff0c\u8fdb\u884c\u4e3b\u89c2VQA\u5b9e\u9a8c\uff0c\u5e76\u8bc4\u4f3011\u79cd\u73b0\u6709VQA\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709VQA\u6a21\u578b\u5728\u590d\u6742\u7684RGC\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9RGC\u89c6\u9891\u7684VQA\u6a21\u578b\uff0cRGCD\u6570\u636e\u5e93\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.23854", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.23854", "abs": "https://arxiv.org/abs/2506.23854", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "AI": {"tldr": "HiNeuS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u8f90\u5c04\u4e0d\u4e00\u81f4\u3001\u65e0\u7eb9\u7406\u533a\u57df\u5173\u952e\u70b9\u7f3a\u5931\u548cEikonal\u7ea6\u675f\u8fc7\u5ea6\u5bfc\u81f4\u7684\u51e0\u4f55\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5fae\u5206\u53ef\u89c1\u6027\u9a8c\u8bc1\u3001\u5e73\u9762\u5171\u5f62\u6b63\u5219\u5316\u548c\u7269\u7406\u57fa\u7840\u7684Eikonal\u677e\u5f1b\u5b9e\u73b0\u4e86\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u96be\u4ee5\u5e73\u8861\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u5149\u5ea6\u4e00\u81f4\u6027\uff0cHiNeuS\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u591a\u89c6\u89d2\u8f90\u5c04\u4e0d\u4e00\u81f4\u3001\u65e0\u7eb9\u7406\u533a\u57df\u5173\u952e\u70b9\u7f3a\u5931\u548cEikonal\u7ea6\u675f\u8fc7\u5ea6\u7684\u95ee\u9898\u3002", "method": "HiNeuS\u5f15\u5165\uff1a1) SDF\u5f15\u5bfc\u7684\u5c04\u7ebf\u8ffd\u8e2a\u5fae\u5206\u53ef\u89c1\u6027\u9a8c\u8bc1\uff1b2) \u5c04\u7ebf\u5bf9\u9f50\u51e0\u4f55\u5757\u7684\u5e73\u9762\u5171\u5f62\u6b63\u5219\u5316\uff1b3) \u57fa\u4e8e\u5c40\u90e8\u8f90\u5c04\u68af\u5ea6\u7684\u7269\u7406\u57fa\u7840Eikonal\u677e\u5f1b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiNeuS\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cChamfer\u8ddd\u79bb\u51cf\u5c1121.4%\uff0cPSNR\u63d0\u53472.32 dB\uff0c\u5e76\u80fd\u6062\u590d\u955c\u9762\u53cd\u5c04\u3001\u5398\u7c73\u7ea7\u57ce\u5e02\u5e03\u5c40\u548c\u65e0\u7eb9\u7406\u8868\u9762\u3002", "conclusion": "HiNeuS\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e0e\u5916\u89c2\u7ea6\u675f\u7684\u7edf\u4e00\uff0c\u5728\u8868\u9762\u91cd\u5efa\u548c\u9006\u6e32\u67d3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23856", "abs": "https://arxiv.org/abs/2506.23856", "authors": ["Ji Zhang", "Shihan Wu", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models", "comment": "18 pages", "summary": "Despite the great promise of Prompt Tuning (PT) in adapting large\nVision-Language Pretrained Models (VLPMs) to downstream tasks, they often\nstruggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better\ntuned to a base task, their ability to generalize to new tasks diminishes.\nRecent work on conditional PT addresses this problem by replacing static\nprompts with dynamic Visual Image Information (VII)-conditioned prompts,\nimproving the model's generalization to new tasks to some extent. In this work,\nwe first identify a critical issue with existing conditional PT methods: using\nVII as the \"condition\" of prompts yields suboptimal performance, and even\nrandom noise-conditioned prompts can outperform the VII-conditioned\ncounterparts. On further analysis, we find that learning dynamic prompts\nconditioned on Textual Class Information (TCI) is the key to solving the BNT\nproblem. Motivated by this, we then propose Class-adaptive Prompt Tuning\n(CaPT), which enables fast adaptation of tuned models to new classes by\nlearning TCI-conditioned prompts from base classes. Remarkably, CaPT can be\nused as a plugin to mitigate the BNT problem for existing unconditional PT\nschemes. Extensive experiments on 11 datasets show that CaPT consistently\nimproves the performance of five strong unconditional PT baselines with\nnegligible additional computational cost. Additionally, by integrating CaPT\nwith our recently proposed DePT framework, we devise a new conditional PT\napproach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art\nconditional PT scheme by 3.49%, averaged over the 11 datasets. Code:\nhttps://github.com/Koorye/CaPT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faClass-adaptive Prompt Tuning (CaPT)\uff0c\u901a\u8fc7\u57fa\u4e8e\u6587\u672c\u7c7b\u522b\u4fe1\u606f\uff08TCI\uff09\u7684\u52a8\u6001\u63d0\u793a\u89e3\u51b3Vision-Language Pretrained Models\uff08VLPMs\uff09\u4e2d\u7684Base-New Tradeoff\uff08BNT\uff09\u95ee\u9898\u3002CaPT\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6761\u4ef6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u56fe\u50cf\u4fe1\u606f\uff08VII\uff09\u7684\u6761\u4ef6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u89e3\u51b3BNT\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u566a\u58f0\u6761\u4ef6\u63d0\u793a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u6587\u672c\u7c7b\u522b\u4fe1\u606f\uff08TCI\uff09\u7684\u52a8\u6001\u63d0\u793a\u662f\u89e3\u51b3BNT\u95ee\u9898\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faCaPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u57fa\u4e8eTCI\u7684\u52a8\u6001\u63d0\u793a\uff0c\u5feb\u901f\u9002\u5e94\u65b0\u7c7b\u522b\u3002CaPT\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u4e0e\u73b0\u6709\u65e0\u6761\u4ef6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u7ed3\u5408\uff0c\u63d0\u5347\u6027\u80fd\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaPT\u663e\u8457\u63d0\u5347\u4e86\u4e94\u79cd\u65e0\u6761\u4ef6\u63d0\u793a\u8c03\u4f18\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002\u7ed3\u5408DePT\u6846\u67b6\u63d0\u51fa\u7684DeCaPT\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6761\u4ef6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd53.49%\u3002", "conclusion": "CaPT\u901a\u8fc7TCI\u52a8\u6001\u63d0\u793a\u6709\u6548\u89e3\u51b3\u4e86BNT\u95ee\u9898\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3aVLPMs\u7684\u63d0\u793a\u8c03\u4f18\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23858", "abs": "https://arxiv.org/abs/2506.23858", "authors": ["Jianzong Wu", "Liang Hou", "Haotian Yang", "Xin Tao", "Ye Tian", "Pengfei Wan", "Di Zhang", "Yunhai Tong"], "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models", "comment": "Code is at https://github.com/KwaiVGI/VMoBA", "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.", "AI": {"tldr": "VMoBA\u662f\u4e00\u79cd\u65b0\u578b\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e13\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5168\u6ce8\u610f\u529b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u957f\u65f6\u957f\u3001\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7684\u80fd\u529b\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u9002\u5e94\u89c6\u9891\u6570\u636e\u7684\u65f6\u7a7a\u7279\u6027\u3002", "method": "VMoBA\u901a\u8fc7\u5c42\u9012\u8fdb\u5757\u5206\u533a\u3001\u5168\u5c40\u5757\u9009\u62e9\u548c\u57fa\u4e8e\u9608\u503c\u7684\u5757\u9009\u62e9\u4e09\u9879\u6539\u8fdb\uff0c\u4f18\u5316\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u7684\u65f6\u7a7a\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVMoBA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u5206\u522b\u5b9e\u73b0\u4e862.92x\u548c2.40x\u7684FLOPs\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u4e0e\u5168\u6ce8\u610f\u529b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "VMoBA\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23863", "abs": "https://arxiv.org/abs/2506.23863", "authors": ["Jiahao Ma", "Lei Wang", "Miaomiao liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction", "comment": "Feed-forward 3D reconstruction, Data Augmentation", "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision.\nRecent methods, such as DUST3R and its successors, directly regress pointmaps\nfrom image pairs without relying on known scene geometry or camera parameters.\nHowever, the performance of these models is constrained by the diversity and\nscale of available training data. In this work, we introduce Puzzles, a data\naugmentation strategy that synthesizes an unbounded volume of high-quality\nposed video-depth data from a single image or video clip. By simulating diverse\ncamera trajectories and realistic scene geometry through targeted image\ntransformations, Puzzles significantly enhances data variety. Extensive\nexperiments show that integrating Puzzles into existing video-based 3D\nreconstruction pipelines consistently boosts performance without modifying the\nunderlying network architecture. Notably, models trained on only ten percent of\nthe original data augmented with Puzzles still achieve accuracy comparable to\nthose trained on the full dataset. Code is available at\nhttps://jiahao-ma.github.io/puzzles/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPuzzles\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u7247\u6bb5\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u59ff\u6001\u89c6\u9891-\u6df1\u5ea6\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u89c4\u6a21\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u591a\u6837\u5316\u7684\u76f8\u673a\u8f68\u8ff9\u548c\u771f\u5b9e\u573a\u666f\u51e0\u4f55\uff0cPuzzles\u7b56\u7565\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPuzzles\u663e\u8457\u63d0\u5347\u4e86\u73b0\u67093D\u91cd\u5efa\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u4ec5\u752810%\u7684\u539f\u59cb\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "Puzzles\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u7f51\u7edc\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u63d0\u53473D\u91cd\u5efa\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23897", "abs": "https://arxiv.org/abs/2506.23897", "authors": ["Longliang Liu", "Miaojie Feng", "Junda Cheng", "Jijun Xiang", "Xuan Zhu", "Xin Yang"], "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View", "comment": "11 pages", "summary": "Panoramic optical flow enables a comprehensive understanding of temporal\ndynamics across wide fields of view. However, severe distortions caused by\nsphere-to-plane projections, such as the equirectangular projection (ERP),\nsignificantly degrade the performance of conventional perspective-based optical\nflow methods, especially in polar regions. To address this challenge, we\npropose PriOr-Flow, a novel dual-branch framework that leverages the\nlow-distortion nature of the orthogonal view to enhance optical flow estimation\nin these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup\n(DCCL) operator, which jointly retrieves correlation information from both the\nprimitive and orthogonal cost volumes, effectively mitigating distortion noise\nduring cost volume construction. Furthermore, our Ortho-Driven Distortion\nCompensation (ODDC) module iteratively refines motion features from both\nbranches, further suppressing polar distortions. Extensive experiments\ndemonstrate that PriOr-Flow is compatible with various perspective-based\niterative optical flow methods and consistently achieves state-of-the-art\nperformance on publicly available panoramic optical flow datasets, setting a\nnew benchmark for wide-field motion estimation. The code is publicly available\nat: https://github.com/longliangLiu/PriOr-Flow.", "AI": {"tldr": "PriOr-Flow\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u89c6\u56fe\u7684\u4f4e\u5931\u771f\u7279\u6027\u63d0\u5347\u5168\u666f\u5149\u6d41\u4f30\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6781\u5730\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u900f\u89c6\u7684\u5149\u6d41\u65b9\u6cd5\u5728\u5168\u666f\u6295\u5f71\uff08\u5982ERP\uff09\u4e2d\u56e0\u4e25\u91cd\u5931\u771f\uff08\u5c24\u5176\u5728\u6781\u5730\u533a\u57df\uff09\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDCCL\u64cd\u4f5c\u7b26\u8054\u5408\u68c0\u7d22\u539f\u59cb\u548c\u6b63\u4ea4\u6210\u672c\u4f53\u79ef\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1ODDC\u6a21\u5757\u8fed\u4ee3\u4f18\u5316\u8fd0\u52a8\u7279\u5f81\u4ee5\u51cf\u5c11\u5931\u771f\u3002", "result": "PriOr-Flow\u5728\u516c\u5f00\u5168\u666f\u5149\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5bb9\u591a\u79cd\u8fed\u4ee3\u5149\u6d41\u65b9\u6cd5\u3002", "conclusion": "PriOr-Flow\u4e3a\u5bbd\u89c6\u573a\u8fd0\u52a8\u4f30\u8ba1\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23903", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23903", "abs": "https://arxiv.org/abs/2506.23903", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "comment": "11 pages, 3 figures, 6 figures", "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u7ed3\u5408Grounding DINO\u548cSAM2\uff0c\u7528\u4e8e\u591a\u5668\u5b98\u8d85\u58f0\u56fe\u50cf\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u56e0\u89e3\u5256\u53d8\u5f02\u6027\u3001\u6210\u50cf\u534f\u8bae\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u752818\u4e2a\u516c\u5171\u8d85\u58f0\u6570\u636e\u96c6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Grounding DINO\uff0c\u7ed3\u5408SAM2\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728\u591a\u6570\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "VLM\u5728\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.23916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23916", "abs": "https://arxiv.org/abs/2506.23916", "authors": ["Radhika Juglan", "Marta Ligero", "Zunamys I. Carrero", "Asier Rabasco", "Tim Lenz", "Leo Misera", "Gregory Patrick Veldhuizen", "Paul Kuntke", "Hagen H. Kitzler", "Sven Nebelung", "Daniel Truhn", "Jakob Nikolas Kather"], "title": "Three-dimensional end-to-end deep learning for brain MRI analysis", "comment": null, "summary": "Deep learning (DL) methods are increasingly outperforming classical\napproaches in brain imaging, yet their generalizability across diverse imaging\ncohorts remains inadequately assessed. As age and sex are key neurobiological\nmarkers in clinical neuroscience, influencing brain structure and disease risk,\nthis study evaluates three of the existing three-dimensional architectures,\nnamely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window\n(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four\nindependent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study\n(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy\ncontrols), and Information eXtraction from Images (IXI, n=319). We found that\nSFCN consistently outperformed more complex architectures with AUC of 1.00\n[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for\nsex classification. For the age prediction task, SFCN demonstrated a mean\nabsolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across\nexternal datasets. Pairwise DeLong and Wilcoxon signed-rank tests with\nBonferroni corrections confirmed SFCN's superiority over Swin Transformer\nacross most cohorts (p<0.017, for three comparisons). Explainability analysis\nfurther demonstrates the regional consistency of model attention across cohorts\nand specific to each task. Our findings reveal that simpler convolutional\nnetworks outperform the denser and more complex attention-based DL\narchitectures in brain image analysis by demonstrating better generalizability\nacross different datasets.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u5377\u79ef\u7f51\u7edc\uff08SFCN\uff09\u5728\u8111\u5f71\u50cf\u5206\u6790\u4e2d\u4f18\u4e8e\u590d\u6742\u7684\u6ce8\u610f\u529b\u67b6\u6784\uff08\u5982Swin Transformer\uff09\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u8111\u5f71\u50cf\u961f\u5217\u4e2d\u5bf9\u5e74\u9f84\u548c\u6027\u522b\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u8003\u8651\u5230\u5e74\u9f84\u548c\u6027\u522b\u662f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u4e09\u7ef4\u67b6\u6784\uff08SFCN\u3001DenseNet\u3001Swin Transformer\uff09\u5728\u56db\u4e2a\u72ec\u7acb\u961f\u5217\uff08UKB\u3001DLBS\u3001PPMI\u3001IXI\uff09\u4e2d\u8fdb\u884c\u5e74\u9f84\u548c\u6027\u522b\u9884\u6d4b\u3002", "result": "SFCN\u5728\u6027\u522b\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff08AUC 1.00-0.85\uff09\uff0c\u5e74\u9f84\u9884\u6d4b\u7684MAE\u4e3a2.66-5.81\uff0c\u4e14\u663e\u8457\u4f18\u4e8eSwin Transformer\u3002", "conclusion": "\u7b80\u5355\u7684\u5377\u79ef\u7f51\u7edc\u5728\u8111\u5f71\u50cf\u5206\u6790\u4e2d\u6bd4\u590d\u6742\u67b6\u6784\u66f4\u5177\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u3002"}}
{"id": "2506.23918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23918", "abs": "https://arxiv.org/abs/2506.23918", "authors": ["Zhaochen Su", "Peng Xia", "Hangyu Guo", "Zhenhua Liu", "Yan Ma", "Xiaoye Qu", "Jiaqi Liu", "Yanshu Li", "Kaide Zeng", "Zhengyuan Yang", "Linjie Li", "Yu Cheng", "Heng Ji", "Junxian He", "Yi R.", "Fung"], "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers", "comment": "We maintain a real-time GitHub repository tracking progress at:\n  https://github.com/zhaochen0110/Awesome_Think_With_Images", "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u4ece\u6587\u672c\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u5230\u89c6\u89c9\u52a8\u6001\u601d\u8003\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u51fa\u4e86\u201c\u7528\u56fe\u50cf\u601d\u8003\u201d\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u8bc4\u4f30\u548c\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u94fe\u5f0f\u601d\u8003\u5728\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u65f6\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u63a8\u52a8AI\u4ece\u9759\u6001\u89c6\u89c9\u8f93\u5165\u5230\u52a8\u6001\u89c6\u89c9\u8ba4\u77e5\u7684\u8f6c\u53d8\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u5916\u90e8\u5de5\u5177\u63a2\u7d22\u3001\u7a0b\u5e8f\u5316\u64cd\u4f5c\u548c\u5185\u5728\u60f3\u8c61\uff0c\u5e76\u7efc\u8ff0\u4e86\u5404\u9636\u6bb5\u7684\u6838\u5fc3\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u201c\u7528\u56fe\u50cf\u601d\u8003\u201d\u8303\u5f0f\u7684\u57fa\u7840\u539f\u5219\uff0c\u5206\u6790\u4e86\u8bc4\u4f30\u6807\u51c6\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u4e3a\u672a\u6765\u591a\u6a21\u6001AI\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u66f4\u5f3a\u5927\u4e14\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5bf9\u9f50\u7684\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2506.23963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23963", "abs": "https://arxiv.org/abs/2506.23963", "authors": ["Vannkinh Nom", "Souhail Bakkali", "Muhammad Muzzamil Luqman", "Mickael Coustaty", "Jean-Marc Ogier"], "title": "Evaluating the Impact of Khmer Font Types on Text Recognition", "comment": null, "summary": "Text recognition is significantly influenced by font types, especially for\ncomplex scripts like Khmer. The variety of Khmer fonts, each with its unique\ncharacter structure, presents challenges for optical character recognition\n(OCR) systems. In this study, we evaluate the impact of 19 randomly selected\nKhmer font types on text recognition accuracy using Pytesseract. The fonts\ninclude Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong\nChhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,\nMetal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth\nFirst. Our comparison of OCR performance across these fonts reveals that Khmer,\nOdor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,\nwhile iSeth First, Bayon, and Dangrek perform poorly. This study underscores\nthe critical importance of font selection in optimizing Khmer text recognition\nand provides valuable insights for developing more robust OCR systems.", "AI": {"tldr": "\u7814\u7a76\u4e8619\u79cd\u9ad8\u68c9\u5b57\u4f53\u5bf9OCR\u51c6\u786e\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u90e8\u5206\u5b57\u4f53\u8868\u73b0\u4f18\u5f02\uff0c\u90e8\u5206\u8f83\u5dee\uff0c\u5f3a\u8c03\u4e86\u5b57\u4f53\u9009\u62e9\u5bf9\u9ad8\u68c9\u6587\u672c\u8bc6\u522b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u9ad8\u68c9\u5b57\u4f53\u591a\u6837\u6027\u5bf9OCR\u7cfb\u7edf\u8bc6\u522b\u590d\u6742\u811a\u672c\uff08\u5982\u9ad8\u68c9\u8bed\uff09\u7684\u51c6\u786e\u6027\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8bc4\u4f30\u4e0d\u540c\u5b57\u4f53\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Pytesseract\u8bc4\u4f3019\u79cd\u968f\u673a\u9009\u62e9\u7684\u9ad8\u68c9\u5b57\u4f53\uff08\u5982Angkor\u3001Battambang\u7b49\uff09\u7684OCR\u6027\u80fd\u3002", "result": "Khmer\u3001Odor MeanChey\u7b49\u5b57\u4f53\u8868\u73b0\u4f18\u5f02\uff0c\u800ciSeth First\u3001Bayon\u7b49\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5b57\u4f53\u9009\u62e9\u5bf9\u9ad8\u68c9\u6587\u672c\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684OCR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2506.23972", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23972", "abs": "https://arxiv.org/abs/2506.23972", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking", "comment": null, "summary": "Prompt-learning-based multi-modal trackers have achieved promising progress\nby employing lightweight visual adapters to incorporate auxiliary modality\nfeatures into frozen foundation models. However, existing approaches often\nstruggle to learn reliable prompts due to limited exploitation of critical cues\nacross frequency and temporal domains. In this paper, we propose a novel visual\nand memory dual adapter (VMDA) to construct more robust and discriminative\nrepresentations for multi-modal tracking. Specifically, we develop a simple but\neffective visual adapter that adaptively transfers discriminative cues from\nauxiliary modality to dominant modality by jointly modeling the frequency,\nspatial, and channel-wise features. Additionally, we design the memory adapter\ninspired by the human memory mechanism, which stores global temporal cues and\nperforms dynamic update and retrieval operations to ensure the consistent\npropagation of reliable temporal information across video sequences. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,\nand RGB-Event tracking. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u4e0e\u8bb0\u5fc6\u53cc\u9002\u914d\u5668\uff08VMDA\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u8ddf\u8e2a\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u9891\u7387\u3001\u7a7a\u95f4\u548c\u901a\u9053\u7279\u5f81\uff0c\u4ee5\u53ca\u5229\u7528\u8bb0\u5fc6\u673a\u5236\u5b58\u50a8\u5168\u5c40\u65f6\u5e8f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u591a\u6a21\u6001\u8ddf\u8e2a\u65b9\u6cd5\u5728\u9891\u7387\u548c\u65f6\u5e8f\u9886\u57df\u7684\u5173\u952e\u7ebf\u7d22\u5229\u7528\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63d0\u793a\u5b66\u4e60\u4e0d\u53ef\u9760\u3002", "method": "\u8bbe\u8ba1\u4e86\u89c6\u89c9\u9002\u914d\u5668\u548c\u8bb0\u5fc6\u9002\u914d\u5668\uff0c\u524d\u8005\u81ea\u9002\u5e94\u5730\u5c06\u8f85\u52a9\u6a21\u6001\u7684\u5224\u522b\u6027\u7ebf\u7d22\u4f20\u9012\u5230\u4e3b\u5bfc\u6a21\u6001\uff0c\u540e\u8005\u5b58\u50a8\u5168\u5c40\u65f6\u5e8f\u4fe1\u606f\u5e76\u52a8\u6001\u66f4\u65b0\u4e0e\u68c0\u7d22\u3002", "result": "\u5728RGB-Thermal\u3001RGB-Depth\u548cRGB-Event\u7b49\u591a\u79cd\u591a\u6a21\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VMDA\u901a\u8fc7\u53cc\u9002\u914d\u5668\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u5224\u522b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.23975", "categories": ["cs.CV", "68T07", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2506.23975", "abs": "https://arxiv.org/abs/2506.23975", "authors": ["Yuliia Kaidashova", "Bettina Finzel", "Ute Schmid"], "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance", "comment": "17 pages, 6 figures, KI2025 - 48th German Conference on Artificial\n  Intelligence", "summary": "Understanding why a classification model prefers one class over another for\nan input instance is the challenge of contrastive explanation. This work\nimplements concept-based contrastive explanations for image classification by\nleveraging the similarity of instance embeddings and relevance of\nhuman-understandable concepts used by a fine-tuned deep learning model. Our\napproach extracts concepts with their relevance score, computes contrasts for\nsimilar instances, and evaluates the resulting contrastive explanations based\non explanation complexity. Robustness is tested for different image\naugmentations. Two research questions are addressed: (1) whether explanation\ncomplexity varies across different relevance ranges, and (2) whether\nexplanation complexity remains consistent under image augmentations such as\nrotation and noise. The results confirm that for our experiments higher concept\nrelevance leads to shorter, less complex explanations, while lower relevance\nresults in longer, more diffuse explanations. Additionally, explanations show\nvarying degrees of robustness. The discussion of these findings offers insights\ninto the potential of building more interpretable and robust AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u7684\u5bf9\u6bd4\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u5b9e\u4f8b\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u548c\u6982\u5ff5\u76f8\u5173\u6027\uff0c\u751f\u6210\u89e3\u91ca\u5e76\u8bc4\u4f30\u5176\u590d\u6742\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5206\u7c7b\u6a21\u578b\u4e3a\u4f55\u504f\u597d\u67d0\u4e00\u7c7b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89e3\u91ca\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u53d6\u6982\u5ff5\u53ca\u5176\u76f8\u5173\u6027\u5206\u6570\uff0c\u8ba1\u7b97\u76f8\u4f3c\u5b9e\u4f8b\u7684\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u89e3\u91ca\u590d\u6742\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u9ad8\u76f8\u5173\u6027\u6982\u5ff5\u751f\u6210\u66f4\u7b80\u6d01\u7684\u89e3\u91ca\uff0c\u4f4e\u76f8\u5173\u6027\u5219\u66f4\u590d\u6742\uff1b\u89e3\u91ca\u5728\u4e0d\u540c\u56fe\u50cf\u589e\u5f3a\u4e0b\u8868\u73b0\u4e0d\u4e00\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2506.24019", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.24019", "abs": "https://arxiv.org/abs/2506.24019", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "title": "Ella: Embodied Social Agents with Lifelong Memory", "comment": null, "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.", "AI": {"tldr": "Ella\u662f\u4e00\u4e2a\u5177\u5907\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u7684\u793e\u4ea4\u4ee3\u7406\uff0c\u901a\u8fc7\u89c6\u89c9\u89c2\u5bdf\u548c\u793e\u4ea4\u4e92\u52a8\u57283D\u5f00\u653e\u4e16\u754c\u4e2d\u79ef\u7d2f\u77e5\u8bc6\u548c\u7ecf\u9a8c\u3002\u5176\u6838\u5fc3\u662f\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u81ea\u4e3b\u51b3\u7b56\u548c\u793e\u4ea4\u884c\u4e3a\u3002\u5b9e\u9a8c\u8868\u660eElla\u80fd\u6709\u6548\u5f71\u54cd\u3001\u9886\u5bfc\u548c\u5408\u4f5c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7cfb\u7edf\u548c\u57fa\u7840\u6a21\u578b\u63d0\u5347\u5177\u8eab\u667a\u80fd\u4f53\u7684\u7ec8\u8eab\u5b66\u4e60\u4e0e\u793e\u4ea4\u80fd\u529b\u3002", "method": "Ella\u91c7\u7528\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\uff08\u8bed\u4e49\u8bb0\u5fc6\u548c\u60c5\u666f\u8bb0\u5fc6\uff09\u7ed3\u5408\u57fa\u7840\u6a21\u578b\uff0c\u5728\u52a8\u60013D\u5f00\u653e\u4e16\u754c\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "Ella\u80fd\u6709\u6548\u5f71\u54cd\u3001\u9886\u5bfc\u548c\u5408\u4f5c\u5176\u4ed6\u4ee3\u7406\uff0c\u5c55\u793a\u51fa\u901a\u8fc7\u89c2\u5bdf\u548c\u793e\u4ea4\u4e92\u52a8\u5b66\u4e60\u7684\u80fd\u529b\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bb0\u5fc6\u7cfb\u7edf\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u7ed3\u5408\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u6f5c\u529b\u3002"}}
{"id": "2506.24039", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.24039", "abs": "https://arxiv.org/abs/2506.24039", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging.", "AI": {"tldr": "Zenesis\u662f\u4e00\u4e2a\u65e0\u9700\u4ee3\u7801\u7684\u4ea4\u4e92\u5f0f\u5e73\u53f0\uff0c\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9002\u5e94\u6280\u672f\u548c\u4eba\u673a\u534f\u4f5c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u548c\u63d0\u793a\u6280\u672f\u96be\u4ee5\u5904\u7406\u7a00\u7f3a\u7684\u79d1\u5b66\u56fe\u50cf\u6570\u636e\uff0cZenesis\u65e8\u5728\u964d\u4f4e\u6570\u636e\u51c6\u5907\u95e8\u69db\uff0c\u63d0\u5347\u79d1\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6548\u7387\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u9002\u5e94\u6280\u672f\uff0c\u652f\u6301\u96f6\u6837\u672c\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u548c\u542f\u53d1\u5f0f\u65f6\u95f4\u589e\u5f3a\u3002", "result": "\u5728FIB-SEM\u6570\u636e\u4e0a\uff0cZenesis\u5e73\u5747\u51c6\u786e\u7387\u8fbe0.947\uff08\u975e\u6676\u6837\u672c\uff09\u548c0.987\uff08\u6676\u4f53\u6837\u672c\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Zenesis\u662f\u79d1\u5b66\u56fe\u50cf\u5206\u6790\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9886\u57df\u3002"}}
{"id": "2506.24063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24063", "abs": "https://arxiv.org/abs/2506.24063", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "comment": null, "summary": "In practice, environments constantly change over time and space, posing\nsignificant challenges for object detectors trained based on a closed-set\nassumption, i.e., training and test data share the same distribution. To this\nend, continual test-time adaptation has attracted much attention, aiming to\nimprove detectors' generalization by fine-tuning a few specific parameters,\ne.g., BatchNorm layers. However, based on a small number of test images,\nfine-tuning certain parameters may affect the representation ability of other\nfixed parameters, leading to performance degradation. Instead, we explore a new\nmechanism, i.e., converting the fine-tuning process to a specific-parameter\ngeneration. Particularly, we first design a dual-path LoRA-based domain-aware\nadapter that disentangles features into domain-invariant and domain-specific\ncomponents, enabling efficient adaptation. Additionally, a conditional\ndiffusion-based parameter generation mechanism is presented to synthesize the\nadapter's parameters based on the current environment, preventing the\noptimization from getting stuck in local optima. Finally, we propose a\nclass-centered optimal transport alignment method to mitigate catastrophic\nforgetting. Extensive experiments conducted on various continuous domain\nadaptive object detection tasks demonstrate the effectiveness. Meanwhile,\nvisualization results show that the representation extracted by the generated\nparameters can capture more object-related information and strengthen the\ngeneralization ability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u7279\u5b9a\u53c2\u6570\u800c\u975e\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73af\u5883\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u53d8\u5316\u5bf9\u57fa\u4e8e\u95ed\u96c6\u5047\u8bbe\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u8def\u5f84LoRA\u57df\u611f\u77e5\u9002\u914d\u5668\u5206\u79bb\u7279\u5f81\uff0c\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u53c2\u6570\u751f\u6210\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u7c7b\u4e2d\u5fc3\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u53c2\u6570\u80fd\u66f4\u597d\u5730\u6355\u6349\u76ee\u6807\u76f8\u5173\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u4e86\u5c40\u90e8\u6700\u4f18\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2506.24085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24085", "abs": "https://arxiv.org/abs/2506.24085", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "comment": "Project website is available at https://imagineforme.github.io/", "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "AI": {"tldr": "IT-Blender\u662f\u4e00\u4e2a\u57fa\u4e8eT2I\u6269\u6563\u9002\u914d\u5668\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u6df7\u5408\u89c6\u89c9\u548c\u6587\u672c\u6982\u5ff5\uff0c\u4ee5\u589e\u5f3a\u4eba\u7c7b\u521b\u9020\u529b\u3002", "motivation": "\u4eba\u7c7b\u5728\u8de8\u6a21\u6001\u6982\u5ff5\u6df7\u5408\u4e2d\u5bb9\u6613\u53d7\u5230\u8ba4\u77e5\u504f\u89c1\uff08\u5982\u8bbe\u8ba1\u56fa\u5b9a\uff09\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u7a7a\u95f4\u7684\u5c40\u90e8\u6700\u4f18\u3002IT-Blender\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff08SD\u548cFLUX\uff09\u6df7\u5408\u5e72\u51c0\u53c2\u8003\u56fe\u50cf\u548c\u566a\u58f0\u751f\u6210\u56fe\u50cf\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "IT-Blender\u5728\u6df7\u5408\u89c6\u89c9\u548c\u6587\u672c\u6982\u5ff5\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IT-Blender\u5c55\u793a\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u589e\u5f3a\u4eba\u7c7b\u521b\u9020\u529b\u65b9\u9762\u7684\u65b0\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.24086", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.24086", "abs": "https://arxiv.org/abs/2506.24086", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "title": "MotionGPT3: Human Motion as a Second Modality", "comment": "21 pages, 8 figures", "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.", "AI": {"tldr": "MotionGPT3\u662f\u4e00\u4e2a\u53cc\u6a21\u6001\u8fd0\u52a8-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u79bb\u8fd0\u52a8\u5efa\u6a21\u53c2\u6570\u89e3\u51b3\u8fd0\u52a8\u4e0e\u8bed\u8a00\u7edf\u4e00\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u4fdd\u7559\u8bed\u8a00\u667a\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6a21\u578b\u5728\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd0\u52a8-\u8bed\u8a00\u7edf\u4e00\u6a21\u578b\u7684\u53d1\u5c55\u4ecd\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u8fd0\u52a8\u6a21\u6001\u4e0e\u79bb\u6563\u8868\u793a\u7684\u5dee\u8ddd\u4ee5\u53ca\u8bed\u8a00\u667a\u80fd\u9000\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\uff0cMotionGPT3\u5c06\u8fd0\u52a8\u4f5c\u4e3a\u7b2c\u4e8c\u6a21\u6001\uff0c\u901a\u8fc7\u5206\u79bb\u53c2\u6570\u5efa\u6a21\u8fd0\u52a8\uff0c\u5e76\u5229\u7528\u5171\u4eab\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u53cc\u5411\u4fe1\u606f\u6d41\u3002\u8fd0\u52a8\u5206\u652f\u4f7f\u7528\u6269\u6563\u5934\u76f4\u63a5\u4ece\u9690\u85cf\u72b6\u6001\u9884\u6d4b\u8fd0\u52a8\u6f5c\u5728\u8868\u793a\uff0c\u7ed5\u8fc7\u79bb\u6563\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMotionGPT3\u5728\u8fd0\u52a8\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "MotionGPT3\u4e3a\u81ea\u56de\u5f52\u6846\u67b6\u4e0b\u7684\u53cc\u6a21\u6001\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u4e0e\u8bed\u8a00\u7edf\u4e00\u5efa\u6a21\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2506.24092", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.24092", "abs": "https://arxiv.org/abs/2506.24092", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "title": "WaRA: Wavelet Low Rank Adaptation", "comment": "Submitted to BMVC 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across\nvarious applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its\nextensions have emerged as particularly effective, allowing efficient model\nadaptation while significantly reducing computational overhead. However,\nexisting approaches typically rely on global low-rank factorizations, which\noverlook local or multi-scale structure, failing to capture complex patterns in\nthe weight updates. To address this, we propose WaRA, a novel PEFT method that\nleverages wavelet transforms to decompose the weight update matrix into a\nmulti-resolution representation. By performing low-rank factorization in the\nwavelet domain and reconstructing updates through an inverse transform, WaRA\nobtains compressed adaptation parameters that harness multi-resolution\nanalysis, enabling it to capture both coarse and fine-grained features while\nproviding greater flexibility and sparser representations than standard LoRA.\nThrough comprehensive experiments and analysis, we demonstrate that WaRA\nperforms superior on diverse vision tasks, including image generation,\nclassification, and semantic segmentation, significantly enhancing generated\nimage quality while reducing computational complexity. Although WaRA was\nprimarily designed for vision tasks, we further showcase its effectiveness in\nlanguage tasks, highlighting its broader applicability and generalizability.\nThe code is publicly available at\n\\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "AI": {"tldr": "WaRA\u662f\u4e00\u79cd\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5206\u89e3\u6743\u91cd\u66f4\u65b0\u77e9\u9635\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5206\u6790\u6355\u6349\u590d\u6742\u6a21\u5f0f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u4f9d\u8d56\u5168\u5c40\u4f4e\u79e9\u5206\u89e3\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u6216\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u6743\u91cd\u66f4\u65b0\u4e2d\u7684\u590d\u6742\u6a21\u5f0f\u3002", "method": "\u63d0\u51faWaRA\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u6743\u91cd\u66f4\u65b0\u77e9\u9635\u5206\u89e3\u4e3a\u591a\u5206\u8fa8\u7387\u8868\u793a\uff0c\u5728\u9891\u57df\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\u5e76\u901a\u8fc7\u9006\u53d8\u6362\u91cd\u6784\u66f4\u65b0\u3002", "result": "WaRA\u5728\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u751f\u6210\u3001\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u4e5f\u6709\u6548\u3002", "conclusion": "WaRA\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5206\u6790\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u7a00\u758f\u7684\u8868\u793a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.24096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24096", "abs": "https://arxiv.org/abs/2506.24096", "authors": ["Antoine Gu\u00e9don", "Diego Gomez", "Nissim Maruani", "Bingchen Gong", "George Drettakis", "Maks Ovsjanikov"], "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction", "comment": "10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE", "summary": "While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.", "AI": {"tldr": "MILo\u662f\u4e00\u79cd\u65b0\u578b\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5730\u4ece3D\u9ad8\u65af\u4e2d\u63d0\u53d6\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u4ece\u4f53\u79ef\u8868\u793a\u5230\u8868\u9762\u8868\u793a\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u9876\u70b9\u6570\u91cf\u5e76\u4fdd\u7559\u4e86\u51e0\u4f55\u7ec6\u8282\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u8fc7\u6602\u8d35\u7684\u540e\u5904\u7406\u6b65\u9aa4\u63d0\u53d6\u8868\u9762\u7f51\u683c\uff0c\u5bfc\u81f4\u51e0\u4f55\u7ec6\u8282\u4e22\u5931\u6216\u751f\u6210\u8fc7\u4e8e\u5bc6\u96c6\u7684\u7f51\u683c\uff0c\u9650\u5236\u4e86\u6700\u7ec8\u7f51\u683c\u4fdd\u7559\u8bad\u7ec3\u4e2d\u6355\u83b7\u7684\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\u3002", "method": "MILo\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u8fc7\u7a0b\uff0c\u76f4\u63a5\u4ece\u9ad8\u65af\u53c2\u6570\u4e2d\u6784\u5efa\u7f51\u683c\uff08\u5305\u62ec\u9876\u70b9\u4f4d\u7f6e\u548c\u8fde\u63a5\u6027\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u5411\u4e00\u81f4\u6027\u6846\u67b6\u3001\u81ea\u9002\u5e94\u7f51\u683c\u63d0\u53d6\u8fc7\u7a0b\u548c\u57fa\u4e8e3D\u9ad8\u65af\u7684\u7b26\u53f7\u8ddd\u79bb\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5148\u8fdb\u7684\u8d28\u91cf\u91cd\u5efa\u5b8c\u6574\u573a\u666f\uff0c\u5305\u62ec\u80cc\u666f\uff0c\u540c\u65f6\u6240\u9700\u7684\u7f51\u683c\u9876\u70b9\u6570\u91cf\u6bd4\u4e4b\u524d\u65b9\u6cd5\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "MILo\u751f\u6210\u7684\u7f51\u683c\u8f7b\u91cf\u4e14\u5185\u90e8\u4e3a\u7a7a\uff0c\u9002\u7528\u4e8e\u7269\u7406\u6a21\u62df\u6216\u52a8\u753b\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2506.24102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24102", "abs": "https://arxiv.org/abs/2506.24102", "authors": ["Xiangtai Li", "Tao Zhang", "Yanwei Li", "Haobo Yuan", "Shihao Chen", "Yikang Zhou", "Jiahao Meng", "Yueyi Sun", "Shilin Xu", "Lu Qi", "Tianheng Cheng", "Yi Lin", "Zilong Huang", "Wenhao Huang", "Jiashi Feng", "Guang Shi"], "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World", "comment": "Datasets and Models: https://github.com/lxtGH/DenseWorld-1M", "summary": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding\nof scenes, benefiting from large-scale and high-quality datasets. Most existing\ncaption datasets lack the ground locations and relations for visual entities.\nSeveral grounded caption datasets face the problems of missing detailed\ndescriptions, relations, and massive object descriptions on high-resolution\nimages. To fill this gap for the community, we present DenseWorld-1M, the first\nmassive, detailed, dense grounded caption dataset in the real world. We design\na three-stage labeling pipeline, containing open-world perception, detailed\nobject caption generation, and dense caption merging. The first stage obtains\nentity-level masks and labels. The second stage generates the object-level,\ndetailed captions with the guidance of masks and labels from the first stage.\nThe final stage merges object captions and masks into spatial and relational\ndense captions. To accelerate the labeling process and improve caption quality,\nwe present two VLM models: the Detailed Region Caption model and the Spatial\nCaption Merging model. Extensive experiments on various settings, including\nvision-language understanding, visual grounding, and region caption generation,\ndemonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DenseWorld-1M\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u8be6\u7ec6\u63cf\u8ff0\u548c\u5b9e\u4f53\u5173\u7cfb\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\u548c\u4e24\u4e2aVLM\u6a21\u578b\u63d0\u5347\u4e86\u6807\u6ce8\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u6807\u9898\u6570\u636e\u96c6\u7f3a\u4e4f\u89c6\u89c9\u5b9e\u4f53\u7684\u4f4d\u7f6e\u548c\u5173\u7cfb\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684\u63a5\u5730\u6807\u9898\u6570\u636e\u96c6\u53c8\u5b58\u5728\u63cf\u8ff0\u4e0d\u8be6\u7ec6\u3001\u5173\u7cfb\u7f3a\u5931\u7b49\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\uff08\u5f00\u653e\u4e16\u754c\u611f\u77e5\u3001\u8be6\u7ec6\u5bf9\u8c61\u6807\u9898\u751f\u6210\u3001\u5bc6\u96c6\u6807\u9898\u5408\u5e76\uff09\u548c\u4e24\u4e2aVLM\u6a21\u578b\uff08Detailed Region Caption\u6a21\u578b\u548cSpatial Caption Merging\u6a21\u578b\uff09\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u3001\u89c6\u89c9\u63a5\u5730\u548c\u533a\u57df\u6807\u9898\u751f\u6210\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86DenseWorld-1M\u6570\u636e\u96c6\u548c\u6807\u6ce8\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "DenseWorld-1M\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u8be6\u7ec6\u4e14\u63a5\u5730\u7684\u6807\u9898\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u6807\u6ce8\u6d41\u7a0b\u548c\u6a21\u578b\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2506.24113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24113", "abs": "https://arxiv.org/abs/2506.24113", "authors": ["Kaiwen Zhang", "Zhenyu Tang", "Xiaotao Hu", "Xingang Pan", "Xiaoyang Guo", "Yuan Liu", "Jingwei Huang", "Li Yuan", "Qian Zhang", "Xiao-Xiao Long", "Xun Cao", "Wei Yin"], "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "comment": "ICCV2025, Project Page: https://kevin-thu.github.io/Epona/", "summary": "Diffusion models have demonstrated exceptional visual quality in video\ngeneration, making them promising for autonomous driving world modeling.\nHowever, existing video diffusion-based world models struggle with\nflexible-length, long-horizon predictions and integrating trajectory planning.\nThis is because conventional video diffusion models rely on global joint\ndistribution modeling of fixed-length frame sequences rather than sequentially\nconstructing localized distributions at each timestep. In this work, we propose\nEpona, an autoregressive diffusion world model that enables localized\nspatiotemporal distribution modeling through two key innovations: 1) Decoupled\nspatiotemporal factorization that separates temporal dynamics modeling from\nfine-grained future world generation, and 2) Modular trajectory and video\nprediction that seamlessly integrate motion planning with visual modeling in an\nend-to-end framework. Our architecture enables high-resolution, long-duration\ngeneration while introducing a novel chain-of-forward training strategy to\naddress error accumulation in autoregressive loops. Experimental results\ndemonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes\nlonger prediction duration compared to prior works. The learned world model\nfurther serves as a real-time motion planner, outperforming strong end-to-end\nplanners on NAVSIM benchmarks. Code will be publicly available at\n\\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "AI": {"tldr": "Epona\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u6269\u6563\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u56e0\u5b50\u5316\u548c\u6a21\u5757\u5316\u8f68\u8ff9\u89c6\u9891\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u5728\u7075\u6d3b\u957f\u5ea6\u3001\u957f\u65f6\u7a0b\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cEpona\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u65f6\u7a7a\u56e0\u5b50\u5316\u548c\u6a21\u5757\u5316\u8f68\u8ff9\u89c6\u9891\u9884\u6d4b\uff0c\u7ed3\u5408\u94fe\u5f0f\u524d\u5411\u8bad\u7ec3\u7b56\u7565\u51cf\u5c11\u81ea\u56de\u5f52\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aFVD\u63d0\u53477.4%\uff0c\u9884\u6d4b\u65f6\u957f\u663e\u8457\u5ef6\u957f\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7aef\u5230\u7aef\u89c4\u5212\u5668\u3002", "conclusion": "Epona\u5728\u89c6\u9891\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.24121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24121", "abs": "https://arxiv.org/abs/2506.24121", "authors": ["Sisi Dai", "Xinxin Su", "Boyan Wan", "Ruizhen Hu", "Kai Xu"], "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation", "comment": null, "summary": "Recent advancements in diffusion generative models significantly advanced\nimage, video, and 3D content creation from user-provided text prompts. However,\nthe challenging problem of dynamic 3D content generation (text-to-4D) with\ndiffusion guidance remains largely unexplored. In this paper, we introduce\nTextMesh4D, a novel framework for high-quality text-to-4D generation. Our\napproach leverages per-face Jacobians as a differentiable mesh representation\nand decomposes 4D generation into two stages: static object creation and\ndynamic motion synthesis. We further propose a flexibility-rigidity\nregularization term to stabilize Jacobian optimization under video diffusion\npriors, ensuring robust geometric performance. Experiments demonstrate that\nTextMesh4D achieves state-of-the-art results in terms of temporal consistency,\nstructural fidelity, and visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead-requiring only a single 24GB GPU-offering a\ncost-effective yet high-quality solution for text-driven 4D mesh generation.\nThe code will be released to facilitate future research in text-to-4D\ngeneration.", "AI": {"tldr": "TextMesh4D\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u6587\u672c\u52304D\u751f\u6210\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u751f\u6210\u9759\u6001\u5bf9\u8c61\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u7075\u6d3b\u6027-\u521a\u6027\u6b63\u5219\u5316\u4f18\u5316Jacobian\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u3001\u89c6\u9891\u548c3D\u5185\u5bb9\u751f\u6210\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u52a8\u60013D\u5185\u5bb9\u751f\u6210\uff08\u6587\u672c\u52304D\uff09\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9762\u7684Jacobian\u4f5c\u4e3a\u53ef\u5fae\u5206\u7f51\u683c\u8868\u793a\uff0c\u5c064D\u751f\u6210\u5206\u4e3a\u9759\u6001\u5bf9\u8c61\u521b\u5efa\u548c\u52a8\u6001\u8fd0\u52a8\u5408\u6210\u4e24\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u7075\u6d3b\u6027-\u521a\u6027\u6b63\u5219\u5316\u3002", "result": "TextMesh4D\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u4ec5\u970024GB GPU\u5185\u5b58\u3002", "conclusion": "TextMesh4D\u4e3a\u6587\u672c\u9a71\u52a8\u76844D\u7f51\u683c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.24123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24123", "abs": "https://arxiv.org/abs/2506.24123", "authors": ["Yue Ma", "Qingyan Bai", "Hao Ouyang", "Ka Leong Cheng", "Qiuyu Wang", "Hongyu Liu", "Zichen Liu", "Haofan Wang", "Jingye Chen", "Yujun Shen", "Qifeng Chen"], "title": "Calligrapher: Freestyle Text Image Customization", "comment": "Project page: https://calligrapher2025.github.io/Calligrapher Code:\n  https://github.com/Calligrapher2025/Calligrapher", "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.", "AI": {"tldr": "Calligrapher\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u5b9a\u5236\u4e0e\u827a\u672f\u5b57\u4f53\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u63a7\u5236\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u673a\u5236\u3001\u5c40\u90e8\u98ce\u683c\u6ce8\u5165\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u673a\u5236\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b57\u4f53\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u4e66\u6cd5\u548c\u8bbe\u8ba1\u4e2d\u7cbe\u786e\u98ce\u683c\u63a7\u5236\u548c\u6570\u636e\u4f9d\u8d56\u7684\u6311\u6218\u3002", "method": "1. \u81ea\u84b8\u998f\u673a\u5236\u6784\u5efa\u98ce\u683c\u57fa\u51c6\uff1b2. \u5c40\u90e8\u98ce\u683c\u6ce8\u5165\u6846\u67b6\u63d0\u53d6\u98ce\u683c\u7279\u5f81\uff1b3. \u4e0a\u4e0b\u6587\u751f\u6210\u673a\u5236\u5d4c\u5165\u53c2\u8003\u56fe\u50cf\u3002", "result": "\u5728\u591a\u79cd\u5b57\u4f53\u548c\u8bbe\u8ba1\u573a\u666f\u4e2d\u51c6\u786e\u590d\u73b0\u98ce\u683c\u7ec6\u8282\u548c\u5b57\u5f62\u5b9a\u4f4d\uff0c\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "Calligrapher\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u5b57\u4f53\uff0c\u4e3a\u6570\u5b57\u827a\u672f\u548c\u54c1\u724c\u8bbe\u8ba1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.24125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24125", "abs": "https://arxiv.org/abs/2506.24125", "authors": ["Jiacheng Cui", "Xinyue Bi", "Yaxin Luo", "Xiaohan Zhao", "Jiacheng Liu", "Zhiqiang Shen"], "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation", "comment": "Code at: https://github.com/Jiacheng8/FADRM", "summary": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6570\u636e\u6b8b\u5dee\u5339\u914d\uff08FADRM\uff09\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u6570\u636e\u5c42\u9762\u5f15\u5165\u6b8b\u5dee\u8fde\u63a5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u84b8\u998f\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u6570\u636e\u6b8b\u5dee\u8fde\u63a5\u5728\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u6570\u636e\u4fe1\u606f\u6d88\u5931\u95ee\u9898\uff0c\u5e73\u8861\u65b0\u77e5\u8bc6\u4e0e\u539f\u59cb\u6570\u636e\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u6570\u636e\u7ea7\u8df3\u8dc3\u8fde\u63a5\u548c\u4f18\u5316\u7ea7\u6539\u8fdb\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u4fe1\u606f\u4fdd\u7559\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1150%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FADRM\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u84b8\u998f\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u6027\u80fd\u7684\u53cc\u91cd\u7a81\u7834\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002"}}
{"id": "2506.24127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24127", "abs": "https://arxiv.org/abs/2506.24127", "authors": ["Matthew Gwilliam", "Roy Zhang", "Namitha Padmanabhan", "Hongyang Du", "Abhinav Shrivastava"], "title": "How to Design and Train Your Implicit Neural Representation for Video Compression", "comment": "21 pages, 41 figures, 5 tables", "summary": "Implicit neural representation (INR) methods for video compression have\nrecently achieved visual quality and compression ratios that are competitive\nwith traditional pipelines. However, due to the need for per-sample network\ntraining, the encoding speeds of these methods are too slow for practical\nadoption. We develop a library to allow us to disentangle and review the\ncomponents of methods from the NeRV family, reframing their performance in\nterms of not only size-quality trade-offs, but also impacts on training time.\nWe uncover principles for effective video INR design and propose a\nstate-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When\nall methods are given equal training time (equivalent to 300 NeRV epochs) for 7\ndifferent UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared\nto the best-performing alternative for each video in our NeRV library. We then\ntackle the encoding speed issue head-on by investigating the viability of\nhyper-networks, which predict INR weights from video inputs, to disentangle\ntraining from encoding to allow for real-time encoding. We propose masking the\nweights of the predicted INR during training to allow for variable, higher\nquality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at\n0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by\n0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar\nspeeds. Our project website is available at https://mgwillia.github.io/vinrb/\nand our code is available at https://github.com/mgwillia/vinrb.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u89c6\u9891\u538b\u7f29\u65b9\u6cd5RNeRV\uff0c\u901a\u8fc7\u4f18\u5316\u7ec4\u4ef6\u8bbe\u8ba1\u548c\u5f15\u5165\u8d85\u7f51\u7edc\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u8d28\u91cf\u548c\u7f16\u7801\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfINR\u65b9\u6cd5\u56e0\u9700\u9010\u6837\u672c\u8bad\u7ec3\u7f51\u7edc\u5bfc\u81f4\u7f16\u7801\u901f\u5ea6\u8fc7\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ec4\u4ef6\u5206\u6790\u548c\u8d85\u7f51\u7edc\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e93\u6765\u5206\u6790NeRV\u5bb6\u65cf\u65b9\u6cd5\u7684\u7ec4\u4ef6\uff0c\u63d0\u51fa\u4e86RNeRV\u914d\u7f6e\uff0c\u5e76\u5f15\u5165\u8d85\u7f51\u7edc\u9884\u6d4bINR\u6743\u91cd\u4ee5\u52a0\u901f\u7f16\u7801\u3002", "result": "RNeRV\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u95f4\u4e0b\u5e73\u5747PSNR\u63d0\u53471.27%\uff1b\u8d85\u7f51\u7edc\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86PSNR\u548cMS-SSIM\uff0c\u540c\u65f6\u4fdd\u6301\u7f16\u7801\u901f\u5ea6\u3002", "conclusion": "RNeRV\u548c\u8d85\u7f51\u7edc\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891INR\u7684\u538b\u7f29\u8d28\u91cf\u548c\u7f16\u7801\u901f\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}

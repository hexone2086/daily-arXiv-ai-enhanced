{"id": "2506.21627", "categories": ["cs.RO", "cs.AI", "F.4.3; I.2.9"], "pdf": "https://arxiv.org/pdf/2506.21627", "abs": "https://arxiv.org/abs/2506.21627", "authors": ["Shiyi Wang", "Wenbo Li", "Yiteng Chen", "Qingyao Wu", "Huiping Zhuang"], "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models", "comment": "15 pages, 4 figures, under review of NeurIPS", "summary": "Developing a general robot manipulation system capable of performing a wide\nrange of tasks in complex, dynamic, and unstructured real-world environments\nhas long been a challenging task. It is widely recognized that achieving\nhuman-like efficiency and robustness manipulation requires the robotic brain to\nintegrate a comprehensive set of functions, such as task planning, policy\ngeneration, anomaly monitoring and handling, and long-term memory, achieving\nhigh-efficiency operation across all functions. Vision-Language Models (VLMs),\npretrained on massive multimodal data, have acquired rich world knowledge,\nexhibiting exceptional scene understanding and multimodal reasoning\ncapabilities. However, existing methods typically focus on realizing only a\nsingle function or a subset of functions within the robotic brain, without\nintegrating them into a unified cognitive architecture. Inspired by a\ndivide-and-conquer strategy and the architecture of the human brain, we propose\nFrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that\nachieves both comprehensive functionality and high operational efficiency. Our\nframework includes a suite of components, decoupling a part of key functions\nfrom frequent VLM calls, striking an optimal balance between functional\ncompleteness and system efficiency. Specifically, we map task planning, policy\ngeneration, memory management, and low-level interfacing to the cortex,\ncerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and\ndesign efficient coordination mechanisms for the modules. We conducted\ncomprehensive experiments in both simulation and real-world robotic\nenvironments, demonstrating that our method offers significant advantages in\nanomaly detection and handling, long-term memory, operational efficiency, and\nstability -- all without requiring any fine-tuning or retraining.", "AI": {"tldr": "FrankenBot\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u8111\u7ed3\u6784\uff0c\u6574\u5408\u4efb\u52a1\u89c4\u5212\u3001\u7b56\u7565\u751f\u6210\u3001\u8bb0\u5fc6\u7ba1\u7406\u7b49\u529f\u80fd\uff0c\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u591a\u6837\u5316\u4efb\u52a1\u7684\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u9700\u8981\u6574\u5408\u591a\u79cd\u529f\u80fd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u529f\u80fd\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u548c\u4eba\u8111\u67b6\u6784\uff0c\u5c06\u529f\u80fd\u6620\u5c04\u5230\u4e0d\u540c\u8111\u533a\uff08\u5982\u76ae\u5c42\u3001\u5c0f\u8111\u7b49\uff09\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u534f\u8c03\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFrankenBot\u5728\u5f02\u5e38\u5904\u7406\u3001\u957f\u671f\u8bb0\u5fc6\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6216\u518d\u8bad\u7ec3\u3002", "conclusion": "FrankenBot\u901a\u8fc7\u7edf\u4e00\u8ba4\u77e5\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u529f\u80fd\u5b8c\u6574\u6027\u548c\u9ad8\u6548\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21628", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21628", "abs": "https://arxiv.org/abs/2506.21628", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "title": "Ark: An Open-source Python-based Framework for Robot Learning", "comment": null, "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "AI": {"tldr": "ARK\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee5Python\u4e3a\u6838\u5fc3\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u590d\u6742\u6027\u548c\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u73b0\u4ee3AI\u751f\u6001\u7684\u4fbf\u6377\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8f6f\u4ef6\u6808\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u5de5\u5177\u788e\u7247\u5316\uff0c\u786c\u4ef6\u96c6\u6210\u590d\u6742\uff0c\u4e0ePython\u4e3b\u5bfc\u7684\u73b0\u4ee3AI\u751f\u6001\u7cfb\u7edf\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u963b\u788d\u4e86\u673a\u5668\u4eba\u6280\u672f\u7684\u5546\u4e1a\u5e94\u7528\u548c\u7814\u7a76\u8fdb\u5c55\u3002", "method": "ARK\u63d0\u4f9bGym\u98ce\u683c\u7684\u73af\u5883\u63a5\u53e3\uff0c\u652f\u6301\u6570\u636e\u6536\u96c6\u3001\u9884\u5904\u7406\u548c\u7b56\u7565\u8bad\u7ec3\uff0c\u7ed3\u5408\u9ad8\u6027\u80fd\u4eff\u771f\u4e0e\u7269\u7406\u673a\u5668\u4eba\u5207\u6362\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5e76\u63d0\u4f9bC/C++\u7ed1\u5b9a\u4ee5\u786e\u4fdd\u5b9e\u65f6\u6027\u80fd\u3002", "result": "ARK\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u3001ROS\u4e92\u64cd\u4f5c\u6027\u548c\u5168\u9762\u6587\u6863\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u786c\u4ef6\u65e0\u7f1d\u5207\u6362\u548c\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5f00\u53d1\u7684\u5165\u95e8\u95e8\u69db\u3002", "conclusion": "ARK\u901a\u8fc7\u7edf\u4e00\u673a\u5668\u4eba\u6280\u672f\u548cAI\u5b9e\u8df5\uff0c\u52a0\u901f\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u7814\u7a76\u548c\u5546\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2506.21630", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21630", "abs": "https://arxiv.org/abs/2506.21630", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "comment": "8 pages, 9 figures, 2025 IJCNN", "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u72ed\u7a84\u3001\u5c0f\u5f84\u5f0f\u8d8a\u91ce\u73af\u5883\u7684Trail-based Off-road Multimodal Dataset (TOMD)\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u5c3a\u5ea6\u6570\u636e\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u53ef\u901a\u884c\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u57ce\u5e02\u73af\u5883\u6216\u5bbd\u9614\u7684\u8d8a\u91ce\u8def\u5f84\uff0c\u65e0\u6cd5\u6ee1\u8db3\u72ed\u7a84\u3001\u5c0f\u5f84\u5f0f\u8d8a\u91ce\u573a\u666f\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u641c\u7d22\u6551\u63f4\u548c\u68ee\u6797\u706b\u707e\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u5f15\u5165TOMD\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u591a\u5c3a\u5ea6\u6570\u636e\u878d\u5408\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u878d\u5408\u7b56\u7565\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5149\u7167\u5bf9\u5206\u5272\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "TOMD\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u8d8a\u91ce\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.21631", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21631", "abs": "https://arxiv.org/abs/2506.21631", "authors": ["Tianliang Yao", "Bingrui Li", "Bo Lu", "Zhiqiang Pei", "Yixuan Yuan", "Peng Qi"], "title": "Real-Time 3D Guidewire Reconstruction from Intraoperative DSA Images for Robot-Assisted Endovascular Interventions", "comment": "This paper has been accepted by IEEE/RSJ IROS 2025", "summary": "Accurate three-dimensional (3D) reconstruction of guidewire shapes is crucial\nfor precise navigation in robot-assisted endovascular interventions.\nConventional 2D Digital Subtraction Angiography (DSA) is limited by the absence\nof depth information, leading to spatial ambiguities that hinder reliable\nguidewire shape sensing. This paper introduces a novel multimodal framework for\nreal-time 3D guidewire reconstruction, combining preoperative 3D Computed\nTomography Angiography (CTA) with intraoperative 2D DSA images. The method\nutilizes robust feature extraction to address noise and distortion in 2D DSA\ndata, followed by deformable image registration to align the 2D projections\nwith the 3D CTA model. Subsequently, the inverse projection algorithm\nreconstructs the 3D guidewire shape, providing real-time, accurate spatial\ninformation. This framework significantly enhances spatial awareness for\nrobotic-assisted endovascular procedures, effectively bridging the gap between\npreoperative planning and intraoperative execution. The system demonstrates\nnotable improvements in real-time processing speed, reconstruction accuracy,\nand computational efficiency. The proposed method achieves a projection error\nof 1.76$\\pm$0.08 pixels and a length deviation of 2.93$\\pm$0.15\\%, with a frame\nrate of 39.3$\\pm$1.5 frames per second (FPS). These advancements have the\npotential to optimize robotic performance and increase the precision of complex\nendovascular interventions, ultimately contributing to better clinical\noutcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u672f\u524d3D CTA\u548c\u672f\u4e2d2D DSA\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f63D\u5bfc\u4e1d\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u5185\u624b\u672f\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf2D DSA\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u7a7a\u95f4\u6a21\u7cca\u6027\uff0c\u5f71\u54cd\u5bfc\u4e1d\u5f62\u72b6\u611f\u77e5\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u9c81\u68d2\u7279\u5f81\u63d0\u53d6\u548c\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\uff0c\u5c062D DSA\u4e0e3D CTA\u5bf9\u9f50\uff0c\u518d\u4f7f\u7528\u9006\u6295\u5f71\u7b97\u6cd5\u91cd\u5efa3D\u5bfc\u4e1d\u5f62\u72b6\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e861.76\u00b10.08\u50cf\u7d20\u7684\u6295\u5f71\u8bef\u5dee\u548c2.93\u00b10.15%\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u5e27\u7387\u4e3a39.3\u00b11.5 FPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u673a\u5668\u4eba\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u8840\u7ba1\u5185\u624b\u672f\u7684\u7cbe\u5ea6\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u6548\u679c\u3002"}}
{"id": "2506.21655", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21655", "abs": "https://arxiv.org/abs/2506.21655", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u975e\u5bf9\u79f0\u7b56\u7565\u4f18\u5316\uff08APO\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574KL\u6563\u5ea6\u6743\u91cd\u548c\u60e9\u7f5a\u8fc7\u957f\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5e94\u7528\u53c8\u5bb9\u6613\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "method": "\u63d0\u51faAPO\u65b9\u6cd5\uff0c\u5c06\u6837\u672c\u5206\u4e3a\u6b63\u8d1f\u4e24\u7ec4\uff1a\u6b63\u6837\u672c\u91c7\u7528\u96be\u5ea6\u81ea\u9002\u5e94\u6563\u5ea6\u8c03\u6574\uff08DADS\uff09\u52a8\u6001\u8c03\u6574KL\u6563\u5ea6\u6743\u91cd\uff1b\u8d1f\u6837\u672c\u91c7\u7528\u6b21\u4f18\u8f68\u8ff9\u590d\u6742\u5ea6\u6b63\u5219\u5316\uff08STCR\uff09\u60e9\u7f5a\u8fc7\u957f\u54cd\u5e94\u3002", "result": "\u5728Qwen2.5-VL-3B\u6a21\u578b\u4e0a\u5e94\u7528APO\uff0c\u5f97\u5230View-R1-3B\uff0c\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u63d0\u53477%\uff0c\u5e76\u5728\u591a\u9879\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u66f4\u5927\u7684MLLMs\u3002", "conclusion": "APO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86DADS\u548cSTCR\u6280\u672f\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.21656", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21656", "abs": "https://arxiv.org/abs/2506.21656", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "comment": "29 pages", "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "AI": {"tldr": "SpatialReasoner-R1\u6a21\u578b\u901a\u8fc7M3CTS\u751f\u6210\u9ad8\u8d28\u91cf\u7a7a\u95f4\u63a8\u7406\u76d1\u7763\u6570\u636e\uff0c\u7ed3\u5408fDPO\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u548c\u591a\u6b65\u903b\u8f91\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u91c7\u7528M3CTS\u751f\u6210\u591a\u6837\u4e14\u903b\u8f91\u4e00\u81f4\u7684LongCoT\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u51fafDPO\u65b9\u6cd5\u4f18\u5316\u7a7a\u95f4\u5956\u52b1\u673a\u5236\u3002", "result": "fDPO\u5728\u7a7a\u95f4\u8d28\u91cf\u548c\u6570\u91cf\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u53474.1%\u548c9.0%\uff0cSpatialReasoner-R1\u5728SPATIALRGPT-Bench\u4e0a\u8fbe\u5230\u65b0SoTA\u3002", "conclusion": "SpatialReasoner-R1\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.21635", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21635", "abs": "https://arxiv.org/abs/2506.21635", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u7740\u9646\u504f\u5dee\u9884\u8b66\u7cfb\u7edfAeroLite-MDNet\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u548c\u5206\u5272\u5206\u652f\u63d0\u9ad8\u68c0\u6d4b\u548c\u65b9\u5411\u4f30\u8ba1\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65b0\u8bc4\u4f30\u6307\u6807AWD\u548c\u65b0\u6570\u636e\u96c6UAVLandData\u3002\u5b9e\u9a8c\u663e\u793a\u7cfb\u7edf\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u65e0\u4eba\u673a\u7740\u9646\u65f6\u56e0GPS\u4fe1\u53f7\u5e72\u6270\u7b49\u95ee\u9898\u96be\u4ee5\u7cbe\u51c6\u964d\u843d\uff0c\u5f71\u54cd\u64cd\u4f5c\u8fde\u7eed\u6027\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faAeroLite-MDNet\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u548c\u5206\u5272\u5206\u652f\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u65b9\u5411\u4f30\u8ba1\uff1b\u5f15\u5165AWD\u6307\u6807\u548c\u65b0\u6570\u636e\u96c6UAVLandData\u3002", "result": "\u7cfb\u7edfAWD\u4e3a0.7\u79d2\uff0c\u504f\u5dee\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe98.6%\uff0c\u663e\u8457\u63d0\u5347\u7740\u9646\u53ef\u9760\u6027\u3002", "conclusion": "AeroLite-MDNet\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u7740\u9646\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.21683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21683", "abs": "https://arxiv.org/abs/2506.21683", "authors": ["Xihong Su", "Jia Lin Hau", "Gersi Doko", "Kishan Panaganti", "Marek Petrik"], "title": "Risk-Averse Total-Reward Reinforcement Learning", "comment": "The paper is under review now", "summary": "Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising\nframework for modeling and solving undiscounted infinite-horizon objectives.\nExisting model-based algorithms for risk measures like the entropic risk\nmeasure (ERM) and entropic value-at-risk (EVaR) are effective in small\nproblems, but require full access to transition probabilities. We propose a\nQ-learning algorithm to compute the optimal stationary policy for total-reward\nERM and EVaR objectives with strong convergence and performance guarantees. The\nalgorithm and its optimality are made possible by ERM's dynamic consistency and\nelicitability. Our numerical results on tabular domains demonstrate quick and\nreliable convergence of the proposed Q-learning algorithm to the optimal\nrisk-averse value function.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdQ-learning\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u603b\u5956\u52b1ERM\u548cEVaR\u76ee\u6807\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5177\u6709\u5f3a\u6536\u655b\u6027\u548c\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u7b97\u6cd5\u5728\u5c0f\u89c4\u6a21\u95ee\u9898\u4e0a\u6709\u6548\uff0c\u4f46\u9700\u8981\u5b8c\u5168\u8bbf\u95ee\u8f6c\u79fb\u6982\u7387\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5229\u7528ERM\u7684\u52a8\u6001\u4e00\u81f4\u6027\u548c\u53ef\u63d0\u53d6\u6027\uff0c\u8bbe\u8ba1Q-learning\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u5728\u8868\u683c\u57df\u4e2d\u5feb\u901f\u53ef\u9760\u5730\u6536\u655b\u5230\u6700\u4f18\u98ce\u9669\u89c4\u907f\u503c\u51fd\u6570\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u98ce\u9669\u89c4\u907f\u603b\u5956\u52b1MDP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21681", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan \u00c7apuk", "Andrew Bond", "Muhammed Burak K\u0131z\u0131l", "Emir G\u00f6\u00e7en", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360\u00b0 Panorama Generation", "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "TanDiT\u662f\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u8986\u76d6360\u5ea6\u89c6\u56fe\u7684\u6b63\u5207\u5e73\u9762\u56fe\u50cf\u7f51\u683c\u6765\u5408\u6210\u5168\u666f\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u548c\u5faa\u73af\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u51e0\u4f55\u5931\u771f\u548c\u5faa\u73af\u4e00\u81f4\u6027\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TanDiT\u4f7f\u7528\u7edf\u4e00\u6269\u6563\u6a21\u578b\u5728\u5355\u6b21\u53bb\u566a\u8fed\u4ee3\u4e2d\u540c\u65f6\u751f\u6210\u6b63\u5207\u5e73\u9762\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u6b65\u9aa4\u589e\u5f3a\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTanDiT\u80fd\u6709\u6548\u6cdb\u5316\u5230\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\uff0c\u5904\u7406\u590d\u6742\u6587\u672c\u63d0\u793a\uff0c\u5e76\u4e0e\u591a\u79cd\u751f\u6210\u6a21\u578b\u96c6\u6210\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\u50cf\u3002", "conclusion": "TanDiT\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u5168\u666f\u56fe\u50cf\u751f\u6210\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u7528\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21689", "abs": "https://arxiv.org/abs/2506.21689", "authors": ["Jason Lim", "Florian Richter", "Zih-Yun Chiu", "Jaeyon Lee", "Ethan Quist", "Nathan Fisher", "Jonathan Chambers", "Steven Hong", "Michael C. Yip"], "title": "Optimal Motion Scaling for Delayed Telesurgery", "comment": "Accepted to IROS 2025", "summary": "Robotic teleoperation over long communication distances poses challenges due\nto delays in commands and feedback from network latency. One simple yet\neffective strategy to reduce errors and increase performance under delay is to\ndownscale the relative motion between the operating surgeon and the robot. The\nquestion remains as to what is the optimal scaling factor, and how this value\nchanges depending on the level of latency as well as operator tendencies. We\npresent user studies investigating the relationship between latency, scaling\nfactor, and performance. The results of our studies demonstrate a statistically\nsignificant difference in performance between users and across scaling factors\nfor certain levels of delay. These findings indicate that the optimal scaling\nfactor for a given level of delay is specific to each user, motivating the need\nfor personalized models for optimal performance. We present techniques to model\nthe user-specific mapping of latency level to scaling factor for optimal\nperformance, leading to an efficient and effective solution to optimizing\nperformance of robotic teleoperation and specifically telesurgery under large\ncommunication delay.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u5ef6\u8fdf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e2a\u6027\u5316\u7f29\u653e\u56e0\u5b50\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u8ddd\u79bb\u901a\u4fe1\u5ef6\u8fdf\u5bfc\u81f4\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u6790\u5ef6\u8fdf\u3001\u7f29\u653e\u56e0\u5b50\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e2a\u6027\u5316\u5efa\u6a21\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7f29\u653e\u56e0\u5b50\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u6700\u4f18\u503c\u56e0\u4eba\u800c\u5f02\u3002", "conclusion": "\u4e2a\u6027\u5316\u6a21\u578b\u80fd\u6709\u6548\u4f18\u5316\u8fdc\u7a0b\u64cd\u4f5c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8fdc\u7a0b\u624b\u672f\u4e2d\u3002"}}
{"id": "2506.21695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21695", "abs": "https://arxiv.org/abs/2506.21695", "authors": ["Oron Nir", "Jay Tenenbaum", "Ariel Shamir"], "title": "Unimodal Strategies in Density-Based Clustering", "comment": null, "summary": "Density-based clustering methods often surpass centroid-based counterparts,\nwhen addressing data with noise or arbitrary data distributions common in\nreal-world problems. In this study, we reveal a key property intrinsic to\ndensity-based clustering methods regarding the relation between the number of\nclusters and the neighborhood radius of core points - we empirically show that\nit is nearly unimodal, and support this claim theoretically in a specific\nsetting. We leverage this property to devise new strategies for finding\nappropriate values for the radius more efficiently based on the Ternary Search\nalgorithm. This is especially important for large scale data that is\nhigh-dimensional, where parameter tuning is computationally intensive. We\nvalidate our methodology through extensive applications across a range of\nhigh-dimensional, large-scale NLP, Audio, and Computer Vision tasks,\ndemonstrating its practical effectiveness and robustness. This work not only\noffers a significant advancement in parameter control for density-based\nclustering but also broadens the understanding regarding the relations between\ntheir guiding parameters. Our code is available at\nhttps://github.com/oronnir/UnimodalStrategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u805a\u7c7b\u7684\u65b0\u7b56\u7565\uff0c\u5229\u7528\u5355\u5cf0\u6027\u8d28\u9ad8\u6548\u786e\u5b9a\u6838\u5fc3\u70b9\u7684\u90bb\u57df\u534a\u5f84\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u5927\u89c4\u6a21\u6570\u636e\u3002", "motivation": "\u5bc6\u5ea6\u805a\u7c7b\u5728\u5904\u7406\u566a\u58f0\u548c\u4efb\u610f\u5206\u5e03\u6570\u636e\u65f6\u4f18\u4e8e\u57fa\u4e8e\u8d28\u5fc3\u7684\u65b9\u6cd5\uff0c\u4f46\u53c2\u6570\u8c03\u4f18\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7Ternary Search\u7b97\u6cd5\u5229\u7528\u90bb\u57df\u534a\u5f84\u4e0e\u805a\u7c7b\u6570\u7684\u5355\u5cf0\u5173\u7cfb\uff0c\u9ad8\u6548\u786e\u5b9a\u53c2\u6570\u3002", "result": "\u5728NLP\u3001\u97f3\u9891\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3a\u5bc6\u5ea6\u805a\u7c7b\u7684\u53c2\u6570\u63a7\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u53c2\u6570\u5173\u7cfb\u7684\u7406\u89e3\u3002"}}
{"id": "2506.21710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21710", "abs": "https://arxiv.org/abs/2506.21710", "authors": ["Liangyu Zhong", "Fabio Rosenthal", "Joachim Sicking", "Fabian H\u00fcger", "Thorsten Bagdonat", "Hanno Gottschalk", "Leo Schwinn"], "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering", "comment": "Preprint. Under review", "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.", "AI": {"tldr": "FOCUS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u88c1\u526a\u65b9\u6cd5\uff0c\u5229\u7528MLLM\u5185\u90e8\u8868\u793a\u6307\u5bfc\u641c\u7d22\u6700\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6VQA\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1MLLM\u5728\u56fe\u50cf-\u6587\u672c\u8f93\u5165\u4e0a\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u7ec6\u7c92\u5ea6VQA\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u89c6\u89c9\u88c1\u526a\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u9700\u6c42\u3001\u6548\u7387\u4f4e\u6216\u4e0e\u9ad8\u6548\u6ce8\u610f\u529b\u5b9e\u73b0\u4e0d\u517c\u5bb9\u7b49\u95ee\u9898\u3002", "method": "FOCUS\u901a\u8fc7\u56db\u6b65\u5b9e\u73b0\uff1a\u8bc6\u522bVQA\u63d0\u793a\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u5229\u7528KV\u7f13\u5b58\u8ba1\u7b97\u5bf9\u8c61\u76f8\u5173\u6027\u56fe\uff0c\u57fa\u4e8e\u56fe\u63d0\u51fa\u5e76\u6392\u5e8f\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff0c\u6700\u540e\u4f7f\u7528\u6392\u540d\u6700\u9ad8\u7684\u533a\u57df\u6267\u884cVQA\u4efb\u52a1\u3002", "result": "FOCUS\u5728\u56db\u4e2a\u7ec6\u7c92\u5ea6VQA\u6570\u636e\u96c6\u548c\u4e24\u79cdMLLM\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u548c\u6548\u7387\u5747\u4f18\u4e8e\u4e09\u79cd\u6d41\u884c\u89c6\u89c9\u88c1\u526a\u65b9\u6cd5\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c113-6.5\u500d\u3002", "conclusion": "FOCUS\u901a\u8fc7\u667a\u80fd\u641c\u7d22\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6VQA\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.21732", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21732", "abs": "https://arxiv.org/abs/2506.21732", "authors": ["Ameya Salvi", "Venkat Krovi"], "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "comment": null, "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u89c9\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6ed1\u79fb\u8f6c\u5411\u8f66\u8f86\u5728\u52a8\u6001\u64cd\u4f5c\u4e2d\u7f3a\u4e4f\u51c6\u786e\u5206\u6790\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6a21\u4eff\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u5b66\u4e60\u89c6\u89c9\u5bfc\u822a\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u8f6f\u4ef6\u6a21\u62df\u3001\u786c\u4ef6\u8bc4\u4f30\u548c\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6ed1\u79fb\u8f6c\u5411\u8f66\u8f86\u7684\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u3002"}}
{"id": "2506.21714", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21714", "abs": "https://arxiv.org/abs/2506.21714", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a7\u5236\u8d28\u91cf-\u590d\u6742\u5ea6\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u65f6\u95f4\u6b65\u957f\u548c\u795e\u7ecf\u7f51\u7edc\u957f\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91c7\u6837\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u51cf\u5c11\u91c7\u6837\u65f6\u95f4\u6b65\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u795e\u7ecf\u7f51\u7edc\u957f\u5ea6\u7684\u5f71\u54cd\u3002\u672c\u6587\u63a2\u7d22\u4e86\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u6b65\u548c\u7f51\u7edc\u957f\u5ea6\u7684\u4e92\u8865\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u91cd\u8fdeTransformer\u67b6\u6784\u4e2d\u7684\u5757\u6765\u6c42\u89e3\u5185\u90e8\u79bb\u6563ODE\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u65f6\u95f4\u548c\u957f\u5ea6\u4e00\u81f4\u6027\u9879\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u95f4\u6b65\u548c\u5757\u7684\u91c7\u6837\u3002", "result": "\u5728CelebA-HQ\u548cImageNet\u4e0a\uff0c\u6700\u9ad8\u6548\u91c7\u6837\u6a21\u5f0f\u4e0b\u5ef6\u8fdf\u964d\u4f4e3\u500d\uff0c\u9ad8\u8d28\u91cf\u91c7\u6837FID\u5206\u6570\u63d0\u53473.5\u5206\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u7ef4\u5ea6\u548c\u957f\u5ea6\u7ef4\u5ea6\u4e0a\u5747\u5177\u6709\u7075\u6d3b\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.21711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21711", "abs": "https://arxiv.org/abs/2506.21711", "authors": ["Aryan Thakre", "Omkar Nagwekar", "Vedang Talekar", "Aparna Santra Biswas"], "title": "CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection", "comment": "50 pages, 6 figures", "summary": "Deepfakes have emerged as a significant threat to digital media authenticity,\nincreasing the need for advanced detection techniques that can identify subtle\nand time-dependent manipulations. CNNs are effective at capturing spatial\nartifacts, and Transformers excel at modeling temporal inconsistencies.\nHowever, many existing CNN-Transformer models process spatial and temporal\nfeatures independently. In particular, attention-based methods often use\nseparate attention mechanisms for spatial and temporal features and combine\nthem using naive approaches like averaging, addition, or concatenation, which\nlimits the depth of spatio-temporal interaction. To address this challenge, we\npropose a unified CAST model that leverages cross-attention to effectively fuse\nspatial and temporal features in a more integrated manner. Our approach allows\ntemporal features to dynamically attend to relevant spatial regions, enhancing\nthe model's ability to detect fine-grained, time-evolving artifacts such as\nflickering eyes or warped lips. This design enables more precise localization\nand deeper contextual understanding, leading to improved performance across\ndiverse and challenging scenarios. We evaluate the performance of our model\nusing the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both\nintra- and cross-dataset settings to affirm the superiority of our approach.\nOur model achieves strong performance with an AUC of 99.49 percent and an\naccuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset\ntesting, it demonstrates impressive generalization by achieving a 93.31 percent\nAUC on the unseen DeepfakeDetection dataset. These results highlight the\neffectiveness of cross-attention-based feature fusion in enhancing the\nrobustness of deepfake video detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAST\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bf9\u6570\u5b57\u5a92\u4f53\u771f\u5b9e\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709CNN-Transformer\u6a21\u578b\u5728\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u878d\u5408\u65f6\u7a7a\u7279\u5f81\uff0c\u589e\u5f3a\u5bf9\u7ec6\u5fae\u65f6\u95f4\u6f14\u5316\u4f2a\u5f71\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cAUC\u8fbe99.49%\uff0c\u51c6\u786e\u738797.57%\uff0c\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5AUC\u4e3a93.31%\u3002", "conclusion": "\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.21853", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21853", "abs": "https://arxiv.org/abs/2506.21853", "authors": ["Dewei Wang", "Chenjia Ba", "Chenhui Li", "Jiyuan Shi", "Yan Ding", "Chi Zhang", "Bin Zhao"], "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface", "comment": "17pages, 6 figures", "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.", "AI": {"tldr": "Skill-Nav\u65b9\u6cd5\u901a\u8fc7\u5c06\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6280\u80fd\u4e0e\u5bfc\u822a\u6846\u67b6\u7ed3\u5408\uff0c\u5229\u7528\u8def\u5f84\u70b9\u4f5c\u4e3a\u63a5\u53e3\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u5730\u5f62\u7684\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u867d\u7136\u5728\u8fd0\u52a8\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fd0\u52a8\u6280\u80fd\u4e0e\u5bfc\u822a\u7684\u7ed3\u5408\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u957f\u8ddd\u79bb\u79fb\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51faSkill-Nav\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8def\u5f84\u70b9\u5f15\u5bfc\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u81ea\u4e3b\u8c03\u6574\u8fd0\u52a8\u6280\u80fd\u4ee5\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u907f\u5f00\u969c\u788d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkill-Nav\u80fd\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u7a7f\u8d8a\u590d\u6742\u5730\u5f62\u5e76\u5b8c\u6210\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "Skill-Nav\u901a\u8fc7\u8def\u5f84\u70b9\u63a5\u53e3\u6574\u5408\u8fd0\u52a8\u6280\u80fd\u4e0e\u5bfc\u822a\uff0c\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u7684\u957f\u8ddd\u79bb\u79fb\u52a8\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21718", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21718", "abs": "https://arxiv.org/abs/2506.21718", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u590d\u6742\u7cfb\u7edf\uff08\u5982\u914d\u7f6e\u6587\u4ef6\u548c\u7cfb\u7edf\u65e5\u5fd7\uff09\u7684\u6307\u6807\u7ed3\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7cfb\u7edf\u6570\u636e\uff08\u5982\u914d\u7f6e\u6587\u4ef6\u548c\u7cfb\u7edf\u65e5\u5fd7\uff09\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u7279\u5f81\u5de5\u7a0b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u91c7\u752860M\u53c2\u6570\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u8bad\u7ec3\uff0c\u8fdb\u884c\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u3002", "result": "\u5728Google\u7684Borg\u8ba1\u7b97\u96c6\u7fa4\u8c03\u5ea6\u7cfb\u7edf\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u76840.99\u7b49\u7ea7\u76f8\u5173\u6027\uff08\u5e73\u57470.9\uff09\uff0c\u4e14\u5747\u65b9\u8bef\u5dee\u6bd4\u8868\u683c\u65b9\u6cd5\u4f4e100\u500d\u3002\u6a21\u578b\u8fd8\u80fd\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u662f\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7ed3\u679c\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.21722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21722", "abs": "https://arxiv.org/abs/2506.21722", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "comment": null, "summary": "While diffusion models demonstrate strong generative capabilities in image\nrestoration (IR) tasks, their complex architectures and iterative processes\nlimit their practical application compared to mainstream reconstruction-based\ngeneral ordinary IR networks. Existing approaches primarily focus on optimizing\nnetwork architecture and diffusion paths but overlook the integration of the\ndiffusion training paradigm within general ordinary IR frameworks. To address\nthese challenges, this paper elucidates key principles for adapting the\ndiffusion training paradigm to general IR training through systematic analysis\nof time-step dependencies, network hierarchies, noise-level relationships, and\nmulti-restoration task correlations, proposing a new IR framework supported by\ndiffusion-based training. To enable IR networks to simultaneously restore\nimages and model generative representations, we introduce a series of\nregularization strategies that align diffusion objectives with IR tasks,\nimproving generalization in single-task scenarios. Furthermore, recognizing\nthat diffusion-based generation exerts varying influences across different IR\ntasks, we develop an incremental training paradigm and task-specific adaptors,\nfurther enhancing performance in multi-task unified IR. Experiments demonstrate\nthat our method significantly improves the generalization of IR networks in\nsingle-task IR and achieves superior performance in multi-task unified IR.\nNotably, the proposed framework can be seamlessly integrated into existing\ngeneral IR architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u878d\u5165\u666e\u901a\u56fe\u50cf\u4fee\u590d\uff08IR\uff09\u7f51\u7edc\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u65f6\u95f4\u6b65\u4f9d\u8d56\u3001\u7f51\u7edc\u5c42\u6b21\u7b49\u5173\u952e\u539f\u5219\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u7b56\u7565\u548c\u589e\u91cf\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1IR\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u590d\u6742\u67b6\u6784\u548c\u8fed\u4ee3\u8fc7\u7a0b\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u6269\u6563\u8def\u5f84\uff0c\u800c\u5ffd\u7565\u4e86\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u878d\u5165\u666e\u901aIR\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u6b65\u4f9d\u8d56\u3001\u7f51\u7edc\u5c42\u6b21\u7b49\u5173\u952e\u539f\u5219\uff0c\u63d0\u51fa\u65b0\u7684IR\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u7b56\u7565\u548c\u589e\u91cf\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u4efb\u52a1IR\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u7edf\u4e00IR\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u666e\u901aIR\u67b6\u6784\u4e2d\uff0c\u4e3a\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u5728IR\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21860", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21860", "abs": "https://arxiv.org/abs/2506.21860", "authors": ["Xiangyu Shi", "Yanyuan Qiao", "Lingqiao Liu", "Feras Dayoub"], "title": "Embodied Domain Adaptation for Object Detection", "comment": "Accepted by IROS 2025", "summary": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e90\u81ea\u7531\u57df\u9002\u5e94\uff08SFDA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u805a\u7c7b\u548c\u591a\u5c3a\u5ea6\u9608\u503c\u878d\u5408\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba4\u5185\u52a8\u6001\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u5c01\u95ed\u96c6\u65b9\u6cd5\u548c\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\uff08OVOD\uff09\u5728\u5ba4\u5185\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u57df\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e90\u81ea\u7531\u57df\u9002\u5e94\uff08SFDA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u805a\u7c7b\u4f18\u5316\u4f2a\u6807\u7b7e\u3001\u591a\u5c3a\u5ea6\u9608\u503c\u878d\u5408\uff0c\u4ee5\u53ca\u57fa\u4e8eMean Teacher\u6846\u67b6\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u548c\u52a8\u6001\u5ba4\u5185\u6761\u4ef6\u9002\u5e94\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684EDAOD\u57fa\u51c6\u548c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21744", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.21744", "abs": "https://arxiv.org/abs/2506.21744", "authors": ["Biying Zhou", "Nanyu Luo", "Feng Ji"], "title": "Federated Item Response Theory Models", "comment": null, "summary": "Item Response Theory (IRT) models have been widely used to estimate\nrespondents' latent abilities and calibrate items' difficulty. Traditional IRT\nestimation requires all individual raw response data to be centralized in one\nplace, thus potentially causing privacy issues. Federated learning is an\nemerging field in computer science and machine learning with added features of\nprivacy protection and distributed computing. To integrate the advances from\nfederated learning with modern psychometrics, we propose a novel framework,\nFederated Item Response Theory (IRT), to enable estimating traditional IRT\nmodels with additional privacy, allowing estimation in a distributed manner\nwithout losing estimation accuracy.\n  Our numerical experiments confirm that FedIRT achieves statistical accuracy\nsimilar to standard IRT estimation using popular R packages, while offering\ncritical advantages: privacy protection and reduced communication costs. We\nalso validate FedIRT's utility through a real-world exam dataset, demonstrating\nits effectiveness in realistic educational contexts. This new framework extends\nIRT's applicability to distributed settings, such as multi-school assessments,\nwithout sacrificing accuracy or security. To support practical adoption, we\nprovide an open-ource R package, FedIRT, implementing the framework for the\ntwo-parameter logistic (2PL) and partial credit models (PCM).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedIRT\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u8054\u90a6\u5b66\u4e60\u4e0e\u4f20\u7edfIRT\u6a21\u578b\u7ed3\u5408\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f30\u8ba1\uff0c\u4fdd\u62a4\u9690\u79c1\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfIRT\u6a21\u578b\u9700\u8981\u96c6\u4e2d\u6240\u6709\u539f\u59cb\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u95ee\u9898\u3002\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u56e0\u6b64\u5e0c\u671b\u5c06\u5176\u4e0e\u5fc3\u7406\u6d4b\u91cf\u5b66\u7ed3\u5408\u3002", "method": "\u63d0\u51faFedIRT\u6846\u67b6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u4f30\u8ba1\u4f20\u7edfIRT\u6a21\u578b\uff08\u59822PL\u548cPCM\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90R\u5305\u5b9e\u73b0\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cFedIRT\u5728\u7edf\u8ba1\u51c6\u786e\u6027\u4e0a\u4e0e\u6807\u51c6IRT\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002\u771f\u5b9e\u8003\u8bd5\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "FedIRT\u6269\u5c55\u4e86IRT\u5728\u5206\u5e03\u5f0f\u73af\u5883\uff08\u5982\u591a\u5b66\u6821\u8bc4\u4f30\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u517c\u987e\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2506.21724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21724", "abs": "https://arxiv.org/abs/2506.21724", "authors": ["Remco F. Leijenaar", "Hamidreza Kasaei"], "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning", "comment": "for associated source code, see\n  https://github.com/RFLeijenaar/AsymDSD", "summary": "Learning semantically meaningful representations from unstructured 3D point\nclouds remains a central challenge in computer vision, especially in the\nabsence of large-scale labeled datasets. While masked point modeling (MPM) is\nwidely used in self-supervised 3D learning, its reconstruction-based objective\ncan limit its ability to capture high-level semantics. We propose AsymDSD, an\nAsymmetric Dual Self-Distillation framework that unifies masked modeling and\ninvariance learning through prediction in the latent space rather than the\ninput space. AsymDSD builds on a joint embedding architecture and introduces\nseveral key design choices: an efficient asymmetric setup, disabling attention\nbetween masked queries to prevent shape leakage, multi-mask sampling, and a\npoint cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results\non ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k\nshapes, surpassing prior methods.", "AI": {"tldr": "AsymDSD\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u8bed\u4e49\u8868\u793a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u77633D\u70b9\u4e91\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u4f20\u7edf\u63a9\u7801\u70b9\u5efa\u6a21\uff08MPM\uff09\u5728\u6355\u6349\u9ad8\u7ea7\u8bed\u4e49\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faAsymDSD\u6846\u67b6\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u53cc\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u3001\u591a\u63a9\u7801\u91c7\u6837\u548c\u591a\u88c1\u526a\u6280\u672f\u3002", "result": "\u5728ScanObjectNN\u4e0a\u8fbe\u523090.53%\u7684\u51c6\u786e\u7387\uff0c\u9884\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u81f393.72%\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AsymDSD\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u8bed\u4e49\u8868\u793a\u7684\u6027\u80fd\uff0c\u4e3a\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21982", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21982", "abs": "https://arxiv.org/abs/2506.21982", "authors": ["Akshay Jaitly", "Jack Cline", "Siavash Farzan"], "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments", "comment": "Accepted to 2025 IEEE International Conference on Automation Science\n  and Engineering (CASE 2025)", "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff0c\u7ed3\u5408PAAMP\u548c\u5e8f\u5217-\u6c42\u89e3\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u53d8\u91cf\u6570\u91cf\u5e76\u63d0\u5347\u6c42\u89e3\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u4e2d\u4f20\u7edf\u65b9\u6cd5\u53d8\u91cf\u591a\u3001\u6c42\u89e3\u6162\u7684\u95ee\u9898\u3002", "method": "\u5d4c\u5165PAAMP\u5230\u5e8f\u5217-\u6c42\u89e3\u6d41\u7a0b\uff0c\u4f7f\u7528\u5927M\u8d85\u5e73\u9762\u6a21\u578b\u786e\u4fdd\u667a\u80fd\u4f53\u5206\u79bb\uff0c\u4ec5\u5bf9\u76f8\u90bb\u533a\u57df\u667a\u80fd\u4f53\u65bd\u52a0\u78b0\u649e\u7ea6\u675f\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\uff0c\u6bd4\u975e\u7ed3\u6784\u5316MILP\u57fa\u7ebf\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2506.21771", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.21771", "abs": "https://arxiv.org/abs/2506.21771", "authors": ["John Wesley Hostetter", "Min Chi"], "title": "Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks", "comment": "45 pages", "summary": "Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function\napproximations that perform as well as conventional neural architectures, but\ntheir knowledge is expressed as linguistic IF-THEN rules. Despite these\nadvantages, their systematic design process remains a challenge. Existing work\nwill often sequentially build NFNs by inefficiently isolating parametric and\nstructural identification, leading to a premature commitment to brittle and\nsubpar architecture. We propose a novel application-independent approach called\ngradient-based neuroplastic adaptation for the concurrent optimization of NFNs'\nparameters and structure. By recognizing that NFNs' parameters and structure\nshould be optimized simultaneously as they are deeply conjoined, settings\npreviously unapproachable for NFNs are now accessible, such as the online\nreinforcement learning of NFNs for vision-based tasks. The effectiveness of\nconcurrently optimizing NFNs is empirically shown as it is trained by online\nreinforcement learning to proficiently play challenging scenarios from a\nvision-based video game called DOOM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u795e\u7ecf\u53ef\u5851\u6027\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u4f18\u5316\u795e\u7ecf\u6a21\u7cca\u7f51\u7edc\u7684\u53c2\u6570\u548c\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5206\u79bb\u4f18\u5316\u5bfc\u81f4\u7684\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u6a21\u7cca\u7f51\u7edc\uff08NFNs\uff09\u5177\u6709\u900f\u660e\u6027\u548c\u7b26\u53f7\u6027\uff0c\u4f46\u5176\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fc7\u7a0b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f4e\u6548\u5730\u5206\u79bb\u53c2\u6570\u548c\u7ed3\u6784\u4f18\u5316\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u57fa\u7840\u7684\u795e\u7ecf\u53ef\u5851\u6027\u9002\u5e94\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316NFNs\u7684\u53c2\u6570\u548c\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u89c6\u9891\u6e38\u620fDOOM\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u540c\u65f6\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u540c\u65f6\u4f18\u5316NFNs\u7684\u53c2\u6570\u548c\u7ed3\u6784\u80fd\u591f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u573a\u666f\uff0c\u5982\u89c6\u89c9\u4efb\u52a1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2506.21731", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21731", "abs": "https://arxiv.org/abs/2506.21731", "authors": ["Chenqiu Zhao", "Anup Basu"], "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "comment": null, "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability\nSpace (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential\nlimitation in probabilistic generative models; namely that learning global\ndistributions leads to memorization rather than generative behavior. MESP\nemerges from our rethinking of the Variational Autoencoder (VAE). We observe\nthat latent variable distributions in VAE exhibit overlap, which leads to an\noptimization conflict between the reconstruction loss and KL-divergence loss. A\nlower bound based on the overlap coefficient is proposed. We refer to this\nphenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary\nLatent Autoencoder (BL-AE) is proposed to encode images into binary latent\nrepresentations. These binary latents are used as the input to our\nAutoregressive Random Variable Model (ARVM), a modified autoregressive model\noutputting histograms. Our ARVM achieves competitive FID scores, outperforming\nstate-of-the-art methods on standard datasets. However, such scores reflect\nmemorization rather than generation. To address this issue, we propose the\nLocal Correlation Hypothesis (LCH), which posits that generative capability\narising from local correlations among latent variables. Comprehensive\nexperiments and discussions are conducted to validate our frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMESP\u548cLCH\u4e24\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u63a2\u8ba8\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5373\u5168\u5c40\u5206\u5e03\u5b66\u4e60\u5bfc\u81f4\u8bb0\u5fc6\u800c\u975e\u751f\u6210\u884c\u4e3a\u3002MESP\u6e90\u4e8e\u5bf9VAE\u7684\u91cd\u65b0\u601d\u8003\uff0c\u63d0\u51fa\u57fa\u4e8e\u91cd\u53e0\u7cfb\u6570\u7684\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1BL-AE\u548cARVM\u6a21\u578b\u3002\u5b9e\u9a8c\u663e\u793aARVM\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u53cd\u6620\u8bb0\u5fc6\u95ee\u9898\u3002LCH\u5047\u8bbe\u5c40\u90e8\u76f8\u5173\u6027\u4fc3\u8fdb\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u6982\u7387\u751f\u6210\u6a21\u578b\u56e0\u5b66\u4e60\u5168\u5c40\u5206\u5e03\u800c\u5bfc\u81f4\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u800c\u975e\u771f\u6b63\u7684\u751f\u6210\u884c\u4e3a\u3002", "method": "\u63d0\u51faMESP\u6846\u67b6\uff0c\u57fa\u4e8eVAE\u7684\u91cd\u53e0\u95ee\u9898\u8bbe\u8ba1BL-AE\u548cARVM\u6a21\u578b\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faLCH\u5047\u8bbe\uff0c\u5f3a\u8c03\u5c40\u90e8\u76f8\u5173\u6027\u5bf9\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "ARVM\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027FID\u5206\u6570\uff0c\u4f46\u53cd\u6620\u8bb0\u5fc6\u95ee\u9898\uff1bLCH\u5047\u8bbe\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5c40\u90e8\u76f8\u5173\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MESP\u548cLCH\u6846\u67b6\u63ed\u793a\u4e86\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u5c40\u90e8\u76f8\u5173\u6027\u662f\u63d0\u5347\u751f\u6210\u80fd\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2506.22028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22028", "abs": "https://arxiv.org/abs/2506.22028", "authors": ["Ossi Parikka", "Roel Pieters"], "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). For further information, videos and\n  code, see https://github.com/ozzyuni/LMPVC", "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u97f3\u63a7\u5236\u67b6\u6784LMPVC\uff0c\u7528\u4e8eROS2\u517c\u5bb9\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u7f16\u7a0b\u548c\u6559\u5b66\u529f\u80fd\uff08Policy Bank\uff09\u5f25\u8865LLM\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u8017\u65f6\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u4e1a\u8d8b\u5411\u4e2a\u6027\u5316\u548c\u590d\u6742\u5316\uff0c\u9700\u8981\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u548c\u5148\u8fdb\u4ea4\u4e92\u65b9\u5f0f\uff08\u5982\u8bed\u97f3\u63a7\u5236\uff09\u3002LLM\u7684\u53d1\u5c55\u4e3a\u673a\u5668\u4eba\u8bed\u97f3\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u63d0\u51faLMPVC\u67b6\u6784\uff0c\u7ed3\u5408\u4ee3\u7801\u751f\u6210\u548c\u7b56\u7565\u7f16\u7a0b\uff08Policy Bank\uff09\uff0c\u7528\u4e8eROS2\u673a\u5668\u4eba\u3002Policy Bank\u5f25\u8865LLM\u4e0d\u8db3\uff0c\u652f\u6301\u5feb\u901f\u4efb\u52a1\u9002\u5e94\u3002", "result": "LMPVC\u80fd\u6709\u6548\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u67b6\u6784\u548c\u6210\u679c\u5df2\u5f00\u6e90\u3002", "conclusion": "LMPVC\u5c55\u793a\u4e86LLM\u5728\u673a\u5668\u4eba\u8bed\u97f3\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0cPolicy Bank\u7684\u8bbe\u8ba1\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.21782", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21782", "abs": "https://arxiv.org/abs/2506.21782", "authors": ["Aditya Narendra", "Dmitry Makarov", "Aleksandr Panov"], "title": "M3PO: Massively Multi-Task Model-Based Policy Optimization", "comment": "6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version,\n  including appendix and implementation details", "summary": "We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a\nscalable model-based reinforcement learning (MBRL) framework designed to\naddress sample inefficiency in single-task settings and poor generalization in\nmulti-task domains. Existing model-based approaches like DreamerV3 rely on\npixel-level generative models that neglect control-centric representations,\nwhile model-free methods such as PPO suffer from high sample complexity and\nweak exploration. M3PO integrates an implicit world model, trained to predict\ntask outcomes without observation reconstruction, with a hybrid exploration\nstrategy that combines model-based planning and model-free uncertainty-driven\nbonuses. This eliminates the bias-variance trade-off in prior methods by using\ndiscrepancies between model-based and model-free value estimates to guide\nexploration, while maintaining stable policy updates through a trust-region\noptimizer. M3PO provides an efficient and robust alternative to existing\nmodel-based policy optimization approaches and achieves state-of-the-art\nperformance across multiple benchmarks.", "AI": {"tldr": "M3PO\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5355\u4efb\u52a1\u6837\u672c\u6548\u7387\u4f4e\u548c\u591a\u4efb\u52a1\u6cdb\u5316\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982DreamerV3\uff09\u5ffd\u7565\u63a7\u5236\u4e2d\u5fc3\u8868\u793a\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982PPO\uff09\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u4e14\u63a2\u7d22\u80fd\u529b\u5f31\u3002M3PO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "M3PO\u6574\u5408\u4e86\u9690\u5f0f\u4e16\u754c\u6a21\u578b\uff08\u9884\u6d4b\u4efb\u52a1\u7ed3\u679c\u800c\u975e\u89c2\u5bdf\u91cd\u5efa\uff09\u548c\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff08\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u5956\u52b1\uff09\u3002", "result": "M3PO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6d88\u9664\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "conclusion": "M3PO\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.21735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21735", "abs": "https://arxiv.org/abs/2506.21735", "authors": ["Nick Lemke", "Mirko Konstantin", "Henry John Krumb", "John Kalkhof", "Jonathan Stieber", "Anirban Mukhopadhyay"], "title": "Equitable Federated Learning with NCA", "comment": null, "summary": "Federated Learning (FL) is enabling collaborative model training across\ninstitutions without sharing sensitive patient data. This approach is\nparticularly valuable in low- and middle-income countries (LMICs), where access\nto trained medical professionals is limited. However, FL adoption in LMICs\nfaces significant barriers, including limited high-performance computing\nresources and unreliable internet connectivity. To address these challenges, we\nintroduce FedNCA, a novel FL system tailored for medical image segmentation\ntasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training\non low-cost edge devices, such as widely available smartphones, while\nminimizing communication costs. Additionally, our encryption-ready FedNCA\nproves to be suitable for compromised network communication. By overcoming\ninfrastructural and security challenges, FedNCA paves the way for inclusive,\nefficient, lightweight, and encryption-ready medical imaging solutions,\nfostering equitable healthcare advancements in resource-constrained regions.", "AI": {"tldr": "FedNCA\u662f\u4e00\u79cd\u4e13\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u5272\u8bbe\u8ba1\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u5730\u533a\uff0c\u652f\u6301\u4f4e\u6210\u672c\u8bbe\u5907\u548c\u5f31\u7f51\u7edc\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\uff08LMICs\uff09\u5728\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u65f6\u9762\u4e34\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3\u548c\u7f51\u7edc\u8fde\u63a5\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u8f7b\u91cf\u7ea7Med-NCA\u67b6\u6784\uff0c\u652f\u6301\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u667a\u80fd\u624b\u673a\uff09\u4e0a\u8bad\u7ec3\uff0c\u5e76\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002\u540c\u65f6\u5177\u5907\u52a0\u5bc6\u529f\u80fd\u4ee5\u9002\u5e94\u4e0d\u5b89\u5168\u7f51\u7edc\u3002", "result": "FedNCA\u6210\u529f\u514b\u670d\u4e86\u57fa\u7840\u8bbe\u65bd\u548c\u5b89\u5168\u6311\u6218\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5730\u533a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u4e14\u652f\u6301\u52a0\u5bc6\u7684\u533b\u7597\u6210\u50cf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FedNCA\u4e3a\u8d44\u6e90\u53d7\u9650\u5730\u533a\u5b9e\u73b0\u516c\u5e73\u7684\u533b\u7597\u6280\u672f\u8fdb\u6b65\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22034", "abs": "https://arxiv.org/abs/2506.22034", "authors": ["Kejia Chen", "Celina Dettmering", "Florian Pachler", "Zhuo Liu", "Yue Zhang", "Tailai Cheng", "Jonas Dirr", "Zhenshan Bing", "Alois Knoll", "R\u00fcdiger Daub"], "title": "Multi-Robot Assembly of Deformable Linear Objects Using Multi-Modal Perception", "comment": null, "summary": "Industrial assembly of deformable linear objects (DLOs) such as cables offers\ngreat potential for many industries. However, DLOs pose several challenges for\nrobot-based automation due to the inherent complexity of deformation and,\nconsequentially, the difficulties in anticipating the behavior of DLOs in\ndynamic situations. Although existing studies have addressed isolated\nsubproblems like shape tracking, grasping, and shape control, there has been\nlimited exploration of integrated workflows that combine these individual\nprocesses. To address this gap, we propose an object-centric perception and\nplanning framework to achieve a comprehensive DLO assembly process throughout\nthe industrial value chain. The framework utilizes visual and tactile\ninformation to track the DLO's shape as well as contact state across different\nstages, which facilitates effective planning of robot actions. Our approach\nencompasses robot-based bin picking of DLOs from cluttered environments,\nfollowed by a coordinated handover to two additional robots that mount the DLOs\nonto designated fixtures. Real-world experiments employing a setup with\nmultiple robots demonstrate the effectiveness of the approach and its relevance\nto industrial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u611f\u77e5\u548c\u89c4\u5212\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u5de5\u4e1a\u4ef7\u503c\u94fe\u4e2d\u5168\u9762\u7684DLO\u7ec4\u88c5\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3DLO\u5728\u52a8\u6001\u60c5\u51b5\u4e0b\u884c\u4e3a\u9884\u6d4b\u7684\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u96c6\u6210\u5de5\u4f5c\u6d41\u7a0b\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u606f\u8ddf\u8e2aDLO\u7684\u5f62\u72b6\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u591a\u673a\u5668\u4eba\u534f\u4f5c\u5b8c\u6210\u7ec4\u88c5\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aDLO\u7684\u5de5\u4e1a\u7ec4\u88c5\u63d0\u4f9b\u4e86\u4e00\u79cd\u96c6\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21788", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "pdf": "https://arxiv.org/pdf/2506.21788", "abs": "https://arxiv.org/abs/2506.21788", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "comment": "15 pages, 4 figures, 2 tables", "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5e76\u884c\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u591a\u6e90\u3001\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u7684\u56fe\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6e90\u3001\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u9884\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u63a2\u7d22\u5316\u5b66\u533a\u57df\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5171\u4eab\u6d88\u606f\u4f20\u9012\u5c42\u5904\u7406\u8f93\u5165\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u89e3\u7801\u5934\u9884\u6d4b\u6570\u636e\u7279\u5b9a\u8f93\u51fa\uff1b\u63d0\u51fa\u591a\u4efb\u52a1\u5e76\u884c\u65b9\u6cd5\uff0c\u5229\u7528GPU\u52a0\u901f\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728\u8d85\u8fc72400\u4e07\u4e2a\u7ed3\u6784\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u4e09\u79cd\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u8fc1\u79fb\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u8d85\u7ea7\u8ba1\u7b97\u673a\u73af\u5883\u3002"}}
{"id": "2506.21742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21742", "abs": "https://arxiv.org/abs/2506.21742", "authors": ["Sirnam Swetha", "Rohit Gupta", "Parth Parag Kulkarni", "David G Shatwell", "Jeffrey A Chan Santiago", "Nyle Siddiqui", "Joseph Fioresi", "Mubarak Shah"], "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "comment": null, "summary": "Video QA has made significant strides by leveraging multimodal learning to\nalign visual and textual modalities. However, current benchmarks overwhelmingly\nfocus on questions answerable through explicit visual content - actions,\nobjects & events directly observable within individual frames or short clips.\nIn contrast, creative and cinematic videos - such as movies, TV shows, and\nnarrative-driven content - employ storytelling techniques that deliberately\nomit certain depictions, requiring viewers to infer motives, causality, and\nrelationships across discontinuous frames. Humans naturally excel at such\nimplicit reasoning, seamlessly integrating information across time and context\nto construct coherent narratives. Current VideoQA systems and benchmarks fail\nto capture this essential dimension of human-like understanding. To bridge this\ngap, we present ImplicitQA, a novel benchmark specifically designed to test\nmodels on implicit reasoning. It comprises 1K meticulously annotated QA pairs\nderived from 320+ high-quality creative video clips, systematically categorized\ninto key reasoning dimensions: lateral and vertical spatial reasoning, depth\nand proximity, viewpoint and visibility, motion and trajectory, causal and\nmotivational reasoning, social interactions, physical context, and inferred\ncounting. These annotations are deliberately challenging, crafted by authors\nensuring high-quality. Our extensive evaluations on leading VideoQA models\nreveals performance degradation, underscoring their reliance on surface-level\nvisual cues and highlighting the difficulty of implicit reasoning. Performance\nvariations across models further illustrate the complexity and diversity of the\nchallenges presented by ImplicitQA. By releasing both the dataset and our data\ncollection framework, we aim to stimulate further research and development in\nthe community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ImplicitQA\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u89c6\u9891\u95ee\u7b54\u7cfb\u7edf\u5728\u9690\u542b\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u663e\u6027\u89c6\u89c9\u5185\u5bb9\uff0c\u800c\u5ffd\u7565\u4e86\u9690\u542b\u63a8\u7406\uff08\u5982\u52a8\u673a\u3001\u56e0\u679c\u5173\u7cfb\u7b49\uff09\uff0c\u4eba\u7c7b\u64c5\u957f\u6b64\u7c7b\u63a8\u7406\u4f46\u73b0\u6709\u7cfb\u7edf\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u4e86ImplicitQA\u57fa\u51c6\uff0c\u5305\u542b1K\u9ad8\u8d28\u91cfQA\u5bf9\uff0c\u6db5\u76d6\u591a\u79cd\u63a8\u7406\u7ef4\u5ea6\uff0c\u5982\u7a7a\u95f4\u3001\u56e0\u679c\u3001\u793e\u4ea4\u7b49\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u9690\u542b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u4f9d\u8d56\u8868\u9762\u89c6\u89c9\u7ebf\u7d22\u3002", "conclusion": "ImplicitQA\u4e3a\u89c6\u9891\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u6311\u6218\uff0c\u63a8\u52a8\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2506.22087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22087", "abs": "https://arxiv.org/abs/2506.22087", "authors": ["Armand Jordana", "Jianghan Zhang", "Joseph Amigo", "Ludovic Righetti"], "title": "An Introduction to Zero-Order Optimization Techniques for Robotics", "comment": null, "summary": "Zero-order optimization techniques are becoming increasingly popular in\nrobotics due to their ability to handle non-differentiable functions and escape\nlocal minima. These advantages make them particularly useful for trajectory\noptimization and policy optimization. In this work, we propose a mathematical\ntutorial on random search. It offers a simple and unifying perspective for\nunderstanding a wide range of algorithms commonly used in robotics. Leveraging\nthis viewpoint, we classify many trajectory optimization methods under a common\nframework and derive novel competitive RL algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u968f\u673a\u641c\u7d22\u7684\u6570\u5b66\u6559\u7a0b\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u4eba\u5b66\u4e2d\u5e38\u7528\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7b80\u5355\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u7c7b\u4e86\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u96f6\u9636\u4f18\u5316\u6280\u672f\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u56e0\u5176\u80fd\u5904\u7406\u4e0d\u53ef\u5fae\u51fd\u6570\u548c\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u9002\u7528\u4e8e\u8f68\u8ff9\u4f18\u5316\u548c\u7b56\u7565\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u6559\u7a0b\uff0c\u901a\u8fc7\u968f\u673a\u641c\u7d22\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u5206\u7c7b\u548c\u5206\u6790\u4e86\u591a\u79cd\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u7edf\u4e00\u89c6\u89d2\uff0c\u63a8\u5bfc\u51fa\u4e86\u65b0\u7684\u7ade\u4e89\u6027\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u96f6\u9636\u4f18\u5316\u6280\u672f\u7684\u5e94\u7528\u3002"}}
{"id": "2506.21797", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21797", "abs": "https://arxiv.org/abs/2506.21797", "authors": ["Peihao Wang", "Zhangyang Wang"], "title": "Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning", "comment": "International Conference on Neuro-symbolic Systems (NeuS), 2025", "summary": "We develop a theoretical framework that explains how discrete symbolic\nstructures can emerge naturally from continuous neural network training\ndynamics. By lifting neural parameters to a measure space and modeling training\nas Wasserstein gradient flow, we show that under geometric constraints, such as\ngroup invariance, the parameter measure $\\mu_t$ undergoes two concurrent\nphenomena: (1) a decoupling of the gradient flow into independent optimization\ntrajectories over some potential functions, and (2) a progressive contraction\non the degree of freedom. These potentials encode algebraic constraints\nrelevant to the task and act as ring homomorphisms under a commutative\nsemi-ring structure on the measure space. As training progresses, the network\ntransitions from a high-dimensional exploration to compositional\nrepresentations that comply with algebraic operations and exhibit a lower\ndegree of freedom. We further establish data scaling laws for realizing\nsymbolic tasks, linking representational capacity to the group invariance that\nfacilitates symbolic solutions. This framework charts a principled foundation\nfor understanding and designing neurosymbolic systems that integrate continuous\nlearning with discrete algebraic reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u79bb\u6563\u7b26\u53f7\u7ed3\u6784\u5982\u4f55\u4ece\u8fde\u7eed\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002\u901a\u8fc7\u5c06\u795e\u7ecf\u53c2\u6570\u63d0\u5347\u5230\u6d4b\u5ea6\u7a7a\u95f4\u5e76\u5efa\u6a21\u4e3aWasserstein\u68af\u5ea6\u6d41\uff0c\u7814\u7a76\u53d1\u73b0\u51e0\u4f55\u7ea6\u675f\uff08\u5982\u7fa4\u4e0d\u53d8\u6027\uff09\u4f1a\u5bfc\u81f4\u53c2\u6570\u6d4b\u5ea6\u7ecf\u5386\u89e3\u8026\u548c\u81ea\u7531\u5ea6\u6536\u7f29\uff0c\u6700\u7ec8\u5f62\u6210\u7b26\u5408\u4ee3\u6570\u64cd\u4f5c\u7684\u7ec4\u5408\u8868\u793a\u3002", "motivation": "\u63a2\u7d22\u8fde\u7eed\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u5982\u4f55\u81ea\u7136\u4ea7\u751f\u79bb\u6563\u7b26\u53f7\u7ed3\u6784\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u795e\u7ecf\u53c2\u6570\u63d0\u5347\u5230\u6d4b\u5ea6\u7a7a\u95f4\uff0c\u5efa\u6a21\u4e3aWasserstein\u68af\u5ea6\u6d41\uff0c\u5206\u6790\u51e0\u4f55\u7ea6\u675f\u4e0b\u7684\u89e3\u8026\u548c\u81ea\u7531\u5ea6\u6536\u7f29\u73b0\u8c61\u3002", "result": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7f51\u7edc\u4ece\u9ad8\u7ef4\u63a2\u7d22\u8fc7\u6e21\u5230\u7b26\u5408\u4ee3\u6570\u64cd\u4f5c\u7684\u7ec4\u5408\u8868\u793a\uff0c\u5e76\u5efa\u7acb\u4e86\u5b9e\u73b0\u7b26\u53f7\u4efb\u52a1\u7684\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u4e0e\u8bbe\u8ba1\u7ed3\u5408\u8fde\u7eed\u5b66\u4e60\u548c\u79bb\u6563\u4ee3\u6570\u63a8\u7406\u7684\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.21770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21770", "abs": "https://arxiv.org/abs/2506.21770", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEfficientNet-B0\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e2d\u68c0\u6d4b\u9752\u5149\u773c\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9752\u5149\u773c\u662f\u4e0d\u53ef\u9006\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u53ef\u6539\u5584\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5177\u6709\u4fb5\u5165\u6027\u4e14\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u3002", "method": "\u4f7f\u7528EfficientNet-B0\u67b6\u6784\uff0c\u901a\u8fc7ACRIMA\u3001ORIGA\u548cRIM-ONE\u6570\u636e\u96c6\u8fdb\u884c\u987a\u5e8f\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u6700\u5c0f\u5316\u9884\u5904\u7406\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5224\u522b\u6027\u80fd\uff0cAUC-ROC\u4f18\u4e8e\u590d\u6742\u9884\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u4e3a\u9752\u5149\u773c\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6f5c\u5728\u4e34\u5e8a\u4ef7\u503c\u3002"}}
{"id": "2506.22116", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22116", "abs": "https://arxiv.org/abs/2506.22116", "authors": ["Noora Sassali", "Roel Pieters"], "title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). Preprint", "summary": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u6001\u4f30\u8ba1\u548c\u51e0\u4f55\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d\u5e73\u9762\u5de5\u4f5c\u533a\u4e2d\u7684\u6307\u5411\u76ee\u6807\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6307\u5411\u624b\u52bf\u662f\u4eba\u673a\u534f\u4f5c\u4e2d\u5e38\u89c1\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4f46\u9700\u8981\u4e00\u79cd\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u5b9a\u4f4d\u76ee\u6807\u3002", "method": "\u4f7f\u7528\u59ff\u6001\u4f30\u8ba1\u548c\u80a9-\u8155\u4f38\u5c55\u7684\u51e0\u4f55\u6a21\u578b\u4eceRGB-D\u6570\u636e\u4e2d\u63d0\u53d6\u624b\u52bf\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u591a\u6a21\u6001\u529f\u80fd\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u548c\u8bed\u97f3\u5904\u7406\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5de5\u5177\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5de5\u5177\u7684\u5c40\u9650\u6027\u548c\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u591a\u6a21\u6001\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u4f5c\u7528\u3002"}}
{"id": "2506.21833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21833", "abs": "https://arxiv.org/abs/2506.21833", "authors": ["Kunjal Panchal", "Sunav Choudhary", "Yuriy Brun", "Hui Guan"], "title": "The Cost of Avoiding Backpropagation", "comment": null, "summary": "Forward-mode automatic differentiation (FmAD) and zero-order (ZO)\noptimization have been proposed as memory-efficient alternatives to\nbackpropagation (BP) for gradient computation, especially in low-resource\nsettings. However, their practical benefits remain unclear due to two key gaps:\na lack of comparison against memory-efficient BP variants, such as activation\ncheckpointing, and a lack of a unified theoretical analysis. This work presents\na comprehensive theoretical and empirical comparison of BP, FmAD, and ZO\nmethods. Our theoretical analysis shows that while FmAD, and ZO can reduce\nmemory usage, they incur significant costs in accuracy, convergence speed, and\ncomputation compared to BP with checkpointing. These drawbacks worsen with\nlarger models or constrained perturbation budgets. Empirical experiments on\nlarge language and vision-language models show that BP with checkpointing\noutperforms FmAD and ZO variants, including those enhanced with variance\nreduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and\n3.8x fewer computations at comparable memory usage. Our results highlight\nfundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as\nthe most effective strategy for model training under memory-constrained\nsettings. Our code is available at\nhttps://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u524d\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08FmAD\uff09\u548c\u96f6\u9636\u4f18\u5316\uff08ZO\uff09\u4e0e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0BP\u7ed3\u5408\u68c0\u67e5\u70b9\u6280\u672f\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3FmAD\u548cZO\u4f5c\u4e3a\u5185\u5b58\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u7684\u5b9e\u9645\u6548\u679c\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u5e76\u4e0e\u5185\u5b58\u4f18\u5316\u7684BP\u53d8\u4f53\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u6bd4\u8f83BP\u3001FmAD\u548cZO\u65b9\u6cd5\uff0c\u5305\u62ec\u68c0\u67e5\u70b9\u6280\u672f\u548c\u65b9\u5dee\u51cf\u5c11\u589e\u5f3a\u7684\u53d8\u4f53\u3002", "result": "BP\u7ed3\u5408\u68c0\u67e5\u70b9\u6280\u672f\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8eFmAD\u548cZO\uff0c\u5c24\u5176\u5728\u5927\u578b\u6a21\u578b\u4e2d\u3002", "conclusion": "BP\u7ed3\u5408\u68c0\u67e5\u70b9\u6280\u672f\u662f\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u6700\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0cFmAD\u548cZO\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002"}}
{"id": "2506.21785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21785", "abs": "https://arxiv.org/abs/2506.21785", "authors": ["Daniel Wen"], "title": "Comparing Learning Paradigms for Egocentric Video Summarization", "comment": null, "summary": "In this study, we investigate various computer vision paradigms - supervised\nlearning, unsupervised learning, and prompt fine-tuning - by assessing their\nability to understand and interpret egocentric video data. Specifically, we\nexamine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM\n(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned\npre-trained model), evaluating their effectiveness in video summarization. Our\nresults demonstrate that current state-of-the-art models perform less\neffectively on first-person videos compared to third-person videos,\nhighlighting the need for further advancements in the egocentric video domain.\nNotably, a prompt fine-tuned general-purpose GPT-4o model outperforms these\nspecialized models, emphasizing the limitations of existing approaches in\nadapting to the unique challenges of first-person perspectives. Although our\nevaluation is conducted on a small subset of egocentric videos from the\nEgo-Exo4D dataset due to resource constraints, the primary objective of this\nresearch is to provide a comprehensive proof-of-concept analysis aimed at\nadvancing the application of computer vision techniques to first-person videos.\nBy exploring novel methodologies and evaluating their potential, we aim to\ncontribute to the ongoing development of models capable of effectively\nprocessing and interpreting egocentric perspectives.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u63d0\u793a\u5fae\u8c03\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u8bc4\u4f30\u4e86Shotluck Holmes\uff08\u76d1\u7763\u5b66\u4e60\uff09\u3001TAC-SUM\uff08\u65e0\u76d1\u7763\u5b66\u4e60\uff09\u548cGPT-4o\uff08\u63d0\u793a\u5fae\u8c03\uff09\u5728\u89c6\u9891\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4f46GPT-4o\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u4ee5\u9002\u5e94\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u7684\u72ec\u7279\u6311\u6218\uff0c\u63d0\u793a\u5fae\u8c03\u65b9\u6cd5\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2506.22170", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "pdf": "https://arxiv.org/pdf/2506.22170", "abs": "https://arxiv.org/abs/2506.22170", "authors": ["Yu Zhang", "Xiao-Song Yang"], "title": "RM-Dijkstra: A surface optimal path planning algorithm based on Riemannian metric", "comment": "7 pages", "summary": "The Dijkstra algorithm is a classic path planning method, which operates in a\ndiscrete graph space to determine the shortest path from a specified source\npoint to a target node or all other nodes based on non-negative edge weights.\nNumerous studies have focused on the Dijkstra algorithm due to its potential\napplication. However, its application in surface path planning for mobile\nrobots remains largely unexplored. In this letter, a surface optimal path\nplanning algorithm called RM-Dijkstra is proposed, which is based on Riemannian\nmetric model. By constructing a new Riemannian metric on the 2D projection\nplane, the surface optimal path planning problem is therefore transformed into\na geometric problem on the 2D plane with new Riemannian metric. Induced by the\nstandard Euclidean metric on surface, the constructed new metric reflects\nenvironmental information of the robot and ensures that the projection map is\nan isometric immersion. By conducting a series of simulation tests, the\nexperimental results demonstrate that the RM-Dijkstra algorithm not only\neffectively solves the optimal path planning problem on surfaces, but also\noutperforms traditional path planning algorithms in terms of path accuracy and\nsmoothness, particularly in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u5ea6\u91cf\u7684RM-Dijkstra\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u8868\u9762\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "Dijkstra\u7b97\u6cd5\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u79fb\u52a8\u673a\u5668\u4eba\u8868\u9762\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u65b0\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5c06\u8868\u9762\u8def\u5f84\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u51e0\u4f55\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u6295\u5f71\u6620\u5c04\u4e3a\u7b49\u8ddd\u6d78\u5165\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cRM-Dijkstra\u7b97\u6cd5\u5728\u8def\u5f84\u7cbe\u5ea6\u548c\u5e73\u6ed1\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002", "conclusion": "RM-Dijkstra\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8868\u9762\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.21844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21844", "abs": "https://arxiv.org/abs/2506.21844", "authors": ["Jun Ohkubo"], "title": "Koopman operator-based discussion on partial observation in stochastic systems", "comment": "23 pages, 5 figures", "summary": "It is sometimes difficult to achieve a complete observation for a full set of\nobservables, and partial observations are necessary. For deterministic systems,\nthe Mori-Zwanzig formalism provides a theoretical framework for handling\npartial observations. Recently, data-driven algorithms based on the Koopman\noperator theory have made significant progress, and there is a discussion to\nconnect the Mori-Zwanzig formalism with the Koopman operator theory. In this\nwork, we discuss the effects of partial observation in stochastic systems using\nthe Koopman operator theory. The discussion clarifies the importance of\ndistinguishing the state space and the function space in stochastic systems.\nEven in stochastic systems, the delay embedding technique is beneficial for\npartial observation, and several numerical experiments showed a power-law\nbehavior of the accuracy for the amplitude of the additive noise. We also\ndiscuss the relation between the exponent of the power-law behavior and the\neffects of partial observation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u968f\u673a\u7cfb\u7edf\u4e2d\u90e8\u5206\u89c2\u6d4b\u7684\u5f71\u54cd\uff0c\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\u548cMori-Zwanzig\u5f62\u5f0f\u4e3b\u4e49\uff0c\u5f3a\u8c03\u4e86\u72b6\u6001\u7a7a\u95f4\u4e0e\u51fd\u6570\u7a7a\u95f4\u7684\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u52a0\u6027\u566a\u58f0\u5e45\u5ea6\u7684\u5e42\u5f8b\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u7cfb\u7edf\u4e2d\u90e8\u5206\u89c2\u6d4b\u7684\u5f71\u54cd\uff0c\u4ee5\u586b\u8865\u786e\u5b9a\u6027\u7cfb\u7edf\u4e2dMori-Zwanzig\u5f62\u5f0f\u4e3b\u4e49\u4e0eKoopman\u7b97\u5b50\u7406\u8bba\u7ed3\u5408\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5206\u6790\u968f\u673a\u7cfb\u7edf\u4e2d\u7684\u90e8\u5206\u89c2\u6d4b\uff0c\u7ed3\u5408\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u52a0\u6027\u566a\u58f0\u5e45\u5ea6\u7684\u5e42\u5f8b\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8\u4e86\u5e42\u5f8b\u6307\u6570\u4e0e\u90e8\u5206\u89c2\u6d4b\u6548\u5e94\u7684\u5173\u7cfb\u3002", "conclusion": "\u5728\u968f\u673a\u7cfb\u7edf\u4e2d\uff0c\u72b6\u6001\u7a7a\u95f4\u4e0e\u51fd\u6570\u7a7a\u95f4\u7684\u533a\u5206\u81f3\u5173\u91cd\u8981\uff0c\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u5bf9\u90e8\u5206\u89c2\u6d4b\u6709\u6548\uff0c\u4e14\u566a\u58f0\u5e45\u5ea6\u7684\u5e42\u5f8b\u884c\u4e3a\u63ed\u793a\u4e86\u89c2\u6d4b\u7cbe\u5ea6\u4e0e\u566a\u58f0\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.21813", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21813", "abs": "https://arxiv.org/abs/2506.21813", "authors": ["Felix Holm", "G\u00f6zde \u00dcnver", "Ghazal Ghazaei", "Nassir Navab"], "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "comment": null, "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u767d\u5185\u969c\u624b\u672f\u573a\u666f\u56fe\u6570\u636e\u96c6CAT-SG\uff0c\u7528\u4e8e\u5168\u9762\u5efa\u6a21\u624b\u672f\u5de5\u5177\u3001\u89e3\u5256\u7ed3\u6784\u548c\u6280\u672f\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u65b0\u6a21\u578bCatSGG\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u5173\u6ce8\u624b\u672f\u5206\u6790\u7684\u5b64\u7acb\u65b9\u9762\uff08\u5982\u5de5\u5177\u68c0\u6d4b\u6216\u9636\u6bb5\u5206\u5272\uff09\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u4f53\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u5168\u9762\u5efa\u6a21\u3002", "method": "\u5f15\u5165CAT-SG\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u3001\u7a0b\u5e8f\u53d8\u5316\u548c\u65f6\u95f4\u4f9d\u8d56\u7684\u7ed3\u6784\u5316\u6807\u6ce8\uff1b\u63d0\u51faCatSGG\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u624b\u672f\u8868\u793a\u3002", "result": "CAT-SG\u63d0\u4f9b\u4e86\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u6574\u4f53\u89c6\u56fe\uff0cCatSGG\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u8868\u793a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAT-SG\u548cCatSGG\u4e3aAI\u9a71\u52a8\u7684\u624b\u672f\u57f9\u8bad\u3001\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u548c\u5de5\u4f5c\u6d41\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63a8\u52a8\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u667a\u80fd\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2506.22174", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22174", "abs": "https://arxiv.org/abs/2506.22174", "authors": ["Bavo Lesy", "Siemen Herremans", "Robin Kerstens", "Jan Steckel", "Walter Daems", "Siegfried Mercelis", "Ali Anwar"], "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research", "comment": "14 Pages, 11 Figures", "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.", "AI": {"tldr": "ASVSim\u662f\u4e00\u4e2a\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u4e13\u4e3a\u5185\u6cb3\u548c\u6e2f\u53e3\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u822a\u8fd0\u7814\u7a76\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u8239\u8236\u52a8\u529b\u5b66\u548c\u6d77\u6d0b\u4f20\u611f\u5668\u6a21\u62df\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USVs\uff09\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5f00\u6e90\u3001\u9ad8\u4fdd\u771f\u4eff\u771f\u6846\u67b6\u548c\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u652f\u6301\u81ea\u4e3b\u822a\u8fd0\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eCosys-AirSim\u5f00\u53d1\uff0c\u7ed3\u5408\u8239\u8236\u52a8\u529b\u5b66\u548c\u4f20\u611f\u5668\u6a21\u62df\uff08\u5982\u96f7\u8fbe\u548c\u6444\u50cf\u5934\uff09\uff0c\u652f\u6301\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86ASVSim\u5728\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ASVSim\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\uff0c\u4e3a\u6d77\u6d0b\u5de5\u7a0b\u793e\u533a\u63d0\u4f9b\u4e86\u81ea\u4e3b\u5bfc\u822a\u7814\u7a76\u7684\u5e73\u53f0\u3002"}}
{"id": "2506.21872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21872", "abs": "https://arxiv.org/abs/2506.21872", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "title": "A Survey of Continual Reinforcement Learning", "comment": "This work has been submitted to the IEEE TPAMI", "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u6838\u5fc3\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u52a8\u6001\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u7684\u5174\u8d77\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u8be6\u7ec6\u56de\u987e\u73b0\u6709\u5de5\u4f5c\uff0c\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u5b58\u50a8\u548c\u8f6c\u79fb\u7684CRL\u65b9\u6cd5\u5206\u7c7b\u6cd5\u3002", "result": "\u5206\u6790\u4e86CRL\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "CRL\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5176\u6311\u6218\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2506.21826", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21826", "abs": "https://arxiv.org/abs/2506.21826", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "comment": "18 pages, accepted at ICDAR2025", "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u5c11\u6837\u672c\u5386\u53f2\u5730\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u662f\u91cd\u8981\u7684\u5386\u53f2\u8d44\u6e90\uff0c\u4f46\u5176\u591a\u6837\u5316\u7684\u89c6\u89c9\u8868\u793a\u548c\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u7ed9\u81ea\u52a8\u5316\u5904\u7406\u5e26\u6765\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u5d4c\u5165\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u5206\u5272\u3002", "result": "\u5728Siegfried\u6570\u636e\u96c6\u4e0a\uff0c\u8461\u8404\u56ed\u548c\u94c1\u8def\u5206\u5272\u7684mIoU\u5206\u522b\u63d0\u53475%\u548c13%\uff1b\u5728ICDAR 2021\u6570\u636e\u96c6\u4e0a\uff0c\u5efa\u7b51\u5757\u5206\u5272\u7684PQ\u8fbe67.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6781\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u63a8\u52a8\u4e86\u5386\u53f2\u5730\u56fe\u7684\u81ea\u52a8\u5316\u5904\u7406\u4e0e\u5206\u6790\u3002"}}
{"id": "2506.22176", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22176", "abs": "https://arxiv.org/abs/2506.22176", "authors": ["Holly Dinkel", "Raghavendra Navaratna", "Jingyi Xiang", "Brian Coltin", "Trey Smith", "Timothy Bretl"], "title": "KnotDLO: Toward Interpretable Knot Tying", "comment": "4 pages, 5 figures, presented at the Workshop on 3D Visual\n  Representations for Manipulation at the 2023 IEEE International Conference on\n  Robotics and Automation in Yokohama, Japan. Video presentation\n  [https://youtu.be/mg30uCUtpOk]. Poster\n  [https://hollydinkel.github.io/assets/pdf/ICRA20243DVRM_poster.pdf] 3DVRM\n  Workshop [https://3d-manipulation-workshop.github.io/]", "summary": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "AI": {"tldr": "KnotDLO\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u6216\u8bad\u7ec3\u7684\u5355\u624b\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u6253\u7ed3\u65b9\u6cd5\uff0c\u5177\u6709\u6297\u906e\u6321\u6027\u3001\u53ef\u91cd\u590d\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3DLO\u6253\u7ed3\u4efb\u52a1\u4e2d\u7684\u906e\u6321\u95ee\u9898\u3001\u521d\u59cb\u914d\u7f6e\u53d8\u5316\u4ee5\u53ca\u65e0\u9700\u4f9d\u8d56\u4eba\u7c7b\u6f14\u793a\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5f53\u524dDLO\u5f62\u72b6\u89c4\u5212\u6293\u53d6\u548c\u76ee\u6807\u8def\u5f84\u70b9\uff0c\u57fa\u4e8e\u5206\u6bb5\u7ebf\u6027\u66f2\u7ebf\u8ba1\u7b97\u6293\u53d6\u59ff\u6001\uff0c\u5e76\u89e3\u8026\u89c6\u89c9\u63a8\u7406\u4e0e\u63a7\u5236\u3002", "result": "\u572816\u6b21\u6253\u7ed3\u8bd5\u9a8c\u4e2d\uff0cKnotDLO\u4ece\u672a\u89c1\u8fc7\u7684\u914d\u7f6e\u4e2d\u6210\u529f\u6253\u7ed3\u7684\u6210\u529f\u7387\u4e3a50%\u3002", "conclusion": "KnotDLO\u5c55\u793a\u4e86\u65e0\u9700\u8bad\u7ec3\u6216\u4eba\u7c7b\u6f14\u793a\u7684DLO\u6253\u7ed3\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.21899", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21899", "abs": "https://arxiv.org/abs/2506.21899", "authors": ["Amara Zuffer", "Michael Burke", "Mehrtash Harandi"], "title": "Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review", "comment": "65 pages, 9 figures", "summary": "The diversity of tasks and dynamic nature of reinforcement learning (RL)\nrequire RL agents to be able to learn sequentially and continuously, a learning\nparadigm known as continuous reinforcement learning. This survey reviews how\ncontinual learning transforms RL agents into dynamic continual learners. This\nenables RL agents to acquire and retain useful and reusable knowledge\nseamlessly. The paper delves into fundamental aspects of continual\nreinforcement learning, exploring key concepts, significant challenges, and\nnovel methodologies. Special emphasis is placed on recent advancements in\ncontinual reinforcement learning within robotics, along with a succinct\noverview of evaluation environments utilized in prominent research,\nfacilitating accessibility for newcomers to the field. The review concludes\nwith a discussion on limitations and promising future directions, providing\nvaluable insights for researchers and practitioners alike.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u7684\u5173\u952e\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u73af\u5883\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3RL\u4efb\u52a1\u591a\u6837\u6027\u548c\u52a8\u6001\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f7fRL\u4ee3\u7406\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u548c\u79ef\u7d2f\u77e5\u8bc6\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86CRL\u7684\u6838\u5fc3\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b0\u5174\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86CRL\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u8bc4\u4f30\u73af\u5883\u548c\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86CRL\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.21832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21832", "abs": "https://arxiv.org/abs/2506.21832", "authors": ["Minh-Loi Nguyen", "Quang-Khai Le", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "comment": null, "summary": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.", "AI": {"tldr": "TaleForge\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6280\u672f\u7684\u4e2a\u6027\u5316\u6545\u4e8b\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u7684\u9762\u90e8\u56fe\u50cf\u5d4c\u5165\u53d9\u4e8b\u548c\u63d2\u56fe\u4e2d\uff0c\u63d0\u5347\u6c89\u6d78\u611f\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7528\u6237\u89c6\u4e3a\u88ab\u52a8\u6d88\u8d39\u8005\uff0c\u63d0\u4f9b\u901a\u7528\u60c5\u8282\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u5f71\u54cd\u4e86\u53c2\u4e0e\u5ea6\u548c\u6c89\u6d78\u611f\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u6545\u4e8b\u751f\u6210\uff08LLMs\u6839\u636e\u7528\u6237\u63d0\u793a\u521b\u5efa\u53d9\u4e8b\u548c\u89d2\u8272\u63cf\u8ff0\uff09\u3001\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff08\u5c06\u7528\u6237\u9762\u90e8\u548c\u670d\u88c5\u9009\u62e9\u878d\u5165\u89d2\u8272\u63d2\u56fe\uff09\u3001\u80cc\u666f\u751f\u6210\uff08\u521b\u5efa\u5305\u542b\u4e2a\u6027\u5316\u89d2\u8272\u7684\u573a\u666f\u80cc\u666f\uff09\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u5f53\u7528\u6237\u4f5c\u4e3a\u4e3b\u89d2\u65f6\uff0c\u53c2\u4e0e\u611f\u548c\u5f52\u5c5e\u611f\u663e\u8457\u63d0\u5347\u3002\u7528\u6237\u8d5e\u8d4f\u5b9e\u65f6\u9884\u89c8\u548c\u76f4\u89c2\u63a7\u5236\uff0c\u4f46\u5e0c\u671b\u6709\u66f4\u7cbe\u7ec6\u7684\u53d9\u4e8b\u7f16\u8f91\u5de5\u5177\u3002", "conclusion": "TaleForge\u901a\u8fc7\u4e2a\u6027\u5316\u6587\u672c\u548c\u56fe\u50cf\u7684\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u6545\u4e8b\u53d9\u8ff0\u7684\u53d1\u5c55\uff0c\u521b\u9020\u4e86\u6c89\u6d78\u5f0f\u7684\u7528\u6237\u4e2d\u5fc3\u4f53\u9a8c\u3002"}}
{"id": "2506.22364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22364", "abs": "https://arxiv.org/abs/2506.22364", "authors": ["Joe Johnson", "Phanender Chalasani", "Arnav Shah", "Ram L. Ray", "Muthukumar Bagavathiannan"], "title": "Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass", "comment": "Accepted in the Extended Abstract, The 22nd International Conference\n  on Ubiquitous Robots (UR 2025), Texas, USA", "summary": "Accurate weed management is essential for mitigating significant crop yield\nlosses, necessitating effective weed suppression strategies in agricultural\nsystems. Integrating cover crops (CC) offers multiple benefits, including soil\nerosion reduction, weed suppression, decreased nitrogen requirements, and\nenhanced carbon sequestration, all of which are closely tied to the aboveground\nbiomass (AGB) they produce. However, biomass production varies significantly\ndue to microsite variability, making accurate estimation and mapping essential\nfor identifying zones of poor weed suppression and optimizing targeted\nmanagement strategies. To address this challenge, developing a comprehensive CC\nmap, including its AGB distribution, will enable informed decision-making\nregarding weed control methods and optimal application rates. Manual visual\ninspection is impractical and labor-intensive, especially given the extensive\nfield size and the wide diversity and variation of weed species and sizes. In\nthis context, optical imagery and Light Detection and Ranging (LiDAR) data are\ntwo prominent sources with unique characteristics that enhance AGB estimation.\nThis study introduces a ground robot-mounted multimodal sensor system designed\nfor agricultural field mapping. The system integrates optical and LiDAR data,\nleveraging machine learning (ML) methods for data fusion to improve biomass\npredictions. The best ML-based model for dry AGB estimation achieved a\ncoefficient of determination value of 0.88, demonstrating robust performance in\ndiverse field conditions. This approach offers valuable insights for\nsite-specific management, enabling precise weed suppression strategies and\npromoting sustainable farming practices.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u5b66\u548cLiDAR\u6570\u636e\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u8986\u76d6\u4f5c\u7269\u751f\u7269\u91cf\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4e3a\u7cbe\u51c6\u6742\u8349\u7ba1\u7406\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u8986\u76d6\u4f5c\u7269\u751f\u7269\u91cf\u4f30\u8ba1\u5bf9\u6742\u8349\u6291\u5236\u548c\u519c\u4e1a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e0d\u7cbe\u786e\u3002", "method": "\u5f00\u53d1\u5730\u9762\u673a\u5668\u4eba\u642d\u8f7d\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u7ed3\u5408\u5149\u5b66\u548cLiDAR\u6570\u636e\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u6570\u636e\u878d\u5408\u548c\u751f\u7269\u91cf\u9884\u6d4b\u3002", "result": "\u6700\u4f73\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u5e72\u71e5\u751f\u7269\u91cf\u7684\u4f30\u8ba1\u51b3\u5b9a\u7cfb\u6570\u4e3a0.88\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u51c6\u6742\u8349\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.21900", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.21900", "abs": "https://arxiv.org/abs/2506.21900", "authors": ["Sheng Yun", "Jianhua Pei", "Ping Wang"], "title": "TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments", "comment": null, "summary": "The evolution toward 6G networks demands a fundamental shift from bit-centric\ntransmission to semantic-aware communication that emphasizes task-relevant\ninformation. This work introduces TOAST (Task-Oriented Adaptive Semantic\nTransmission), a unified framework designed to address the core challenge of\nmulti-task optimization in dynamic wireless environments through three\ncomplementary components. First, we formulate adaptive task balancing as a\nMarkov decision process, employing deep reinforcement learning to dynamically\nadjust the trade-off between image reconstruction fidelity and semantic\nclassification accuracy based on real-time channel conditions. Second, we\nintegrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our\nSwin Transformer-based joint source-channel coding architecture, enabling\nparameter-efficient fine-tuning that dramatically reduces adaptation overhead\nwhile maintaining full performance across diverse channel impairments including\nAdditive White Gaussian Noise (AWGN), fading, phase noise, and impulse\ninterference. Third, we incorporate an Elucidating diffusion model that\noperates in the latent space to restore features corrupted by channel noises,\nproviding substantial quality improvements compared to baseline approaches.\nExtensive experiments across multiple datasets demonstrate that TOAST achieves\nsuperior performance compared to baseline approaches, with significant\nimprovements in both classification accuracy and reconstruction quality at low\nSignal-to-Noise Ratio (SNR) conditions while maintaining robust performance\nacross all tested scenarios.", "AI": {"tldr": "TOAST\u662f\u4e00\u4e2a\u9762\u54116G\u7f51\u7edc\u7684\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5e73\u8861\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u566a\u58f0\u6062\u590d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u4ece\u6bd4\u7279\u4f20\u8f93\u8f6c\u5411\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\uff0c\u5f3a\u8c03\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002TOAST\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u591a\u4efb\u52a1\u4f18\u5316\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "1) \u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u5e73\u8861\uff1b2) \u96c6\u6210LoRA\u673a\u5236\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b3) \u5f15\u5165\u6269\u6563\u6a21\u578b\u6062\u590d\u566a\u58f0\u635f\u574f\u7684\u7279\u5f81\u3002", "result": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0cTOAST\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "TOAST\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21834", "abs": "https://arxiv.org/abs/2506.21834", "authors": ["Duy-Bao Bui", "Hoang-Khang Nguyen", "Trung-Nghia Le"], "title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "comment": null, "summary": "Inpainting, the process of filling missing or corrupted image parts, has\nbroad applications, including medical imaging. However, in specialized fields\nlike medical polyps imaging, where accuracy and reliability are critical,\ninpainting models can generate inaccurate images, leading to significant errors\nin medical diagnosis and treatment. To ensure reliability, medical images\nshould be annotated by experts like oncologists for effective model training.\nWe propose PrefPaint, an approach that incorporates human feedback into the\ntraining process of Stable Diffusion Inpainting, bypassing the need for\ncomputationally expensive reward models. In addition, we develop a web-based\ninterface streamlines training, fine-tuning, and inference. This interactive\ninterface provides a smooth and intuitive user experience, making it easier to\noffer feedback and manage the fine-tuning process. User study on various\ndomains shows that PrefPaint outperforms existing methods, reducing visual\ninconsistencies and improving image rendering, particularly in medical\ncontexts, where our model generates more realistic polyps images.", "AI": {"tldr": "PrefPaint\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u6539\u8fdbStable Diffusion Inpainting\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4fee\u590d\uff08\u5982\u606f\u8089\u56fe\u50cf\uff09\u7684\u51c6\u786e\u6027\u5bf9\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0d\u51c6\u786e\u56fe\u50cf\uff0c\u9700\u4e13\u5bb6\u6807\u6ce8\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faPrefPaint\uff0c\u5c06\u4eba\u7c7b\u53cd\u9988\u76f4\u63a5\u878d\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u907f\u514d\u6602\u8d35\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eWeb\u7684\u4ea4\u4e92\u754c\u9762\u7b80\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "PrefPaint\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u89c6\u89c9\u4e0d\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u751f\u6210\u66f4\u771f\u5b9e\u7684\u606f\u8089\u56fe\u50cf\u3002", "conclusion": "PrefPaint\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u4ea4\u4e92\u754c\u9762\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2506.21937", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21937", "abs": "https://arxiv.org/abs/2506.21937", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification", "comment": null, "summary": "We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain\ntumor classification using MRI images. Trained on a dataset of 7,576 scans\ncovering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC\nintegrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized\nvia AdamW and a composite loss blending cross-entropy and attention\nconsistency.\n  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical\nbaseline (86.72%). It delivers higher precision and F1-scores, especially for\nglioma detection. t-SNE projections reveal enhanced feature separability in\nquantum space, and confusion matrices show lower misclassification. Attention\nmap analysis (Jaccard Index) confirms more accurate and focused tumor\nlocalization at high-confidence thresholds.\n  These results highlight the promise of quantum-enhanced models in medical\nimaging, advancing both diagnostic accuracy and interpretability for clinical\nbrain tumor assessment.", "AI": {"tldr": "HQCM-EBTC\u662f\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u7528\u4e8eMRI\u56fe\u50cf\u7684\u8111\u80bf\u7624\u81ea\u52a8\u5206\u7c7b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u3002", "motivation": "\u63d0\u9ad8\u8111\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a2\u7d22\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7ed3\u54085\u91cf\u5b50\u6bd4\u7279\u3001\u6df1\u5ea62\u7684\u91cf\u5b50\u5c42\u548c5\u4e2a\u5e76\u884c\u7535\u8def\uff0c\u4f7f\u7528AdamW\u4f18\u5316\u5668\u548c\u6df7\u5408\u635f\u5931\u51fd\u6570\uff08\u4ea4\u53c9\u71b5\u548c\u6ce8\u610f\u529b\u4e00\u81f4\u6027\uff09\u3002", "result": "\u57287,576\u5f20\u626b\u63cf\u56fe\u50cf\u4e0a\u8fbe\u523096.48%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\uff0886.72%\uff09\uff0c\u5c24\u5176\u5728\u80f6\u8d28\u7624\u68c0\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.21835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21835", "abs": "https://arxiv.org/abs/2506.21835", "authors": ["Xiaoqi Wang", "Clint Sebastian", "Wenbin He", "Liu Ren"], "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts", "comment": null, "summary": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation.", "AI": {"tldr": "ProSAM\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u89c9\u53c2\u8003\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u63d0\u793a\u7f16\u7801\u5668\u9884\u6d4b\u591a\u53d8\u91cf\u63d0\u793a\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709SAM\u65b9\u6cd5\u5728\u8fb9\u754c\u751f\u6210\u63d0\u793a\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SAM\u65b9\u6cd5\u5728\u89c6\u89c9\u53c2\u8003\u5206\u5272\u4e2d\u56e0\u63d0\u793a\u7f16\u7801\u5668\u4e0d\u7406\u60f3\u5bfc\u81f4\u8fb9\u754c\u63d0\u793a\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u5206\u5272\u6548\u679c\u3002", "method": "ProSAM\u901a\u8fc7\u5b66\u4e60\u53d8\u5206\u63d0\u793a\u7f16\u7801\u5668\u9884\u6d4b\u591a\u53d8\u91cf\u63d0\u793a\u5206\u5e03\uff0c\u907f\u514d\u5728\u4e0d\u7a33\u5b9a\u533a\u57df\u751f\u6210\u63d0\u793a\u3002", "result": "\u5728Pascal-5$^i$\u548cCOCO-20$^i$\u6570\u636e\u96c6\u4e0a\uff0cProSAM\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProSAM\u4e3a\u89c6\u89c9\u53c2\u8003\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21885", "categories": ["cs.CV", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21885", "abs": "https://arxiv.org/abs/2506.21885", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "comment": "Accepted by IEEE IV 2025", "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7b56\u7565\u3001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u591a\u4f20\u611f\u5668\u878d\u5408\u80fd\u514b\u670d\u5355\u4e00\u4f20\u611f\u5668\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5c06\u591a\u4f20\u611f\u5668\u878d\u5408\u7b56\u7565\u5206\u4e3a\u6570\u636e\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\uff0c\u5e76\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5bf9\u5e94\u65b9\u6cd5\u3002", "result": "\u8ba8\u8bba\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u65b0\u5174\u8d8b\u52bf\uff08\u5982\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u878d\u5408\uff09\u3002", "conclusion": "\u591a\u4f20\u611f\u5668\u878d\u5408\u5728\u63d0\u5347\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.21940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21940", "abs": "https://arxiv.org/abs/2506.21940", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus", "comment": null, "summary": "Variational Quantum Algorithms (VQAs) offer potential for near-term quantum\nadvantage but face challenges from barren plateaus, where gradients vanish, and\npoorly conditioned optimization landscapes. We introduce GuiderNet, a\nmeta-learning framework that conditions Parameterized Quantum Circuits (PQCs)\nusing data-dependent parameter shifts aimed at minimizing the log condition\nnumber of the Fubini-Study metric tensor. Implemented as a classical neural\nnetwork, GuiderNet is meta-trained to guide PQC parameters into geometrically\nfavorable regions and is embedded within hybrid quantum-classical pipelines to\nsteer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces\ncumulative training loss by over 5x, improves test accuracy from 75.3% to\n98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also\nsuppresses gradient explosion and stabilizes parameter updates, enabling\nsmoother and more robust optimization. These results demonstrate that geometric\nmeta-conditioning can mitigate barren plateaus and ill-conditioning, providing\na scalable approach to enhance trainability and generalization in quantum\nmachine learning.", "AI": {"tldr": "GuiderNet\u662f\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u7684\u53c2\u6570\u8c03\u6574\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u51e0\u4f55\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQAs\uff09\u5728\u8fd1\u671f\u91cf\u5b50\u4f18\u52bf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u68af\u5ea6\u6d88\u5931\uff08barren plateaus\uff09\u548c\u4f18\u5316\u6761\u4ef6\u5dee\u7684\u95ee\u9898\u3002", "method": "GuiderNet\u4f5c\u4e3a\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Fubini-Study\u5ea6\u91cf\u5f20\u91cf\u7684\u5bf9\u6570\u6761\u4ef6\u6570\uff0c\u5f15\u5bfc\u91cf\u5b50\u7535\u8def\u53c2\u6570\u8fdb\u5165\u51e0\u4f55\u6709\u5229\u533a\u57df\u3002", "result": "\u5728\u7cd6\u5c3f\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGuiderNet\u5c06\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece75.3%\u63d0\u5347\u81f398.6%\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u635f\u5931\u548c\u68af\u5ea6\u7206\u70b8\u3002", "conclusion": "\u51e0\u4f55\u5143\u6761\u4ef6\u5316\u53ef\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u548c\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21839", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21839", "abs": "https://arxiv.org/abs/2506.21839", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "comment": null, "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u89c6\u89c9\u5438\u5f15\u3001\u903b\u8f91\u4e25\u5bc6\u4e14\u5bcc\u6709\u6311\u6218\u6027\u7684\u9003\u8131\u623f\u95f4\u8c1c\u9898\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u57fa\u7840\u56fe\u50cf\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u548c\u529f\u80fd\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u6311\u6218\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u9003\u8131\u623f\u95f4\u8c1c\u9898\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u8981\u6c42\u5176\u89c6\u89c9\u5438\u5f15\u3001\u903b\u8f91\u4e25\u5bc6\u4e14\u5bcc\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u529f\u80fd\u8bbe\u8ba1\u3001\u7b26\u53f7\u573a\u666f\u56fe\u63a8\u7406\u3001\u5e03\u5c40\u5408\u6210\u548c\u5c40\u90e8\u56fe\u50cf\u7f16\u8f91\u7b49\u9636\u6bb5\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8fed\u4ee3\u53cd\u9988\u786e\u4fdd\u573a\u666f\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u529f\u80fd\u53ef\u89e3\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u9ad8\u4e86\u8f93\u51fa\u8d28\u91cf\uff0c\u5305\u62ec\u53ef\u89e3\u6027\u3001\u907f\u514d\u6377\u5f84\u548c\u529f\u80fd\u6e05\u6670\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u9003\u8131\u623f\u95f4\u8c1c\u9898\u56fe\u50cf\u3002"}}
{"id": "2506.21976", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21976", "abs": "https://arxiv.org/abs/2506.21976", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "comment": "Accepted to CVPR 2025", "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "AI": {"tldr": "CitySim\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578bSceneDiffuser++\uff0c\u7528\u4e8e\u57ce\u5e02\u89c4\u6a21\u7684\u4ea4\u901a\u6a21\u62df\uff0c\u6574\u5408\u4e86\u573a\u666f\u751f\u6210\u3001\u52a8\u6001\u4ee3\u7406\u884c\u4e3a\u5efa\u6a21\u7b49\u6280\u672f\u3002", "motivation": "\u901a\u8fc7\u6a21\u62df\u5927\u91cf\u5408\u6210\u91cc\u7a0b\u6765\u8865\u5145\u6709\u9650\u7684\u771f\u5b9e\u9a7e\u9a76\u91cc\u7a0b\uff0c\u4ee5\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u3002", "method": "\u63d0\u51faSceneDiffuser++\uff0c\u4e00\u79cd\u57fa\u4e8e\u5355\u4e00\u635f\u5931\u51fd\u6570\u7684\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u652f\u6301\u57ce\u5e02\u89c4\u6a21\u7684A\u70b9\u5230B\u70b9\u6a21\u62df\u3002", "result": "\u5728\u6269\u5c55\u7248\u7684Waymo Open Motion Dataset\u4e0a\u9a8c\u8bc1\u4e86SceneDiffuser++\u7684\u57ce\u5e02\u89c4\u6a21\u6a21\u62df\u80fd\u529b\u548c\u957f\u671f\u6a21\u62df\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "SceneDiffuser++\u4e3a\u57ce\u5e02\u89c4\u6a21\u7684\u4ea4\u901a\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u771f\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21952", "categories": ["cs.LG", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.21952", "abs": "https://arxiv.org/abs/2506.21952", "authors": ["Yangyang Wan", "Haotian Wang", "Xuhui Yu", "Jiageng Chen", "Xinyu Fan", "Zuyuan He"], "title": "Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications", "comment": null, "summary": "Distributed acoustic sensing (DAS) has attracted considerable attention\nacross various fields and artificial intelligence (AI) technology plays an\nimportant role in DAS applications to realize event recognition and denoising.\nExisting AI models require real-world data (RWD), whether labeled or not, for\ntraining, which is contradictory to the fact of limited available event data in\nreal-world scenarios. Here, a physics-informed DAS neural network paradigm is\nproposed, which does not need real-world events data for training. By\nphysically modeling target events and the constraints of real world and DAS\nsystem, physical functions are derived to train a generative network for\ngeneration of DAS events data. DAS debackground net is trained by using the\ngenerated DAS events data to eliminate background noise in DAS data. The\neffectiveness of the proposed paradigm is verified in event identification\napplication based on a public dataset of DAS spatiotemporal data and in belt\nconveyor fault monitoring application based on DAS time-frequency data, and\nachieved comparable or better performance than data-driven networks trained\nwith RWD. Owing to the introduction of physical information and capability of\nbackground noise removal, the paradigm demonstrates generalization in same\napplication on different sites. A fault diagnosis accuracy of 91.8% is achieved\nin belt conveyor field with networks which transferred from simulation test\nsite without any fault events data of test site and field for training. The\nproposed paradigm is a prospective solution to address significant obstacles of\ndata acquisition and intense noise in practical DAS applications and explore\nmore potential fields for DAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684DAS\u795e\u7ecf\u7f51\u7edc\u8303\u5f0f\uff0c\u65e0\u9700\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u8bad\u7ec3\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210\u6570\u636e\uff0c\u5e76\u5728\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3DAS\u5e94\u7528\u4e2d\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210DAS\u4e8b\u4ef6\u6570\u636e\uff0c\u8bad\u7ec3\u751f\u6210\u7f51\u7edc\u548c\u53bb\u80cc\u666f\u7f51\u7edc\uff0c\u5e94\u7528\u4e8e\u4e8b\u4ef6\u8bc6\u522b\u548c\u6545\u969c\u76d1\u6d4b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd1\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u6545\u969c\u8bca\u65ad\u51c6\u786e\u7387\u8fbe91.8%\u3002", "conclusion": "\u8be5\u8303\u5f0f\u89e3\u51b3\u4e86DAS\u6570\u636e\u83b7\u53d6\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21843", "abs": "https://arxiv.org/abs/2506.21843", "authors": ["Yuxiang Ge", "Jionghao Cheng", "Ruiquan Ge", "Zhaojie Fang", "Gangyong Jia", "Xiang Wan", "Nannan Li", "Ahmed Elazab", "Changmiao Wang"], "title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "comment": null, "summary": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds\nsignificant potential for applications in Brain-Computer Interfaces (BCIs) and\naiding individuals with communication disorders. Traditionally, efforts have\nfocused on converting brain activity into 2D images, neglecting the translation\nof EEG data into 3D objects. This limitation is noteworthy, as the human brain\ninherently processes three-dimensional spatial information regardless of\nwhether observing 2D images or the real world. The neural activities captured\nby EEG contain rich spatial information that is inevitably lost when\nreconstructing only 2D images, thus limiting its practical applications in BCI.\nThe transition from EEG data to 3D object reconstruction faces considerable\nobstacles. These include the presence of extensive noise within EEG signals and\na scarcity of datasets that include both EEG and 3D information, which\ncomplicates the extraction process of 3D visual data. Addressing this\nchallenging task, we propose an innovative EEG encoder architecture that\nintegrates a dual self-attention mechanism. We use a hybrid training strategy\nto train the EEG Encoder, which includes cross-attention, contrastive learning,\nand self-supervised learning techniques. Additionally, by employing stable\ndiffusion as a prior distribution and utilizing Variational Score Distillation\nto train a neural radiation field, we successfully generate 3D objects with\nsimilar content and structure from EEG data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684EEG\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u53cc\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u4eceEEG\u6570\u636e\u91cd\u5efa3D\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5c06\u8111\u7535\u6d3b\u52a8\u8f6c\u6362\u4e3a2D\u56fe\u50cf\uff0c\u5ffd\u7565\u4e863D\u7a7a\u95f4\u4fe1\u606f\u7684\u91cd\u5efa\uff0c\u9650\u5236\u4e86BCI\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cc\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684EEG\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u7a33\u5b9a\u6269\u6563\u548c\u53d8\u5206\u8bc4\u5206\u84b8\u998f\u8bad\u7ec3\u795e\u7ecf\u8f90\u5c04\u573a\u3002", "result": "\u6210\u529f\u4eceEEG\u6570\u636e\u751f\u6210\u5177\u6709\u76f8\u4f3c\u5185\u5bb9\u548c\u7ed3\u6784\u76843D\u7269\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eceEEG\u6570\u636e\u91cd\u5efa3D\u89c6\u89c9\u523a\u6fc0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u62d3\u5c55\u4e86BCI\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22191", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22191", "abs": "https://arxiv.org/abs/2506.22191", "authors": ["Yuxin Cui", "Rui Song", "Yibin Li", "Max Q. -H. Meng", "Zhe Min"], "title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints", "comment": "ICRA 2025", "summary": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u89d22D/3D\u521a\u6027\u914d\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u4ea4\u53c9\u89c6\u89d2\u7ea6\u675f\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u89c6\u89d2\u672f\u4e2d\u56fe\u50cf\u89c6\u91ce\u6709\u9650\u7684\u95ee\u9898\uff0c\u63d0\u53472D/3D\u914d\u51c6\u7684\u51c6\u786e\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bbe\u8ba1\u8054\u5408\u635f\u5931\u51fd\u6570\u548c\u4ea4\u53c9\u89c6\u89d2\u8bad\u7ec3\u635f\u5931\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u7ec6\u5316\u4f30\u8ba1\u59ff\u6001\u3002", "result": "\u5728DeepFluoro\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.79\u00b12.17 mm\u7684\u5e73\u5747\u76ee\u6807\u914d\u51c6\u8bef\u5dee\uff08mTRE\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u89c6\u89d2\u7ea6\u675f\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.21956", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21956", "abs": "https://arxiv.org/abs/2506.21956", "authors": ["Hao Jiang", "Yongxiang Tang", "Yanxiang Zeng", "Pengjia Yuan", "Yanhua Cheng", "Teng Sha", "Xialong Liu", "Peng Jiang"], "title": "Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement", "comment": null, "summary": "In the realm of online advertising, advertisers partake in ad auctions to\nobtain advertising slots, frequently taking advantage of auto-bidding tools\nprovided by demand-side platforms. To improve the automation of these bidding\nsystems, we adopt generative models, namely the Decision Transformer (DT), to\ntackle the difficulties inherent in automated bidding. Applying the Decision\nTransformer to the auto-bidding task enables a unified approach to sequential\nmodeling, which efficiently overcomes short-sightedness by capturing long-term\ndependencies between past bidding actions and user behavior. Nevertheless,\nconventional DT has certain drawbacks: (1) DT necessitates a preset\nreturn-to-go (RTG) value before generating actions, which is not inherently\nproduced; (2) The policy learned by DT is restricted by its training data,\nwhich is consists of mixed-quality trajectories. To address these challenges,\nwe introduce the R* Decision Transformer (R* DT), developed in a three-step\nprocess: (1) R DT: Similar to traditional DT, R DT stores actions based on\nstate and RTG value, as well as memorizing the RTG for a given state using the\ntraining set; (2) R^ DT: We forecast the highest value (within the training\nset) of RTG for a given state, deriving a suboptimal policy based on the\ncurrent state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,\nwe generate trajectories and select those with high rewards (using a simulator)\nto augment our training dataset. This data enhancement has been shown to\nimprove the RTG of trajectories in the training data and gradually leads the\nsuboptimal policy towards optimality. Comprehensive tests on a publicly\navailable bidding dataset validate the R* DT's efficacy and highlight its\nsuperiority when dealing with mixed-quality trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u51b3\u7b56\u53d8\u6362\u5668\uff08R* DT\uff09\uff0c\u7528\u4e8e\u5728\u7ebf\u5e7f\u544a\u81ea\u52a8\u7ade\u4ef7\u4efb\u52a1\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548cRTG\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u5728\u81ea\u52a8\u7ade\u4ef7\u4efb\u52a1\u4e2d\u7684\u9884\u8bbeRTG\u503c\u9650\u5236\u548c\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4e0d\u5747\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faR* DT\uff0c\u5206\u4e09\u6b65\uff1aR DT\u5b58\u50a8\u72b6\u6001\u548cRTG\u503c\uff1bR^ DT\u9884\u6d4b\u6700\u4f18RTG\u503c\uff1bR* DT\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u4f18\u5316\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u516c\u5f00\u7ade\u4ef7\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86R* DT\u7684\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u6df7\u5408\u8d28\u91cf\u8f68\u8ff9\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "R* DT\u901a\u8fc7\u6539\u8fdbRTG\u751f\u6210\u548c\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21851", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.21851", "abs": "https://arxiv.org/abs/2506.21851", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdRGB-IR\u56fe\u50cf\u5bf9\u7684\u8054\u5408\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u71b5\u6a21\u578b\uff08CCEM\uff09\u548c\u4f4e\u9891\u4e0a\u4e0b\u6587\u63d0\u53d6\u4e0e\u878d\u5408\u5757\uff08LCEB\u548cLCFB\uff09\u4f18\u5316\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u968f\u7740RGB-IR\u56fe\u50cf\u5bf9\u5728\u667a\u80fd\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u6210\u500d\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u9053\u7ea7\u8de8\u6a21\u6001\u71b5\u6a21\u578b\uff08CCEM\uff09\uff0c\u5305\u542b\u4f4e\u9891\u4e0a\u4e0b\u6587\u63d0\u53d6\u5757\uff08LCEB\uff09\u548c\u4f4e\u9891\u4e0a\u4e0b\u6587\u878d\u5408\u5757\uff08LCFB\uff09\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u805a\u5408\u8de8\u6a21\u6001\u7684\u4f4e\u9891\u4fe1\u606f\uff0c\u4f18\u5316\u71b5\u53c2\u6570\u9884\u6d4b\u3002", "result": "\u5728LLVIP\u548cKAIST\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684RGB-IR\u56fe\u50cf\u5bf9\u548c\u5355\u6a21\u6001\u538b\u7f29\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728LLVIP\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8623.1%\u7684\u6bd4\u7279\u7387\u8282\u7701\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u8de8\u6a21\u6001\u5148\u9a8c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-IR\u56fe\u50cf\u5bf9\u7684\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2506.22365", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22365", "abs": "https://arxiv.org/abs/2506.22365", "authors": ["Tao Li", "Haozhe Lei", "Mingsheng Yin", "Yaqi Hu"], "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation", "comment": "Spotlight paper at Reinforcement Learning Conference 2025, Workshop\n  on Inductive Biases in Reinforcement Learning", "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b26\u53f7\u5316\u65b9\u6cd5PiPRL\uff0c\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4eba\u5de5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002PiPRL\u65e8\u5728\u901a\u8fc7\u7b26\u53f7\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u7684\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u7a0b\u5e8f\uff08\u7f16\u7801\u7269\u7406\u5148\u9a8c\uff09\u548c\u795e\u7ecf\u63a7\u5236\u5668\uff0c\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "PiPRL\u5728\u5ba4\u5185\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7eaf\u7b26\u53f7\u6216\u795e\u7ecf\u7b56\u7565\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1126%\u3002", "conclusion": "PiPRL\u901a\u8fc7\u7b26\u53f7\u5316\u65b9\u6cd5\u6709\u6548\u6574\u5408\u7269\u7406\u5148\u9a8c\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.21855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21855", "abs": "https://arxiv.org/abs/2506.21855", "authors": ["Jiho Choi", "Sang Jun Lee"], "title": "Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation", "comment": null, "summary": "In this paper, we propose a method that learns a general representation of\nperiodic signals from unlabeled facial videos by capturing subtle changes in\nskin tone over time. The proposed framework employs the video masked\nautoencoder to learn a high-dimensional spatio-temporal representation of the\nfacial region through self-supervised learning. Capturing quasi-periodic\nsignals in the video is crucial for remote photoplethysmography (rPPG)\nestimation. To account for signal periodicity, we apply frame masking in terms\nof video sampling, which allows the model to capture resampled quasi-periodic\nsignals during the pre-training stage. Moreover, the framework incorporates\nphysiological bandlimit constraints, leveraging the property that physiological\nsignals are sparse within their frequency bandwidth to provide pulse cues to\nthe model. The pre-trained encoder is then transferred to the rPPG task, where\nit is used to extract physiological signals from facial videos. We evaluate the\nproposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and\nV4V datasets. Our results demonstrate significant performance improvements,\nparticularly in challenging cross-dataset evaluations. Our code is available at\nhttps://github.com/ziiho08/Periodic-MAE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u65e0\u6807\u7b7e\u9762\u90e8\u89c6\u9891\u4e2d\u5b66\u4e60\u5468\u671f\u6027\u4fe1\u53f7\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\u672f\uff08rPPG\uff09\u4f30\u8ba1\u3002", "motivation": "\u6355\u6349\u9762\u90e8\u89c6\u9891\u4e2d\u76ae\u80a4\u8272\u8c03\u7684\u5fae\u5999\u53d8\u5316\u4ee5\u63d0\u53d6\u751f\u7406\u4fe1\u53f7\uff0c\u89e3\u51b3rPPG\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u89c6\u9891\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u65f6\u7a7a\u8868\u793a\uff0c\u7ed3\u5408\u5e27\u63a9\u7801\u548c\u751f\u7406\u5e26\u5bbd\u9650\u5236\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u53d6\u5468\u671f\u6027\u751f\u7406\u4fe1\u53f7\uff0c\u4e3arPPG\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22423", "categories": ["cs.LG", "cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22423", "abs": "https://arxiv.org/abs/2506.22423", "authors": ["Pritam Dash", "Ethan Chan", "Nathan P. Lawrence", "Karthik Pattabiraman"], "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.", "AI": {"tldr": "ARMOR\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u653b\u51fb\u5f39\u6027\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5b66\u4e60\u9c81\u68d2\u7684\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5728\u4f20\u611f\u5668\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "motivation": "\u65e0\u4eba\u673a\u4f20\u611f\u5668\u6613\u53d7\u7269\u7406\u653b\u51fb\uff08\u5982GPS\u6b3a\u9a97\uff09\uff0c\u4f20\u7edf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6b64\u65e0\u6548\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u3002", "method": "ARMOR\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u7279\u6743\u653b\u51fb\u4fe1\u606f\u8bad\u7ec3\u6559\u5e08\u7f16\u7801\u5668\u751f\u6210\u653b\u51fb\u611f\u77e5\u6f5c\u5728\u72b6\u6001\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5b66\u751f\u7f16\u7801\u5668\uff0c\u4ec5\u4f7f\u7528\u5386\u53f2\u4f20\u611f\u5668\u6570\u636e\u8fd1\u4f3c\u6559\u5e08\u6f5c\u5728\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660eARMOR\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5b89\u5168\uff0c\u63d0\u9ad8\u5bf9\u672a\u89c1\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "ARMOR\u4e3a\u65e0\u4eba\u673a\u5728\u5bf9\u6297\u6027\u4f20\u611f\u5668\u653b\u51fb\u4e0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21997", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "pdf": "https://arxiv.org/pdf/2506.21997", "abs": "https://arxiv.org/abs/2506.21997", "authors": ["Rafael Sojo", "Javier D\u00edaz-Rozo", "Concha Bielza", "Pedro Larra\u00f1aga"], "title": "Binned semiparametric Bayesian networks", "comment": null, "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u534a\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u5206\u7bb1\u964d\u4f4e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u89e3\u51b3\u4e86\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u975e\u53c2\u6570\u5206\u5e03\u4e2d\u7684\u6838\u5bc6\u5ea6\u4f30\u8ba1\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u4f20\u7edf\u5206\u7bb1\u6a21\u578b\u53d7\u7ef4\u5ea6\u707e\u96be\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff08\u7a00\u758f\u5206\u7bb1\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5085\u91cc\u53f6\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff09\uff0c\u5229\u7528\u7a00\u758f\u5f20\u91cf\u548c\u9650\u5236\u7236\u8282\u70b9\u6570\u91cf\u89e3\u51b3\u7ef4\u5ea6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u7ed3\u6784\u5b66\u4e60\u548c\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u4e0a\u4e0e\u65e0\u5206\u7bb1\u7248\u672c\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.21857", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21857", "abs": "https://arxiv.org/abs/2506.21857", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "comment": null, "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "AI": {"tldr": "SPADE\u662f\u4e00\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u521b\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u75c5\u7406\u5b66\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u53d1\u5c55\u4e2d\uff0c\u6574\u5408\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u4e0e\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u6570\u636e\u7684\u5168\u9762\u65b9\u6cd5\u4ecd\u5b58\u5728\u7a7a\u767d\uff0c\u8fd9\u5bf9\u4e8e\u6355\u6349\u5206\u5b50\u5f02\u8d28\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "SPADE\u91c7\u7528\u6df7\u5408\u6570\u636e\u4e13\u5bb6\u6280\u672f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7279\u5f81\u7a7a\u95f4\u805a\u7c7b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b66\u4e60\u5171\u914d\u51c6\u7684WSI\u5757\u548c\u57fa\u56e0\u8868\u8fbe\u8c31\u7684\u8868\u793a\u3002", "result": "\u5728HEST-1k\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cSPADE\u572814\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u5c11\u6837\u672c\u6027\u80fd\u3002", "conclusion": "SPADE\u901a\u8fc7\u6574\u5408\u5f62\u6001\u5b66\u548c\u5206\u5b50\u4fe1\u606f\u5230\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.22004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22004", "abs": "https://arxiv.org/abs/2506.22004", "authors": ["Mohammad Sabbaqi", "Riccardo Taormina", "Elvin Isufi"], "title": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning", "comment": null, "summary": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u56fe\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u4e86\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002", "motivation": "\u56fe\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u57ce\u5e02\u6c34\u7f51\u7edc\u3001\u7ecf\u6d4e\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\uff09\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u80fd\u6355\u6349\u56fe-\u65f6\u95f4\u6a21\u5f0f\u7684\u6a21\u578b\u3002", "method": "\u6a21\u578b\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u9690\u72b6\u6001\u65b9\u7a0b\u548c\u89c2\u6d4b\u65b9\u7a0b\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u63a8\u65ad\u53c2\u6570\uff0c\u5e76\u8fdb\u4e00\u6b65\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6709\u6548\u5b66\u4e60\u53c2\u6570\u5e76\u8ddf\u8e2a\u72b6\u6001\uff0c\u9002\u7528\u4e8e\u9884\u6d4b\u548c\u63d2\u8865\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u56fe\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2506.21862", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.21862", "abs": "https://arxiv.org/abs/2506.21862", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "comment": "21 pages, 4 figures, 7 tables", "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "AI": {"tldr": "LLaVA-Scissor\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4ee4\u724c\u538b\u7f29\u7b56\u7565\uff0c\u7528\u4e8e\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u8fde\u901a\u7ec4\u4ef6\uff08SCC\uff09\u5b9e\u73b0\u5168\u9762\u7684\u8bed\u4e49\u8986\u76d6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u5f97\u5206\u7684\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u6240\u6709\u8bed\u4e49\u533a\u57df\u4e14\u6613\u5bfc\u81f4\u5197\u4f59\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u538b\u7f29\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eSCC\u7684\u4e24\u6b65\u65f6\u7a7a\u4ee4\u724c\u538b\u7f29\u7b56\u7565\uff0c\u5c06\u4ee4\u724c\u5206\u914d\u5230\u4e0d\u540c\u7684\u8bed\u4e49\u533a\u57df\uff0c\u5b9e\u73b0\u975e\u91cd\u53e0\u8bed\u4e49\u4ee4\u724c\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4f4e\u4ee4\u724c\u4fdd\u7559\u7387\u4e0b\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "LLaVA-Scissor\u901a\u8fc7SCC\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4ee4\u724c\u538b\u7f29\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22008", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22008", "abs": "https://arxiv.org/abs/2506.22008", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gissl\u00e9n"], "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "AI": {"tldr": "TROFI\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u5373\u53ef\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6570\u636e\u96c6\u901a\u5e38\u9700\u8981\u5956\u52b1\u51fd\u6570\u6807\u6ce8\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u6e38\u620f\u5f00\u53d1\uff09\u5956\u52b1\u51fd\u6570\u53ef\u80fd\u4e0d\u53ef\u7528\u3002", "method": "TROFI\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u7684\u5956\u52b1\u51fd\u6570\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7136\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u6700\u4f18\u8f68\u8ff9\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTROFI\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f7f\u7528\u771f\u5b9e\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\u76f8\u5f53\u3002\u57283D\u6e38\u620f\u73af\u5883\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5956\u52b1\u51fd\u6570\u7684\u8bbe\u8ba1\u5bf9\u503c\u51fd\u6570\u4e0e\u5b9e\u9645\u672a\u6765\u6298\u6263\u5956\u52b1\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.21863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21863", "abs": "https://arxiv.org/abs/2506.21863", "authors": ["Sungjune Park", "Yeongyun Kim", "Se Yeon Kim", "Yong Man Ro"], "title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "comment": "13 pages including reference pages, 7 tables, and 6 figures", "summary": "Large Vision and Language Models (LVLMs) have shown strong performance across\nvarious vision-language tasks in natural image domains. However, their\napplication to remote sensing (RS) remains underexplored due to significant\ndomain differences in visual appearances, object scales, and semantics. These\ndiscrepancies hider the effective understanding of RS scenes, which contain\nrich, multi-level semantic information spanning from coarse-to-fine levels.\nHence, it limits the direct adaptation of existing LVLMs to RS imagery. To\naddress this gap, we propose a novel LVLM framework tailored for RS\nunderstanding, incorporating two core components: Semantic-augmented\nMulti-level Alignment and Semantic-aware Expert Modeling. First, to align\nmulti-level visual features, we introduce the retrieval-based Semantic\nAugmentation Module which enriches the visual features with relevant semantics\nacross fine-to-coarse levels (e.g., object- and scene-level information). It is\ndesigned to retrieve relevant semantic cues from a RS semantic knowledge\ndatabase, followed by aggregation of semantic cues with user query and\nmulti-level visual features, resulting in semantically enriched representation\nacross multiple levels. Second, for Semantic-aware Expert Modeling, we design\nsemantic experts, where each expert is responsible for processing semantic\nrepresentation at different levels separately. This enables hierarchical\nsemantic understanding from coarse to fine levels. Evaluations across multiple\nRS tasks-including scene classification and VQA, etc.-demonstrate that the\nproposed framework achieves consistent improvements across multiple semantic\nlevels. This highlights its capability and effectiveness in bridging the gap\nbetween general LVLMs and unique demands of RS-specific vision-language\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9065\u611f\uff08RS\uff09\u56fe\u50cf\u7406\u89e3\u7684\u65b0\u578b\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u7684\u591a\u7ea7\u5bf9\u9f50\u548c\u8bed\u4e49\u611f\u77e5\u4e13\u5bb6\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u901a\u7528LVLM\u5728RS\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u901a\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u81ea\u7136\u56fe\u50cf\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u9065\u611f\u56fe\u50cf\u5728\u89c6\u89c9\u5916\u89c2\u3001\u5bf9\u8c61\u5c3a\u5ea6\u548c\u8bed\u4e49\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5176\u76f4\u63a5\u5e94\u7528\u4e8eRS\u9886\u57df\u6548\u679c\u53d7\u9650\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u68c0\u7d22\u7684\u8bed\u4e49\u589e\u5f3a\u6a21\u5757\uff0c\u7528\u4e8e\u591a\u7ea7\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff1b2\uff09\u8bed\u4e49\u611f\u77e5\u4e13\u5bb6\u5efa\u6a21\uff0c\u5206\u522b\u5904\u7406\u4e0d\u540c\u5c42\u6b21\u7684\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2aRS\u4efb\u52a1\uff08\u5982\u573a\u666f\u5206\u7c7b\u548c\u89c6\u89c9\u95ee\u7b54\uff09\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u591a\u4e2a\u8bed\u4e49\u5c42\u6b21\u7684\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u586b\u8865\u4e86\u901a\u7528LVLM\u4e0eRS\u7279\u5b9a\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c55\u793a\u4e86\u5176\u5728RS\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u80fd\u529b\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.22036", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.22036", "abs": "https://arxiv.org/abs/2506.22036", "authors": ["Ying Zhang", "Yu Zhao", "Xuhui Sui", "Baohang Zhou", "Xiangrui Cai", "Li Shen", "Xiaojie Yuan", "Dacheng Tao"], "title": "Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion", "comment": "Submitted to the IEEE for possible publication", "summary": "With the increasing multimodal knowledge privatization requirements,\nmultimodal knowledge graphs in different institutes are usually decentralized,\nlacking of effective collaboration system with both stronger reasoning ability\nand transmission safety guarantees. In this paper, we propose the Federated\nMultimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over\nfederated MKGs for better predicting the missing links in clients without\nsharing sensitive knowledge. We propose a framework named MMFeD3-HidE for\naddressing multimodal uncertain unavailability and multimodal client\nheterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed\nHyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete\nmultimodal distributions from incomplete entity embeddings constrained by\navailable modalities. (2) Among clients, our proposed Multimodal FeDerated Dual\nDistillation (MMFeD3) transfers knowledge mutually between clients and the\nserver with logit and feature distillation to improve both global convergence\nand semantic consistency. We propose a FedMKGC benchmark for a comprehensive\nevaluation, consisting of a general FedMKGC backbone named MMFedE, datasets\nwith heterogeneous multimodal information, and three groups of constructed\nbaselines. Experiments conducted on our benchmark validate the effectiveness,\nsemantic consistency, and convergence robustness of MMFeD3-HidE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\uff08FedMKGC\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86MMFeD3-HidE\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u77e5\u8bc6\u79c1\u6709\u5316\u9700\u6c42\u7684\u589e\u52a0\uff0c\u5206\u6563\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u6709\u6548\u7684\u534f\u4f5c\u7cfb\u7edf\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u8f93\u5b89\u5168\u4fdd\u969c\u3002", "method": "\u63d0\u51faHidE\u6a21\u578b\u6062\u590d\u4e0d\u5b8c\u6574\u5b9e\u4f53\u5d4c\u5165\u7684\u591a\u6a21\u6001\u5206\u5e03\uff0c\u5e76\u8bbe\u8ba1MMFeD3\u6846\u67b6\u901a\u8fc7\u7279\u5f81\u548clogit\u84b8\u998f\u5b9e\u73b0\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u77e5\u8bc6\u4f20\u9012\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MMFeD3-HidE\u7684\u6709\u6548\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6536\u655b\u9c81\u68d2\u6027\u3002", "conclusion": "MMFeD3-HidE\u4e3a\u8054\u90a6\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21866", "abs": "https://arxiv.org/abs/2506.21866", "authors": ["Yanguang Sun", "Jiexi Yan", "Jianjun Qian", "Chunyan Xu", "Jian Yang", "Lei Luo"], "title": "Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images", "comment": "Accepted by IJCAI 2025", "summary": "Automatically segmenting objects from optical remote sensing images (ORSIs)\nis an important task. Most existing models are primarily based on either\nconvolutional or Transformer features, each offering distinct advantages.\nExploiting both advantages is valuable research, but it presents several\nchallenges, including the heterogeneity between the two types of features, high\ncomplexity, and large parameters of the model. However, these issues are often\noverlooked in existing the ORSIs methods, causing sub-optimal segmentation. For\nthat, we propose a novel Dual-Perspective United Transformer (DPU-Former) with\na unique structure designed to simultaneously integrate long-range dependencies\nand spatial details. In particular, we design the global-local mixed attention,\nwhich captures diverse information through two perspectives and introduces a\nFourier-space merging strategy to obviate deviations for efficient fusion.\nFurthermore, we present a gated linear feed-forward network to increase the\nexpressive ability. Additionally, we construct a DPU-Former decoder to\naggregate and strength features at different layers. Consequently, the\nDPU-Former model outperforms the state-of-the-art methods on multiple datasets.\nCode: https://github.com/CSYSI/DPU-Former.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u89c6\u89d2\u7edf\u4e00Transformer\uff08DPU-Former\uff09\uff0c\u7528\u4e8e\u5149\u5b66\u9065\u611f\u56fe\u50cf\uff08ORSIs\uff09\u7684\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u548cTransformer\u7279\u5f81\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709ORSIs\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5377\u79ef\u6216Transformer\u7279\u5f81\uff0c\u4f46\u672a\u80fd\u6709\u6548\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5bfc\u81f4\u5206\u5272\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u5168\u5c40-\u5c40\u90e8\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u5085\u91cc\u53f6\u7a7a\u95f4\u5408\u5e76\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u95e8\u63a7\u7ebf\u6027\u524d\u9988\u7f51\u7edc\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u6784\u5efa\u4e86DPU-Former\u89e3\u7801\u5668\u3002", "result": "DPU-Former\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "DPU-Former\u901a\u8fc7\u7ed3\u5408\u957f\u7a0b\u4f9d\u8d56\u548c\u7a7a\u95f4\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u4e86ORSIs\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2506.22039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22039", "abs": "https://arxiv.org/abs/2506.22039", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "comment": null, "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "AI": {"tldr": "UniCA\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5f02\u6784\u534f\u53d8\u91cf\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u4ec5\u9488\u5bf9\u5b9e\u503c\u5e8f\u5217\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5305\u542b\u5f02\u6784\u534f\u53d8\u91cf\uff08\u5982\u5206\u7c7b\u53d8\u91cf\u3001\u591a\u6a21\u6001\u6570\u636e\uff09\u7684\u9884\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51faUniCA\u6846\u67b6\uff0c\u5305\u62ec\u534f\u53d8\u91cf\u540c\u8d28\u5316\u548c\u7edf\u4e00\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u5c06\u5f02\u6784\u534f\u53d8\u91cf\u8f6c\u5316\u4e3a\u540c\u8d28\u5e8f\u5217\u8868\u793a\u5e76\u878d\u5408\u3002", "result": "\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u534f\u53d8\u91cf\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86UniCA\u7684\u6709\u6548\u6027\u3002", "conclusion": "UniCA\u4e3a\u534f\u53d8\u91cf\u611f\u77e5\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21873", "abs": "https://arxiv.org/abs/2506.21873", "authors": ["Tzu-Chun Chien", "Chieh-Kai Lin", "Shiang-Feng Tsai", "Ruei-Chi Lai", "Hung-Jen Chen", "Min Sun"], "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u4f1a\u663e\u8457\u964d\u4f4e\u5176\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u4f4d\u7f6eID\u9519\u4f4d\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u63d0\u51faGAP\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6062\u590d90%\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u4f1a\u4e25\u91cd\u524a\u5f31\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "method": "\u63d0\u51faGrounding-Aware Token Pruning\uff08GAP\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u4f4d\u7f6eID\u89e3\u51b3\u526a\u679d\u540e\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "result": "GAP\u65b9\u6cd5\u5728RefCOCO\u9a8c\u8bc1\u96c6\u4e0a\u5c06LLaVA\u7684\u51c6\u786e\u7387\u4ece\u526a\u679d\u540e\u768415.34%\u6062\u590d\u523051.42%\uff0c\u63a5\u8fd1\u539f\u59cb\u6027\u80fd\u768490%\u3002", "conclusion": "GAP\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u548c\u526a\u679d\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.22049", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22049", "abs": "https://arxiv.org/abs/2506.22049", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPAS\u7684\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3Pre-LN Transformer\u4e2d\u6fc0\u6d3b\u65b9\u5dee\u6307\u6570\u589e\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7f29\u653e\u4e2d\u95f4\u6fc0\u6d3b\u503c\u4f46\u4fdd\u6301\u68af\u5ea6\u4e0d\u53d8\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Pre-LN Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u6fc0\u6d3b\u65b9\u5dee\u6307\u6570\u589e\u957f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6b8b\u5dee\u8def\u5f84\u4e3b\u5bfc\u5b50\u5c42\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u6df1\u5c42\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faGradient-Preserving Activation Scaling (GPAS)\uff0c\u901a\u8fc7\u7f29\u653e\u4e2d\u95f4\u6fc0\u6d3b\u503c\u4f46\u4fdd\u6301\u68af\u5ea6\u4e0d\u53d8\uff0c\u907f\u514d\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u572871M\u52301B\u7684\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\uff0cGPAS\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u9002\u7528\u4e8e\u5176\u4ed6\u67b6\u6784\u5982Sandwich-LN\u548cDeepNorm\u3002", "conclusion": "GPAS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u6539\u5584Pre-LN Transformer\u53ca\u5176\u4ed6\u67b6\u6784\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21883", "abs": "https://arxiv.org/abs/2506.21883", "authors": ["Basudha Pal", "Sharif Amit Kamran", "Brendon Lutnick", "Molly Lucas", "Chaitanya Parmar", "Asha Patel Shah", "David Apfel", "Steven Fakharzadeh", "Lloyd Miller", "Gabriela Cula", "Kristopher Standish"], "title": "GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification", "comment": null, "summary": "Psoriasis (PsO) severity scoring is important for clinical trials but is\nhindered by inter-rater variability and the burden of in person clinical\nevaluation. Remote imaging using patient captured mobile photos offers\nscalability but introduces challenges, such as variation in lighting,\nbackground, and device quality that are often imperceptible to humans but can\nimpact model performance. These factors, along with inconsistencies in\ndermatologist annotations, reduce the reliability of automated severity\nscoring. We propose a framework to automatically flag problematic training\nimages that introduce spurious correlations which degrade model generalization,\nusing a gradient based interpretability approach. By tracing the gradients of\nmisclassified validation images, we detect training samples where model errors\nalign with inconsistently rated examples or are affected by subtle, nonclinical\nartifacts. We apply this method to a ConvNeXT based weakly supervised model\ndesigned to classify PsO severity from phone images. Removing 8.2% of flagged\nimages improves model AUC-ROC by 5% (85% to 90%) on a held out test set.\nCommonly, multiple annotators and an adjudication process ensure annotation\naccuracy, which is expensive and time consuming. Our method detects training\nimages with annotation inconsistencies, potentially removing the need for\nmanual review. When applied to a subset of training data rated by two\ndermatologists, the method identifies over 90% of cases with inter-rater\ndisagreement by reviewing only the top 30% of samples. This improves automated\nscoring for remote assessments, ensuring robustness despite data collection\nvariability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u89e3\u91ca\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u6807\u8bb0\u8bad\u7ec3\u56fe\u50cf\u4e2d\u7684\u95ee\u9898\u6837\u672c\uff0c\u4ee5\u63d0\u5347\u94f6\u5c51\u75c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u94f6\u5c51\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5206\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b58\u5728\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\u548c\u4e34\u5e8a\u8bc4\u4f30\u8d1f\u62c5\u7684\u95ee\u9898\u3002\u8fdc\u7a0b\u6210\u50cf\u867d\u5177\u6269\u5c55\u6027\uff0c\u4f46\u53d7\u5149\u7167\u3001\u80cc\u666f\u548c\u8bbe\u5907\u8d28\u91cf\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u964d\u4f4e\u4e86\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u8ffd\u8e2a\u8bef\u5206\u7c7b\u9a8c\u8bc1\u56fe\u50cf\u7684\u68af\u5ea6\uff0c\u68c0\u6d4b\u8bad\u7ec3\u6837\u672c\u4e2d\u4e0e\u4e0d\u4e00\u81f4\u8bc4\u5206\u6216\u975e\u4e34\u5e8a\u4f2a\u5f71\u76f8\u5173\u7684\u9519\u8bef\u3002\u5e94\u7528\u4e8e\u57fa\u4e8eConvNeXT\u7684\u5f31\u76d1\u7763\u6a21\u578b\u3002", "result": "\u79fb\u96648.2%\u7684\u95ee\u9898\u56fe\u50cf\u540e\uff0c\u6a21\u578bAUC-ROC\u63d0\u53475%\uff0885%\u81f390%\uff09\u3002\u5728\u53cc\u76ae\u80a4\u79d1\u533b\u751f\u8bc4\u5206\u7684\u6570\u636e\u5b50\u96c6\u4e2d\uff0c\u65b9\u6cd5\u8bc6\u522b\u51fa90%\u4ee5\u4e0a\u8bc4\u5206\u4e0d\u4e00\u81f4\u7684\u6848\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u9700\u6c42\uff0c\u63d0\u5347\u8fdc\u7a0b\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22055", "abs": "https://arxiv.org/abs/2506.22055", "authors": ["Mehul Gautam"], "title": "crypto price prediction using lstm+xgboost", "comment": null, "summary": "The volatility and complex dynamics of cryptocurrency markets present unique\nchallenges for accurate price forecasting. This research proposes a hybrid deep\nlearning and machine learning model that integrates Long Short-Term Memory\n(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency\nprice prediction. The LSTM component captures temporal dependencies in\nhistorical price data, while XGBoost enhances prediction by modeling nonlinear\nrelationships with auxiliary features such as sentiment scores and\nmacroeconomic indicators. The model is evaluated on historical datasets of\nBitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and\nlocalized exchange data. Comparative analysis using Mean Absolute Percentage\nError (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)\ndemonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone\nmodels and traditional forecasting methods. This study underscores the\npotential of hybrid architectures in financial forecasting and provides\ninsights into model adaptability across different cryptocurrencies and market\ncontexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cXGBoost\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u9884\u6d4b\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u6ce2\u52a8\u6027\u548c\u590d\u6742\u6027\u5bf9\u4ef7\u683c\u9884\u6d4b\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u6a21\u578b\u3002", "method": "\u7ed3\u5408LSTM\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548cXGBoost\u5efa\u6a21\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u5229\u7528\u5386\u53f2\u4ef7\u683c\u6570\u636e\u548c\u8f85\u52a9\u7279\u5f81\uff08\u5982\u60c5\u611f\u5206\u6570\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\uff09\u3002", "result": "\u5728\u6bd4\u7279\u5e01\u3001\u4ee5\u592a\u574a\u7b49\u52a0\u5bc6\u8d27\u5e01\u6570\u636e\u96c6\u4e0a\uff0c\u6df7\u5408\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u72ec\u7acb\u6a21\u578b\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u52a0\u5bc6\u8d27\u5e01\u548c\u5e02\u573a\u73af\u5883\u3002"}}
{"id": "2506.22084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22084", "abs": "https://arxiv.org/abs/2506.22084", "authors": ["Chaitanya K. Joshi"], "title": "Transformers are Graph Neural Networks", "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "AI": {"tldr": "Transformer\u67b6\u6784\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8868\u793a\u5b66\u4e60\u4e0a\u7684\u8054\u7cfb\u88ab\u63ed\u793a\uff0cTransformer\u53ef\u89c6\u4e3a\u5728\u5b8c\u5168\u8fde\u63a5\u56fe\u4e0a\u64cd\u4f5c\u7684\u6d88\u606f\u4f20\u9012GNN\u3002", "motivation": "\u63a2\u7d22Transformer\u4e0eGNNs\u4e4b\u95f4\u7684\u6570\u5b66\u8054\u7cfb\uff0c\u63ed\u793a\u5176\u4f5c\u4e3a\u9ad8\u6548\u96c6\u5408\u5904\u7406\u7f51\u7edc\u7684\u6f5c\u529b\u3002", "method": "\u5c06Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u91ca\u4e3aGNN\u4e2d\u7684\u6d88\u606f\u4f20\u9012\uff0c\u5e76\u5206\u6790\u5176\u786c\u4ef6\u6548\u7387\u4f18\u52bf\u3002", "result": "Transformer\u901a\u8fc7\u5bc6\u96c6\u77e9\u9635\u64cd\u4f5c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\uff0c\u786c\u4ef6\u9002\u5e94\u6027\u4f18\u4e8e\u4f20\u7edf\u7a00\u758f\u6d88\u606f\u4f20\u9012\u7684GNN\u3002", "conclusion": "Transformer\u662f\u5f53\u524d\u786c\u4ef6\u73af\u5883\u4e0b\u66f4\u9ad8\u6548\u7684GNN\u5b9e\u73b0\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.21891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21891", "abs": "https://arxiv.org/abs/2506.21891", "authors": ["Umihiro Kamoto", "Tatsuya Ishibashi", "Noriyuki Kugo"], "title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "comment": null, "summary": "In this report, we present the winning solution that achieved the 1st place\nin the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This\nchallenge evaluates the ability to generate accurate natural language answers\nto questions about diverse, real-world video clips. It uses the Complex Video\nReasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists\nof 214 unique videos and 2,400 question-answer pairs spanning 11 categories.\nOur method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative\nreasoning approach, in which each input question is semantically decomposed and\nsolved through stepwise reasoning and progressive inference. This enables our\nsystem to provide highly accurate and contextually appropriate answers to even\nthe most complex queries. Applied to the CVRR-ES benchmark, our approach\nachieves 81.44% accuracy on the test set, securing the top position among all\nparticipants. This report details our methodology and provides a comprehensive\nanalysis of the experimental results, demonstrating the effectiveness of our\niterative reasoning framework in achieving robust video question answering. The\ncode is available at https://github.com/PanasonicConnect/DIVE", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57282025\u5e74\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e0e\u9c81\u68d2\u6027\u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u7684\u89e3\u51b3\u65b9\u6848DIVE\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u5b9e\u73b0\u4e8681.44%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u591a\u6837\u5316\u89c6\u9891\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u91c7\u7528DIVE\uff08\u6df1\u5ea6\u641c\u7d22\u8fed\u4ee3\u89c6\u9891\u63a2\u7d22\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u548c\u9010\u6b65\u63a8\u7406\u5b9e\u73b0\u590d\u6742\u95ee\u9898\u7684\u89e3\u7b54\u3002", "result": "\u5728CVRR-ES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523081.44%\u7684\u51c6\u786e\u7387\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "DIVE\u7684\u8fed\u4ee3\u63a8\u7406\u6846\u67b6\u5728\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22095", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22095", "abs": "https://arxiv.org/abs/2506.22095", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Bal\u00e1zs Kulcs\u00e1r"], "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "comment": "18 pages, 5 Figures", "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u591a\u56fe\u8def\u7531\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u591a\u56fe\u73af\u5883\u4e0b\u7684\u591a\u76ee\u6807\u8def\u5f84\u89c4\u5212\u95ee\u9898\u5177\u6709\u5b9e\u9645\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u7b2c\u4e00\u79cd\u65b9\u6cd5\u76f4\u63a5\u5728\u591a\u56fe\u4e0a\u81ea\u56de\u5f52\u9009\u62e9\u8fb9\uff1b\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5148\u5c06\u591a\u56fe\u526a\u679d\u4e3a\u7b80\u5355\u56fe\u518d\u6784\u5efa\u8def\u5f84\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728TSP\u548cCVRP\u7b49\u591a\u79cd\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u591a\u56fe\u8def\u7531\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.21892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21892", "abs": "https://arxiv.org/abs/2506.21892", "authors": ["Adam Goodge", "Xun Xu", "Bryan Hooi", "Wee Siong Ng", "Jingyi Liao", "Yongyi Su", "Xulei Yang"], "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "comment": null, "summary": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSODA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u90bb\u57df\u8bc4\u5206\u4f20\u64ad\u65b9\u6848\u6539\u8fdb\u70b9\u4e91\u6570\u636e\u7684OOD\u68c0\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u70b9\u4e91\u6570\u636e\u7684OOD\u68c0\u6d4b\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8f83\u5c11\uff0c\u4e14\u9884\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u5408\u6210\u5230\u73b0\u5b9e\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u5229\u75283D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff083D VLMs\uff09\uff0c\u63d0\u51faSODA\u65b9\u6cd5\uff0c\u901a\u8fc7\u90bb\u57df\u8bc4\u5206\u4f20\u64ad\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\u3002", "result": "SODA\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u95ee\u9898\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SODA\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u5230\u73b0\u5b9e\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91OOD\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22096", "abs": "https://arxiv.org/abs/2506.22096", "authors": ["Tin Lai", "Farnaz Farid", "Yueyang Kuan", "Xintian Zhang"], "title": "Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments", "comment": null, "summary": "Detecting heavy metal pollution in soils and seaports is vital for regional\nenvironmental monitoring. The Pollution Load Index (PLI), an international\nstandard, is commonly used to assess heavy metal containment. However, the\nconventional PLI assessment involves laborious procedures and data analysis of\nsediment samples. To address this challenge, we propose a deep-learning-based\nmodel that simplifies the heavy metal assessment process. Our model tackles the\nissue of data scarcity in the water-sediment domain, which is traditionally\nplagued by challenges in data collection and varying standards across nations.\nBy leveraging transfer learning, we develop an accurate quantitative assessment\nmethod for predicting PLI. Our approach allows the transfer of learned features\nacross domains with different sets of features. We evaluate our model using\ndata from six major ports in New South Wales, Australia: Port Yamba, Port\nNewcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results\ndemonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute\nPercentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared\nto other models. Our model performance is up to 2 orders of magnitude than\nother baseline models. Our proposed model offers an innovative, accessible, and\ncost-effective approach to predicting water quality, benefiting marine life\nconservation, aquaculture, and industrial pollution monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u7b80\u5316\u91cd\u91d1\u5c5e\u6c61\u67d3\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPLI\u65b9\u6cd5\u7684\u7e41\u7410\u95ee\u9898\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u7684\u6c34\u57df-\u6c89\u79ef\u7269\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfPLI\u8bc4\u4f30\u65b9\u6cd5\u7e41\u7410\u4e14\u6570\u636e\u6536\u96c6\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9a\u91cf\u8bc4\u4f30PLI\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u5dee\u5f02\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u6fb3\u5927\u5229\u4e9a\u516d\u4e2a\u4e3b\u8981\u6e2f\u53e3\u7684\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMAE\u548cMAPE\u5206\u522b\u7ea6\u4e3a0.5\u548c0.03\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u6c34\u8d28\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u3001\u6613\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u6d77\u6d0b\u751f\u6001\u4fdd\u62a4\u548c\u6c34\u4ea7\u517b\u6b96\u7b49\u884c\u4e1a\u3002"}}
{"id": "2506.21895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21895", "abs": "https://arxiv.org/abs/2506.21895", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Gang Wang", "Bing Liu", "Zhenan Sun"], "title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning", "comment": null, "summary": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5fae\u8c03\u7684\u4eba\u8138\u53cd\u6b3a\u9a97\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4f9d\u8d56\u6570\u636e\u6a21\u5f0f\u8bb0\u5fc6\uff0c\u4ee5\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u53ef\u9a8c\u8bc1\u7684\u7c7b\u522b\u4e00\u81f4\u5956\u52b1\u548c\u63a8\u7406\u4e00\u81f4\u5956\u52b1\uff0c\u91c7\u7528GRPO\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bd5\u9519\u5b66\u4e60\u63d0\u53d6\u9ad8\u6cdb\u5316\u6027\u51b3\u7b56\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u672a\u77e5\u653b\u51fb\u7c7b\u578b\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u4f9d\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u53cd\u6b3a\u9a97\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002"}}
{"id": "2506.22129", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22129", "abs": "https://arxiv.org/abs/2506.22129", "authors": ["Anurag Panda", "Gaurav Kumar Yadav"], "title": "Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models", "comment": "3rd International Conference on Applied Mathematics in Science and\n  Engineering", "summary": "In the aftermath of major earthquakes, evaluating structural and\ninfrastructural damage is vital for coordinating post-disaster response\nefforts. This includes assessing damage's extent and spatial distribution to\nprioritize rescue operations and resource allocation. Accurately estimating\ndamage grades to buildings post-earthquake is paramount for effective response\nand recovery, given the significant impact on lives and properties,\nunderscoring the urgency of streamlining relief fund allocation processes.\nPrevious studies have shown the effectiveness of multi-class classification,\nespecially XGBoost, along with other machine learning models and ensembling\nmethods, incorporating regularization to address class imbalance. One\nconsequence of class imbalance is that it may give rise to skewed models that\nundervalue minority classes and give preference to the majority class. This\nresearch deals with the problem of class imbalance with the help of the\nsynthetic minority oversampling technique (SMOTE). We delve into multiple\nmulti-class classification machine learning, deep learning models, and\nensembling methods to forecast structural damage grades. The study elucidates\nperformance determinants through comprehensive feature manipulation experiments\nand diverse training approaches. It identifies key factors contributing to\nseismic vulnerability while evaluating model performance using techniques like\nthe confusion matrix further to enhance understanding of the effectiveness of\nearthquake damage prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982XGBoost\u548cSMOTE\uff09\u9884\u6d4b\u5730\u9707\u540e\u5efa\u7b51\u7ed3\u6784\u635f\u574f\u7b49\u7ea7\uff0c\u4ee5\u4f18\u5316\u707e\u540e\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u5730\u9707\u540e\u5feb\u901f\u51c6\u786e\u8bc4\u4f30\u5efa\u7b51\u635f\u574f\u7b49\u7ea7\u5bf9\u6551\u63f4\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528SMOTE\u6280\u672f\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u6bd4\u8f83\u591a\u79cd\u591a\u5206\u7c7b\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7279\u5f81\u5b9e\u9a8c\u548c\u8bad\u7ec3\u65b9\u6cd5\u4f18\u5316\uff0c\u786e\u5b9a\u4e86\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u8bc4\u4f30\u4e86\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408SMOTE\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u9884\u6d4b\u5730\u9707\u635f\u574f\u7b49\u7ea7\uff0c\u4e3a\u707e\u540e\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.21903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21903", "abs": "https://arxiv.org/abs/2506.21903", "authors": ["Dipayan Biswas", "Shishir Shah", "Jaspal Subhlok"], "title": "Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment", "comment": "This is an extended version of a paper accepted to MIPR 2025", "summary": "Video is transforming education with online courses and recorded lectures\nsupplementing and replacing classroom teaching. Recent research has focused on\nenhancing information retrieval for video lectures with advanced navigation,\nsearchability, summarization, as well as question answering chatbots. Visual\nelements like tables, charts, and illustrations are central to comprehension,\nretention, and data presentation in lecture videos, yet their full potential\nfor improving access to video content remains underutilized. A major factor is\nthat accurate automatic detection of visual elements in a lecture video is\nchallenging; reasons include i) most visual elements, such as charts, graphs,\ntables, and illustrations, are artificially created and lack any standard\nstructure, and ii) coherent visual objects may lack clear boundaries and may be\ncomposed of connected text and visual components. Despite advancements in deep\nlearning based object detection, current models do not yield satisfactory\nperformance due to the unique nature of visual content in lectures and scarcity\nof annotated datasets. This paper reports on a transfer learning approach for\ndetecting visual elements in lecture video frames. A suite of state of the art\nobject detection models were evaluated for their performance on lecture video\ndatasets. YOLO emerged as the most promising model for this task. Subsequently\nYOLO was optimized for lecture video object detection with training on multiple\nbenchmark datasets and deploying a semi-supervised auto labeling strategy.\nResults evaluate the success of this approach, also in developing a general\nsolution to the problem of object detection in lecture videos. Paper\ncontributions include a publicly released benchmark of annotated lecture video\nframes, along with the source code to facilitate future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u4f18\u5316\u4e86YOLO\u6a21\u578b\uff0c\u5e76\u516c\u5f00\u4e86\u6807\u6ce8\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\uff08\u5982\u56fe\u8868\u3001\u8868\u683c\uff09\u5bf9\u7406\u89e3\u548c\u68c0\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u68c0\u6d4b\u8fd9\u4e9b\u5143\u7d20\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u6807\u51c6\u7ed3\u6784\u548c\u6807\u6ce8\u6570\u636e\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u4f18\u5316YOLO\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u534a\u76d1\u7763\u81ea\u52a8\u6807\u6ce8\u7b56\u7565\u3002", "result": "YOLO\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5e76\u5f00\u53d1\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.22186", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22186", "abs": "https://arxiv.org/abs/2506.22186", "authors": ["Kaikai Zheng", "Dawei Shi", "Yang Shi", "Long Wang"], "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems", "comment": null, "summary": "Thompson sampling (TS) is an effective method to explore parametric\nuncertainties and can therefore be used for active learning-based controller\ndesign. However, TS relies on finite parametric representations, which limits\nits applicability to more general spaces, which are more commonly encountered\nin control system design. To address this issue, this work pro poses a\nparameterization method for control law learning using reproducing kernel\nHilbert spaces and designs a data-driven active learning control approach.\nSpecifically, the proposed method treats the control law as an element in a\nfunction space, allowing the design of control laws without imposing\nrestrictions on the system structure or the form of the controller. A TS\nframework is proposed in this work to explore potential optimal control laws,\nand the convergence guarantees are further provided for the learning process.\nTheoretical analysis shows that the proposed method learns the relationship\nbetween control laws and closed-loop performance metrics at an exponential\nrate, and the upper bound of control regret is also derived. Numerical\nexperiments on controlling unknown nonlinear systems validate the effectiveness\nof the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3b\u52a8\u5b66\u4e60\u63a7\u5236\u8bbe\u8ba1\uff0c\u6269\u5c55\u4e86Thompson\u91c7\u6837\u7684\u9002\u7528\u8303\u56f4\u3002", "motivation": "Thompson\u91c7\u6837\u4f9d\u8d56\u4e8e\u6709\u9650\u53c2\u6570\u8868\u793a\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u4e00\u822c\u7a7a\u95f4\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u3002", "method": "\u4f7f\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u53c2\u6570\u5316\u63a7\u5236\u5f8b\uff0c\u8bbe\u8ba1\u6570\u636e\u9a71\u52a8\u7684\u4e3b\u52a8\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u65e0\u9700\u9650\u5236\u7cfb\u7edf\u7ed3\u6784\u6216\u63a7\u5236\u5668\u5f62\u5f0f\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u6307\u6570\u901f\u7387\u5b66\u4e60\u63a7\u5236\u5f8b\u4e0e\u95ed\u73af\u6027\u80fd\u6307\u6807\u7684\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfc\u4e86\u63a7\u5236\u9057\u61be\u7684\u4e0a\u754c\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6269\u5c55\u4e86Thompson\u91c7\u6837\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u63a7\u5236\u5f8b\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2506.21905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21905", "abs": "https://arxiv.org/abs/2506.21905", "authors": ["Mingquan Liu"], "title": "RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network", "comment": null, "summary": "Fine Grained Visual Categorization (FGVC) remains a challenging task in\ncomputer vision due to subtle inter class differences and fragile feature\nrepresentations. Existing methods struggle in fine grained scenarios,\nespecially when labeled data is scarce. We propose a semi supervised method\ncombining Mamba based feature modeling, region attention, and Bayesian\nuncertainty. Our approach enhances local to global feature modeling while\nfocusing on key areas during learning. Bayesian inference selects high quality\npseudo labels for stability. Experiments show strong performance on FGVC\nbenchmarks with occlusions, demonstrating robustness when labeled data is\nlimited. Code is available at https://github.com/wxqnl/RAUM Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Mamba\u7279\u5f81\u5efa\u6a21\u3001\u533a\u57df\u6ce8\u610f\u529b\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\uff08FGVC\uff09\uff0c\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u56e0\u7c7b\u95f4\u5dee\u5f02\u7ec6\u5fae\u548c\u7279\u5f81\u8868\u793a\u8106\u5f31\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408Mamba\u7279\u5f81\u5efa\u6a21\u3001\u533a\u57df\u6ce8\u610f\u529b\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\uff0c\u589e\u5f3a\u5c40\u90e8\u5230\u5168\u5c40\u7279\u5f81\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u9009\u62e9\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "result": "\u5728FGVC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.22189", "categories": ["cs.LG", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22189", "abs": "https://arxiv.org/abs/2506.22189", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sj\u00f6lund"], "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6a21\u5757\u5316\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u548c\u4ee3\u7406\u7c7b\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0Claude\u548cGPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u901a\u5e38\u4f18\u4e8e\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff0c\u4f46\u7ed3\u679c\u56e0\u95ee\u9898\u548c\u6a21\u578b\u800c\u5f02\u3002", "motivation": "\u63a2\u7d22LLM\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6a21\u5757\u5316\u6f5c\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4e0d\u540cLLM\u548c\u4ee3\u7406\u7c7b\u578b\uff08\u5de5\u5177\u8c03\u7528\u4e0e\u4ee3\u7801\u751f\u6210\uff09\u7684\u6027\u80fd\uff0c\u4f7f\u7528LLM-as-a-judge\u8bc4\u5206\u3002", "result": "Claude-3.5-Sonnet\u3001Claude-3.7-Sonnet\u548cGPT-4o\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5e73\u5747\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u7ed3\u679c\u56e0\u95ee\u9898\u548c\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u5757\u5316\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5f00\u53d1\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21909", "abs": "https://arxiv.org/abs/2506.21909", "authors": ["Justin Reinman", "Sunwoong Choi"], "title": "CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability", "comment": null, "summary": "CERBERUS is a synthetic benchmark designed to help train and evaluate AI\nmodels for detecting cracks and other defects in infrastructure. It includes a\ncrack image generator and realistic 3D inspection scenarios built in Unity. The\nbenchmark features two types of setups: a simple Fly-By wall inspection and a\nmore complex Underpass scene with lighting and geometry challenges. We tested a\npopular object detection model (YOLO) using different combinations of synthetic\nand real crack data. Results show that combining synthetic and real data\nimproves performance on real-world images. CERBERUS provides a flexible,\nrepeatable way to test defect detection systems and supports future research in\nautomated infrastructure inspection. CERBERUS is publicly available at\nhttps://github.com/justinreinman/Cerberus-Defect-Generator.", "AI": {"tldr": "CERBERUS\u662f\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u68c0\u6d4b\u57fa\u7840\u8bbe\u65bd\u88c2\u7f1d\u7684AI\u6a21\u578b\uff0c\u5305\u542b\u88c2\u7f1d\u56fe\u50cf\u751f\u6210\u5668\u548c3D\u573a\u666f\u3002\u6d4b\u8bd5\u8868\u660e\uff0c\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e3a\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u63d0\u4f9b\u7075\u6d3b\u3001\u53ef\u91cd\u590d\u7684\u6d4b\u8bd5\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "method": "\u4f7f\u7528Unity\u6784\u5efa3D\u573a\u666f\uff0c\u751f\u6210\u5408\u6210\u88c2\u7f1d\u6570\u636e\uff0c\u7ed3\u5408YOLO\u6a21\u578b\u6d4b\u8bd5\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u6df7\u5408\u6548\u679c\u3002", "result": "\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "CERBERUS\u4e3a\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.22190", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22190", "abs": "https://arxiv.org/abs/2506.22190", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "title": "dreaMLearning: Data Compression Assisted Machine Learning", "comment": "18 pages, 11 figures", "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "AI": {"tldr": "dreaMLearning\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u6570\u636e\u76f4\u63a5\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u9700\u6c42\u95ee\u9898\u3002", "method": "\u57fa\u4e8eEntroGeDe\u538b\u7f29\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u4efb\u52a1\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u53478.8\u500d\uff0c\u5185\u5b58\u548c\u5b58\u50a8\u9700\u6c42\u5927\u5e45\u964d\u4f4e\uff0c\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "conclusion": "dreaMLearning\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2506.21912", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.21912", "abs": "https://arxiv.org/abs/2506.21912", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "comment": null, "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u8bed\u4e49\u548c\u4eba\u7c7b\u5c5e\u6027\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u548c\u5c5e\u6027\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u5f15\u5165\u5e26\u5c5e\u6027\u6807\u6ce8\u7684\u6570\u636e\u96c6HumanAttr\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u7684\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001\u4f53\u91cd\u3001\u8eab\u9ad8\uff09\u5bf9\u52a8\u4f5c\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u8bed\u4e49\u4e0e\u4eba\u7c7b\u5c5e\u6027\u89e3\u8026\uff0c\u652f\u6301\u6587\u672c\u5230\u8bed\u4e49\u7684\u9884\u6d4b\u548c\u5c5e\u6027\u63a7\u5236\u7684\u751f\u6210\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u7528\u6237\u6587\u672c\u548c\u5c5e\u6027\u8f93\u5165\u4e00\u81f4\u7684\u771f\u5b9e\u52a8\u4f5c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u5c5e\u6027\u611f\u77e5\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u8bbe\u5b9a\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2506.22199", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22199", "abs": "https://arxiv.org/abs/2506.22199", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "REDELEX: A Framework for Relational Deep Learning Exploration", "comment": "Accepted to ECMLPKDD 2025 at Porto, Portugal", "summary": "Relational databases (RDBs) are widely regarded as the gold standard for\nstoring structured information. Consequently, predictive tasks leveraging this\ndata format hold significant application promise. Recently, Relational Deep\nLearning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized\nas graph structures, enabling the application of various graph neural\narchitectures to effectively address these tasks. However, given its novelty,\nthere is a lack of analysis into the relationships between the performance of\nvarious RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for\nevaluating RDL models of varying complexity on the most diverse collection of\nover 70 RDBs, which we make available to the community. Benchmarked alongside\nkey representatives of classic methods, we confirm the generally superior\nperformance of RDL while providing insights into the main factors shaping\nperformance, including model complexity, database sizes and their structural\nproperties.", "AI": {"tldr": "REDELEX\u6846\u67b6\u8bc4\u4f30\u4e86\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u6570\u636e\u5e93\u7279\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0e\u5e95\u5c42RDB\u7279\u6027\u5173\u7cfb\u7684\u5206\u6790\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u63d0\u51faREDELEX\u6846\u67b6\uff0c\u8bc4\u4f30\u591a\u79cdRDL\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u7ecf\u5178\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "RDL\u8868\u73b0\u666e\u904d\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u6027\u80fd\u53d7\u6a21\u578b\u590d\u6742\u5ea6\u3001\u6570\u636e\u5e93\u5927\u5c0f\u53ca\u7ed3\u6784\u7279\u6027\u5f71\u54cd\u3002", "conclusion": "REDELEX\u4e3aRDL\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u5168\u9762\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2506.21920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21920", "abs": "https://arxiv.org/abs/2506.21920", "authors": ["Nam Quan Nguyen", "Xuan Phong Pham", "Tuan-Anh Tran"], "title": "SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition", "comment": null, "summary": "The automated reconstruction of the logical arrangement of tables from image\ndata, termed Table Structure Recognition (TSR), is fundamental for semantic\ndata extraction. Recently, researchers have explored a wide range of techniques\nto tackle this problem, demonstrating significant progress. Each table is a set\nof vertical and horizontal separators. Following this realization, we present\nSepFormer, which integrates the split-and-merge paradigm into a single step\nthrough separator regression with a DETR-style architecture, improving speed\nand robustness. SepFormer is a coarse-to-fine approach that predicts table\nseparators from single-line to line-strip separators with a stack of two\ntransformer decoders. In the coarse-grained stage, the model learns to\ngradually refine single-line segments through decoder layers with additional\nangle loss. At the end of the fine-grained stage, the model predicts line-strip\nseparators by refining sampled points from each single-line segment. Our\nSepFormer can run on average at 25.6 FPS while achieving comparable performance\nwith state-of-the-art methods on several benchmark datasets, including SciTSR,\nPubTabNet, WTW, and iFLYTAB.", "AI": {"tldr": "SepFormer\u662f\u4e00\u79cd\u57fa\u4e8eDETR\u67b6\u6784\u7684\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b65\u5206\u9694\u7b26\u56de\u5f52\u5b9e\u73b0\u5feb\u901f\u4e14\u9c81\u68d2\u7684\u8868\u683c\u91cd\u6784\u3002", "motivation": "\u8868\u683c\u7ed3\u6784\u8bc6\u522b\uff08TSR\uff09\u662f\u8bed\u4e49\u6570\u636e\u63d0\u53d6\u7684\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "SepFormer\u91c7\u7528\u5206\u5408\u5e76\u8303\u5f0f\uff0c\u901a\u8fc7\u4e24\u4e2aTransformer\u89e3\u7801\u5668\u5806\u53e0\uff0c\u4ece\u5355\u7ebf\u5230\u7ebf\u5e26\u5206\u9694\u7b26\u9010\u6b65\u7ec6\u5316\u9884\u6d4b\u3002", "result": "SepFormer\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523025.6 FPS\u7684\u901f\u5ea6\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "SepFormer\u901a\u8fc7\u5355\u6b65\u5206\u9694\u7b26\u56de\u5f52\u548c\u5206\u5408\u5e76\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22200", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22200", "abs": "https://arxiv.org/abs/2506.22200", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "AI": {"tldr": "EFRame\u6846\u67b6\u901a\u8fc7\u63a2\u7d22\u3001\u8fc7\u6ee4\u548c\u56de\u653e\u673a\u5236\u589e\u5f3aGRPO\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "GRPO\u5728\u63a2\u7d22\u3001\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "EFRame\u901a\u8fc7\u989d\u5916\u63a2\u7d22\u3001\u5728\u7ebf\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\u548c\u5229\u7528\u7ecf\u9a8c\u56de\u653e\uff0c\u6784\u5efa\u5b8c\u6574\u7a33\u5b9a\u7684\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEFRame\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u89e3\u9501\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "EFRame\u4e0d\u4ec5\u4f18\u5316\u4e86GRPO\u7684\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u8bad\u7ec3\u6837\u672c\u8d21\u732e\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002"}}
{"id": "2506.21923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21923", "abs": "https://arxiv.org/abs/2506.21923", "authors": ["Juming Xiong", "Ruining Deng", "Jialin Yue", "Siqi Lu", "Junlin Guo", "Marilyn Lionts", "Tianyuan Yao", "Can Cui", "Junchao Zhu", "Chongyu Qu", "Mengmeng Yin", "Haichun Yang", "Yuankai Huo"], "title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction", "comment": null, "summary": "Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D", "AI": {"tldr": "ZeroReg3D\u662f\u4e00\u79cd\u65b0\u578b\u96f6\u6837\u672c\u6ce8\u518c\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u8fde\u7eed\u7ec4\u7ec7\u5207\u7247\u4e2d\u6784\u5efa\u7cbe\u786e\u76843D\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7ec4\u7ec7\u53d8\u5f62\u3001\u5207\u7247\u4f2a\u5f71\u548c\u5149\u7167\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u76842D\u7ec4\u7ec7\u5b66\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u5173\u952e\u76843D\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e14\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\uff0c\u975e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u96f6\u6837\u672c\u6df1\u5ea6\u5b66\u4e60\u5173\u952e\u70b9\u5339\u914d\u4e0e\u57fa\u4e8e\u4f18\u5316\u7684\u4eff\u5c04\u548c\u975e\u521a\u6027\u914d\u51c6\u6280\u672f\u3002", "result": "\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u7ec7\u53d8\u5f62\u3001\u5207\u7247\u4f2a\u5f71\u3001\u67d3\u8272\u53d8\u5f02\u548c\u5149\u7167\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "conclusion": "ZeroReg3D\u4e3a\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u76843D\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22253", "abs": "https://arxiv.org/abs/2506.22253", "authors": ["Shunta Nonaga", "Koji Tabata", "Yuta Mizuno", "Tamiki Komatsuzaki"], "title": "Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence", "comment": null, "summary": "Decision making under uncertain environments in the maximization of expected\nreward while minimizing its risk is one of the ubiquitous problems in many\nsubjects. Here, we introduce a novel problem setting in stochastic bandit\noptimization that jointly addresses two critical aspects of decision-making:\nmaximizing expected reward and minimizing associated uncertainty, quantified\nvia the mean-variance(MV) criterion. Unlike traditional bandit formulations\nthat focus solely on expected returns, our objective is to efficiently and\naccurately identify the Pareto-optimal set of arms that strikes the best\ntrade-off between expected performance and risk. We propose a unified\nmeta-algorithmic framework capable of operating under both fixed-confidence and\nfixed-budget regimes, achieved through adaptive design of confidence intervals\ntailored to each scenario using the same sample exploration strategy. We\nprovide theoretical guarantees on the correctness of the returned solutions in\nboth settings. To complement this theoretical analysis, we conduct extensive\nempirical evaluations across synthetic benchmarks, demonstrating that our\napproach outperforms existing methods in terms of both accuracy and sample\nefficiency, highlighting its broad applicability to risk-aware decision-making\ntasks in uncertain environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u8bbe\u7f6e\uff0c\u65e8\u5728\u540c\u65f6\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\u548c\u6700\u5c0f\u5316\u98ce\u9669\uff08\u901a\u8fc7\u5747\u503c-\u65b9\u5dee\u51c6\u5219\uff09\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5143\u7b97\u6cd5\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u4e24\u79cd\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u51b3\u7b56\u65f6\u540c\u65f6\u4f18\u5316\u671f\u671b\u5956\u52b1\u548c\u98ce\u9669\u7684\u95ee\u9898\uff0c\u5f25\u8865\u4f20\u7edf\u8001\u864e\u673a\u65b9\u6cd5\u4ec5\u5173\u6ce8\u671f\u671b\u56de\u62a5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u5143\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bbe\u8ba1\u7f6e\u4fe1\u533a\u95f4\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u4e24\u79cd\u573a\u666f\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u6837\u672c\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u51c6\u786e\u6027\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u627e\u5230\u671f\u671b\u6027\u80fd\u548c\u98ce\u9669\u4e4b\u95f4\u7684\u6700\u4f18\u6743\u8861\u3002"}}
{"id": "2506.21924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21924", "abs": "https://arxiv.org/abs/2506.21924", "authors": ["Zhao Jin", "Rong-Cheng Tu", "Jingyi Liao", "Wenhao Sun", "Xiao Luo", "Shunyu Liu", "Dacheng Tao"], "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "comment": null, "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "AI": {"tldr": "SPAZER\u7ed3\u54083D\u548c2D\u6a21\u6001\u8fdb\u884c\u6e10\u8fdb\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c3D\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u51cf\u5c11\u5bf9\u6602\u8d353D\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u54083D\u6e32\u67d3\u548c2D\u56fe\u50cf\uff0c\u8fdb\u884c\u951a\u70b9\u5f15\u5bfc\u7b5b\u9009\u548c3D-2D\u8054\u5408\u51b3\u7b56\u3002", "result": "\u5728ScanRefer\u548cNr3D\u57fa\u51c6\u4e0a\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u53479.0%\u548c10.9%\u3002", "conclusion": "SPAZER\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u4e0e\u8bed\u4e49\u63a8\u7406\uff0c\u65e0\u97003D\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9c81\u68d2\u7684\u96f6\u6837\u672c\u5b9a\u4f4d\u3002"}}
{"id": "2506.22255", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22255", "abs": "https://arxiv.org/abs/2506.22255", "authors": ["Maciej Stefaniak", "Micha\u0142 Krutul", "Jan Ma\u0142a\u015bnicki", "Maciej Pi\u00f3ro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "comment": null, "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProjected Compression\u7684\u65b0\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u6295\u5f71\u6a21\u5757\u51cf\u5c11\u6a21\u578b\u6743\u91cd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u53c2\u6570\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u548c\u8ba1\u7b97\u9700\u6c42\u4e0a\u5347\uff0c\u56e0\u6b64\u9700\u8981\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u6295\u5f71\u6743\u91cd\uff0c\u5e76\u5c06\u5176\u5408\u5e76\u4e3a\u4f4e\u7ef4\u4e58\u79ef\u77e9\u9635\uff0c\u4ece\u800c\u51cf\u5c11\u6807\u51c6Transformer\u6a21\u578b\u7684\u5c3a\u5bf8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u8d28\u91cf\u6a21\u578b\u4e0a\u4f18\u4e8e\u786c\u526a\u679d\u548c\u518d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u968ftoken\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "Projected Compression\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002"}}
{"id": "2506.21925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21925", "abs": "https://arxiv.org/abs/2506.21925", "authors": ["Liu Yang", "Huiyu Duan", "Jiarui Wang", "Jing Liu", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "title": "Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images", "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) techniques, AI generated images (AIGIs) have attracted widespread\nattention, among which AI generated omnidirectional images (AIGODIs) hold\nsignificant potential for Virtual Reality (VR) and Augmented Reality (AR)\napplications. AI generated omnidirectional images exhibit unique quality\nissues, however, research on the quality assessment and optimization of\nAI-generated omnidirectional images is still lacking. To this end, this work\nfirst studies the quality assessment and distortion-aware saliency prediction\nproblems for AIGODIs, and further presents a corresponding optimization\nprocess. Specifically, we first establish a comprehensive database to reflect\nhuman feedback for AI-generated omnidirectionals, termed OHF2024, which\nincludes both subjective quality ratings evaluated from three perspectives and\ndistortion-aware salient regions. Based on the constructed OHF2024 database, we\npropose two models with shared encoders based on the BLIP-2 model to evaluate\nthe human visual experience and predict distortion-aware saliency for\nAI-generated omnidirectional images, which are named as BLIP2OIQA and\nBLIP2OISal, respectively. Finally, based on the proposed models, we present an\nautomatic optimization process that utilizes the predicted visual experience\nscores and distortion regions to further enhance the visual quality of an\nAI-generated omnidirectional image. Extensive experiments show that our\nBLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in\nthe human visual experience evaluation task and the distortion-aware saliency\nprediction task for AI generated omnidirectional images, and can be effectively\nused in the optimization process. The database and codes will be released on\nhttps://github.com/IntMeGroup/AIGCOIQA to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u7684\u5168\u666f\u56fe\u50cf\uff08AIGODIs\uff09\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eBLIP-2\u7684\u6a21\u578bBLIP2OIQA\u548cBLIP2OISal\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u4f18\u5316\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740AIGC\u6280\u672f\u7684\u53d1\u5c55\uff0cAI\u751f\u6210\u7684\u5168\u666f\u56fe\u50cf\u5728VR/AR\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u5efa\u7acb\u4e86OHF2024\u6570\u636e\u5e93\uff0c\u5305\u542b\u4e3b\u89c2\u8d28\u91cf\u8bc4\u5206\u548c\u5931\u771f\u663e\u8457\u533a\u57df\uff0c\u5e76\u57fa\u4e8eBLIP-2\u6a21\u578b\u63d0\u51fa\u4e86BLIP2OIQA\u548cBLIP2OISal\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBLIP2OIQA\u548cBLIP2OISal\u5728\u8d28\u91cf\u8bc4\u4f30\u548c\u663e\u8457\u533a\u57df\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6548\u679c\uff0c\u5e76\u80fd\u6709\u6548\u7528\u4e8e\u4f18\u5316\u6d41\u7a0b\u3002", "conclusion": "\u672c\u6587\u4e3aAIGODIs\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6570\u636e\u5e93\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.22295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22295", "abs": "https://arxiv.org/abs/2506.22295", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Score-Based Model for Low-Rank Tensor Recovery", "comment": null, "summary": "Low-rank tensor decompositions (TDs) provide an effective framework for\nmultiway data analysis. Traditional TD methods rely on predefined structural\nassumptions, such as CP or Tucker decompositions. From a probabilistic\nperspective, these can be viewed as using Dirac delta distributions to model\nthe relationships between shared factors and the low-rank tensor. However, such\nprior knowledge is rarely available in practical scenarios, particularly\nregarding the optimal rank structure and contraction rules. The optimization\nprocedures based on fixed contraction rules are complex, and approximations\nmade during these processes often lead to accuracy loss. To address this issue,\nwe propose a score-based model that eliminates the need for predefined\nstructural or distributional assumptions, enabling the learning of\ncompatibility between tensors and shared factors. Specifically, a neural\nnetwork is designed to learn the energy function, which is optimized via score\nmatching to capture the gradient of the joint log-probability of tensor entries\nand shared factors. Our method allows for modeling structures and distributions\nbeyond the Dirac delta assumption. Moreover, integrating the block coordinate\ndescent (BCD) algorithm with the proposed smooth regularization enables the\nmodel to perform both tensor completion and denoising. Experimental results\ndemonstrate significant performance improvements across various tensor types,\nincluding sparse and continuous-time tensors, as well as visual data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u5339\u914d\u7684\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7ed3\u6784\u6216\u5206\u5e03\u5047\u8bbe\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u80fd\u91cf\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f20\u91cf\u5b8c\u6210\u548c\u53bb\u566a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u7ed3\u6784\u5047\u8bbe\uff08\u5982CP\u6216Tucker\u5206\u89e3\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5f80\u5f80\u4e0d\u9002\u7528\uff0c\u4e14\u56fa\u5b9a\u6536\u7f29\u89c4\u5219\u5bfc\u81f4\u4f18\u5316\u590d\u6742\u548c\u7cbe\u5ea6\u635f\u5931\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u4f18\u5316\uff0c\u6355\u6349\u5f20\u91cf\u6761\u76ee\u548c\u5171\u4eab\u56e0\u5b50\u7684\u8054\u5408\u5bf9\u6570\u6982\u7387\u68af\u5ea6\uff0c\u7ed3\u5408\u5757\u5750\u6807\u4e0b\u964d\uff08BCD\uff09\u7b97\u6cd5\u548c\u5e73\u6ed1\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u5f20\u91cf\u3001\u8fde\u7eed\u65f6\u95f4\u5f20\u91cf\u548c\u89c6\u89c9\u6570\u636e\u7b49\u591a\u79cd\u5f20\u91cf\u7c7b\u578b\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u72c4\u62c9\u514b\u03b4\u5206\u5e03\u5047\u8bbe\u7684\u9650\u5236\uff0c\u80fd\u591f\u7075\u6d3b\u5efa\u6a21\u7ed3\u6784\u548c\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u5f20\u91cf\u5b8c\u6210\u548c\u53bb\u566a\u4efb\u52a1\u3002"}}
{"id": "2506.21945", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21945", "abs": "https://arxiv.org/abs/2506.21945", "authors": ["Naftaly Wambugu", "Ruisheng Wang", "Bo Guo", "Tianshu Yu", "Sheng Xu", "Mohammed Elhassan"], "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "comment": null, "summary": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5806\u53e0\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08SDRNet\uff09\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u5dee\u5f02\u3001\u906e\u6321\u548c\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u9762\u4e34\u7c7b\u522b\u5dee\u5f02\u3001\u906e\u6321\u548c\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\u7684\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u63d0\u53d6\u8db3\u591f\u7279\u5f81\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u5806\u53e0\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u548c\u6269\u5f20\u6b8b\u5dee\u5757\uff08DRB\uff09\uff0c\u4ee5\u6355\u83b7\u957f\u8ddd\u79bb\u8bed\u4e49\u5e76\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728ISPRS Vaihingen\u548cPotsdam\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSDRNet\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "SDRNet\u901a\u8fc7\u591a\u4e0a\u4e0b\u6587\u7279\u5f81\u5b66\u4e60\u548c\u5168\u5c40-\u5c40\u90e8\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6548\u679c\u3002"}}
{"id": "2506.22299", "categories": ["cs.LG", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2506.22299", "abs": "https://arxiv.org/abs/2506.22299", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "comment": "icmr", "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "AI": {"tldr": "CoATA\u662f\u4e00\u4e2a\u53cc\u901a\u9053GNN\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u589e\u5f3a\u62d3\u6251\u548c\u5c5e\u6027\u6765\u89e3\u51b3\u56fe\u6570\u636e\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u6570\u636e\u901a\u5e38\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4e00\u7ef4\u5ea6\u7684\u589e\u5f3a\uff0c\u5ffd\u7565\u4e86\u62d3\u6251\u548c\u5c5e\u6027\u4e4b\u95f4\u7684\u6df1\u5c42\u4ea4\u4e92\u3002", "method": "CoATA\u901a\u8fc7\u7ed3\u6784\u4fe1\u53f7\u4f20\u64ad\u589e\u5f3a\u8282\u70b9\u5c5e\u6027\uff0c\u6784\u5efa\u8282\u70b9-\u5c5e\u6027\u4e8c\u5206\u56fe\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u76f8\u4e92\u6821\u6b63\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCoATA\u4f18\u4e8e\u5341\u4e00\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CoATA\u6210\u529f\u6355\u6349\u4e86\u62d3\u6251\u548c\u5c5e\u6027\u4e4b\u95f4\u7684\u534f\u540c\u5173\u7cfb\uff0c\u63d0\u5347\u4e86GNN\u5728\u566a\u58f0\u56fe\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2506.21957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21957", "abs": "https://arxiv.org/abs/2506.21957", "authors": ["Yixin Zha", "Chuxin Wang", "Wenfei Yang", "Tianzhu Zhang"], "title": "Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding", "comment": "Accepted by IJCAI 2025", "summary": "Point cloud understanding aims to acquire robust and general feature\nrepresentations from unlabeled data. Masked point modeling-based methods have\nrecently shown significant performance across various downstream tasks. These\npre-training methods rely on random masking strategies to establish the\nperception of point clouds by restoring corrupted point cloud inputs, which\nleads to the failure of capturing reasonable semantic relationships by the\nself-supervised models. To address this issue, we propose Semantic Masked\nAutoencoder, which comprises two main components: a prototype-based component\nsemantic modeling module and a component semantic-enhanced masking strategy.\nSpecifically, in the component semantic modeling module, we design a component\nsemantic guidance mechanism to direct a set of learnable prototypes in\ncapturing the semantics of different components from objects. Leveraging these\nprototypes, we develop a component semantic-enhanced masking strategy that\naddresses the limitations of random masking in effectively covering complete\ncomponent structures. Furthermore, we introduce a component semantic-enhanced\nprompt-tuning strategy, which further leverages these prototypes to improve the\nperformance of pre-trained models in downstream tasks. Extensive experiments\nconducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart\ndemonstrate the effectiveness of our proposed modules.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u8bed\u4e49\u5efa\u6a21\u548c\u589e\u5f3a\u63a9\u7801\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u968f\u673a\u63a9\u7801\u5728\u70b9\u4e91\u7406\u89e3\u4e2d\u8bed\u4e49\u5173\u7cfb\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u673a\u63a9\u7801\u7b56\u7565\u5728\u70b9\u4e91\u9884\u8bad\u7ec3\u4e2d\u96be\u4ee5\u6355\u6349\u5408\u7406\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u81ea\u76d1\u7763\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u539f\u578b\u8bed\u4e49\u5efa\u6a21\u6a21\u5757\u548c\u8bed\u4e49\u589e\u5f3a\u63a9\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u539f\u578b\u6355\u6349\u7ec4\u4ef6\u8bed\u4e49\uff0c\u5e76\u6539\u8fdb\u63a9\u7801\u7b56\u7565\u4ee5\u8986\u76d6\u5b8c\u6574\u7ec4\u4ef6\u7ed3\u6784\u3002", "result": "\u5728ScanObjectNN\u3001ModelNet40\u548cShapeNetPart\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u7406\u89e3\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6355\u6349\u8bed\u4e49\u5173\u7cfb\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.22301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22301", "abs": "https://arxiv.org/abs/2506.22301", "authors": ["Takumi Okuo", "Shinnosuke Matsuo", "Shota Harada", "Kiyohito Tanaka", "Ryoma Bise"], "title": "Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling", "comment": "Accepted at IJCNN2025", "summary": "Domain shift is a significant challenge in machine learning, particularly in\nmedical applications where data distributions differ across institutions due to\nvariations in data collection practices, equipment, and procedures. This can\ndegrade performance when models trained on source domain data are applied to\nthe target domain. Domain adaptation methods have been widely studied to\naddress this issue, but most struggle when class proportions between the source\nand target domains differ. In this paper, we propose a weakly-supervised domain\nadaptation method that leverages class proportion information from the target\ndomain, which is often accessible in medical datasets through prior knowledge\nor statistical reports. Our method assigns pseudo-labels to the unlabeled\ntarget data based on class proportion (called proportion-constrained\npseudo-labeling), improving performance without the need for additional\nannotations. Experiments on two endoscopic datasets demonstrate that our method\noutperforms semi-supervised domain adaptation techniques, even when 5% of the\ntarget domain is labeled. Additionally, the experimental results with noisy\nproportion labels highlight the robustness of our method, further demonstrating\nits effectiveness in real-world application scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u57df\u7684\u7c7b\u522b\u6bd4\u4f8b\u4fe1\u606f\uff0c\u901a\u8fc7\u6bd4\u4f8b\u7ea6\u675f\u4f2a\u6807\u7b7e\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u9886\u57df\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u7c7b\u522b\u6bd4\u4f8b\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u57fa\u4e8e\u76ee\u6807\u57df\u7c7b\u522b\u6bd4\u4f8b\u4fe1\u606f\uff0c\u5bf9\u672a\u6807\u6ce8\u76ee\u6807\u6570\u636e\u5206\u914d\u4f2a\u6807\u7b7e\uff08\u6bd4\u4f8b\u7ea6\u675f\u4f2a\u6807\u7b7e\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u534a\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5373\u4f7f\u76ee\u6807\u57df\u4ec55%\u6807\u6ce8\u6570\u636e\u3002\u5bf9\u566a\u58f0\u6bd4\u4f8b\u6807\u7b7e\u4e5f\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u533b\u5b66\u573a\u666f\u4e2d\u6709\u6548\u4e14\u7a33\u5065\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7c7b\u522b\u6bd4\u4f8b\u5dee\u5f02\u5927\u7684\u9886\u57df\u81ea\u9002\u5e94\u4efb\u52a1\u3002"}}
{"id": "2506.21975", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21975", "abs": "https://arxiv.org/abs/2506.21975", "authors": ["Meng Yu", "Te Cui", "Qitong Chu", "Wenjie Song", "Yi Yang", "Yufeng Yue"], "title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "comment": "6 pages, accepted for publication in lEEE/RSJ international\n  Conference on Intelligent Robots and Systems (lROS 2025)", "summary": "Reliable semantic segmentation of open environments is essential for\nintelligent systems, yet significant problems remain: 1) Existing RGB-T\nsemantic segmentation models mainly rely on low-level visual features and lack\nhigh-level textual information, which struggle with accurate segmentation when\ncategories share similar visual characteristics. 2) While SAM excels in\ninstance-level segmentation, integrating it with thermal images and text is\nhindered by modality heterogeneity and computational inefficiency. To address\nthese, we propose TASeg, a text-aware RGB-T segmentation framework by using\nLow-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation\nmodels. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the\nimage encoder, which effectively merges features from multiple visual\nmodalities while freezing SAM's original transformer blocks. Additionally, we\nincorporate CLIP-generated text embeddings in the mask decoder to enable\nsemantic alignment, which further rectifies the classification error and\nimproves the semantic understanding accuracy. Experimental results across\ndiverse datasets demonstrate that our method achieves superior performance in\nchallenging scenarios with fewer trainable parameters.", "AI": {"tldr": "TASeg\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u611f\u77e5\u7684RGB-T\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u6280\u672f\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08DFFM\uff09\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u7ed3\u5408CLIP\u6587\u672c\u5d4c\u5165\u6539\u5584\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RGB-T\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u3001\u7f3a\u4e4f\u9ad8\u5c42\u6587\u672c\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4ee5\u53caSAM\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u4f7f\u7528LoRA\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u51faDFFM\u6a21\u5757\u878d\u5408\u591a\u6a21\u6001\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5f15\u5165CLIP\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u4e14\u8bad\u7ec3\u53c2\u6570\u8f83\u5c11\u3002", "conclusion": "TASeg\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u6587\u672c\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.22304", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22304", "abs": "https://arxiv.org/abs/2506.22304", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "comment": null, "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKoopman\u7b97\u5b50\u7406\u8bba\u7684\u6539\u8fdb\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u751f\u6210\u52a8\u6001\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfCFM\u91c7\u6837\u4f9d\u8d56\u975e\u7ebf\u6027ODE\u6c42\u89e3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u89e3\u91ca\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u672a\u63ed\u793a\u751f\u6210\u8fc7\u7a0b\u7684\u7ed3\u6784\u3002", "method": "\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\uff0c\u8bbe\u8ba1\u65e0\u89e3\u7801\u5668\u7684Koopman-CFM\u67b6\u6784\uff0c\u5728\u5b66\u4e60\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u7ebf\u6027\u52a8\u6001\uff0c\u652f\u6301\u95ed\u5f0f\u4e00\u6b65\u91c7\u6837\u3002", "result": "\u57282D\u6570\u636e\u96c6\u548cMNIST\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u52a0\u901f\u91c7\u6837\uff0c\u540c\u65f6\u901a\u8fc7Koopman\u751f\u6210\u5668\u7684\u8c31\u7279\u6027\u63d0\u4f9b\u751f\u6210\u884c\u4e3a\u7684\u5206\u6790\u5de5\u5177\u3002", "conclusion": "Koopman\u589e\u5f3a\u7684CFM\u7ed3\u5408\u4e86\u91c7\u6837\u6548\u7387\u548c\u89e3\u6790\u7ed3\u6784\uff0c\u4e3a\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.21980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21980", "abs": "https://arxiv.org/abs/2506.21980", "authors": ["Biao Wang", "Wenwen Li"], "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "comment": "7 pages, 2 figures", "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u89c6\u89c9\u5355\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5R1-Track\uff0c\u901a\u8fc7GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5fae\u8c03Qwen2.5-VL\uff0c\u5728GOT-10k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u76d1\u7763\u8bad\u7ec3\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800cMLLMs\u5728\u57fa\u7840\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8ddf\u8e2a\u4efb\u52a1\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen2.5-VL\uff0c\u751f\u6210\u6a21\u578bR1-Track\u3002", "result": "R1-Track\u5728GOT-10k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u652f\u6301\u901a\u8fc7\u8fb9\u754c\u6846\u6216\u6587\u672c\u63cf\u8ff0\u7075\u6d3b\u521d\u59cb\u5316\u3002", "conclusion": "R1-Track\u5c55\u793a\u4e86MLLMs\u5728\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u672a\u6765\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2506.22331", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22331", "abs": "https://arxiv.org/abs/2506.22331", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "title": "Less Greedy Equivalence Search", "comment": "35 total pages. 14 figures", "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "AI": {"tldr": "LGES\u662fGES\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u4f18\u5316\u8d2a\u5a6a\u6b65\u9aa4\u548c\u5229\u7528\u5148\u9a8c\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u652f\u6301\u5e72\u9884\u6570\u636e\u3002", "motivation": "GES\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u6837\u672c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cLGES\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LGES\u907f\u514d\u5728\u6761\u4ef6\u72ec\u7acb\u7684\u53d8\u91cf\u95f4\u63d2\u5165\u8fb9\uff0c\u5e76\u5229\u7528\u5148\u9a8c\u5047\u8bbe\u548c\u5e72\u9884\u6570\u636e\u4f18\u5316\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLGES\u6bd4GES\u5feb10\u500d\uff0c\u7ed3\u6784\u9519\u8bef\u66f4\u5c11\uff0c\u4e14\u5bf9\u9519\u8bef\u5047\u8bbe\u66f4\u9c81\u68d2\u3002", "conclusion": "LGES\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u4f18\u4e8eGES\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u3002"}}
{"id": "2506.22007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22007", "abs": "https://arxiv.org/abs/2506.22007", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "comment": "8 pages, 6 figures", "summary": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u957f\u65f6\u7a0b\u89c6\u9891\uff0c\u907f\u514d\u4e86\u81ea\u56de\u5f52\u751f\u6210\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u5173\u952e\u5e27\u751f\u6210\u548c\u8bed\u4e49\u4fdd\u6301\u6a21\u5757\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u56e0\u81ea\u56de\u5f52\u751f\u6210\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "method": "1) \u5c06\u9ad8\u7ea7\u76ee\u6807\u5206\u89e3\u4e3a\u539f\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u5173\u952e\u5e27\uff1b2) \u4f7f\u7528\u6269\u6563\u6a21\u578b\u63d2\u503c\u5173\u952e\u5e27\uff1b3) \u5f15\u5165\u8bed\u4e49\u4fdd\u6301\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u89c6\u9891\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u5e76\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e4b\u524d\u7684\u7b56\u7565\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u89c6\u9891\u751f\u6210\u7684\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2506.22342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22342", "abs": "https://arxiv.org/abs/2506.22342", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "comment": "17 pages, 6 figures", "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d41\u884c\u75c5\u9884\u6d4b\u548c\u4f20\u64ad\u673a\u5236\u5efa\u6a21\uff0c\u540c\u65f6\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\uff09\u3002", "motivation": "\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u5728\u6d41\u884c\u75c5\u5b66\u548c\u516c\u5171\u536b\u751f\u5206\u6790\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u90e8\u5206\u6570\u636e\u654f\u611f\uff0c\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u3002\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u56e0\u5176\u5f3a\u4fdd\u969c\u6210\u4e3a\u6807\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ecDP\u4fdd\u62a4\u7684\u6570\u636e\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u5efa\u6a21\u3002", "result": "\u4f7f\u7528\u5408\u6210\u91d1\u878d\u6570\u636e\u96c6\uff08\u5e26DP\uff09\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5373\u4f7f\u6709DP\u4fdd\u62a4\uff0c\u6570\u636e\u4ecd\u5bf9\u9884\u6d4b\u548c\u5efa\u6a21\u6709\u663e\u8457\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6d41\u884c\u75c5\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u6574\u5408\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86DP\u6570\u636e\u5728\u6d41\u884c\u75c5\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22015", "abs": "https://arxiv.org/abs/2506.22015", "authors": ["Sarthak Ketanbhai Modi", "Lim Zi Pong", "Shourya Kuchhal", "Yoshi Cao", "Yupeng Cheng", "Teo Yon Shin", "Lin Shang-Wei", "Zhiming Li"], "title": "Towards Universal & Efficient Model Compression via Exponential Torque Pruning", "comment": null, "summary": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6307\u6570\u626d\u77e9\u526a\u679d\uff08ETP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u6570\u529b\u5e94\u7528\u65b9\u6848\u6539\u8fdb\u73b0\u6709\u526a\u679d\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u7387\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u5feb\u901f\u589e\u957f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\u3002\u73b0\u6709\u57fa\u4e8e\u626d\u77e9\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u526a\u679d\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u7f51\u7edc\u4ecd\u5bc6\u96c6\u4e14\u51c6\u786e\u6027\u4e0b\u964d\u660e\u663e\u3002", "method": "\u63d0\u51fa\u6307\u6570\u626d\u77e9\u526a\u679d\uff08ETP\uff09\uff0c\u91c7\u7528\u6307\u6570\u529b\u5e94\u7528\u65b9\u6848\u5bf9\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u6709\u6548\u526a\u9664\u5197\u4f59\u548c\u8fdc\u8ddd\u79bb\u6a21\u5757\uff0c\u4fdd\u7559\u5fc5\u8981\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cETP\u5728\u591a\u79cd\u9886\u57df\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u7387\uff0c\u4e14\u51c6\u786e\u6027\u4e0b\u964d\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u3002", "conclusion": "ETP\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21DNNs\u7684\u538b\u7f29\u9700\u6c42\u3002"}}
{"id": "2506.22022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22022", "abs": "https://arxiv.org/abs/2506.22022", "authors": ["Zhanyi Lu", "Yue Zhou"], "title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "comment": null, "summary": "Facial stylization aims to transform facial images into appealing,\nhigh-quality stylized portraits, with the critical challenge of accurately\nlearning the target style while maintaining content consistency with the\noriginal image. Although previous StyleGAN-based methods have made significant\nadvancements, the generated results still suffer from artifacts or insufficient\nfidelity to the source image. We argue that these issues stem from neglecting\nsemantic shift of the generator during stylization. Therefore, we propose a\nfacial stylization method that integrates semantic preservation constraint and\npseudo-paired supervision to enhance the content correspondence and improve the\nstylization effect. Additionally, we develop a methodology for creating\nmulti-level pseudo-paired datasets to implement supervisory constraint.\nFurthermore, building upon our facial stylization framework, we achieve more\nflexible multimodal and reference-guided stylization without complex network\narchitecture designs or additional training. Experimental results demonstrate\nthat our approach produces high-fidelity, aesthetically pleasing facial style\ntransfer that surpasses previous methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u4fdd\u7559\u7ea6\u675f\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\u7684\u9762\u90e8\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u7ed3\u679c\u4e2d\u7684\u4f2a\u5f71\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u548c\u53c2\u8003\u5f15\u5bfc\u7684\u98ce\u683c\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eStyleGAN\u7684\u65b9\u6cd5\u5728\u9762\u90e8\u98ce\u683c\u5316\u4e2d\u5b58\u5728\u4f2a\u5f71\u6216\u6e90\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u539f\u56e0\u662f\u5ffd\u7565\u4e86\u751f\u6210\u5668\u5728\u98ce\u683c\u5316\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u504f\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u8bed\u4e49\u4fdd\u7559\u7ea6\u675f\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u521b\u5efa\u591a\u7ea7\u4f2a\u914d\u5bf9\u6570\u636e\u96c6\u7684\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u9762\u90e8\u98ce\u683c\u5316\u7ed3\u679c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c\u7f8e\u5b66\u5438\u5f15\u529b\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u4fdd\u7559\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u98ce\u683c\u5316\u7684\u6548\u679c\uff0c\u4e14\u65e0\u9700\u590d\u6742\u7f51\u7edc\u8bbe\u8ba1\u6216\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u6a21\u6001\u548c\u53c2\u8003\u5f15\u5bfc\u98ce\u683c\u5316\u3002"}}
{"id": "2506.22374", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22374", "abs": "https://arxiv.org/abs/2506.22374", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "comment": "13 pages, 9 figures", "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "AI": {"tldr": "\u63d0\u51faSheaf-DMFL\u6846\u67b6\uff0c\u5229\u7528\u5c42\u7406\u8bba\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Sheaf-DMFL-Att\u7b97\u6cd5\u589e\u5f3a\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355\u6a21\u6001\u6570\u636e\u4e14\u6a21\u578b\u67b6\u6784\u76f8\u540c\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "Sheaf-DMFL\u6846\u67b6\u901a\u8fc7\u5c42\u7406\u8bba\u6355\u6349\u5ba2\u6237\u7aef\u4efb\u52a1\u5c42\u7684\u5185\u5728\u5173\u8054\uff0cSheaf-DMFL-Att\u7b97\u6cd5\u5219\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u76f8\u5173\u6027\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u7684\u94fe\u8def\u963b\u585e\u9884\u6d4b\u548c\u6beb\u7c73\u6ce2\u6ce2\u675f\u6210\u5f62\u5b9e\u9a8c\u4e2d\uff0c\u7b97\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "Sheaf-DMFL\u6846\u67b6\u548cSheaf-DMFL-Att\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22027", "abs": "https://arxiv.org/abs/2506.22027", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "comment": "Accepted to ICCV 2025", "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u5b66\u548cSAR\u4f20\u611f\u5668\u7684\u4f4e\u5730\u7403\u8f68\u9053\u661f\u5ea7\u6570\u636e\u96c6\uff08HOSS ReID\uff09\uff0c\u7528\u4e8e\u8239\u8236\u8ddf\u8e2a\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eVision Transformer\u7684\u8de8\u6a21\u6001\u8239\u8236\u518d\u8bc6\u522b\u65b9\u6cd5TransOSS\u3002", "motivation": "\u5f53\u524d\u8239\u8236\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u7684\u536b\u661f\u5b58\u5728\u5206\u8fa8\u7387\u4f4e\u3001\u53d7\u5929\u6c14\u5f71\u54cd\u6216\u62cd\u6444\u65f6\u95f4\u77ed\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u63d0\u51faHOSS ReID\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5149\u5b66\u548cSAR\u4f20\u611f\u5668\uff1b\u63d0\u51faTransOSS\u65b9\u6cd5\uff0c\u6539\u8fdbVision Transformer\u4ee5\u5904\u7406\u8de8\u6a21\u6001\u4efb\u52a1\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u3002", "result": "HOSS ReID\u6570\u636e\u96c6\u652f\u6301\u5168\u5929\u5019\u8ddf\u8e2a\uff0cTransOSS\u80fd\u63d0\u53d6\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u3002", "conclusion": "HOSS ReID\u6570\u636e\u96c6\u548cTransOSS\u65b9\u6cd5\u4e3a\u8239\u8236\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22376", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22376", "abs": "https://arxiv.org/abs/2506.22376", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "title": "Probabilistic Optimality for Inference-time Scaling", "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6846\u67b6\u7684\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u65b9\u6cd5OptScale\uff0c\u901a\u8fc7\u52a8\u6001\u786e\u5b9a\u6700\u4f18\u6837\u672c\u6570\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u73b0\u6709\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u91c7\u6837\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u65e0\u6cd5\u9ad8\u6548\u6307\u5bfc\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u6982\u7387\u6846\u67b6\uff0c\u5047\u8bbe\u6837\u672c\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u63a8\u5bfc\u7406\u8bba\u4e0b\u9650\uff0c\u5f00\u53d1OptScale\u7b97\u6cd5\u52a8\u6001\u786e\u5b9a\u6837\u672c\u6570\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptScale\u663e\u8457\u51cf\u5c11\u91c7\u6837\u5f00\u9500\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u6301\u5e73\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4e3aLLM\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22032", "abs": "https://arxiv.org/abs/2506.22032", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation", "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen\nclasses using supervision from only seen classes. Beyond adaptation-based\nmethods, distillation-based approaches transfer vision-language alignment of\nvision-language model, e.g., CLIP, to segmentation models. However, such\nknowledge transfer remains challenging due to: (1) the difficulty of aligning\nvision-based features with the textual space, which requires combining spatial\nprecision with vision-language alignment; and (2) the semantic gap between\nCLIP's global representations and the local, fine-grained features of\nsegmentation models. To address challenge (1), we propose Chimera-Seg, which\nintegrates a segmentation backbone as the body and a CLIP-based semantic head\nas the head, like the Chimera in Greek mythology, combining spatial precision\nwith vision-language alignment. Specifically, Chimera-Seg comprises a trainable\nsegmentation model and a CLIP Semantic Head (CSH), which maps dense features\ninto the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed\nprojection layers from the CLIP visual encoder, along with lightweight\ntrainable components. The partial module from CLIP visual encoder, paired with\nthe segmentation model, retains segmentation capability while easing the\nmapping to CLIP's semantic space. To address challenge (2), we propose\nSelective Global Distillation (SGD), which distills knowledge from dense\nfeatures exhibiting high similarity to the CLIP CLS token, while gradually\nreducing the number of features used for alignment as training progresses.\nBesides, we also use a Semantic Alignment Module (SAM) to further align dense\nvisual features with semantic embeddings extracted from the frozen CLIP text\nencoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in\nhIoU.", "AI": {"tldr": "Chimera-Seg\u548cSGD\u65b9\u6cd5\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u548c\u7279\u5f81\u7c92\u5ea6\u5dee\u5f02\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u56f0\u96be\u53caCLIP\u5168\u5c40\u8868\u793a\u4e0e\u5206\u5272\u6a21\u578b\u5c40\u90e8\u7279\u5f81\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faChimera-Seg\u7ed3\u5408\u5206\u5272\u4e3b\u5e72\u548cCLIP\u8bed\u4e49\u5934\uff0c\u4ee5\u53caSGD\u9009\u62e9\u6027\u5730\u84b8\u998f\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165SAM\u8fdb\u4e00\u6b65\u5bf9\u9f50\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0chIoU\u5206\u522b\u63d0\u5347\u4e860.9%\u548c1.2%\u3002", "conclusion": "Chimera-Seg\u548cSGD\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.22389", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22389", "abs": "https://arxiv.org/abs/2506.22389", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "title": "Towards Distributed Neural Architectures", "comment": "36 pages, 25 figures", "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u795e\u7ecf\u67b6\u6784\uff08DNA\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u6a21\u5757\u7ec4\u5408\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u548c\u53c2\u6570\u5171\u4eab\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3001\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53c2\u6570\u5229\u7528\u7387\u3002", "method": "\u4f7f\u7528\u5305\u542b\u591a\u79cd\u6a21\u5757\uff08\u5982Transformer\u3001MLP\u3001\u6ce8\u610f\u529b\u7b49\uff09\u7684\u539f\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u8ba9\u8f93\u5165\u5185\u5bb9\u7075\u6d3b\u9009\u62e9\u6a21\u5757\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4f18\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u6a21\u5f0f\u3002", "result": "DNA\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u4e0e\u5bc6\u96c6\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u5b66\u4e60\u8ba1\u7b97\u6548\u7387\u548c\u53c2\u6570\u5171\u4eab\u3002\u8def\u5f84\u5206\u5e03\u7b26\u5408\u5e42\u5f8b\uff0c\u90e8\u5206\u6a21\u5757\u7ec4\u8868\u73b0\u51fa\u4e13\u4e1a\u5316\u7279\u6027\u3002", "conclusion": "DNA\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u6a21\u5757\u7ec4\u5408\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u548c\u53c2\u6570\u5171\u4eab\uff0c\u8def\u5f84\u5206\u5e03\u548c\u6a21\u5757\u4e13\u4e1a\u5316\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.22044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22044", "abs": "https://arxiv.org/abs/2506.22044", "authors": ["Hong Nie", "Fuyuan Cao", "Lu Chen", "Fengxin Chen", "Yuefeng Zou", "Jun Yu"], "title": "Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field", "comment": null, "summary": "Reconstruction and rendering-based talking head synthesis methods achieve\nhigh-quality results with strong identity preservation but are limited by their\ndependence on identity-specific models. Each new identity requires training\nfrom scratch, incurring high computational costs and reduced scalability\ncompared to generative model-based approaches. To overcome this limitation, we\npropose FIAG, a novel 3D speaking head synthesis framework that enables\nefficient identity-specific adaptation using only a few training footage. FIAG\nincorporates Global Gaussian Field, which supports the representation of\nmultiple identities within a shared field, and Universal Motion Field, which\ncaptures the common motion dynamics across diverse identities. Benefiting from\nthe shared facial structure information encoded in the Global Gaussian Field\nand the general motion priors learned in the motion field, our framework\nenables rapid adaptation from canonical identity representations to specific\nones with minimal data. Extensive comparative and ablation experiments\ndemonstrate that our method outperforms existing state-of-the-art approaches,\nvalidating both the effectiveness and generalizability of the proposed\nframework. Code is available at: \\textit{https://github.com/gme-hong/FIAG}.", "AI": {"tldr": "FIAG\u662f\u4e00\u79cd\u65b0\u578b3D\u8bf4\u8bdd\u5934\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7684\u5168\u5c40\u9ad8\u65af\u573a\u548c\u901a\u7528\u8fd0\u52a8\u573a\u5b9e\u73b0\u9ad8\u6548\u8eab\u4efd\u9002\u5e94\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u91cd\u5efa\u548c\u6e32\u67d3\u7684\u8bf4\u8bdd\u5934\u5408\u6210\u65b9\u6cd5\u5bf9\u8eab\u4efd\u7279\u5b9a\u6a21\u578b\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faFIAG\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u9ad8\u65af\u573a\uff08\u652f\u6301\u591a\u8eab\u4efd\u5171\u4eab\u8868\u793a\uff09\u548c\u901a\u7528\u8fd0\u52a8\u573a\uff08\u6355\u6349\u8de8\u8eab\u4efd\u8fd0\u52a8\u52a8\u6001\uff09\uff0c\u5b9e\u73b0\u5feb\u901f\u8eab\u4efd\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFIAG\u5728\u8d28\u91cf\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u9ad8\u6548\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "FIAG\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u7ed3\u6784\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u5408\u6210\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.22393", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22393", "abs": "https://arxiv.org/abs/2506.22393", "authors": ["YongKyung Oh", "Alex Bui"], "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "comment": null, "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u7684\u8de8\u9886\u57df\u9002\u5e94\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u4e2d\u590d\u6742\u7684\u65f6\u5e8f\u4f9d\u8d56\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u8de8\u9886\u57df\u9002\u5e94\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6574\u5408\u65f6\u5e8f\u6a21\u5f0f\u3001\u5bfc\u6570\u52a8\u6001\u548c\u9891\u57df\u7279\u5f81\uff0c\u91c7\u7528\u72ec\u7acb\u7f16\u7801\u5668\u548c\u5206\u5c42\u878d\u5408\u673a\u5236\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u6570\u636e\u96c6\uff08\u5982EEG\u3001ECG\u3001EMG\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22063", "abs": "https://arxiv.org/abs/2506.22063", "authors": ["Durgesh K. Singh", "Ahcene Boubekki", "Qing Cao", "Svein Arne Aase", "Robert Jenssen", "Michael Kampffmeyer"], "title": "EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode", "comment": null, "summary": "Linear measurements of the left ventricle (LV) in the Parasternal Long Axis\n(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.\nThese involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular\nto the LV axis near the mitral valve tips. Manual placement is time-consuming\nand error-prone, while existing deep learning methods often misalign landmarks,\ncausing inaccurate measurements. We propose a novel framework that enhances LV\nmeasurement accuracy by enforcing straight-line constraints. A landmark\ndetector is trained on Anatomical M-Mode (AMM) images, computed in real time\nfrom B-mode videos, then transformed back to B-mode space. This approach\naddresses misalignment and reduces measurement errors. Experiments show\nimproved accuracy over standard B-mode methods, and the framework generalizes\nwell across network architectures. Our semi-automatic design includes a\nhuman-in-the-loop step where the user only places the SL, simplifying\ninteraction while preserving alignment flexibility and clinical relevance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u76f4\u7ebf\u7ea6\u675f\u589e\u5f3a\u5de6\u5fc3\u5ba4\u6d4b\u91cf\u7cbe\u5ea6\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u89e3\u5256M\u6a21\u5f0f\u56fe\u50cf\u8bad\u7ec3\u6807\u5fd7\u70b9\u68c0\u6d4b\u5668\uff0c\u51cf\u5c11\u8bef\u5dee\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u653e\u7f6e\u6807\u5fd7\u70b9\u8017\u65f6\u4e14\u6613\u9519\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5e38\u5bfc\u81f4\u6807\u5fd7\u70b9\u9519\u4f4d\uff0c\u5f71\u54cd\u6d4b\u91cf\u7cbe\u5ea6\u3002", "method": "\u8bad\u7ec3\u6807\u5fd7\u70b9\u68c0\u6d4b\u5668\u4e8e\u89e3\u5256M\u6a21\u5f0f\u56fe\u50cf\uff0c\u5b9e\u65f6\u4eceB\u6a21\u5f0f\u89c6\u9891\u8ba1\u7b97\u5e76\u8f6c\u6362\u56deB\u6a21\u5f0f\u7a7a\u95f4\uff0c\u7ed3\u5408\u534a\u81ea\u52a8\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6bd4\u6807\u51c6B\u6a21\u5f0f\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u7b80\u5316\u4e86\u7528\u6237\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u5bf9\u9f50\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.22401", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.22401", "abs": "https://arxiv.org/abs/2506.22401", "authors": ["Tong Yang", "Bo Dai", "Lin Xiao", "Yuejie Chi"], "title": "Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL", "comment": null, "summary": "Online reinforcement learning (RL) with complex function approximations such\nas transformers and deep neural networks plays a significant role in the modern\npractice of artificial intelligence. Despite its popularity and importance,\nbalancing the fundamental trade-off between exploration and exploitation\nremains a long-standing challenge; in particular, we are still in lack of\nefficient and practical schemes that are backed by theoretical performance\nguarantees. Motivated by recent developments in exploration via optimistic\nregularization, this paper provides an interpretation of the principle of\noptimism through the lens of primal-dual optimization. From this fresh\nperspective, we set forth a new value-incentivized actor-critic (VAC) method,\nwhich optimizes a single easy-to-optimize objective integrating exploration and\nexploitation -- it promotes state-action and policy estimates that are both\nconsistent with collected data transitions and result in higher value\nfunctions. Theoretically, the proposed VAC method has near-optimal regret\nguarantees under linear Markov decision processes (MDPs) in both finite-horizon\nand infinite-horizon settings, which can be extended to the general function\napproximation setting under appropriate assumptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u59cb-\u5bf9\u5076\u4f18\u5316\u7684\u65b0\u65b9\u6cd5VAC\uff0c\u7528\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u5e73\u8861\uff0c\u5177\u6709\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\u957f\u671f\u5b58\u5728\uff0c\u7f3a\u4e4f\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e50\u89c2\u6b63\u5219\u5316\u7684\u89c6\u89d2\uff0c\u63d0\u51faVAC\u65b9\u6cd5\uff0c\u4f18\u5316\u4e00\u4e2a\u6574\u5408\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5355\u4e00\u76ee\u6807\u3002", "result": "VAC\u5728\u6709\u9650\u548c\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u7684\u7ebf\u6027MDP\u4e2d\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u4e00\u822c\u51fd\u6570\u903c\u8fd1\u3002", "conclusion": "VAC\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.22065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22065", "abs": "https://arxiv.org/abs/2506.22065", "authors": ["Dechao Meng", "Steven Xiao", "Xindi Zhang", "Guangyuan Wang", "Peng Zhang", "Qi Wang", "Bang Zhang", "Liefeng Bo"], "title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "comment": "8 pages, 6 figures", "summary": "Audio-driven portrait animation, which synthesizes realistic videos from\nreference images using audio signals, faces significant challenges in real-time\ngeneration of high-fidelity, temporally coherent animations. While recent\ndiffusion-based methods improve generation quality by integrating audio into\ndenoising processes, their reliance on frame-by-frame UNet architectures\nintroduces prohibitive latency and struggles with temporal consistency. This\npaper introduces MirrorMe, a real-time, controllable framework built on the LTX\nvideo model, a diffusion transformer that compresses video spatially and\ntemporally for efficient latent space denoising. To address LTX's trade-offs\nbetween compression and semantic fidelity, we propose three innovations: 1. A\nreference identity injection mechanism via VAE-encoded image concatenation and\nself-attention, ensuring identity consistency; 2. A causal audio encoder and\nadapter tailored to LTX's temporal structure, enabling precise audio-expression\nsynchronization; and 3. A progressive training strategy combining close-up\nfacial training, half-body synthesis with facial masking, and hand pose\nintegration for enhanced gesture control. Extensive experiments on the EMTD\nBenchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,\nlip-sync accuracy, and temporal stability.", "AI": {"tldr": "MirrorMe\u662f\u4e00\u4e2a\u57fa\u4e8eLTX\u89c6\u9891\u6a21\u578b\u7684\u5b9e\u65f6\u3001\u53ef\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8eab\u4efd\u6ce8\u5165\u673a\u5236\u3001\u56e0\u679c\u97f3\u9891\u7f16\u7801\u5668\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u7684\u751f\u6210\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u5728\u5b9e\u65f6\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u9010\u5e27\u5904\u7406\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u3002", "method": "1. \u901a\u8fc7VAE\u7f16\u7801\u56fe\u50cf\u62fc\u63a5\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6ce8\u5165\u53c2\u8003\u8eab\u4efd\uff1b2. \u4e3aLTX\u6a21\u578b\u8bbe\u8ba1\u56e0\u679c\u97f3\u9891\u7f16\u7801\u5668\u548c\u9002\u914d\u5668\uff1b3. \u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u9762\u90e8\u3001\u534a\u8eab\u548c\u624b\u52bf\u63a7\u5236\u3002", "result": "\u5728EMTD Benchmark\u4e0a\uff0cMirrorMe\u5728\u4fdd\u771f\u5ea6\u3001\u5507\u540c\u6b65\u51c6\u786e\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MirrorMe\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u751f\u6210\u3002"}}
{"id": "2506.22069", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2506.22069", "abs": "https://arxiv.org/abs/2506.22069", "authors": ["Petr Hruby", "Marc Pollefeys"], "title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras", "comment": "ICCV 2025, 15 pages, 5 figures, 12 tables", "summary": "We propose a novel approach for estimating the relative pose between rolling\nshutter cameras using the intersections of line projections with a single\nscanline per image. This allows pose estimation without explicitly modeling\ncamera motion. Alternatively, scanlines can be selected within a single image,\nenabling single-view relative pose estimation for scanlines of rolling shutter\ncameras. Our approach is designed as a foundational building block for rolling\nshutter structure-from-motion (SfM), where no motion model is required, and\neach scanline's pose can be computed independently. % We classify minimal\nsolvers for this problem in both generic and specialized settings, including\ncases with parallel lines and known gravity direction, assuming known\nintrinsics and no lens distortion. Furthermore, we develop minimal solvers for\nthe parallel-lines scenario, both with and without gravity priors, by\nleveraging connections between this problem and the estimation of 2D structure\nfrom 1D cameras. % Experiments on rolling shutter images from the Fastec\ndataset demonstrate the feasibility of our approach for initializing rolling\nshutter SfM, highlighting its potential for further development. % The code\nwill be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u4e2d\u5355\u6761\u626b\u63cf\u7ebf\u7684\u7ebf\u6295\u5f71\u4ea4\u70b9\u4f30\u8ba1\u76f8\u5bf9\u4f4d\u59ff\uff0c\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\u76f8\u673a\u8fd0\u52a8\u3002", "motivation": "\u65e8\u5728\u4e3a\u6eda\u52a8\u5feb\u95e8\u7ed3\u6784\u4ece\u8fd0\u52a8\uff08SfM\uff09\u63d0\u4f9b\u57fa\u7840\u6784\u5efa\u5757\uff0c\u65e0\u9700\u8fd0\u52a8\u6a21\u578b\uff0c\u6bcf\u6761\u626b\u63cf\u7ebf\u7684\u4f4d\u59ff\u53ef\u72ec\u7acb\u8ba1\u7b97\u3002", "method": "\u5229\u7528\u7ebf\u6295\u5f71\u4e0e\u5355\u6761\u626b\u63cf\u7ebf\u7684\u4ea4\u70b9\u8fdb\u884c\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5f00\u53d1\u4e86\u6700\u5c0f\u6c42\u89e3\u5668\uff0c\u9002\u7528\u4e8e\u901a\u7528\u548c\u7279\u5b9a\u573a\u666f\uff08\u5982\u5e73\u884c\u7ebf\u548c\u5df2\u77e5\u91cd\u529b\u65b9\u5411\uff09\u3002", "result": "\u5728Fastec\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6eda\u52a8\u5feb\u95e8SfM\u521d\u59cb\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6eda\u52a8\u5feb\u95e8SfM\u63d0\u4f9b\u4e86\u65e0\u9700\u8fd0\u52a8\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8fdb\u4e00\u6b65\u5f00\u53d1\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22427", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22427", "abs": "https://arxiv.org/abs/2506.22427", "authors": ["Randeep Bhatia", "Nikos Papadis", "Murali Kodialam", "TV Lakshman", "Sayak Chakrabarty"], "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings", "comment": "31 pages, 4 figures", "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.", "AI": {"tldr": "CLoVE\u662f\u4e00\u79cd\u7528\u4e8e\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u635f\u5931\u5411\u91cf\u5d4c\u5165\u5b9e\u73b0\u805a\u7c7b\uff0c\u65e0\u9700\u6a21\u578b\u521d\u59cb\u5316\uff0c\u9002\u7528\u4e8e\u76d1\u7763\u548c\u975e\u76d1\u7763\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3CFL\u4e2d\u5ba2\u6237\u805a\u7c7b\u8bc6\u522b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5229\u7528\u635f\u5931\u503c\u76f8\u4f3c\u6027\u533a\u5206\u4e0d\u540c\u96c6\u7fa4\u3002", "method": "\u57fa\u4e8e\u5ba2\u6237\u635f\u5931\u5411\u91cf\u5d4c\u5165\uff0c\u8fed\u4ee3\u8bc6\u522b\u548c\u5206\u79bb\u4e0d\u540c\u96c6\u7fa4\uff0c\u4f18\u5316\u96c6\u7fa4\u7279\u5b9a\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660eCLoVE\u80fd\u51c6\u786e\u6062\u590d\u96c6\u7fa4\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5c11\u91cf\u8bad\u7ec3\u8f6e\u6b21\u5185\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u805a\u7c7b\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "CLoVE\u5728CFL\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002"}}
{"id": "2506.22075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22075", "abs": "https://arxiv.org/abs/2506.22075", "authors": ["Shaheer U. Saeed", "Yipei Wang", "Veeru Kasivisvanathan", "Brian R. Davidson", "Matthew J. Clarkson", "Yipeng Hu", "Daniel C. Alexander"], "title": "Reasoning in machine vision: learning to think fast and slow", "comment": null, "summary": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u65f6\u95f4\uff08\u8ba1\u7b97\u8d44\u6e90\uff09\u6765\u63d0\u5347\u673a\u5668\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5feb\u901f\u601d\u8003\u7684System I\u548c\u6162\u901f\u601d\u8003\u7684System II\u6a21\u5757\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5e76\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u76d1\u7763\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u751a\u81f3\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u800c\u673a\u5668\u4ecd\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u6cd5\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u673a\u5668\u63a8\u7406\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u8bed\u8a00\u9886\u57df\uff0c\u975e\u8bed\u8a00\u63a8\u7406\uff08\u5982\u89c6\u89c9\u611f\u77e5\u3001\u533b\u5b66\u8bca\u65ad\uff09\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u53d7\u5fc3\u7406\u5b66\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51fa\u7ed3\u5408\u5feb\u901f\u601d\u8003\uff08System I\uff09\u548c\u6162\u901f\u601d\u8003\uff08System II\uff09\u6a21\u5757\u7684\u5b66\u4e60\u8303\u5f0f\u3002System II\u901a\u8fc7\u81ea\u5bf9\u5f08\u5f3a\u5316\u5b66\u4e60\u8fed\u4ee3\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u89c6\u89c9\u4efb\u52a1\uff08\u5305\u62ec\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\u548c\u533b\u5b66\u56fe\u50cf\u764c\u75c7\u5b9a\u4f4d\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5ef6\u957f\u63a8\u7406\u65f6\u95f4\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u89c4\u6a21\u76d1\u7763\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u8be5\u8303\u5f0f\u4e3a\u975e\u8bed\u8a00\u673a\u5668\u63a8\u7406\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u548c\u533b\u5b66\u8bca\u65ad\u9886\u57df\u3002"}}
{"id": "2109.05721", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faADL\u548cAAM\u65b9\u6cd5\u89e3\u51b3\u4eba\u8138\u5bf9\u9f50\u4e2d\u7684\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728ADNet\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u5bf9\u9f50\u4e2d\u8bef\u5dee\u5206\u5e03\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u6a21\u578b\u6536\u655b\uff0c\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u3002", "method": "\u63d0\u51faADL\uff08\u5404\u5411\u5f02\u6027\u65b9\u5411\u635f\u5931\uff09\u548cAAM\uff08\u5404\u5411\u5f02\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5750\u6807\u548c\u70ed\u56fe\u56de\u5f52\u3002", "result": "\u5728300W\u3001WFLW\u548cCOFW\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "ADNet\u901a\u8fc7\u7ed3\u5408ADL\u548cAAM\u6709\u6548\u89e3\u51b3\u4e86\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4eba\u8138\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22078", "abs": "https://arxiv.org/abs/2506.22078", "authors": ["Pei-Kai Huanga", "Ya-Ting Chan", "Kuan-Wen Chen", "Yen-Chun Chou", "Shih-Yu Yang", "Chiou-Ting Hsu"], "title": "Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction", "comment": null, "summary": "Many remote Heart Rate (HR) measurement methods focus on estimating remote\nphotoplethysmography (rPPG) signals from video clips lasting around 10 seconds\nbut often overlook the need for HR estimation from ultra-short video clips. In\nthis paper, we aim to accurately measure HR from ultra-short 2-second video\nclips by specifically addressing two key challenges. First, to overcome the\nlimited number of heartbeat cycles in ultra-short video clips, we propose an\neffective periodicity-guided rPPG estimation method that enforces consistent\nperiodicity between rPPG signals estimated from ultra-short clips and their\nmuch longer ground truth signals. Next, to mitigate estimation inaccuracies due\nto spectral leakage, we propose including a generator to reconstruct longer\nrPPG signals from ultra-short ones while preserving their periodic consistency\nto enable more accurate HR measurement. Extensive experiments on four rPPG\nestimation benchmark datasets demonstrate that our proposed method not only\naccurately measures HR from ultra-short video clips but also outperform\nprevious rPPG estimation techniques to achieve state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8d85\u77ed2\u79d2\u89c6\u9891\u4e2d\u51c6\u786e\u6d4b\u91cf\u5fc3\u7387\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u5f15\u5bfc\u7684rPPG\u4f30\u8ba1\u548c\u4fe1\u53f7\u91cd\u6784\u514b\u670d\u4e86\u9891\u8c31\u6cc4\u6f0f\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u8fdc\u7a0b\u5fc3\u7387\u6d4b\u91cf\u65b9\u6cd5\u591a\u5173\u6ce810\u79d2\u89c6\u9891\uff0c\u800c\u5ffd\u7565\u4e86\u8d85\u77ed\u89c6\u9891\u7684\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5468\u671f\u6027\u5f15\u5bfc\u7684rPPG\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5668\u91cd\u6784\u957f\u4fe1\u53f7\u4ee5\u4fdd\u6301\u5468\u671f\u6027\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u9891\u8c31\u6cc4\u6f0f\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u51c6\u786e\u6d4b\u91cf\u8d85\u77ed\u89c6\u9891\u7684\u5fc3\u7387\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d85\u77ed\u89c6\u9891\u5fc3\u7387\u6d4b\u91cf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2212.09525", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u5730\u6807\u6570\u636e\u96c6\uff08\u5982300W\u548cWFLW\uff09\u751f\u6210\u5bc6\u96c6\u9762\u90e8\u5730\u6807\u7684\u6846\u67b6\uff0c\u91c7\u7528\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u5bf9\u9f50\u7814\u7a76\u591a\u5173\u6ce8\u7a00\u758f\u5730\u6807\uff0c\u800c\u5bc6\u96c6\u5730\u6807\u5728\u7f8e\u5bb9\u533b\u5b66\u7b49\u573a\u666f\u4e2d\u9700\u6c42\u9ad8\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u8bed\u4e49\u8f6e\u5ed3\u4e0a\u7684\u5c40\u90e8\u5757\u5916\u89c2\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u5f31\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u591a\u4e2a\u64cd\u4f5c\u7b26\u5b9e\u73b0\u5730\u6807\u5bc6\u5ea6\u63d0\u5347\u3002", "result": "\u5728\u5bc6\u96c6300W\u6d4b\u8bd5\u96c6\u53ca\u539f\u59cb\u7a00\u758f300W\u548cWFLW\u6d4b\u8bd5\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u989d\u5916\u6210\u672c\u5373\u53ef\u63d0\u5347\u5730\u6807\u5bc6\u5ea6\uff0c\u4e14\u6027\u80fd\u4f18\u5f02\u3002"}}
{"id": "2506.22099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22099", "abs": "https://arxiv.org/abs/2506.22099", "authors": ["Zipei Ma", "Junzhe Jiang", "Yurui Chen", "Li Zhang"], "title": "B\u00e9zierGS: Dynamic Urban Scene Reconstruction with B\u00e9zier Curve Gaussian Splatting", "comment": "Accepted at ICCV 2025, Project Page:\n  https://github.com/fudan-zvg/BezierGS", "summary": "The realistic reconstruction of street scenes is critical for developing\nreal-world simulators in autonomous driving. Most existing methods rely on\nobject pose annotations, using these poses to reconstruct dynamic objects and\nmove them during the rendering process. This dependence on high-precision\nobject annotations limits large-scale and extensive scene reconstruction. To\naddress this challenge, we propose B\\'ezier curve Gaussian splatting\n(B\\'ezierGS), which represents the motion trajectories of dynamic objects using\nlearnable B\\'ezier curves. This approach fully leverages the temporal\ninformation of dynamic objects and, through learnable curve modeling,\nautomatically corrects pose errors. By introducing additional supervision on\ndynamic object rendering and inter-curve consistency constraints, we achieve\nreasonable and accurate separation and reconstruction of scene elements.\nExtensive experiments on the Waymo Open Dataset and the nuPlan benchmark\ndemonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both\ndynamic and static scene components reconstruction and novel view synthesis.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22101", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22101", "abs": "https://arxiv.org/abs/2506.22101", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "comment": "Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo\n  will be made publicly available soon", "summary": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "AI": {"tldr": "TPM\u6539\u8fdbADNet\uff0c\u901a\u8fc7\u7ed1\u5b9a\u539f\u578b\u4f4d\u7f6e\u548c\u591a\u7c7b\u5206\u5272\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ADNet\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5355\u539f\u578b\u3001\u4e8c\u5206\u7c7b\u548c\u56fa\u5b9a\u9608\u503c\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faTied Prototype Model (TPM)\uff0c\u7ed1\u5b9a\u524d\u666f\u548c\u80cc\u666f\u5206\u5e03\u7684\u539f\u578b\u4f4d\u7f6e\uff0c\u652f\u6301\u591a\u539f\u578b\u548c\u591a\u7c7b\u5206\u5272\uff0c\u5e76\u5229\u7528\u7c7b\u5148\u9a8c\u4f18\u5316\u81ea\u9002\u5e94\u9608\u503c\u3002", "result": "TPM\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "TPM\u4e3a\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.22111", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22111", "abs": "https://arxiv.org/abs/2506.22111", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "comment": null, "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5370\u5ea6\u9a7e\u9a76\u884c\u4eba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u884c\u4eba\u884c\u4e3a\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u884c\u4e3a\u5bf9\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6ee1\u8db3\u975e\u7ed3\u6784\u5316\u73af\u5883\u7684\u9700\u6c42\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5370\u5ea6\u9a7e\u9a76\u884c\u4eba\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u6c34\u5e73\u548c\u4f4e\u6c34\u5e73\u7684\u8be6\u7ec6\u6ce8\u91ca\uff0c\u91cd\u70b9\u5173\u6ce8\u9700\u8981\u81ea\u8f66\u6ce8\u610f\u7684\u884c\u4eba\u884c\u4e3a\u3002", "result": "\u5728\u8be5\u6570\u636e\u96c6\u4e0a\uff0c\u73b0\u6709\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe15%\uff0c\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u7684MSE\u589e\u52a0\u9ad8\u8fbe1208\uff0c\u8868\u73b0\u4e0d\u53ca\u6807\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u884c\u4eba\u884c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2506.22118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22118", "abs": "https://arxiv.org/abs/2506.22118", "authors": ["Antje Alex", "Jannis Stoppe"], "title": "Pipe Reconstruction from Point Cloud Data", "comment": null, "summary": "Accurate digital twins of industrial assets, such as ships and offshore\nplatforms, rely on the precise reconstruction of complex pipe networks.\nHowever, manual modelling of pipes from laser scan data is a time-consuming and\nlabor-intensive process. This paper presents a pipeline for automated pipe\nreconstruction from incomplete laser scan data. The approach estimates a\nskeleton curve using Laplacian-based contraction, followed by curve elongation.\nThe skeleton axis is then recentred using a rolling sphere technique combined\nwith 2D circle fitting, and refined with a 3D smoothing step. This enables the\ndetermination of pipe properties, including radius, length and orientation, and\nfacilitates the creation of detailed 3D models of complex pipe networks. By\nautomating pipe reconstruction, this approach supports the development of\ndigital twins, allowing for rapid and accurate modeling while reducing costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6fc0\u5149\u626b\u63cf\u6570\u636e\u81ea\u52a8\u91cd\u5efa\u7ba1\u9053\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u9aa8\u67b6\u66f2\u7ebf\u4f30\u8ba1\u548c\u4f18\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u7cbe\u786e\u7684\u7ba1\u9053\u5efa\u6a21\u3002", "motivation": "\u624b\u52a8\u4ece\u6fc0\u5149\u626b\u63cf\u6570\u636e\u5efa\u6a21\u7ba1\u9053\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u652f\u6301\u6570\u5b57\u5b6a\u751f\u7684\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u7684\u6536\u7f29\u4f30\u8ba1\u9aa8\u67b6\u66f2\u7ebf\uff0c\u7ed3\u5408\u6eda\u52a8\u7403\u6280\u672f\u548c2D\u5706\u62df\u5408\u4f18\u5316\uff0c\u6700\u540e\u8fdb\u884c3D\u5e73\u6ed1\u5904\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u7ba1\u9053\u534a\u5f84\u3001\u957f\u5ea6\u548c\u65b9\u5411\u7684\u7cbe\u786e\u786e\u5b9a\uff0c\u652f\u6301\u590d\u6742\u7ba1\u9053\u7f51\u7edc\u7684\u8be6\u7ec63D\u5efa\u6a21\u3002", "conclusion": "\u81ea\u52a8\u5316\u7ba1\u9053\u91cd\u5efa\u65b9\u6cd5\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u6570\u5b57\u5b6a\u751f\u5efa\u6a21\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22134", "abs": "https://arxiv.org/abs/2506.22134", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization", "comment": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "summary": "Higher-order tensors are well-suited for representing multi-dimensional data,\nsuch as color images and videos. Low-rank tensor representation has become\nessential in machine learning and computer vision, but existing methods like\nTucker decomposition offer flexibility at the expense of interpretability. In\ncontrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more\nnatural and interpretable tensor structure, obtaining sparse solutions remains\nchallenging. Leveraging the rich properties of CP decomposition, we propose a\nCP-based low-rank tensor function parameterized by neural networks for implicit\nneural representation (CP-INR). This approach enables continuous data\nrepresentation beyond structured grids, fully exploiting the non-linearity of\ntensor data with theoretical guarantees on excess risk bounds. To achieve a\nsparse CP decomposition, we introduce a variational form of the Schatten-p\nquasi-norm and prove its relationship to multilinear rank minimization. For\nsmoothness, we propose a regularization term based on the spectral norm of the\nJacobian and Hutchinson's trace estimator. Our proposed smoothness\nregularization is SVD-free and avoids explicit chain rule derivations. It can\nserve as an alternative to Total Variation (TV) regularization in image\ndenoising tasks and is naturally applicable to continuous data. Extensive\nexperiments on multi-dimensional data recovery tasks, including image\ninpainting, denoising, and point cloud upsampling, demonstrate the superiority\nand versatility of our method compared to state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCP\u5206\u89e3\u7684\u4f4e\u79e9\u5f20\u91cf\u51fd\u6570\uff08CP-INR\uff09\uff0c\u7528\u4e8e\u8fde\u7eed\u6570\u636e\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u6b63\u5219\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\uff08\u5982Tucker\u548cCP\uff09\u5728\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u7a00\u758f\u89e3\u96be\u4ee5\u83b7\u5f97\u3002CP-INR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7684CP\u5206\u89e3\uff0c\u5f15\u5165Schatten-p\u62df\u8303\u6570\u5b9e\u73b0\u7a00\u758f\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8c31\u8303\u6570\u7684\u5e73\u6ed1\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u3001\u53bb\u566a\u548c\u70b9\u4e91\u4e0a\u91c7\u6837\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CP-INR\u5728\u8fde\u7eed\u6570\u636e\u8868\u793a\u548c\u591a\u7ef4\u6570\u636e\u6062\u590d\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.22139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22139", "abs": "https://arxiv.org/abs/2506.22139", "authors": ["Shaojie Zhang", "Jiahui Yang", "Jianqin Yin", "Zhenbo Luo", "Jian Luan"], "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs", "comment": "Accepted at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.", "AI": {"tldr": "Q-Frame\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5e27\u9009\u62e9\u548c\u591a\u5206\u8fa8\u7387\u7f29\u653e\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891-LLM\u5728\u5747\u5300\u5e27\u91c7\u6837\u4e0b\u96be\u4ee5\u6709\u6548\u6355\u6349\u89c6\u9891\u7684\u5173\u952e\u65f6\u7a7a\u7ebf\u7d22\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "Q-Frame\u901a\u8fc7\u65e0\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u7b56\u7565\uff0c\u7ed3\u5408\u6587\u672c-\u56fe\u50cf\u5339\u914d\u7f51\u7edc\uff08\u5982CLIP\uff09\u548cGumbel-Max\u6280\u5de7\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5e27\u548c\u591a\u5206\u8fa8\u7387\u7f29\u653e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQ-Frame\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002", "conclusion": "Q-Frame\u901a\u8fc7\u9ad8\u6548\u5e27\u9009\u62e9\u548c\u591a\u5206\u8fa8\u7387\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891-LLM\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22146", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22146", "abs": "https://arxiv.org/abs/2506.22146", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "comment": null, "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "AI": {"tldr": "\u901a\u8fc7\u6dfb\u52a0\u4f4e\u5c42\u6b21\u7a7a\u95f4\u7ed3\u6784\uff08\u5982\u6c34\u5e73\u7ebf\uff09\u548c\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u7279\u5f81\u65f6\u7f3a\u4e4f\u7a7a\u95f4\u5e8f\u5217\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u8ba1\u6570\u3001\u89c6\u89c9\u641c\u7d22\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5728\u89c6\u89c9\u8f93\u5165\u4e2d\u5f15\u5165\u4f4e\u5c42\u6b21\u7a7a\u95f4\u7ed3\u6784\uff0c\u5e76\u914d\u5408\u9f13\u52b1\u5e8f\u5217\u5316\u3001\u7a7a\u95f4\u611f\u77e5\u89e3\u6790\u7684\u6587\u672c\u63d0\u793a\u3002", "result": "\u5728\u89c6\u89c9\u641c\u7d22\u3001\u8ba1\u6570\u3001\u573a\u666f\u63cf\u8ff0\u548c\u7a7a\u95f4\u5173\u7cfb\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u5347\u4e8625.00%\u300126.83%\u30010.32\u7f16\u8f91\u8ddd\u79bb\u8bef\u5dee\u548c9.50%\u7684\u6027\u80fd\u3002", "conclusion": "\u4f4e\u5c42\u6b21\u89c6\u89c9\u7ed3\u6784\u662f\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u6709\u6548\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u65b9\u5411\u3002"}}
{"id": "2506.22149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22149", "abs": "https://arxiv.org/abs/2506.22149", "authors": ["Ronald Fecso", "Jos\u00e9 Morano", "Ursula Schmidt-Erfurth", "Hrvoje Bogunovi\u0107"], "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models", "comment": "Accepted for presentation at MICCAI 2025", "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.", "AI": {"tldr": "RetFiner\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u89c6\u89c9\u8bed\u8a00\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u6570\u636e\u7684\u76d1\u7763\u4fe1\u53f7\u63d0\u5347\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u5373\u53ef\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709OCT\u57fa\u7840\u6a21\u578b\u4ec5\u4f9d\u8d56\u56fe\u50cf\u6570\u636e\uff0c\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\u3002", "method": "\u63d0\u51faRetFiner\uff0c\u5229\u7528\u6587\u672c\u6570\u636e\u7684\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u591a\u6837\u5316\u8bad\u7ec3\u76ee\u6807\u4f18\u5316\u6a21\u578b\u8868\u793a\u3002", "result": "\u5728\u4e03\u9879OCT\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cRetFiner\u663e\u8457\u63d0\u5347\u4e86RETFound\u3001UrFound\u548cVisionFM\u7684\u6027\u80fd\uff0c\u5e73\u5747\u5206\u522b\u63d0\u9ad85.8\u30013.9\u548c2.1\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "RetFiner\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002"}}
{"id": "2506.22161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22161", "abs": "https://arxiv.org/abs/2506.22161", "authors": ["Taijin Zhao", "Heqian Qiu", "Yu Dai", "Lanxiao Wang", "Fanman Meng", "Qingbo Wu", "Hongliang Li"], "title": "Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection", "comment": null, "summary": "Few-shot object detection (FSOD) aims to detect objects with limited samples\nfor novel classes, while relying on abundant data for base classes. Existing\nFSOD approaches, predominantly built on the Faster R-CNN detector, entangle\nobjectness recognition and foreground classification within shared feature\nspaces. This paradigm inherently establishes class-specific objectness criteria\nand suffers from unrepresentative novel class samples. To resolve this\nlimitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization\nframework. First, UOFS decouples the feature space into two orthogonal\ncomponents, where magnitude encodes objectness and angle encodes\nclassification. This decoupling enables transferring class-agnostic objectness\nknowledge from base classes to novel classes. Moreover, implementing the\ndisentanglement requires careful attention to two challenges: (1) Base set\nimages contain unlabeled foreground instances, causing confusion between\npotential novel class instances and backgrounds. (2) Angular optimization\ndepends exclusively on base class foreground instances, inducing overfitting of\nangular distributions to base classes. To address these challenges, we propose\na Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure\nbackground base set by removing unlabeled instances in original images to\nprovide unbiased magnitude-based objectness supervision. (2) Incorporating\nunlabeled foreground instances in the original base set into angular\noptimization to enhance distribution uniformity. Additionally, we propose a\nSpatial-wise Attention Disentanglement and Association (SADA) module to address\ntask conflicts between class-agnostic and class-specific tasks. Experiments\ndemonstrate that our method significantly outperforms existing approaches based\non entangled feature spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUOFS\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7279\u5f81\u7a7a\u95f4\u4e3a\u5e45\u5ea6\u548c\u89d2\u5ea6\u4e24\u90e8\u5206\uff0c\u5206\u522b\u7f16\u7801\u7269\u4f53\u6027\u548c\u5206\u7c7b\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7279\u5f81\u7a7a\u95f4\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u4e2d\u7ea0\u7f20\u7269\u4f53\u6027\u8bc6\u522b\u548c\u524d\u666f\u5206\u7c7b\uff0c\u5bfc\u81f4\u7c7b\u7279\u5b9a\u7269\u4f53\u6027\u6807\u51c6\u548c\u6837\u672c\u4e0d\u5177\u4ee3\u8868\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faUOFS\u6846\u67b6\uff0c\u89e3\u8026\u7279\u5f81\u7a7a\u95f4\u4e3a\u5e45\u5ea6\u548c\u89d2\u5ea6\uff1b\u91c7\u7528HBO\u7b56\u7565\u5904\u7406\u672a\u6807\u8bb0\u524d\u666f\u5b9e\u4f8b\uff1b\u5f15\u5165SADA\u6a21\u5757\u89e3\u51b3\u4efb\u52a1\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u7ea0\u7f20\u7279\u5f81\u7a7a\u95f4\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UOFS\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7279\u5f81\u7a7a\u95f4\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22179", "abs": "https://arxiv.org/abs/2506.22179", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Hongfei Xue", "Aidong Lu"], "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "comment": "Accepted to ICCV 2025", "summary": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u5206\u89e3\u7684FS-VAE\u6a21\u578b\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u548c\u8bed\u4e49\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6a21\u5f0f\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u8bc6\u522b\u6548\u679c\u53d7\u9650\u3002", "method": "FS-VAE\u5305\u542b\u9891\u7387\u589e\u5f3a\u6a21\u5757\u3001\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u548c\u6821\u51c6\u4ea4\u53c9\u5bf9\u9f50\u635f\u5931\uff0c\u4ee5\u4f18\u5316\u9aa8\u67b6\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u9891\u7387\u589e\u5f3a\u7684\u8bed\u4e49\u7279\u5f81\u80fd\u6709\u6548\u533a\u5206\u89c6\u89c9\u548c\u8bed\u4e49\u76f8\u4f3c\u7684\u52a8\u4f5c\u7c07\uff0c\u63d0\u5347\u96f6\u6837\u672c\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "FS-VAE\u901a\u8fc7\u9891\u7387\u548c\u8bed\u4e49\u7684\u8054\u5408\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22216", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22216", "abs": "https://arxiv.org/abs/2506.22216", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "comment": "6 pages, 8 figures, accepted by ICME2025", "summary": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.", "AI": {"tldr": "ReF-LLE\u662f\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u9891\u57df\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u8be5\u9886\u57df\uff0c\u901a\u8fc7\u96f6\u53c2\u8003\u56fe\u50cf\u8bc4\u4f30\u7b56\u7565\u548c\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u8fed\u4ee3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u5dee\u5f02\u5927\uff0c\u4ee5\u53ca\u589e\u5f3a\u6548\u679c\u53d7\u4e3b\u89c2\u504f\u597d\u548c\u7528\u6237\u610f\u56fe\u5f71\u54cd\u3002", "method": "\u5728\u5085\u91cc\u53f6\u9891\u57df\u4e2d\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u65f6\u5f15\u5165\u96f6\u53c2\u8003\u56fe\u50cf\u8bc4\u4f30\u7b56\u7565\uff0c\u63a8\u7406\u65f6\u91c7\u7528\u57fa\u4e8e\u96f6\u9891\u7387\u5206\u91cf\u7684\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u8fed\u4ee3\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u611f\u77e5\u8d28\u91cf\u548c\u4e2a\u6027\u5316\u9002\u5e94\u6027\u3002", "conclusion": "ReF-LLE\u4e3a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22241", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.22241", "abs": "https://arxiv.org/abs/2506.22241", "authors": ["Matthias Tsch\u00f6pe", "Vitor Fortes Rey", "Sogo Pierre Sanon", "Paul Lukowicz", "Nikolaos Palaiodimopoulos", "Maximilian Kiefer-Emmanouilidis"], "title": "Boosting Classification with Quantum-Inspired Augmentations", "comment": null, "summary": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "AI": {"tldr": "\u91cf\u5b50\u95e8\u6270\u52a8\u53ef\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u5347\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6027\u80fd\uff0c\u4f46\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u6709\u9650\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u95e8\u6270\u52a8\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u5176\u5728\u7ecf\u5178\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u968f\u673aBloch\u7403\u65cb\u8f6c\u4f5c\u4e3a\u91cf\u5b50\u542f\u53d1\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5e94\u7528\u4e8eImageNet\u6570\u636e\u96c6\u3002", "result": "Top-1\u51c6\u786e\u7387\u63d0\u53473%\uff0cTop-5\u51c6\u786e\u7387\u63d0\u53472.5%\uff0cF1\u5206\u6570\u4ece8%\u589e\u81f312%\u3002", "conclusion": "\u91cf\u5b50\u6270\u52a8\u589e\u5f3a\u6280\u672f\u6709\u6548\uff0c\u4f46\u5bf9\u9690\u79c1\u4fdd\u62a4\u65e0\u663e\u8457\u8d21\u732e\u3002"}}
{"id": "2506.22360", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22360", "abs": "https://arxiv.org/abs/2506.22360", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "AI": {"tldr": "\u6bd4\u8f83\u4e86CNN\uff08ResNet34\uff09\u548cViT\uff08ViT B16\uff09\u5728\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0cResNet34\u5728\u51c6\u786e\u7387\u4e0a\u7565\u4f18\uff0c\u4f46ViT B16\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "motivation": "\u7814\u7a76\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u65e0\u4eba\u673a\u548c\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e24\u79cd\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528ResNet34\u548cViT B16\u5728GEN1\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u8bc4\u4f30\u5176\u5728\u6807\u51c6\u6761\u4ef6\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "ResNet34\u548cViT B16\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a88%\u548c86%\uff0cViT B16\u5728\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u5c3d\u7ba1ResNet34\u51c6\u786e\u7387\u7565\u9ad8\uff0cViT B16\u7684\u7a33\u5065\u6027\u4f7f\u5176\u66f4\u9002\u5408\u52a8\u6001\u73af\u5883\uff0c\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u65e0\u4eba\u673a\u9886\u57df\u3002"}}
{"id": "2506.22242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22242", "abs": "https://arxiv.org/abs/2506.22242", "authors": ["Jiahui Zhang", "Yurui Chen", "Yueming Xu", "Ze Huang", "Yanpeng Zhou", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration", "comment": null, "summary": "Leveraging diverse robotic data for pretraining remains a critical challenge.\nExisting methods typically model the dataset's action distribution using simple\nobservations as inputs. However, these inputs are often incomplete, resulting\nin a dispersed conditional action distribution-an issue we refer to as\ncoordinate system chaos and state chaos. This inconsistency significantly\nhampers pretraining efficiency. To address this, we propose 4D-VLA, a novel\napproach that effectively integrates 4D information into the input to mitigate\nthese sources of chaos. Our model introduces depth and temporal information\ninto visual features with sequential RGB-D inputs, aligning the coordinate\nsystems of the robot and the scene. This alignment endows the model with strong\nspatiotemporal reasoning capabilities while minimizing training overhead.\nAdditionally, we introduce memory bank sampling, a frame sampling strategy\ndesigned to extract informative frames from historical images, further\nimproving effectiveness and efficiency. Experimental results demonstrate that\nour pretraining method and architectural components substantially enhance model\nperformance. In both simulated and real-world experiments, our model achieves a\nsignificant increase in success rate over OpenVLA. To further assess spatial\nperception and generalization to novel views, we introduce MV-Bench, a\nmulti-view simulation benchmark. Our model consistently outperforms existing\nmethods, demonstrating stronger spatial understanding and adaptability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa4D-VLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u54084D\u4fe1\u606f\u89e3\u51b3\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u4e2d\u7684\u5750\u6807\u7cfb\u548c\u72b6\u6001\u6df7\u4e71\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u8f93\u5165\u4e0d\u5b8c\u6574\u5bfc\u81f4\u6761\u4ef6\u52a8\u4f5c\u5206\u5e03\u5206\u6563\uff0c\u5f71\u54cd\u9884\u8bad\u7ec3\u6548\u7387\uff0c\u9700\u89e3\u51b3\u5750\u6807\u7cfb\u548c\u72b6\u6001\u6df7\u4e71\u95ee\u9898\u3002", "method": "\u5f15\u51654D\u4fe1\u606f\uff08\u6df1\u5ea6\u548c\u65f6\u95f4\uff09\u5230\u89c6\u89c9\u7279\u5f81\u4e2d\uff0c\u4f7f\u7528\u987a\u5e8fRGB-D\u8f93\u5165\u5bf9\u9f50\u5750\u6807\u7cfb\uff0c\u5e76\u63d0\u51fa\u5185\u5b58\u5e93\u91c7\u6837\u7b56\u7565\u63d0\u53d6\u5173\u952e\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4D-VLA\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\uff0c\u5e76\u5728\u591a\u89c6\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "4D-VLA\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u4fe1\u606f\u548c\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.22246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22246", "abs": "https://arxiv.org/abs/2506.22246", "authors": ["Yu-Cheng Lin", "Yu-Syuan Xu", "Hao-Wei Chen", "Hsien-Kai Kuo", "Chun-Yi Lee"], "title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration", "comment": "ICCV 2025", "summary": "Image restoration is a key task in low-level computer vision that aims to\nreconstruct high-quality images from degraded inputs. The emergence of Vision\nMamba, which draws inspiration from the advanced state space model Mamba, marks\na significant advancement in this field. Vision Mamba demonstrates excellence\nin modeling long-range dependencies with linear complexity, a crucial advantage\nfor image restoration tasks. Despite its strengths, Vision Mamba encounters\nchallenges in low-level vision tasks, including computational complexity that\nscales with the number of scanning sequences and local pixel forgetting. To\naddress these limitations, this study introduces Efficient All-Around Mamba\n(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan\nModule (MHSSM) with an all-around scanning mechanism. MHSSM efficiently\naggregates multiple scanning sequences, which avoids increases in computational\ncomplexity and parameter count. The all-around scanning strategy implements\nmultiple patterns to capture holistic information and resolves the local pixel\nforgetting issue. Our experimental evaluations validate these innovations\nacross several restoration tasks, including super resolution, denoising,\ndeblurring, and dehazing. The results validate that EAMamba achieves a\nsignificant 31-89% reduction in FLOPs while maintaining favorable performance\ncompared to existing low-level Vision Mamba methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEAMamba\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5934\u9009\u62e9\u6027\u626b\u63cf\u6a21\u5757\u548c\u5168\u65b9\u4f4d\u626b\u63cf\u673a\u5236\u6539\u8fdbVision Mamba\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u89e3\u51b3\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "Vision Mamba\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165EAMamba\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5934\u9009\u62e9\u6027\u626b\u63cf\u6a21\u5757\uff08MHSSM\uff09\u548c\u5168\u65b9\u4f4d\u626b\u63cf\u673a\u5236\uff0c\u4f18\u5316\u626b\u63cf\u5e8f\u5217\u5e76\u6355\u83b7\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEAMamba\u5728\u591a\u79cd\u6062\u590d\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4eFLOPs\uff0831-89%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "conclusion": "EAMamba\u6709\u6548\u89e3\u51b3\u4e86Vision Mamba\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22274", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22274", "abs": "https://arxiv.org/abs/2506.22274", "authors": ["Filippo Merlo", "Ece Takmaz", "Wenkai Chen", "Albert Gatt"], "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "comment": null, "summary": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u662f\u5426\u4f9d\u8d56\u573a\u666f\u4e0a\u4e0b\u6587\u751f\u6210\u5bf9\u8c61\u5f15\u7528\uff0c\u5e76\u5f15\u5165COOCO\u6570\u636e\u96c6\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f-\u5bf9\u8c61\u4e00\u81f4\u6027\u548c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22VLMs\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u4f9d\u8d56\u573a\u666f\u4e0a\u4e0b\u6587\u8fdb\u884c\u5bf9\u8c61\u8bc6\u522b\u548c\u5f15\u7528\u3002", "method": "\u4f7f\u7528COOCO\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f-\u5bf9\u8c61\u4e00\u81f4\u6027\u548c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u6790\u3002", "result": "\u6a21\u578b\u4f1a\u6839\u636e\u573a\u666f-\u5bf9\u8c61\u8bed\u4e49\u76f8\u5173\u6027\u548c\u566a\u58f0\u6c34\u5e73\u52a8\u6001\u8c03\u6574\u5bf9\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\uff0c\u4e2d\u5c42\u6b21\u6ce8\u610f\u529b\u5728\u76ee\u6807\u5206\u7c7b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "VLMs\u80fd\u591f\u52a8\u6001\u5e73\u8861\u5c40\u90e8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u76ee\u6807-\u573a\u666f\u9ad8\u4e00\u81f4\u6027\u6216\u5bf9\u8c61\u9000\u5316\u65f6\u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2506.22283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22283", "abs": "https://arxiv.org/abs/2506.22283", "authors": ["Rui Xu", "Yunke Wang", "Yong Luo", "Bo Du"], "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment", "comment": null, "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.", "AI": {"tldr": "VisionDrop\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u4ee4\u724c\u4fee\u526a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5185\u6ce8\u610f\u529b\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9636\u6bb5\u4fee\u526a\u4e2d\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u8fdc\u591a\u4e8e\u6587\u672c\u4ee4\u724c\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u6761\u4ef6\u4ea4\u4e92\uff0c\u4f46\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u5f71\u54cd\u4e86\u5176\u6548\u679c\u3002", "method": "\u63d0\u51faVisionDrop\u6846\u67b6\uff0c\u57fa\u4e8e\u89c6\u89c9\u5185\u6ce8\u610f\u529b\u9009\u62e9\u4ee4\u724c\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u4fee\u526a\u7ba1\u9053\uff0c\u5728\u591a\u4e2a\u9636\u6bb5\u8fdb\u884c\u4e3b\u5bfc\u4ee4\u724c\u9009\u62e9\u548c\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u5408\u5e76\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVisionDrop\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u5e76\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "VisionDrop\u901a\u8fc7\u89c6\u89c9\u5185\u6ce8\u610f\u529b\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u89c6\u89c9\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\u3002"}}
{"id": "2506.22291", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22291", "abs": "https://arxiv.org/abs/2506.22291", "authors": ["Mengqi Zhou", "Xipeng Wang", "Yuxi Wang", "Zhaoxiang Zhang"], "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation", "comment": null, "summary": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.", "AI": {"tldr": "RoomCraft\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ed3\u5408\u573a\u666f\u751f\u6210\u548c\u7ea6\u675f\u4f18\u5316\uff0c\u4ece\u7528\u6237\u8f93\u5165\u751f\u6210\u8fde\u8d2f\u76843D\u5ba4\u5185\u573a\u666f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u62103D\u5ba4\u5185\u573a\u666f\u65f6\u56e0\u5168\u5c40\u7a7a\u95f4\u63a8\u7406\u4e0d\u8db3\u6216\u7ea6\u675f\u5904\u7406\u80fd\u529b\u6709\u9650\u5bfc\u81f4\u7684\u91cd\u590d\u6027\u3001\u78b0\u649e\u548c\u5e03\u5c40\u4e0d\u5b8c\u6574\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u573a\u666f\u751f\u6210\u7ba1\u9053\u548c\u7ea6\u675f\u9a71\u52a8\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u3001\u7a7a\u95f4\u5173\u7cfb\u7f51\u7edc\u3001HDFS\u7b97\u6cd5\u3001\u7edf\u4e00\u7ea6\u675f\u8868\u793a\u548cCAPS\u7b56\u7565\u3002", "result": "RoomCraft\u5728\u591a\u6837\u8f93\u5165\u6a21\u6001\u4e0b\u751f\u6210\u66f4\u771f\u5b9e\u3001\u8bed\u4e49\u8fde\u8d2f\u4e14\u89c6\u89c9\u5438\u5f15\u7684\u5ba4\u5185\u5e03\u5c40\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RoomCraft\u901a\u8fc7\u591a\u9636\u6bb5\u4f18\u5316\u548c\u52a8\u6001\u7ea6\u675f\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u573a\u666f\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.22298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22298", "abs": "https://arxiv.org/abs/2506.22298", "authors": ["Linhao Zhong", "Fan Li", "Yi Huang", "Jianzhuang Liu", "Renjing Pei", "Fenglong Song"], "title": "OutDreamer: Video Outpainting with a Diffusion Transformer", "comment": null, "summary": "Video outpainting is a challenging task that generates new video content by\nextending beyond the boundaries of an original input video, requiring both\ntemporal and spatial consistency. Many state-of-the-art methods utilize latent\ndiffusion models with U-Net backbones but still struggle to achieve high\nquality and adaptability in generated content. Diffusion transformers (DiTs)\nhave emerged as a promising alternative because of their superior performance.\nWe introduce OutDreamer, a DiT-based video outpainting framework comprising two\nmain components: an efficient video control branch and a conditional\noutpainting branch. The efficient video control branch effectively extracts\nmasked video information, while the conditional outpainting branch generates\nmissing content based on these extracted conditions. Additionally, we propose a\nmask-driven self-attention layer that dynamically integrates the given mask\ninformation, further enhancing the model's adaptability to outpainting tasks.\nFurthermore, we introduce a latent alignment loss to maintain overall\nconsistency both within and between frames. For long video outpainting, we\nemploy a cross-video-clip refiner to iteratively generate missing content,\nensuring temporal consistency across video clips. Extensive evaluations\ndemonstrate that our zero-shot OutDreamer outperforms state-of-the-art\nzero-shot methods on widely recognized benchmarks.", "AI": {"tldr": "OutDreamer\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u5916\u7ed8\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u89c6\u9891\u63a7\u5236\u5206\u652f\u548c\u6761\u4ef6\u5916\u7ed8\u5206\u652f\uff0c\u7ed3\u5408\u63a9\u7801\u9a71\u52a8\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u6f5c\u5728\u5bf9\u9f50\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5916\u7ed8\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u5916\u7ed8\u4efb\u52a1\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u751f\u6210\u5185\u5bb9\u7684\u9ad8\u8d28\u91cf\uff0c\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u56e0\u5176\u4f18\u8d8a\u6027\u80fd\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOutDreamer\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u6548\u89c6\u9891\u63a7\u5236\u5206\u652f\u548c\u6761\u4ef6\u5916\u7ed8\u5206\u652f\uff0c\u5f15\u5165\u63a9\u7801\u9a71\u52a8\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u6f5c\u5728\u5bf9\u9f50\u635f\u5931\uff0c\u5e76\u91c7\u7528\u8de8\u89c6\u9891\u7247\u6bb5\u7ec6\u5316\u5668\u5904\u7406\u957f\u89c6\u9891\u3002", "result": "\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOutDreamer\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "OutDreamer\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5916\u7ed8\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.22336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22336", "abs": "https://arxiv.org/abs/2506.22336", "authors": ["Paula Carb\u00f3 Cubero", "Alberto Jaenal G\u00e1lvez", "Andr\u00e9 Mateus", "Jos\u00e9 Ara\u00fajo", "Patric Jensfelt"], "title": "MatChA: Cross-Algorithm Matching with Feature Augmentation", "comment": null, "summary": "State-of-the-art methods fail to solve visual localization in scenarios where\ndifferent devices use different sparse feature extraction algorithms to obtain\nkeypoints and their corresponding descriptors. Translating feature descriptors\nis enough to enable matching. However, performance is drastically reduced in\ncross-feature detector cases, because current solutions assume common\nkeypoints. This means that the same detector has to be used, which is rarely\nthe case in practice when different descriptors are used. The low repeatability\nof keypoints, in addition to non-discriminatory and non-distinctive\ndescriptors, make the identification of true correspondences extremely\nchallenging. We present the first method tackling this problem, which performs\nfeature descriptor augmentation targeting cross-detector feature matching, and\nthen feature translation to a latent space. We show that our method\nsignificantly improves image matching and visual localization in the\ncross-feature scenario and evaluate the proposed method on several benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8de8\u68c0\u6d4b\u5668\u7279\u5f81\u5339\u914d\u7684\u589e\u5f3a\u548c\u8f6c\u6362\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5339\u914d\u548c\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u8bbe\u5907\u4f7f\u7528\u4e0d\u540c\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u5047\u8bbe\u4f7f\u7528\u76f8\u540c\u68c0\u6d4b\u5668\uff0c\u800c\u5b9e\u9645\u4e2d\u5f88\u5c11\u5982\u6b64\u3002", "method": "\u901a\u8fc7\u7279\u5f81\u63cf\u8ff0\u7b26\u589e\u5f3a\u548c\u8f6c\u6362\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u89e3\u51b3\u8de8\u68c0\u6d4b\u5668\u7279\u5f81\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5339\u914d\u548c\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u68c0\u6d4b\u5668\u7279\u5f81\u5339\u914d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22338", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22338", "abs": "https://arxiv.org/abs/2506.22338", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing", "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u65e5\u671fVHR SAR\u56fe\u50cf\u548c\u591a\u6e90\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u68c0\u6d4b\u5efa\u7b51\u7269\u635f\u574f\uff0c\u65e0\u9700\u4f9d\u8d56\u707e\u524d\u6570\u636e\u3002", "motivation": "\u707e\u540e\u5feb\u901f\u8bc6\u522b\u5efa\u7b51\u7269\u635f\u574f\u5bf9\u5e94\u6025\u54cd\u5e94\u548c\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5149\u5b66\u536b\u661f\u56fe\u50cf\u5e38\u53d7\u4e91\u5c42\u6216\u7f3a\u4e4f\u707e\u524d\u6570\u636e\u9650\u5236\u3002", "method": "\u7ed3\u5408SAR\u56fe\u50cf\u3001OSM\u5efa\u7b51\u8f6e\u5ed3\u3001DSM\u6570\u636e\u548cGEM\u5c5e\u6027\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u707e\u540e\u6570\u636e\u3002", "result": "\u5728\u571f\u8033\u51762023\u5e74\u5730\u9707\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u7279\u5f81\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u707e\u524d\u6570\u636e\uff0c\u53ef\u5feb\u901f\u3001\u53ef\u9760\u5730\u8bc4\u4f30\u5efa\u7b51\u7269\u635f\u574f\uff0c\u652f\u6301\u707e\u5bb3\u7ba1\u7406\u548c\u6062\u590d\u3002"}}
{"id": "2506.22347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22347", "abs": "https://arxiv.org/abs/2506.22347", "authors": ["Hans Gei\u00dfner", "Christian Rathgeb"], "title": "Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults", "comment": "10 pages, 4 figures, 4 tables", "summary": "This paper analyses and addresses the performance gap in the fuzzy\nvault-based \\ac{BCS}. We identify unstable error correction capabilities, which\nare caused by variable feature set sizes and their influence on similarity\nthresholds, as a key source of performance degradation. This issue is further\ncompounded by information loss introduced through feature type transformations.\nTo address both problems, we propose a novel feature quantization method based\non \\it{equal frequent intervals}. This method guarantees fixed feature set\nsizes and supports training-free adaptation to any number of intervals. The\nproposed approach significantly reduces the performance gap introduced by\ntemplate protection. Additionally, it integrates seamlessly with existing\nsystems to minimize the negative effects of feature transformation. Experiments\non state-of-the-art face, fingerprint, and iris recognition systems confirm\nthat only minimal performance degradation remains, demonstrating the\neffectiveness of the method across major biometric modalities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b49\u9891\u533a\u95f4\u7684\u7279\u5f81\u91cf\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6a21\u7cca\u4fdd\u9669\u5e93\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u6a21\u7cca\u4fdd\u9669\u5e93\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7cfb\u7edf\u4e2d\uff0c\u4e0d\u7a33\u5b9a\u7684\u7ea0\u9519\u80fd\u529b\u548c\u7279\u5f81\u7c7b\u578b\u8f6c\u6362\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u662f\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b49\u9891\u533a\u95f4\u7684\u7279\u5f81\u91cf\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u56fa\u5b9a\u7279\u5f81\u96c6\u5927\u5c0f\uff0c\u5e76\u652f\u6301\u65e0\u8bad\u7ec3\u9002\u5e94\u4efb\u610f\u533a\u95f4\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u677f\u4fdd\u62a4\u5f15\u5165\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u5728\u4e3b\u6d41\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2506.22375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22375", "abs": "https://arxiv.org/abs/2506.22375", "authors": ["Tiankai Chen", "Yushu Li", "Adam Goodge", "Fei Teng", "Xulei Yang", "Tianrui Li", "Xun Xu"], "title": "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation", "comment": "Accepted by ICCV 2025", "summary": "Out-of-distribution (OOD) detection in 3D point cloud data remains a\nchallenge, particularly in applications where safe and robust perception is\ncritical. While existing OOD detection methods have shown progress for 2D image\ndata, extending these to 3D environments involves unique obstacles. This paper\nintroduces a training-free framework that leverages Vision-Language Models\n(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph\nbased on class prototypes and testing data, we exploit the data manifold\nstructure to enhancing the effectiveness of VLMs for 3D OOD detection. We\npropose a novel Graph Score Propagation (GSP) method that incorporates prompt\nclustering and self-training negative prompting to improve OOD scoring with\nVLM. Our method is also adaptable to few-shot scenarios, providing options for\npractical applications. We demonstrate that GSP consistently outperforms\nstate-of-the-art methods across synthetic and real-world datasets 3D point\ncloud OOD detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e3D\u70b9\u4e91\u6570\u636e\u7684OOD\u68c0\u6d4b\uff0c\u901a\u8fc7\u56fe\u5206\u6570\u4f20\u64ad\uff08GSP\uff09\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "3D\u70b9\u4e91\u6570\u636e\u7684OOD\u68c0\u6d4b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf92D\u56fe\u50cf\uff0c\u96be\u4ee5\u76f4\u63a5\u6269\u5c55\u52303D\u73af\u5883\u3002", "method": "\u5229\u7528VLM\u6784\u5efa\u57fa\u4e8e\u7c7b\u539f\u578b\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u56fe\u7ed3\u6784\uff0c\u63d0\u51faGSP\u65b9\u6cd5\uff0c\u7ed3\u5408\u63d0\u793a\u805a\u7c7b\u548c\u81ea\u8bad\u7ec3\u8d1f\u63d0\u793a\u4f18\u5316OOD\u8bc4\u5206\u3002", "result": "GSP\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5c11\u6837\u672c\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3D\u70b9\u4e91OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22385", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22385", "abs": "https://arxiv.org/abs/2506.22385", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "comment": null, "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Defeasible Video Entailment (DVidE)\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u9891\u5927\u6a21\u578b\u5728\u62bd\u8c61\u548c\u81ea\u9002\u5e94\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u65b0\u4efb\u52a1\u548c\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86\u52a8\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5927\u6a21\u578b\u5728\u62bd\u8c61\u548c\u81ea\u9002\u5e94\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6839\u636e\u65b0\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7ed3\u8bba\u3002", "method": "\u63d0\u51fa\u4e86DVidE\u4efb\u52a1\uff0c\u5305\u62ec\u5206\u7c7b\u548c\u751f\u6210\u7248\u672c\uff0c\u5206\u522b\u91c7\u7528Chain of Counterfactual Thought\u6846\u67b6\u548c\u7ed3\u5408ASR\u4e0eLLM\u7684\u751f\u6210\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "DVidE\u4efb\u52a1\u53ca\u76f8\u5173\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u89c6\u9891\u5927\u6a21\u578b\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.22395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22395", "abs": "https://arxiv.org/abs/2506.22395", "authors": ["Shih-Han Chou", "Shivam Chandhok", "James J. Little", "Leonid Sigal"], "title": "Test-Time Consistency in Vision Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u518d\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u4e00\u81f4\u6027\u635f\u5931\u548c\u4f2a\u6807\u7b7e\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u4e49\u7b49\u6548\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u4e00\u81f4\u6027\u635f\u5931\u548c\u4f2a\u6807\u7b7e\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5229\u7528\u6d4b\u8bd5\u8f93\u5165\u81ea\u8eab\u4fe1\u606f\u63d0\u5347\u4e00\u81f4\u6027\u3002", "result": "\u5728MM-R3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e00\u81f4\u6027\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u7684\u63a8\u7406\u65f6\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.22432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22432", "abs": "https://arxiv.org/abs/2506.22432", "authors": ["Yuhao Liu", "Tengfei Wang", "Fang Liu", "Zhenwei Wang", "Rynson W. H. Lau"], "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy", "comment": null, "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/", "AI": {"tldr": "Shape-for-Motion\u6846\u67b6\u901a\u8fc73D\u4ee3\u7406\u5b9e\u73b0\u7cbe\u786e\u4e14\u4e00\u81f4\u7684\u89c6\u9891\u7f16\u8f91\uff0c\u652f\u6301\u591a\u79cd\u7269\u7406\u4e00\u81f4\u7684\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u786e\u4fdd\u7ec6\u7c92\u5ea6\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u4e00\u81f4\u7684\u63a7\u5236\u5de5\u5177\u3002", "method": "\u5229\u75283D\u4ee3\u7406\uff08\u65f6\u95f4\u4e00\u81f4\u7684\u7f51\u683c\uff09\u8fdb\u884c\u7f16\u8f91\uff0c\u901a\u8fc7\u53cc\u4f20\u64ad\u7b56\u7565\u7b80\u5316\u7f16\u8f91\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7ed3\u679c\u3002", "result": "\u652f\u6301\u591a\u79cd\u7cbe\u786e\u64cd\u4f5c\uff08\u5982\u59ff\u6001\u7f16\u8f91\u3001\u7eb9\u7406\u4fee\u6539\u7b49\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "Shape-for-Motion\u662f\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u5de5\u4f5c\u6d41\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2506.22433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22433", "abs": "https://arxiv.org/abs/2506.22433", "authors": ["Sadra Safadoust", "Fabio Tosi", "Fatma G\u00fcney", "Matteo Poggi"], "title": "WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields", "comment": "Project page: https://kuis-ai.github.io/WarpRF/", "summary": "We introduce WarpRF, a training-free general-purpose framework for\nquantifying the uncertainty of radiance fields. Built upon the assumption that\nphotometric and geometric consistency should hold among images rendered by an\naccurate model, WarpRF quantifies its underlying uncertainty from an unseen\npoint of view by leveraging backward warping across viewpoints, projecting\nreliable renderings to the unseen viewpoint and measuring the consistency with\nimages rendered there. WarpRF is simple and inexpensive, does not require any\ntraining, and can be applied to any radiance field implementation for free.\nWarpRF excels at both uncertainty quantification and downstream tasks, e.g.,\nactive view selection and active mapping, outperforming any existing method\ntailored to specific frameworks.", "AI": {"tldr": "WarpRF\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u8f90\u5c04\u573a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u53cd\u5411\u53d8\u5f62\u548c\u4e00\u81f4\u6027\u6d4b\u91cf\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u6846\u67b6\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u4e14\u9700\u8981\u8bad\u7ec3\uff0cWarpRF\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u53cd\u5411\u53d8\u5f62\u5c06\u53ef\u9760\u6e32\u67d3\u6295\u5f71\u5230\u65b0\u89c6\u89d2\uff0c\u6d4b\u91cf\u4e0e\u6e32\u67d3\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u3002", "result": "WarpRF\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WarpRF\u7b80\u5355\u3001\u4f4e\u6210\u672c\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u8f90\u5c04\u573a\u5b9e\u73b0\u3002"}}
{"id": "2506.22434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22434", "abs": "https://arxiv.org/abs/2506.22434", "authors": ["Xi Chen", "Mingkang Zhu", "Shaoteng Liu", "Xiaoyang Wu", "Xiaogang Xu", "Yu Liu", "Xiang Bai", "Hengshuang Zhao"], "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "comment": null, "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u6765\u589e\u5f3a\u591a\u56fe\u50cf\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u95ee\u7b54\u5bf9\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u548c\u591a\u56fe\u50cf\u590d\u6742\u903b\u8f91\u65f6\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b\u540c\u4e00\u56fe\u50cf\u7684\u4e24\u4e2a\u589e\u5f3a\u89c6\u56fe\u548c\u4e00\u4e2a\u76f8\u4f3c\u4f46\u4e0d\u540c\u56fe\u50cf\u7684\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u56fe\u50cf\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u53ef\u6709\u6548\u63d0\u5347\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002"}}

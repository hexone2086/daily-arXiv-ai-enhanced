<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.LG](#cs.LG) [Total: 71]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 基于多模态视觉语言模型(VLM)的端到端自主驾驶方案，仅使用单相机即可在驾驶任务中达到领先性能


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型(尤其是多模态视觉语言模型)是否能够提升端到端自主驾驶任务的性能

Method: 结合端到端架构设计与知识丰富的多模态视觉语言模型(VLM)

Result: 在驾驶任务上表现出艰人的性能，成为目前最佳的仅使用单相机的解决方案

Conclusion: 证明了基于视觉的驾驶方法的有效性，并显示了端到端驾驶任务的很大潜力

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [2] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 该论文提出了MoCentric-Bench基准测试，用于评估视频多模态大语言模型在像素级视觉定位中对运动信息的理解能力，发现现有模型过度依赖静态外观线索而忽视真实运动推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLMs在像素级视觉定位任务中主要关注视频问答和字幕生成，但对运动信息的利用研究不足，且现有基准测试存在单帧即可完成任务的问题，无法真正评估时序推理能力。

Method: 提出了四种运动中心探测技术，构建了MoCentric-Bench基准测试，确保模型必须利用运动与语言的交互关系，而非仅依赖静态外观线索。同时建立了强单帧基线并探索了运动中心适应技术。

Result: 研究发现现有视频MLLMs在区分真实运动和虚假运动、理解运动顺序方面存在不足。提出的运动中心适应技术在MoCentric-Bench上达到了最先进性能，单帧基线甚至优于先前方法。

Conclusion: 该工作挑战未来模型需要改进密集时空定位和像素级视频理解能力，强调了运动信息在视觉定位中的重要性，并为相关研究提供了新的评估基准。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [3] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: 基于HG-TNet混合深度学习模型，结合Transformer咈CNN优势，通过自监睦旋转预测任务，在结肠癌病理图像分类中取得优异性能


<details>
  <summary>Details</summary>
Motivation: 早期检测结肠癌对预防病情恶化至关重要，需要更准确的图像分析技术

Method: 使用HG-TNet混合模型，包含Transformer分支提取全局上下文关系咈CNN分支捐捕局部细节，结合自监睦旋转预测任务咈囊网络保持空间结构信息

Result: 在LC25000数据集上表现超过标准架构，在准确率咈损失函数等指标上均有明显收益

Conclusion: 多规模特征融合与自监睦学习的混合方法能够有效提升病理图像分析的识别精度，为癌病早期诊断提供了有效技术支撑

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [4] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 一种基于强化学习的主动视频采集框架，通过动态选择最信息富雅超声视频，在使用47%视频的情况下达到80.6%的主动脉瓦穿分类准确度。


<details>
  <summary>Details</summary>
Motivation: 主动脉瓦穿是一种威胎生命的病变，但金标准诊断方法心超声检查在资源有限地区难以普及。床边超声虽更易获得，但受限于操作者经验和选择最优成像视角的挑战。

Method: 提出一种基于强化学习的主动视频采集框架，动态选择每个患者最信息富的心超视频，持续评估是否需要额外成像。

Result: 在2,572名患者数据上进行测试，该方法在仅使用47%心超视频的情况下，达到了类比全部采集的80.6%的分类准确性。

Conclusion: 这种主动特征采集方法有力于改善主动脉瓦穿的诊断，使心超声评估更加高效、可扩展和个性化。

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [5] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard是一个开源的激光雷达软件框架，旨在解决激光雷达研究中代码重复开发的问题，提供数据I/O、预处理/后处理和常用算法的内置支持，支持快速开发和结果可视化。


<details>
  <summary>Details</summary>
Motivation: 激光雷达研究中存在大量代码重复开发的问题，不同研究在数据I/O、预处理/后处理和算法实现等方面存在大量重复工作，且数据或算法的小幅变化就需要大幅修改代码。

Method: 开发了LiGuard开源框架，提供内置的数据I/O支持、预处理/后处理功能、常用算法库，支持交互式算法添加/删除/重排序和参数调整，以及分类、检测、分割和跟踪任务的结果可视化。

Result: LiGuard通过案例研究证明了其有效性，能够帮助研究人员快速开发激光雷达项目代码，并支持项目组件的高效共享和重用。

Conclusion: LiGuard框架成功解决了激光雷达研究中的代码重复开发问题，提供了高效的开发环境和组件共享机制，有助于促进激光雷达研究的协作和发展。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [6] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 本文提出了一种使用高保真数字双胞生成大规模高质量合成数据集的方法论，以解决LiDAR感知系统中标注数据成本高的问题。


<details>
  <summary>Details</summary>
Motivation: LiDAR基于感知系统需要大规模标注数据进行深度学习训练，但人工标注成本高、耗时较长，限制了系统扩展性。Sim2Real学习提供了可扩展的替代方案，但效果取决于模拟的保真度。

Method: 提出了一套严格可复现的方法论，使用高保真数字双胞生成大规模合成数据集。方法包括：静态几何建模、道路基础设施复制、动态交通场景生成。利用卫星影像、OpenStreetMap数据等开源资源，给出了建立稳健合成环境的具体指导。

Result: 该方法能够生成大规模、高质量、多样化的合成数据集，为Sim2Real学习提供了可靠的基础。

Conclusion: 通过高保真数字双胞技术，可以实现可扩展、成本效益高的合成数据集生成，有效解决LiDAR感知系统中标注数据成本高的挑战，推动智能交通系统的发展。

Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [7] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 本文提出了一种高保真数字双胞框架（HiFi DT），通过统计方法构建模拟环境来减少LiDAR感知的Sim2Real域迁移问题，使得在模拟数据训练的模型在真实数据上表现超过了使用真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR感知模型在从模拟环境迁移到真实世界时因分布偏移导致的性能下降问题，提供一种成本效益高可扩展的方案来支持智能交通系统的发展。

Method: 提出HiFi DT框架，包含真实世界背景几何、车道级路网拓扑和传感器特定规格位置，通过多种距离指标（CD、MMD、EMD、FD）在原始输入和隐藏特征层面量化域对齐程度。

Result: 在HiFi DT生成的合成数据上训练的3D物体检测器在真实数据上表现超过使用真实数据训练的模型4.8%，显著减少了域偏移并提高了通用性。

Conclusion: 高保真数字双胞在实现可靠的模拟基LiDAR感知方面发挥了重要作用，为真实世界智能交通系统应用提供了有力支持。

Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [8] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

TL;DR: KG-DG是一个神经符号框架，通过整合视觉变换器和专家引导的符号推理，在糖尿病视网膜病变分类中实现了跨域泛化的显著提升，准确率最高提升6%。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中模型在真实世界分布偏移下泛化能力差的问题，特别是在糖尿病视网膜病变分类中的域泛化挑战。

Method: 提出神经符号框架KG-DG，整合视觉变换器与专家指导的符号推理，利用临床病变本体和视网膜血管分割，通过置信度加权集成策略融合深度视觉表示，最小化域嵌入间的KL散度来对齐高层临床语义。

Result: 在四个公共数据集上实验显示：跨域设置准确率提升达5.2%，比基线ViT模型提升6%；符号模型在MDG中平均准确率63.67%；神经符号集成在SDG场景中达到最高准确率；病变特征准确率84.65%，显著优于纯神经方法。

Conclusion: 神经符号集成是构建临床鲁棒、域不变医学AI系统的有前景范式，符号组件不仅是可解释性增强，更是有效的正则化器。

Abstract: Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>


### [9] [A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images](https://arxiv.org/abs/2509.02928)
*Zhicheng Tang,Jinwen Tang,Yi Shang*

Main category: cs.CV

TL;DR: DDR-Net是基于RetinaNet的数据驱动深度学习模型，专门用于增强航空图像中小物体的检测能力，通过自主确定最优特征图和锚点估计，在有限数据条件下实现高效训练和精确检测。


<details>
  <summary>Details</summary>
Motivation: 航空图像中小物体检测在环境监测、城市规划、危机管理等多个领域具有关键应用价值，但现有方法在数据有限条件下效果不佳，需要开发更高效的检测模型。

Method: 提出DDR-Net模型，引入数据驱动技术自动确定最优特征图和锚点估计，开发创新的采样技术来提升在有限数据训练条件下的模型效能。

Result: 在多种航空鸟类图像数据集上的实证评估表明，DDR-Net显著优于RetinaNet和其他当代模型，大幅降低了数据收集和训练的成本与时间。

Conclusion: DDR-Net的创新技术推动了当前航空图像分析技术的发展，在农业、安全和考古等多个领域具有广泛的应用前景和重要影响。

Abstract: In the realm of aerial imaging, the ability to detect small objects is
pivotal for a myriad of applications, encompassing environmental surveillance,
urban design, and crisis management. Leveraging RetinaNet, this work unveils
DDR-Net: a data-driven, deep-learning model devised to enhance the detection of
diminutive objects. DDR-Net introduces novel, data-driven techniques to
autonomously ascertain optimal feature maps and anchor estimations, cultivating
a tailored and proficient training process while maintaining precision.
Additionally, this paper presents an innovative sampling technique to bolster
model efficacy under limited data training constraints. The model's enhanced
detection capabilities support critical applications including wildlife and
habitat monitoring, traffic flow optimization, and public safety improvements
through accurate identification of small objects like vehicles and pedestrians.
DDR-Net significantly reduces the cost and time required for data collection
and training, offering efficient performance even with limited data. Empirical
assessments over assorted aerial avian imagery datasets demonstrate that
DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These
innovations advance current aerial image analysis technologies and promise
wide-ranging impacts across multiple sectors including agriculture, security,
and archaeology.

</details>


### [10] [STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images](https://arxiv.org/abs/2509.02952)
*Zeyu Liu,Shengwei Ding*

Main category: cs.CV

TL;DR: STAR是一个快速、轻量级的开源框架，用于全切片组织病理图像的刚性配准，特别适用于连续切片的多染色对齐，具有分层相关策略和内置质量控制。


<details>
  <summary>Details</summary>
Motivation: 现有的序列全切片组织病理图像配准方法通常依赖计算密集且难以复现的复杂可变形或深度学习方法，而适用于连续切片场景的轻量级刚性配准框架开发不足。

Method: STAR整合了染色条件预处理、分层粗到精相关策略、自适应核缩放和内置质量控制，实现跨异质组织类型和染色方案的可靠刚性配准。

Result: 在ANHIR 2019和ACROBAT 2022数据集上的评估显示，STAR能在几分钟内为每张切片产生稳定的对齐结果，对跨染色变异性和部分组织重叠具有鲁棒性。

Conclusion: STAR作为一个开源轻量级工具，为临床采用降低了门槛，并为下一代计算病理学的大规模配对数据准备提供了可复现的基线。

Abstract: Registration of serial whole-slide histopathological images (WSIs) is
critical for enabling direct comparison across diverse stains and for preparing
paired datasets in artificial intelligence (AI) workflows such as virtual
staining and biomarker prediction. While existing methods often rely on complex
deformable or deep learning approaches that are computationally intensive and
difficult to reproduce, lightweight rigid frameworks-sufficient for many
consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial
Tissue Alignment for Rigid registration), a fast and robust open-source
framework for multi-WSI alignment. STAR integrates stain-conditioned
preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive
kernel scaling, and built-in quality control, achieving reliable rigid
registration across heterogeneous tissue types and staining protocols,
including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,
PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).
Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs
and scanning conditions, STAR consistently produced stable alignments within
minutes per slide, demonstrating robustness to cross-stain variability and
partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC
alignment, construction of multi-IHC panels, and typical failure modes,
underscoring both utility and limitations. Released as an open and lightweight
tool, STAR provides a reproducible baseline that lowers the barrier for
clinical adoption and enables large-scale paired data preparation for
next-generation computational pathology.

</details>


### [11] [Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability](https://arxiv.org/abs/2509.02962)
*Shuai Jiang,Yunfeng Ma,Jingyu Zhou,Yuan Bian,Yaonan Wang,Min Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种面向多模态工业表面缺陷检测的方法，解决因传感器可用性不确定导致的模态缺失问题，通过跨模态提示学习和对称对比学习来提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 多模态工业表面缺陷检测中存在传感器可用性不确定导致的模态缺失问题，传统多模态融合方法遇到模式转换和信息缺失等挑战。

Method: 提出跨模态提示学习（包括跨模态一致性提示、模态特定提示和缺失感知提示）和对称对比学习（利用文本模态作为双视觉模态融合的桥梁）。

Result: 在RGB和3D模态总缺失率0.7的情况下，方法达到73.83% I-AUROC和93.05% P-AUROC，超过现有最佳方法3.84%和5.58%，在不同缺失类型和率下都表现优异。

Conclusion: 该方法有效解决了多模态工业表面缺陷检测中的模态缺失问题，通过创新的提示学习和对比学习机制显著提高了检测性能。

Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and
locate defect in industrial products by fusing RGB and 3D modalities. This
article focuses on modality-missing problems caused by uncertain sensors
availability in MISDD. In this context, the fusion of multiple modalities
encounters several troubles, including learning mode transformation and
information vacancy. To this end, we first propose cross-modal prompt learning,
which includes: i) the cross-modal consistency prompt serves the establishment
of information consistency of dual visual modalities; ii) the modality-specific
prompt is inserted to adapt different input patterns; iii) the missing-aware
prompt is attached to compensate for the information vacancy caused by dynamic
modalities-missing. In addition, we propose symmetric contrastive learning,
which utilizes text modality as a bridge for fusion of dual vision modalities.
Specifically, a paired antithetical text prompt is designed to generate binary
text semantics, and triple-modal contrastive pre-training is offered to
accomplish multimodal learning. Experiment results show that our proposed
method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7
for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%
respectively), and outperforms existing approaches to varying degrees under
different missing types and rates. The source code will be available at
https://github.com/SvyJ/MISDD-MM.

</details>


### [12] [EdgeAttNet: Towards Barb-Aware Filament Segmentation](https://arxiv.org/abs/2509.02964)
*Victor Solomon,Piet Martens,Jingyu Liu,Rafal Angryk*

Main category: cs.CV

TL;DR: EdgeAttNet是一种基于U-Net的太阳细丝分割架构，通过引入可学习的边缘图来增强注意力机制，显著提高了细丝边界和分支的识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在H-alpha观测中难以捕捉太阳细丝的精细结构（特别是分支），主要原因是长程依赖建模能力和空间细节处理能力有限。

Method: 在U-Net基础上引入直接从输入图像推导的可学习边缘图，通过线性变换注意力机制的Key和Query矩阵，将边缘信息整合到网络瓶颈处的自注意力计算中。

Result: 在MAGFILO数据集上优于U-Net和其他基于U-Net的transformer基线，实现了更高的分割精度和显著更好的细丝分支识别，推理速度更快。

Conclusion: 通过将结构先验显式整合到注意力计算中，EdgeAttNet增强了空间敏感性和分割精度，同时减少了可训练参数数量，适合实际部署。

Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical
for determining filament chirality, a key factor in the behavior of Coronal
Mass Ejections (CMEs). However, existing methods often fail to capture
fine-scale filament structures, particularly barbs, due to a limited ability to
model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone
by introducing a novel, learnable edge map derived directly from the input
image. This edge map is incorporated into the model by linearly transforming
the attention Key and Query matrices with the edge information, thereby guiding
the self-attention mechanism at the network's bottleneck to more effectively
capture filament boundaries and barbs. By explicitly integrating this
structural prior into the attention computations, EdgeAttNet enhances spatial
sensitivity and segmentation accuracy while reducing the number of trainable
parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based
transformer baselines on the MAGFILO dataset. It achieves higher segmentation
accuracy and significantly better recognition of filament barbs, with faster
inference performance suitable for practical deployment.

</details>


### [13] [KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966)
*Yujin Wang,Tianyi Wang,Quanfeng Liu,Wenxian Fan,Junfeng Jiao,Christian Claudel,Yunbing Yan,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: KEPT是一个知识增强的视觉语言模型框架，通过检索场景对齐的范例和链式思维提示，在nuScenes数据集上实现了最先进的短视距轨迹预测性能，平均L2误差0.31m，碰撞率0.07%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶轨迹预测中难以有效基于场景动态和领域知识进行推理，需要提升预测准确性和安全性。

Method: 结合时间频率-空间融合视频编码器、k-means+HNSW检索堆栈提供场景对齐范例，通过链式思维提示嵌入规划约束，采用三阶段微调策略对齐空间线索、物理可行运动和时间条件规划。

Result: 在nuScenes数据集上，NoAvg协议下达到0.70m平均L2误差和0.21%碰撞率；TemAvg协议下达到0.31m平均L2误差和0.07%碰撞率，检索延迟亚毫秒级。

Conclusion: 检索增强、链式思维引导的视觉语言模型为可解释和可信的自动驾驶提供了一条有前景的数据高效路径。

Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable
autonomous driving, yet existing vision-language models (VLMs) often fail to
effectively ground their reasoning in scene dynamics and domain knowledge. To
address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM
framework that predicts ego trajectories directly from consecutive front-view
driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video
encoder, trained via self-supervised learning with hard-negative mining, with a
scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.
Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit
planning constraints, while a triple-stage fine-tuning schedule incrementally
aligns the language head to metric spatial cues, physically feasible motion,
and temporally conditioned front-view planning. Evaluated on nuScenes dataset,
KEPT achieves state-of-the-art performance across open-loop protocols: under
NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg
with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision
rate. Ablation studies show that all three fine-tuning stages contribute
complementary benefits, and that using Top-2 retrieved exemplars yields the
best accuracy-safety trade-off. The k-means-clustered HNSW index delivers
sub-millisecond retrieval latency, supporting practical deployment. These
results indicate that retrieval-augmented, CoT-guided VLMs offer a promising,
data-efficient pathway toward interpretable and trustworthy autonomous driving.

</details>


### [14] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025挑战赛聚焦于社交媒体平台用户生成短视频的参与度预测，使用包含真实用户互动数据的新数据集，吸引了97名参与者提交15份有效测试方案。


<details>
  <summary>Details</summary>
Motivation: 理解和建模社交媒体平台上用户生成内容（UGC）短视频的受欢迎程度，促进能够捕捉影响用户参与度复杂因素的鲁棒建模策略发展。

Method: 使用新的短格式UGC数据集，包含源自真实用户互动的参与度指标，参与者探索了包括视觉内容、音频和创作者提供的元数据在内的多模态特征。

Result: 挑战赛吸引了97名参与者，收到了15份有效的测试提交，在短格式UGC视频参与度预测方面取得了显著进展。

Conclusion: 该挑战赛成功推动了短格式UGC视频参与度预测领域的发展，为理解用户参与行为提供了重要数据和模型基础。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [15] [InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System](https://arxiv.org/abs/2509.02973)
*Xianbao Hou,Yonghao He,Zeyd Boukhers,John See,Hu Su,Wei Sui,Cong Yang*

Main category: cs.CV

TL;DR: InstaDA是一个无需训练的双代理系统，通过LLM和扩散模型的深度协作来增强实例分割数据集，在LVIS验证集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的实例分割数据标注成本高且存在类别不平衡问题，现有方法缺乏LLM与扩散模型的深度协作，未能充分利用训练数据中的丰富信息。

Method: 提出双代理系统：1) Text-Agent通过Prompt Rethink机制迭代优化提示词，促进LLM与扩散模型协作；2) Image-Agent基于训练图像生成新实例来丰富数据分布。两个代理都是独立自动化的。

Result: 在LVIS 1.0验证集上，相比基线方法box AP提升+4.0，mask AP提升+3.3；相比领先模型DiverGen，box AP提升+0.3，mask AP提升+0.1，在常见类别上表现尤为突出。

Conclusion: InstaDA通过LLM与扩散模型的深度协作有效解决了实例分割数据稀缺和类别不平衡问题，无需训练即可显著提升模型性能。

Abstract: Acquiring high-quality instance segmentation data is challenging due to the
labor-intensive nature of the annotation process and significant class
imbalances within datasets. Recent studies have utilized the integration of
Copy-Paste and diffusion models to create more diverse datasets. However, these
studies often lack deep collaboration between large language models (LLMs) and
diffusion models, and underutilize the rich information within the existing
training data. To address these limitations, we propose InstaDA, a novel,
training-free Dual-Agent system designed to augment instance segmentation
datasets. First, we introduce a Text-Agent (T-Agent) that enhances data
diversity through collaboration between LLMs and diffusion models. This agent
features a novel Prompt Rethink mechanism, which iteratively refines prompts
based on the generated images. This process not only fosters collaboration but
also increases image utilization and optimizes the prompts themselves.
Additionally, we present an Image-Agent (I-Agent) aimed at enriching the
overall data distribution. This agent augments the training set by generating
new instances conditioned on the training images. To ensure practicality and
efficiency, both agents operate as independent and automated workflows,
enhancing usability. Experiments conducted on the LVIS 1.0 validation set
indicate that InstaDA achieves significant improvements, with an increase of
+4.0 in box average precision (AP) and +3.3 in mask AP compared to the
baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in
box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common
categories and mask AP gains of +0.2 on common categories and +0.5 on frequent
categories.

</details>


### [16] [SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation](https://arxiv.org/abs/2509.02993)
*Chao Fan,Xibin Jia,Anqi Xiao,Hongyuan Yu,Zhenghan Yang,Dawei Yang,Hui Xu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出SPENet网络解决少样本医学图像分割中单原型方法忽略类内变化的问题，通过多级原型生成和查询引导的局部原型增强模块提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的方法通常为支持图像生成单一全局原型来匹配查询图像，忽略了类内变化，导致性能受限

Method: 提出SPENet网络，包含多级原型生成模块（同时生成全局原型和自适应数量的局部原型）和查询引导的局部原型增强模块（利用查询图像指导优化支持原型）

Result: 在三个公共医学数据集上的大量实验表明，SPENet优于现有最先进方法，实现了优越性能

Conclusion: SPENet通过多粒度原型匹配和查询引导的原型增强，有效解决了少样本医学图像分割中的类内变化问题，取得了state-of-the-art性能

Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of
medical objects using only a few labeled images. Prototype-based methods have
made significant progress in addressing FSMIS. However, they typically generate
a single global prototype for the support image to match with the query image,
overlooking intra-class variations. To address this issue, we propose a
Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce
a Multi-level Prototype Generation (MPG) module, which enables
multi-granularity measurement between the support and query images by
simultaneously generating a global prototype and an adaptive number of local
prototypes. Additionally, we observe that not all local prototypes in the
support image are beneficial for matching, especially when there are
substantial discrepancies between the support and query images. To alleviate
this issue, we propose a Query-guided Local Prototype Enhancement (QLPE)
module, which adaptively refines support prototypes by incorporating guidance
from the query image, thus mitigating the negative effects of such
discrepancies. Extensive experiments on three public medical datasets
demonstrate that SPENet outperforms existing state-of-the-art methods,
achieving superior performance.

</details>


### [17] [SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2509.03002)
*Chenhao Wang,Yingrui Ji,Yu Meng,Yunjian Zhang,Yao Zhu*

Main category: cs.CV

TL;DR: 提出了SOPSeg框架，专门用于遥感图像中的小目标分割，通过区域自适应放大策略和定制解码器解决小目标分割难题，并构建了首个小目标实例分割数据集。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中小目标提取在多个应用中至关重要，但现有研究主要关注小目标检测，实例分割领域缺乏专门数据集和技术方案。SAM模型在小目标分割上性能显著下降，主要由于粗粒度特征分辨率导致空间细节丢失。

Method: SOPSeg框架包含：1）区域自适应放大策略保持细粒度细节；2）集成边缘预测和渐进细化的定制解码器；3）针对遥感应用中广泛采用的定向边界框的提示机制。

Result: SOPSeg在小目标分割方面优于现有方法，并能有效支持遥感任务的数据集构建。基于SODA-A构建了全面的小目标实例分割数据集。

Conclusion: SOPSeg为解决遥感图像小目标分割问题提供了有效方案，填补了该领域的技术空白，模型和数据集将公开发布以支持未来研究。

Abstract: Extracting small objects from remote sensing imagery plays a vital role in
various applications, including urban planning, environmental monitoring, and
disaster management. While current research primarily focuses on small object
detection, instance segmentation for small objects remains underexplored, with
no dedicated datasets available. This gap stems from the technical challenges
and high costs of pixel-level annotation for small objects. While the Segment
Anything Model (SAM) demonstrates impressive zero-shot generalization, its
performance on small-object segmentation deteriorates significantly, largely
due to the coarse 1/16 feature resolution that causes severe loss of fine
spatial details. To this end, we propose SOPSeg, a prompt-based framework
specifically designed for small object segmentation in remote sensing imagery.
It incorporates a region-adaptive magnification strategy to preserve
fine-grained details, and employs a customized decoder that integrates edge
prediction and progressive refinement for accurate boundary delineation.
Moreover, we introduce a novel prompting mechanism tailored to the oriented
bounding boxes widely adopted in remote sensing applications. SOPSeg
outperforms existing methods in small object segmentation and facilitates
efficient dataset construction for remote sensing tasks. We further construct a
comprehensive small object instance segmentation dataset based on SODA-A, and
will release both the model and dataset to support future research.

</details>


### [18] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种后处理水印方法，通过集成攻击网络训练增强水印鲁棒性，在WAVES基准测试中显著提升了基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度水印研究主要关注处理中水印，而后处理水印更具灵活性，可应用于任何生成模型的输出且无需访问模型内部结构，还能为单个图像嵌入独特水印。

Method: 构建基于CNN和Transformer的空间域和频域攻击网络组合，研究不同组合对水印模型鲁棒性的影响，采用集成攻击网络进行训练。

Result: 空间域CNN攻击网络与频域Transformer攻击网络的组合在水印模型中表现出最高鲁棒性，在WAVES基准测试中平均比特准确率显著提升，特别是对再生攻击，StegaStamp方法提升了18.743%。

Conclusion: 集成攻击网络训练能有效增强后处理水印的鲁棒性，空间域CNN与频域Transformer的组合是最优攻击网络配置。

Abstract: Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [19] [Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations](https://arxiv.org/abs/2509.03011)
*Alexis Ivan Lopez Escamilla,Gilberto Ochoa,Sharib Al*

Main category: cs.CV

TL;DR: 提出了一种用于溃疡性结肠炎的病灶感知图像字幕框架，整合了ResNet嵌入、Grad-CAM热图和CBAM增强注意力机制，结合T5解码器生成结构化临床描述。


<details>
  <summary>Details</summary>
Motivation: 为溃疡性结肠炎内镜检查提供自动化的结构化报告生成，提高临床报告的质量和一致性，支持可靠的 endoscopic 报告。

Method: 集成ResNet特征提取、Grad-CAM热图可视化、CBAM注意力增强机制，使用T5解码器，并注入临床元数据（MES评分0-3、血管模式、出血、红斑、易碎性、溃疡等）作为自然语言提示。

Result: 相比基线方法，该方法在字幕质量和MES分类准确性方面都有显著提升。

Conclusion: 该病灶感知图像字幕框架能够生成与临床实践一致的结构化、可解释的描述，同时提供MES分类和病灶标签，支持可靠的 endoscopic 报告系统。

Abstract: We present a lesion-aware image captioning framework for ulcerative colitis
(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and
CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,
vascular pattern, bleeding, erythema, friability, ulceration) is injected as
natural-language prompts to guide caption generation. The system produces
structured, interpretable descriptions aligned with clinical practice and
provides MES classification and lesion tags. Compared with baselines, our
approach improves caption quality and MES classification accuracy, supporting
reliable endoscopic reporting.

</details>


### [20] [Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens](https://arxiv.org/abs/2509.03025)
*Sohee Kim,Soohyun Ryu,Joonhyung Park,Eunho Yang*

Main category: cs.CV

TL;DR: LVLMs存在将文本输入误认为图像内容的幻觉问题，研究发现特定FFN神经元(VA神经元)能够识别视觉缺失，基于此开发了检测模块和方法来修正输出。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)经常错误地将缺乏视觉证据的文本输入感知为图像的一部分，导致错误响应。需要探究模型是否具备判断文本概念是否基于图像内容的内在能力。

Method: 1) 发现特定FFN神经元(VA神经元)通过独特的激活模式指示视觉缺失；2) 开发检测模块系统分类输入token是否视觉接地；3) 基于预测重新解释问题提示或在生成过程中替换检测到的缺失token

Result: 实验表明该方法有效缓解了模型错误假设文本输入视觉存在的倾向，并在各种LVLMs中具有通用性。

Conclusion: 通过识别和利用VA神经元的激活模式，可以系统性地检测和修正LVLMs中的视觉幻觉问题，提高模型输出的准确性。

Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses
by jointly interpreting visual and textual inputs. However, our finding reveals
they often mistakenly perceive text inputs lacking visual evidence as being
part of the image, leading to erroneous responses. In light of this finding, we
probe whether LVLMs possess an internal capability to determine if textual
concepts are grounded in the image, and discover a specific subset of
Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,
that consistently signal the visual absence through a distinctive activation
pattern. Leveraging these patterns, we develop a detection module that
systematically classifies whether an input token is visually grounded. Guided
by its prediction, we propose a method to refine the outputs by reinterpreting
question prompts or replacing the detected absent tokens during generation.
Extensive experiments show that our method effectively mitigates the models'
tendency to falsely presume the visual presence of text input and its
generality across various LVLMs.

</details>


### [21] [Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification](https://arxiv.org/abs/2509.03032)
*Kaicong Huang,Talha Azfar,Jack M. Reilly,Thomas Guggisberg,Ruimin Ke*

Main category: cs.CV

TL;DR: 提出了一种端到端的双分支跨模态特征提取框架，联合建模前景和背景信息，通过语义对齐和对抗学习策略提升行人重识别性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注前景信息而忽略背景线索的价值，但人类感知中背景语义与前景语义同等重要。需要同时利用前景和背景信息来提升模型对复杂遮挡场景的处理能力

Method: 双分支跨模态特征提取管道，提出域内语义对齐和域间语义对抗学习策略：对齐相同语义的视觉和文本特征，同时惩罚前景和背景特征之间的相似性

Result: 在两个整体和两个遮挡ReID基准测试上进行了全面实验，结果匹配或超越了当前最先进方法，证明了方法的有效性和通用性

Conclusion: 背景语义在行人重识别中具有重要价值，联合建模前景和背景信息的双分支框架能有效提升模型性能，特别是在复杂遮挡场景下

Abstract: Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network's discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.

</details>


### [22] [MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model](https://arxiv.org/abs/2509.03041)
*Pengyang Yu,Haoquan Wang,Gerard Marks,Tahar Kechadi,Laurence T. Yang,Sahraoui Dhelim,Nyothiri Aung*

Main category: cs.CV

TL;DR: MedLiteNet是一个轻量级的CNN-Transformer混合网络，专门用于皮肤病变分割，通过分层特征提取和多尺度上下文聚合实现高精度分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：CNN感受野有限，难以建模长距离依赖；Vision Transformer虽然能捕获全局上下文，但二次复杂度和大量参数限制了在小型医学数据集上的应用。

Method: 使用深度可分离Mobile Inverted Bottleneck块构建编码器以减少计算量，插入瓶颈级跨尺度token混合单元在不同分辨率间交换信息，嵌入边界感知自注意力模块来锐化病变轮廓。

Result: 该方法在皮肤镜分割任务中实现了高精度，同时保持了轻量级特性。

Conclusion: MedLiteNet为皮肤癌计算机辅助诊断提供了一种高效准确的病变分割解决方案，特别适合小样本医学数据集。

Abstract: Accurate skin-lesion segmentation remains a key technical challenge for
computer-aided diagnosis of skin cancer. Convolutional neural networks, while
effective, are constrained by limited receptive fields and thus struggle to
model long-range dependencies. Vision Transformers capture global context, yet
their quadratic complexity and large parameter budgets hinder use on the
small-sample medical datasets common in dermatology. We introduce the
MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic
segmentation that achieves high precision through hierarchical feature
extraction and multi-scale context aggregation. The encoder stacks depth-wise
Mobile Inverted Bottleneck blocks to curb computation, inserts a
bottleneck-level cross-scale token-mixing unit to exchange information between
resolutions, and embeds a boundary-aware self-attention module to sharpen
lesion contours.

</details>


### [23] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: 这篇论文提出了一种动态条件双激洞桥训练范式，以解决条件激洞模型在多任务场景中的挑战，特别是在训练数据缺乏的不良问题中。


<details>
  <summary>Details</summary>
Motivation: 条件激洞模型在多份务场景中难以利用任务间的内在关联性，传统静态条件控制方式不适合具有动态演化特性的多任务学习。

Method: 提出动态条件双激洞桥训练范式，将激洞和条件生成过程解耦，使用相同的噪声调度生成动态条件，逐渐调整统计特征并嵌入时间相关信息。

Result: 在去霜和可见-红外融合两个典型不良问题多任务场景中，在公开数据集上达到了多个指标的最佳性能。

Conclusion: 动态条件方法显著提升了模型在多任务场景中的学习效果，特别是在训练数据缺乏的不良问题中显示出优勃性能。

Abstract: Conditional diffusion models have made impressive progress in the field of
image processing, but the characteristics of constructing data distribution
pathways make it difficult to exploit the intrinsic correlation between tasks
in multi-task scenarios, which is even worse in ill-posed tasks with a lack of
training data. In addition, traditional static condition control makes it
difficult for networks to learn in multi-task scenarios with its dynamically
evolving characteristics. To address these challenges, we propose a dynamic
conditional double diffusion bridge training paradigm to build a general
framework for ill-posed multi-tasks. Firstly, this paradigm decouples the
diffusion and condition generation processes, avoiding the dependence of the
diffusion model on supervised data in ill-posed tasks. Secondly, generated by
the same noise schedule, dynamic conditions are used to gradually adjust their
statistical characteristics, naturally embed time-related information, and
reduce the difficulty of network learning. We analyze the learning objectives
of the network under different conditional forms in the single-step denoising
process and compare the changes in its attention weights in the network,
demonstrating the superiority of our dynamic conditions. Taking dehazing and
visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve
the best performance in multiple indicators on public datasets. The code has
been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [24] [Isolated Bangla Handwritten Character Classification using Transfer Learning](https://arxiv.org/abs/2509.03061)
*Abdul Karim,S M Rafiuddin,Jahidul Islam Razin,Tahira Alam*

Main category: cs.CV

TL;DR: 使用迁移学习和多种深度神经网络技术（3DCNN、ResNet、MobileNet）对孟加拉语手写字符进行分类，在包含166,105个样本的数据集上取得了99.46%的测试准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语包含50个基本字符和许多复合字符，现有研究主要针对基本字符识别。本文旨在开发一个能够同时识别基本字符和复合字符的端到端分类模型，并解决梯度消失问题。

Method: 采用迁移学习方法，结合3D卷积神经网络、残差神经网络和MobileNet等多种深度神经网络技术，构建端到端的分类模型。使用Bangla Lekha Isolated数据集（84个类别，166,105个样本）进行训练和测试。

Result: 模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。与现有最先进的孟加拉语手写字符分类基准相比，该模型取得了更好的准确率。

Conclusion: 提出的基于迁移学习和多种深度神经网络的方法能够有效识别孟加拉语手写字符（包括基本字符和复合字符），在准确率方面优于现有技术，为解决复杂字符识别问题提供了有效解决方案。

Abstract: Bangla language consists of fifty distinct characters and many compound
characters. Several notable studies have been performed to recognize Bangla
characters, both handwritten and optical. Our approach uses transfer learning
to classify the basic, distinct, as well as compound Bangla handwritten
characters while avoiding the vanishing gradient problem. Deep Neural Network
techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural
Network (ResNet), and MobileNet are applied to generate an end-to-end
classification of all possible standard formations of handwritten characters in
the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105
Bangla character image samples categorized into 84 distinct classes, is used
for this classification model. The model achieved 99.82% accuracy on training
data and 99.46% accuracy on test data. Comparisons with various
state-of-the-art benchmarks of Bangla handwritten character classification show
that the proposed model achieves better accuracy in classifying the data.

</details>


### [25] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 使用GAN生成器网络生成假手写字符图片，通过添加对抗性威胁噪声和过滤低信心度样本来扩充训练数据，ADA-GAN模型在复杂和草书字符分类中显示出更好的稳健性和效果。


<details>
  <summary>Details</summary>
Motivation: 手写字符因其复杂性和草书性质，比简单非草书字符更难分类，需要开发更稳健有效的分类方法。

Method: 提出ADA-GAN模型，包括生成器网络生成假手写字符图片，通过添加对抗性威胁噪声和过滤低信心度样本来扩充训练数据集。

Result: 卷积神经网络的准确性随字符复杂度增加而下降，但ADA-GAN模型在复杂和草书字符分类中显示出更好的稳健性和效果。

Conclusion: ADA-GAN通过生成对抗网络和数据增帽技术，能够有效提高复杂手写字符分类的性能，为手写字符识别领域提供了新的解决方案。

Abstract: Handwritten characters can be trickier to classify due to their complex and
cursive nature compared to simple and non-cursive characters. We present an
external classifier along with a Generative Adversarial Network that can
classify highly cursive and complex characters. The generator network produces
fake handwritten character images, which are then used to augment the training
data after adding adversarially perturbed noise and achieving a confidence
score above a threshold with the discriminator network. The results show that
the accuracy of convolutional neural networks decreases as character complexity
increases, but our proposed model, ADA-GAN, remains more robust and effective
for both cursive and complex characters.

</details>


### [26] [TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis](https://arxiv.org/abs/2509.03095)
*Clément Hervé,Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.CV

TL;DR: 通过利用非医疗大规模3D数据集训练的TRELLIS生成模型的几何嵌入特征，提升了脱罩动脉穩的分类、分割咈血流模拟任务的性能


<details>
  <summary>Details</summary>
Motivation: 脱罩动脉穩临床风险高但缺乏注释3D数据，需要有效的检测咈模型方法

Method: 采用跨域特征转移方法，将TRELLIS模型学习的表面特征替代传统点法向或网格描述子，应用于分类、分割咈血流预测任务

Result: 在准确率、F1分数咈分割质量方面显著提升，模拟误差减少15%

Conclusion: 证明了将通用生成模型的3D表示转移到专业医疗任务的广阔潜力

Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to
detect, delineate and model due to limited annotated 3D data. We propose a
cross-domain feature-transfer approach that leverages the latent geometric
embeddings learned by TRELLIS, a generative model trained on large-scale
non-medical 3D datasets, to augment neural networks for aneurysm analysis. By
replacing conventional point normals or mesh descriptors with TRELLIS surface
features, we systematically enhance three downstream tasks: (i) classifying
aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting
aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving
blood-flow fields using a graph neural network on the AnXplore dataset. Our
experiments show that the inclusion of these features yields strong gains in
accuracy, F1-score and segmentation quality over state-of-the-art baselines,
and reduces simulation error by 15\%. These results illustrate the broader
potential of transferring 3D representations from general-purpose generative
models to specialized medical tasks.

</details>


### [27] [Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods](https://arxiv.org/abs/2509.03108)
*Shota Iwamatsu,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 本文提出了一种针对人脸反欺骗检测系统的后门投毒攻击方法，通过在活体人脸图像中嵌入欺骗攻击特征，使特定欺骗攻击能够绕过检测而不引起视觉变化。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到使用用户照片的欺骗攻击，现有反欺骗检测方法依赖深度学习需要大量训练数据，如果训练数据被恶意注入后门，可能导致特定欺骗攻击被错误分类为活体。

Method: 提出后门投毒攻击方法，从欺骗攻击的人脸图像中提取特征并嵌入到活体人脸图像中，不产生可察觉的视觉变化，从而使特定欺骗攻击能够绕过检测。

Result: 在公共数据集上的实验表明，该方法对现有欺骗攻击检测系统构成实际威胁。

Conclusion: 该方法证明了人脸反欺骗检测中后门投毒的潜在威胁，需要加强对此类攻击的防御措施。

Abstract: Face recognition systems are robust against environmental changes and noise,
and thus may be vulnerable to illegal authentication attempts using user face
photos, such as spoofing attacks. To prevent such spoofing attacks, it is
crucial to discriminate whether the input image is a live user image or a
spoofed image prior to the face recognition process. Most existing spoofing
attack detection methods utilize deep learning, which necessitates a
substantial amount of training data. Consequently, if malicious data is
injected into a portion of the training dataset, a specific spoofing attack may
be erroneously classified as live, leading to false positives.In this paper, we
propose a novel backdoor poisoning attack method to demonstrate the latent
threat of backdoor poisoning within face anti-spoofing detection. The proposed
method enables certain spoofing attacks to bypass detection by embedding
features extracted from the spoofing attack's face image into a live face image
without inducing any perceptible visual alterations.Through experiments
conducted on public datasets, we demonstrate that the proposed method
constitutes a realistic threat to existing spoofing attack detection systems.

</details>


### [28] [Information transmission: Inferring change area from change moment in time series remote sensing images](https://arxiv.org/abs/2509.03112)
*Jialu Li,Chen Wu,Meiqi Hu*

Main category: cs.CV

TL;DR: CAIM-Net是一个时间序列变化检测网络，通过从变化时刻推断变化区域，确保变化区域和变化时刻结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将变化区域检测和变化时刻识别作为独立任务处理，而实际上变化区域可以从变化时刻推断出来，因此需要确保两者结果的一致性。

Method: CAIM-Net包含三个关键步骤：差异提取与增强、粗粒度变化时刻提取、细粒度变化时刻提取与变化区域推断。使用轻量级编码器提取差异特征，通过边界增强卷积放大特征，进行时空相关性分析，最后利用多尺度时间CAM模块加权变化时刻并推断变化区域。

Result: 该方法能够同时指示变化发生的位置和时间，确保变化区域和变化时刻结果的一致性。

Conclusion: CAIM-Net通过从变化时刻推断变化区域的方法，有效解决了时间序列变化检测中变化区域和变化时刻结果不一致的问题，为生态系统动态探索提供了更可靠的工具。

Abstract: Time series change detection is a critical task for exploring ecosystem
dynamics using time series remote sensing images, because it can simultaneously
indicate where and when change occur. While deep learning has shown excellent
performance in this domain, it continues to approach change area detection and
change moment identification as distinct tasks. Given that change area can be
inferred from change moment, we propose a time series change detection network,
named CAIM-Net (Change Area Inference from Moment Network), to ensure
consistency between change area and change moment results. CAIM-Net infers
change area from change moment based on the intrinsic relationship between time
series analysis and spatial change detection. The CAIM-Net comprises three key
steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,
and Fine Change Moment Extraction and Change Area Inference. In the Difference
Extraction and Enhancement, a lightweight encoder with batch dimension stacking
is designed to rapidly extract difference features. Subsequently, boundary
enhancement convolution is applied to amplify these difference features. In the
Coarse Change Moment Extraction, the enhanced difference features from the
first step are used to spatiotemporal correlation analysis, and then two
distinct methods are employed to determine coarse change moments. In the Fine
Change Moment Extraction and Change Area Inference, a multiscale temporal Class
Activation Mapping (CAM) module first increases the weight of the
change-occurring moment from coarse change moments. Then the weighted change
moment is used to infer change area based on the fact that pixels with the
change moment must have undergone a change.

</details>


### [29] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 提出基于梯度自反思的token影响力估计方法，通过影响感知对比解码框架同时缓解文本-视觉偏差和共现偏差，无需额外资源即可有效减少多模态大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有方法启发式地处理多模态大语言模型中的文本-视觉偏差和共现偏差，但缺乏对实例间波动偏差水平的理解，需要更精准的偏差缓解方法

Method: 使用基于梯度的自反思方法估计不同类型token（视觉、提示、先前输出）的影响力，检测物体相关视觉token，并将其整合到影响感知对比解码框架中

Result: 在LLaVA-QA90上实现了高达92%的准确率提升，有效减少了幻觉现象

Conclusion: 该方法无需额外微调、额外模型或数据统计，能够同时缓解两种类型的偏差，显著提升多模态语言模型的性能

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


### [30] [Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge](https://arxiv.org/abs/2509.03114)
*Miao Xu,Xiangyu Zhu,Xusheng Liang,Zidu Wang,Jinlin Wu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出GravityDB方法，通过引力场驱动的扩散桥解决手-物体交互中的穿透、间隙和手部变形问题，生成物理合理且语义指导的交互效果。


<details>
  <summary>Details</summary>
Motivation: 现有手-物体姿态估计方法存在穿透、接触区域间隙问题，且难以捕捉手部在交互过程中的真实变形，需要更精确的交互建模方法。

Method: 将手-物体交互建模为引力场驱动过程，提出基于引力场的扩散桥(GravityDB)方法，模拟可变形手部表面与刚性物体的交互，并引入文本语义信息指导引力场构建。

Result: 在多个数据集上的定性和定量实验表明，该方法能有效消除穿透、确保稳定抓握、捕捉真实手部变形，生成物理合理的交互效果。

Conclusion: GravityDB方法通过引力场驱动的扩散桥成功解决了手-物体交互中的关键问题，实现了无穿透、稳定抓握且包含真实变形的物理合理交互建模。

Abstract: Existing reconstruction or hand-object pose estimation methods are capable of
producing coarse interaction states. However, due to the complex and diverse
geometry of both human hands and objects, these approaches often suffer from
interpenetration or leave noticeable gaps in regions that are supposed to be in
contact. Moreover, the surface of a real human hand undergoes non-negligible
deformations during interaction, which are difficult to capture and represent
with previous methods. To tackle these challenges, we formulate hand-object
interaction as an attraction-driven process and propose a Gravity-Field Based
Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand
surface and rigid objects. Our approach effectively resolves the aforementioned
issues by generating physically plausible interactions that are free of
interpenetration, ensure stable grasping, and capture realistic hand
deformations. Furthermore, we incorporate semantic information from textual
descriptions to guide the construction of the gravitational field, enabling
more semantically meaningful interaction regions. Extensive qualitative and
quantitative experiments on multiple datasets demonstrate the effectiveness of
our method.

</details>


### [31] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出TADM-3D模型，使用3D扩散模型和脑龄估计器来预测MRI脑部结构随时间的变化，解决了现有方法在时间关系建模和3D上下文利用方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI预测方法存在三个主要问题：(1)无法明确捕捉结构变化与时间间隔的关系；(2)仅依赖扫描插值，缺乏临床实用性；(3)大多基于2D切片架构，忽略了完整的3D解剖上下文。需要开发能够准确预测脑部病理进展的3D时间感知模型。

Method: 提出TADM-3D（3D时间感知扩散模型），使用预训练的脑龄估计器(BAE)指导扩散模型生成反映预期年龄差异的MRI。还提出Back-In-Time正则化(BITR)，通过双向训练（从基线到随访的前向预测和从随访到基线的后向预测）来提升时间准确性。

Result: 在OASIS-3数据集上训练和评估，并在NACC数据集的外部测试集上验证了泛化性能。模型能够生成更准确反映时间变化的MRI扫描。

Conclusion: TADM-3D通过结合脑龄估计器和双向训练正则化，成功解决了现有方法在时间关系建模和3D上下文利用方面的局限性，为临床提供更准确的脑部病理进展预测工具。

Abstract: Generating realistic MRIs to accurately predict future changes in the
structure of brain is an invaluable tool for clinicians in assessing clinical
outcomes and analysing the disease progression at the patient level. However,
current existing methods present some limitations: (i) some approaches fail to
explicitly capture the relationship between structural changes and time
intervals, especially when trained on age-imbalanced datasets; (ii) others rely
only on scan interpolation, which lack clinical utility, as they generate
intermediate images between timepoints rather than future pathological
progression; and (iii) most approaches rely on 2D slice-based architectures,
thereby disregarding full 3D anatomical context, which is essential for
accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion
Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To
better model the relationship between time interval and brain changes, TADM-3D
uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in
the generation of MRIs that accurately reflect the expected age difference
between baseline and generated follow-up scans. Additionally, to further
improve the temporal awareness of TADM-3D, we propose the Back-In-Time
Regularisation (BITR), by training TADM-3D to predict bidirectionally from the
baseline to follow-up (forward), as well as from the follow-up to baseline
(backward). Although predicting past scans has limited clinical applications,
this regularisation helps the model generate temporally more accurate scans. We
train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the
generalisation performance on an external test set from the NACC dataset. The
code will be available upon acceptance.

</details>


### [32] [Preserving instance continuity and length in segmentation through connectivity-aware loss computation](https://arxiv.org/abs/2509.03154)
*Karol Szustakowski,Luk Frank,Julia Esser,Jan Gründemann,Marie Piraud*

Main category: cs.CV

TL;DR: 这篇论文提出了两种新的损失函数（负向中心线损失和简化拓扑损失），通过在卷积神经网络中应用这些损失函数来保持生物医学分割任务中延伸结构的连续性和长度准确性。


<details>
  <summary>Details</summary>
Motivation: 在生物医学分割任务中，延伸结构的连续性和长度保持比像素级准确性更为重要。由于信号丢失，当前的分割方法容易产生断续问题，影响下游应用的可靠性。

Method: 提出两种新的损失函数：Negative Centerline Loss和Simplified Topology Loss，并在卷积神经网络中应用。讨论了实验设计特征如下量和间距置换等方法来获得连续的分割面具。

Result: 在3D光切激光显微镜数据集上评估，与标准CNN和现有拓扑意识损失相比，新方法减少了每个实例的分割断续数量，尤其是在输入信号缺失的区域，从而改善了下游应用中的实例长度计算。

Conclusion: 论文证明了将结构先验知语强制到损失设计中可以显著提高生物应用中分割结果的可靠性。

Abstract: In many biomedical segmentation tasks, the preservation of elongated
structure continuity and length is more important than voxel-wise accuracy. We
propose two novel loss functions, Negative Centerline Loss and Simplified
Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help
preserve connectivity of output instances. Moreover, we discuss characteristics
of experiment design, such as downscaling and spacing correction, that help
obtain continuous segmentation masks. We evaluate our approach on a 3D
light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a
task prone to discontinuity due to signal dropout. Compared to standard CNNs
and existing topology-aware losses, our methods reduce the number of
segmentation discontinuities per instance, particularly in regions with missing
input signal, resulting in improved instance length calculation in downstream
applications. Our findings demonstrate that structural priors embedded in the
loss design can significantly enhance the reliability of segmentation for
biological applications.

</details>


### [33] [Count2Density: Crowd Density Estimation without Location-level Annotations](https://arxiv.org/abs/2509.03170)
*Mattia Litrico,Feng Chen,Michael Pound,Sotirios A Tsaftaris,Sebastiano Battiato,Mario Valerio Giuffrida*

Main category: cs.CV

TL;DR: Count2Density是一种仅使用计数级标注训练密度估计模型的新方法，通过历史地图库生成伪密度图，结合对比空间正则化，显著优于现有半监督方法


<details>
  <summary>Details</summary>
Motivation: 解决人群密度估计中细粒度位置标注收集困难、耗时且难以扩展的问题，降低对详细标注的依赖

Method: 使用历史地图库生成伪密度图，通过无监督显著性估计提供初始空间先验，采用EMA更新策略，结合超几何分布采样和自监督对比空间正则化

Result: 在多个数据集上显著优于跨域适应方法，在半监督设置下达到优于最新方法的结果，能够有效从计数标注中恢复空间信息

Conclusion: Count2Density成功证明了仅使用计数级标注即可训练出能够生成有意义密度图的有效模型，为实际应用提供了可扩展的解决方案

Abstract: Crowd density estimation is a well-known computer vision task aimed at
estimating the density distribution of people in an image. The main challenge
in this domain is the reliance on fine-grained location-level annotations,
(i.e. points placed on top of each individual) to train deep networks.
Collecting such detailed annotations is both tedious, time-consuming, and poses
a significant barrier to scalability for real-world applications. To alleviate
this burden, we present Count2Density: a novel pipeline designed to predict
meaningful density maps containing quantitative spatial information using only
count-level annotations (i.e., total number of people) during training. To
achieve this, Count2Density generates pseudo-density maps leveraging past
predictions stored in a Historical Map Bank, thereby reducing confirmation
bias. This bank is initialised using an unsupervised saliency estimator to
provide an initial spatial prior and is iteratively updated with an EMA of
predicted density maps. These pseudo-density maps are obtained by sampling
locations from estimated crowd areas using a hypergeometric distribution, with
the number of samplings determined by the count-level annotations. To further
enhance the spatial awareness of the model, we add a self-supervised
contrastive spatial regulariser to encourage similar feature representations
within crowded regions while maximising dissimilarity with background regions.
Experimental results demonstrate that our approach significantly outperforms
cross-domain adaptation methods and achieves better results than recent
state-of-the-art approaches in semi-supervised settings across several
datasets. Additional analyses validate the effectiveness of each individual
component of our pipeline, confirming the ability of Count2Density to
effectively retrieve spatial information from count-level annotations and
enabling accurate subregion counting.

</details>


### [34] [AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain](https://arxiv.org/abs/2509.03179)
*Alma M. Liezenga,Stefan Wijnja,Puck de Haan,Niels W. T. Brink,Jip J. van Stijn,Yori Kamphuis,Klamer Schutte*

Main category: cs.CV

TL;DR: 本文研究军事目标检测系统中的投毒攻击效果与检测方法，创建军事车辆数据集MilCivVeh，提出基于自动编码器的轻量级检测方法AutoDetect，发现现有检测方法存在不足，并强调需要更多军事领域数据集来评估风险。


<details>
  <summary>Details</summary>
Motivation: 军事AI系统中投毒攻击威胁日益严重，但针对目标检测系统的投毒攻击应用和检测研究有限，军事领域的攻击可能带来严重后果，需要深入研究。

Method: 创建军事车辆数据集MilCivVeh，实施改进的BadDet补丁式投毒攻击，测试专用投毒检测方法和视觉工业异常检测方法，提出基于自动编码器的AutoDetect方法使用图像切片重建误差进行检测。

Result: 投毒攻击虽然可以达到一定的成功率，但需要污染大量数据，实用性存疑；现有检测方法效果不佳；AutoDetect方法在区分干净和污染样本方面表现优于现有方法，且计算资源需求更低。

Conclusion: 军事领域需要大型代表性数据集来进一步评估投毒攻击风险，AutoDetect为轻量高效的补丁检测提供了有前景的解决方案，但投毒攻击的实际应用可行性仍需更多研究。

Abstract: Poisoning attacks pose an increasing threat to the security and robustness of
Artificial Intelligence systems in the military domain. The widespread use of
open-source datasets and pretrained models exacerbates this risk. Despite the
severity of this threat, there is limited research on the application and
detection of poisoning attacks on object detection systems. This is especially
problematic in the military domain, where attacks can have grave consequences.
In this work, we both investigate the effect of poisoning attacks on military
object detectors in practice, and the best approach to detect these attacks. To
support this research, we create a small, custom dataset featuring military
vehicles: MilCivVeh. We explore the vulnerability of military object detectors
for poisoning attacks by implementing a modified version of the BadDet attack:
a patch-based poisoning attack. We then assess its impact, finding that while a
positive attack success rate is achievable, it requires a substantial portion
of the data to be poisoned -- raising questions about its practical
applicability. To address the detection challenge, we test both specialized
poisoning detection methods and anomaly detection methods from the visual
industrial inspection domain. Since our research shows that both classes of
methods are lacking, we introduce our own patch detection method: AutoDetect, a
simple, fast, and lightweight autoencoder-based method. Our method shows
promising results in separating clean from poisoned samples using the
reconstruction error of image slices, outperforming existing methods, while
being less time- and memory-intensive. We urge that the availability of large,
representative datasets in the military domain is a prerequisite to further
evaluate risks of poisoning attacks and opportunities patch detection.

</details>


### [35] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 基于强化学习和编码器-解码器结构的PPORLD-EDNetLDCT方法，通过后置策略优化算法实时优化去噪策略，在LDCT图像去噪中获得了更高的PSNR、SSIM和更低的RMSE，并在新冠病毒分类任务中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 低强度计算机断展扫描(LDCT)对减少放射危险至关重要，但导致噪声增加和图像质量下降。传统的迭代优化或监督学习方法在保持图像质量方面存在不足。

Method: 提出PPORLD-EDNetLDCT方法，结合强化学习(RL)和编码器-解码器结构。使用后置策略优化(PPO)算法，基于图像质量反馈实时优化去噪策略，通过自定义gym环境进行训练。

Result: 在低强度CT图像和投影数据集上，PSNR达到41.87，SSIM为0.9814，RMSE为0.00236。在NIH-AAPM-Mayo挑战数据集上PSNR为41.52，SSIM为0.9723，RMSE为0.0051。在新冠病毒LDCT数据集的分类任务中，准确率提升到94%，比非RL去噪方法提高4%。

Conclusion: 该方法为LDCT成像提供了更安全和准确的解决方案，在图像质量和临床应用性方面都显示出优异性能。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation
exposure, but it often leads to increased noise and reduced image quality.
Traditional denoising methods, such as iterative optimization or supervised
learning, often fail to preserve image quality. To address these challenges, we
introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with
Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in
which an advanced posterior policy optimization (PPO) algorithm is used to
optimize denoising policies in real time, based on image quality feedback,
trained via a custom gym environment. The experimental results on the low dose
CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT
model outperforms traditional denoising techniques and other DL-based methods,
achieving a peak signal-to-noise ratio of 41.87, a structural similarity index
measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in
NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of
41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality
of denoising using a classification task in the COVID-19 LDCT dataset, where
the images processed by our method improved the classification accuracy to
94\%, achieving 4\% higher accuracy compared to denoising without RL-based
denoising. This method offers a promising solution for safer and more accurate
LDCT imaging.

</details>


### [36] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: 该论文提出了一个名为\ours的多模态情感感知AI伴侣，通过融合文本、语音和视觉信号来增强LLMs的情感理解能力，实现更具同理心的人机交互。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型仅限于单模态文本处理，无法解读非语言情感信号，限制了沉浸式和共情式人机交互的发展。

Method: 提出多模态情感感知网络(MSPN)，使用跨模态融合transformer和监督对比学习来提取情感线索；开发情感感知提示工程策略生成共情响应；集成TTS系统和动画化身体现模块。

Result: 构建了一个能够捕捉多模态情感线索的情感感知AI代理框架，实现了情感对齐的动画化人机交互。

Conclusion: 该工作为情感感知代理提供了一个完整框架，在伴侣机器人、社会关怀、心理健康和以人为本的AI等领域具有广泛应用前景。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved
natural language understanding and generation, enhancing Human-Computer
Interaction (HCI). However, LLMs are limited to unimodal text processing and
lack the ability to interpret emotional cues from non-verbal signals, hindering
more immersive and empathetic interactions. This work explores integrating
multimodal sentiment perception into LLMs to create emotion-aware agents. We
propose \ours, an AI-based virtual companion that captures multimodal sentiment
cues, enabling emotionally aligned and animated HCI. \ours introduces a
Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion
transformer and supervised contrastive learning to provide emotional cues.
Additionally, we develop an emotion-aware prompt engineering strategy for
generating empathetic responses and integrate a Text-to-Speech (TTS) system and
animated avatar module for expressive interactions. \ours provides a framework
for emotion-aware agents with applications in companion robotics, social care,
mental health, and human-centered AI.

</details>


### [37] [RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion](https://arxiv.org/abs/2509.03214)
*Junhao Jia,Yifei Sun,Yunyou Liu,Cheng Yang,Changmiao Wang,Feiwei Qin,Yong Peng,Wenwen Min*

Main category: cs.CV

TL;DR: RTGMFF是一个融合ROI级文本生成和多模态特征融合的fMRI脑疾病诊断框架，通过自动生成文本描述、混合频域-空间编码和自适应语义对齐，显著提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: fMRI在临床诊断中面临信噪比低、个体差异大以及现有CNN和Transformer模型频率感知有限的问题，且缺乏文本标注来理解脑区激活和连接模式。

Method: 框架包含三个组件：1) ROI驱动的fMRI文本生成；2) 混合频域-空间编码器（小波-mamba分支+跨尺度Transformer）；3) 自适应语义对齐模块，通过正则化余弦相似度损失缩小模态差距。

Result: 在ADHD-200和ABIDE基准测试中，RTGMFF在诊断准确性、敏感性、特异性和ROC曲线下面积方面均超越了现有方法。

Conclusion: RTGMFF通过整合文本生成和多模态特征融合，有效解决了fMRI诊断中的关键挑战，为脑疾病诊断提供了新的有效工具。

Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing
brain function, yet reliable clinical diagnosis is hampered by low
signal-to-noise ratios, inter-subject variability, and the limited frequency
awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI
datasets lack textual annotations that could contextualize regional activation
and connectivity patterns. We introduce RTGMFF, a framework that unifies
automatic ROI-level text generation with multimodal feature fusion for
brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven
fMRI text generation deterministically condenses each subject's activation,
connectivity, age, and sex into reproducible text tokens; (ii) Hybrid
frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a
cross-scale Transformer encoder to capture frequency-domain structure alongside
long-range spatial dependencies; and (iii) Adaptive semantic alignment module
embeds the ROI token sequence and visual features in a shared space, using a
regularized cosine-similarity loss to narrow the modality gap. Extensive
experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses
current methods in diagnostic accuracy, achieving notable gains in sensitivity,
specificity, and area under the ROC curve. Code is available at
https://github.com/BeistMedAI/RTGMFF.

</details>


### [38] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于深度学习的自动化无损坏性组织团分割和跟踪方法LGBP-OrgaNet，能够准确分割、跟踪和定量组织团，避免传统药光标记方法对组织结构的损害。


<details>
  <summary>Details</summary>
Motivation: 组织团能够复制器官结构和功能，在肿瘤治疗和药物筛选领域具有重要价值。但传统的药光标记方法存在损害组织结构的风险，需要一种无损坏的自动化分割和跟踪方法来分析组织团的形状和大小。

Method: 提出LGBP-OrgaNet深度学习模型，利用CNN和Transformer模块提取互补信息，通过创新的可学习高斯带通融合模块(Learnable Gaussian Band Pass Fusion)合并两个分支的数据。在解码器中使用双向交叉融合块融合多尺度特征，最终通过逐步连接和上采样完成解码。

Result: SROrga在组织团分割数据集上展现了满意的分割准确性和稳健性，为组织团研究提供了强大的工具。

Conclusion: 该研究提出的LGBP-OrgaNet模型能够有效地解决组织团分割和跟踪问题，避免了传统标记方法的缺陷，为生物医学研究提供了一种无损坏、自动化的分析方案。

Abstract: Organoids replicate organ structure and function, playing a crucial role in
fields such as tumor treatment and drug screening. Their shape and size can
indicate their developmental status, but traditional fluorescence labeling
methods risk compromising their structure. Therefore, this paper proposes an
automated, non-destructive approach to organoid segmentation and tracking. We
introduced the LGBP-OrgaNet, a deep learning-based system proficient in
accurately segmenting, tracking, and quantifying organoids. The model leverages
complementary information extracted from CNN and Transformer modules and
introduces the innovative feature fusion module, Learnable Gaussian Band Pass
Fusion, to merge data from two branches. Additionally, in the decoder, the
model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,
and finally completes the decoding through progressive concatenation and
upsampling. SROrga demonstrates satisfactory segmentation accuracy and
robustness on organoids segmentation datasets, providing a potent tool for
organoid research.

</details>


### [39] [PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR](https://arxiv.org/abs/2509.03262)
*Fabio F. Oberweger,Michael Schwingshackl,Vanessa Staderini*

Main category: cs.CV

TL;DR: PI3DETR是一个端到端的3D参数化曲线检测框架，直接从点云预测多种曲线实例，无需中间表示和多阶段处理，在ABC数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要中间表示和多阶段处理的复杂性，提高对噪声和不同采样密度的鲁棒性，应对真实LiDAR和3D传感场景的挑战。

Method: 基于3DETR扩展，引入几何感知匹配策略和专门损失函数，统一检测多种参数化曲线类型（三次贝塞尔曲线、线段、圆、圆弧），单次前向传播完成检测。

Result: 在ABC数据集上达到新的最先进水平，对真实传感器数据具有良好的泛化能力，对噪声和采样密度变化表现出更强的鲁棒性。

Conclusion: PI3DETR提供了一个简单而强大的3D边缘和曲线估计解决方案，通过端到端设计和统一检测框架，显著简化了3D曲线检测流程并提高了性能。

Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D
parametric curve instances from raw point clouds, avoiding the intermediate
representations and multi-stage processing common in prior work. Extending
3DETR, our model introduces a geometry-aware matching strategy and specialized
loss functions that enable unified detection of differently parameterized curve
types, including cubic B\'ezier curves, line segments, circles, and arcs, in a
single forward pass. Optional post-processing steps further refine predictions
without adding complexity. This streamlined design improves robustness to noise
and varying sampling densities, addressing critical challenges in real world
LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC
dataset and generalizes effectively to real sensor data, offering a simple yet
powerful solution for 3D edge and curve estimation.

</details>


### [40] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 一种基于3D医疗滿散模型SynBT的高质量乳腹肿瘤生成方法，通过片段到体积自动编码器和掩码条件滿散模型，在大视野MRI中生成实伻的乳腹肿瘤，提升分割模型性能2-3% Dice分数。


<details>
  <summary>Details</summary>
Motivation: 现有的肿瘤生成方法在处理占据大空间体积的肿瘤时表现不佳，特别是在大视野MRI中的乳腹肿瘤分割任务中，而常用的肿瘤生成方法多基于小片段。

Method: 提出SynBT模型，包含片段到体积自动编码器压缩高分辨率MRI到紧凑的潜在空间，保持大视野体积的分辨率，然后使用掩码条件滿散模型在选定乳腹组织区域生成实伻肿瘤。

Result: 在肿瘤分割任务中，该方法在大型公开数据集上对常见分割模型带来性能提升，Dice分数提高2-3%。

Conclusion: 提出的高质量肿瘤生成方法能够有效提升MRI图像中肿瘤分割的性能，为医学图像分析提供了实用价值。

Abstract: Synthetic tumors in medical images offer controllable characteristics that
facilitate the training of machine learning models, leading to an improved
segmentation performance. However, the existing methods of tumor synthesis
yield suboptimal performances when tumor occupies a large spatial volume, such
as breast tumor segmentation in MRI with a large field-of-view (FOV), while
commonly used tumor generation methods are based on small patches. In this
paper, we propose a 3D medical diffusion model, called SynBT, to generate
high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed
model consists of a patch-to-volume autoencoder, which is able to compress the
high-resolution MRIs into compact latent space, while preserving the resolution
of volumes with large FOV. Using the obtained latent space feature vector, a
mask-conditioned diffusion model is used to synthesize breast tumors within
selected regions of breast tissue, resulting in realistic tumor appearances. We
evaluated the proposed method for a tumor segmentation task, which demonstrated
the proposed high-quality tumor synthesis method can facilitate the common
segmentation models with performance improvement of 2-3% Dice Score on a large
public dataset, and therefore provides benefits for tumor segmentation in MRI
images.

</details>


### [41] [PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection](https://arxiv.org/abs/2509.03277)
*Qihang Zhou,Shibo He,Jiangtao Yan,Wenchao Meng,Jiming Chen*

Main category: cs.CV

TL;DR: 基于CLIP的2D扩展能力，PointAD+通过点云和像素级利用，统一隐式和显式3D表征来检测和分割未见对象的3D异常


<details>
  <summary>Details</summary>
Motivation: 将CLIP的稳健2D扩展能力转移到3D领域，识别高度多样化类语义的未见对象中的3D异常

Method: 首先设计PointAD（隐式3D表征），利用点-像素对应关系通过渲染像素表征检测3D异常。然后PointAD+增加显式3D表征，通过G-aggregation涵盖几何信息使点表征具有空间意识，采用层次表征学习和跨层次对比对齐来兼须渲染和空间异常

Result: 大量实验证明PointAD+在未见对象的零样本3D异常检测中体现优势，能够插件式集成RGB信息不断提升检测性能

Conclusion: PointAD+通过统一隐式和显式3D表征学习，成功将CLIP的2D扩展能力转移到3D异常检测，实现了对异常的全面理解

Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization
capabilities to identify 3D anomalies across unseen objects of highly diverse
class semantics. To this end, we propose a unified framework to comprehensively
detect and segment 3D anomalies by leveraging both point- and pixel-level
information. We first design PointAD, which leverages point-pixel
correspondence to represent 3D anomalies through their associated rendering
pixel representations. This approach is referred to as implicit 3D
representation, as it focuses solely on rendering pixel anomalies but neglects
the inherent spatial relationships within point clouds. Then, we propose
PointAD+ to further broaden the interpretation of 3D anomalies by introducing
explicit 3D representation, emphasizing spatial abnormality to uncover abnormal
spatial relationships. Hence, we propose G-aggregation to involve geometry
information to enable the aggregated point representations spatially aware. To
simultaneously capture rendering and spatial abnormality, PointAD+ proposes
hierarchical representation learning, incorporating implicit and explicit
anomaly semantics into hierarchical text prompts: rendering prompts for the
rendering layer and geometry prompts for the geometry layer. A cross-hierarchy
contrastive alignment is further introduced to promote the interaction between
the rendering and geometry layers, facilitating mutual anomaly learning.
Finally, PointAD+ integrates anomaly semantics from both layers to capture the
generalized anomaly semantics. During the test, PointAD+ can integrate RGB
information in a plug-and-play manner and further improve its detection
performance. Extensive experiments demonstrate the superiority of PointAD+ in
ZS 3D anomaly detection across unseen objects with highly diverse class
semantics, achieving a holistic understanding of abnormality.

</details>


### [42] [Empowering Lightweight MLLMs with Reasoning via Long CoT SFT](https://arxiv.org/abs/2509.03321)
*Linyu Ou*

Main category: cs.CV

TL;DR: 长链式思维数据对于提升轻量级多模态语言模型的推理能力至关重要，先进行监督微调再结合强化学习可获得最佳效果


<details>
  <summary>Details</summary>
Motivation: 探索验证奖励强化学习在少于70亿参数的轻量级多模态语言模型中的有效性，以及长链式思维数据对提升推理能力的作用

Method: 使用长链式思维数据进行监督微调(SFT)，随后进行强化学习(RL)阶段

Result: 长CoT数据的SFT显著提升MLLM推理能力，后续RL阶段可带来额外性能增益

Conclusion: 使用长链式思维数据的SFT阶段是开发轻量级MLLM推理能力的关键先决条件

Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the
reasoning of large-scale language models (LLMs), its efficacy for lightweight
multimodal language models (MLLMs) with fewer than seven billion parameters
remains underexplored. This paper investigates the role of long
Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such
MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT
data significantly improves MLLM reasoning. Furthermore, we observe that after
this initial SFT phase, MLLMs can achieve additional performance gains through
a subsequent RL stage. We conclude that a SFT stage with long CoT data is a
critical prerequisite for developing the reasoning capabilities of lightweight
MLLMs.

</details>


### [43] [Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions](https://arxiv.org/abs/2509.03323)
*Xizhe Zhang,Jiayang Zhu*

Main category: cs.CV

TL;DR: 提出了一种混合CNN-Transformer检测器，用于自动检测组织学图像中的星形胶质细胞，在ALDH1L1和GFAP染色数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 星形胶质细胞的复杂分支结构和染色依赖性变异使得自动检测极具挑战性，需要解决小细胞和密集簇的检测问题

Method: 混合CNN-Transformer架构，结合局部特征提取和全局上下文推理，使用热图引导查询机制生成空间锚点，轻量级Transformer模块改善密集簇中的区分能力

Result: 在ALDH1L1和GFAP染色数据集上持续优于Faster R-CNN、YOLOv11和DETR，实现更高灵敏度且假阳性更少，FROC分析证实了性能提升

Conclusion: 混合CNN-Transformer架构在星形胶质细胞检测方面具有强大潜力，为先进计算病理学工具奠定了基础

Abstract: Astrocytes are critical glial cells whose altered morphology and density are
hallmarks of many neurological disorders. However, their intricate branching
and stain dependent variability make automated detection of histological images
a highly challenging task. To address these challenges, we propose a hybrid CNN
Transformer detector that combines local feature extraction with global
contextual reasoning. A heatmap guided query mechanism generates spatially
grounded anchors for small and faint astrocytes, while a lightweight
Transformer module improves discrimination in dense clusters. Evaluated on
ALDH1L1 and GFAP stained astrocyte datasets, the model consistently
outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with
fewer false positives, as confirmed by FROC analysis. These results highlight
the potential of hybrid CNN Transformer architectures for robust astrocyte
detection and provide a foundation for advanced computational pathology tools.

</details>


### [44] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion是一个零样本框架，通过虚拟相机将砖石点云转换为深度图，并使用DDNM进行恢复，无需特定任务训练即可提升深度图的视觉清晰度和几何一致性，显著改善了砖块级分割效果。


<details>
  <summary>Details</summary>
Motivation: 在低光照环境下（如砖石隧道），获取高分辨率RGB图像不切实际，而点云虽然对暗光环境鲁棒，但通常是非结构化、稀疏且有噪声的，限制了细粒度分割。需要一种方法能够利用点云数据进行精细的砖块级缺陷检测。

Method: 提出InfraDiffusion框架：1）使用虚拟相机将砖石点云投影为深度图；2）采用去噪扩散零空间模型（DDNM）进行恢复；3）无需任务特定训练即可增强深度图的视觉清晰度和几何一致性；4）使用Segment Anything Model（SAM）进行砖块级分割。

Result: 在砖石桥梁和隧道点云数据集上的实验表明，该方法在砖块级分割方面取得了显著改进，证明了其在砖石资产自动化检测方面的潜力。

Conclusion: InfraDiffusion通过将点云转换为深度图并利用扩散模型进行增强，成功解决了低光照环境下砖石结构细粒度分割的挑战，为零样本的自动化基础设施检测提供了有效解决方案。

Abstract: Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


### [45] [Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing](https://arxiv.org/abs/2509.03376)
*Hui Chen,Liangyu Liu,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出T-CAGU框架，结合Transformer和内容自适应图神经网络，同时捕捉全局依赖和局部一致性，提升高光谱解混性能


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以同时表征全局依赖和局部一致性，无法兼顾长程交互和边界细节

Method: 使用Transformer捕捉全局依赖，引入内容自适应图神经网络增强局部关系，集成多传播阶次动态学习图结构，采用图残差机制保留全局信息

Result: 实验结果表明该方法优于现有最先进方法

Conclusion: T-CAGU框架有效解决了高光谱解混中全局和局部特征平衡的问题，具有鲁棒性和优越性能

Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote
sensing images into a set of endmembers and their corresponding abundances.
Despite significant progress in this field using deep learning, most methods
fail to simultaneously characterize global dependencies and local consistency,
making it difficult to preserve both long-range interactions and boundary
details. This letter proposes a novel transformer-guided content-adaptive graph
unmixing framework (T-CAGU), which overcomes these challenges by employing a
transformer to capture global dependencies and introducing a content-adaptive
graph neural network to enhance local relationships. Unlike previous work,
T-CAGU integrates multiple propagation orders to dynamically learn the graph
structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a
graph residual mechanism to preserve global information and stabilize training.
Experimental results demonstrate its superiority over the state-of-the-art
methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.

</details>


### [46] [TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers](https://arxiv.org/abs/2509.03379)
*Guoxin Wang,Qingyuan Wang,Binhua Huang,Shaowu Chen,Deepu John*

Main category: cs.CV

TL;DR: TinyDrop是一个无需训练、即插即用的token丢弃框架，通过轻量级视觉模型指导ViT模型在推理时选择性丢弃低重要性token，可减少80%计算量且精度损失极小


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在图像分类中表现优异但计算成本高昂，需要在不损失精度的前提下降低大型ViT模型的推理成本

Method: 使用轻量级视觉模型在推理时估计token重要性，选择性丢弃低重要性token，无需修改架构且兼容各种ViT模型

Result: 在标准图像分类基准测试中，FLOPs减少高达80%，精度下降极小

Conclusion: 该框架具有出色的泛化能力和实际应用价值，为高效ViT分类提供了实用解决方案

Abstract: Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.

</details>


### [47] [Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation](https://arxiv.org/abs/2509.03385)
*Reina Ishikawa,Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出了D-GPTScore评估方法和CC-AlignBench基准数据集，用于评估概念定制任务，特别是在多概念场景下，该方法与人类偏好具有更高相关性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在概念定制任务中存在局限性，要么过于狭隘要么过于泛化，与人类偏好不一致，特别是在评估多概念交互时更加困难。

Method: 提出分解式GPT评分(D-GPTScore)，将评估标准分解为更细粒度的方面，并使用多模态大语言模型进行方面级评估。同时发布了包含单概念和多概念任务的人类偏好对齐概念定制基准(CC-AlignBench)。

Result: 该方法在基准测试中显著优于现有方法，与人类偏好表现出更高的相关性。

Conclusion: 这项工作为评估概念定制建立了新标准，并突出了未来研究的关键挑战。

Abstract: Evaluating concept customization is challenging, as it requires a
comprehensive assessment of fidelity to generative prompts and concept images.
Moreover, evaluating multiple concepts is considerably more difficult than
evaluating a single concept, as it demands detailed assessment not only for
each individual concept but also for the interactions among concepts. While
humans can intuitively assess generated images, existing metrics often provide
either overly narrow or overly generalized evaluations, resulting in
misalignment with human preference. To address this, we propose Decomposed GPT
Score (D-GPTScore), a novel human-aligned evaluation method that decomposes
evaluation criteria into finer aspects and incorporates aspect-wise assessments
using Multimodal Large Language Model (MLLM). Additionally, we release Human
Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark
dataset containing both single- and multi-concept tasks, enabling stage-wise
evaluation across a wide difficulty range -- from individual actions to
multi-person interactions. Our method significantly outperforms existing
approaches on this benchmark, exhibiting higher correlation with human
preferences. This work establishes a new standard for evaluating concept
customization and highlights key challenges for future research. The benchmark
and associated materials are available at
https://github.com/ReinaIshikawa/D-GPTScore.

</details>


### [48] [Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping](https://arxiv.org/abs/2509.03408)
*Mohammed Amer,Mohamed A. Suliman,Tu Bui,Nuria Garcia,Serban Georgescu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种可扩展的多模态框架，通过结合复制数量变异(CNV)、临床记录和组织病理图像来改善乳腾癌分子分型的性能。该框架采用双重表征方法处理全手片图像(WSI)，并提出新的多模态融合策略，在各种多模态条件下都显著提升了表现。


<details>
  <summary>Details</summary>
Motivation: 医疗健康应用本质上是多模态的，但不同临床环境中可用的模态异越性较大。乳腾癌分子分型作为重要的临床任务，能够促进个性化治疗和改善患者预后。

Method: 提出可扩展的松耦合多模态框架，支持随时添加新模态而无需重新训练现有模态。引入双重表征方法处理WSI图像，结合传统图像基础和图象基础表征。提出新的多模态融合策略。

Result: 结果显示，将双重WSI表征与CNV和临床健康记录相结合，内置管道和融合策略，在乳腾癌分子分型任务上超过了现有最先进方法的性能。

Conclusion: 该研究提出的多模态框架不仅在乳腾癌分型中表现优异，其可扩展性和灵活性也使得该方法同样适用于其他类型的癌症分析。

Abstract: Healthcare applications are inherently multimodal, benefiting greatly from
the integration of diverse data sources. However, the modalities available in
clinical settings can vary across different locations and patients. A key area
that stands to gain from multimodal integration is breast cancer molecular
subtyping, an important clinical task that can facilitate personalized
treatment and improve patient prognosis. In this work, we propose a scalable
and loosely-coupled multimodal framework that seamlessly integrates data from
various modalities, including copy number variation (CNV), clinical records,
and histopathology images, to enhance breast cancer subtyping. While our
primary focus is on breast cancer, our framework is designed to easily
accommodate additional modalities, offering the flexibility to scale up or down
with minimal overhead without requiring re-training of existing modalities,
making it applicable to other types of cancers as well. We introduce a
dual-based representation for whole slide images (WSIs), combining traditional
image-based and graph-based WSI representations. This novel dual approach
results in significant performance improvements. Moreover, we present a new
multimodal fusion strategy, demonstrating its ability to enhance performance
across a range of multimodal conditions. Our comprehensive results show that
integrating our dual-based WSI representation with CNV and clinical health
records, along with our pipeline and fusion strategy, outperforms
state-of-the-art methods in breast cancer subtyping.

</details>


### [49] [Time-Scaling State-Space Models for Dense Video Captioning](https://arxiv.org/abs/2509.03426)
*AJ Piergiovanni,Ganesh Satish Mallya,Dahun Kim,Anelia Angelova*

Main category: cs.CV

TL;DR: 通过时间缩放状态空间模型(SSM)和转移状态机制，提出了一种可在线处理长视频的密集视频描述方法，减少了7倍计算开销，支持流式处理。


<details>
  <summary>Details</summary>
Motivation: 现有的密集视频描述方法在处理长视频时遇到计算复杂度和内存限制问题，且需要整个视频作为输入，无法支持在线处理。

Method: 提出带有转移状态的状态空间模型(SSM)，结合SSM的长序列和递归特性，充分利用其长远程依赖学习能力，解决了SSM在很长上下文中无法维持状态的问题。

Result: 方法在密集视频描述任务中表现良好，能够良好地适应不同长度的视频，计算开销减少了7倍，支持实时流式处理。

Conclusion: 该方法通过扩展SSM的时间缩放能力，提供了一种高效的在线密集视频描述解决方案，具有强烈的实践应用价值。

Abstract: Dense video captioning is a challenging video understanding task which aims
to simultaneously segment the video into a sequence of meaningful consecutive
events and to generate detailed captions to accurately describe each event.
Existing methods often encounter difficulties when working with the long videos
associated with dense video captioning, due to the computational complexity and
memory limitations. Furthermore, traditional approaches require the entire
video as input, in order to produce an answer, which precludes online
processing of the video. We address these challenges by time-scaling
State-Space Models (SSMs) to even longer sequences than before. Our approach,
State-Space Models with Transfer State, combines both the long-sequence and
recurrent properties of SSMs and addresses the main limitation of SSMs which
are otherwise not able to sustain their state for very long contexts,
effectively scaling SSMs further in time. The proposed model is particularly
suitable for generating captions on-the-fly, in an online or streaming manner,
without having to wait for the full video to be processed, which is more
beneficial in practice. When applied to dense video captioning, our approach
scales well with video lengths and uses 7x fewer FLOPs.

</details>


### [50] [Decoding Visual Neural Representations by Multimodal with Dynamic Balancing](https://arxiv.org/abs/2509.03433)
*Kaili sun,Xingyu Miao,Bing Zhai,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 通过集成EEG、图像和文本多模态数据，提出了一种创新框架来解码视觉神经表征，在ThingsEEG数据集上实现了独创的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决低信器比EEG信号解码视视觉神经表征的挑战，通过引入文本模态来增强EEG信号与视觉内容之间的语义对应关系。

Method: 提出了一个集成框架，包括：1)使用文本模态提供显式语义标签；2)设计适配器模块来利用预训练的视觉和文本表征；3)模态一致性动态平衡(MCDB)策略；4)随机扰动正则化(SPR)项。

Result: 在ThingsEEG数据集上，方法在Top-1和Top-5准确率指标上都超过了之前的最佳方法，分别提升了2.0%和4.7%。

Conclusion: 该框架通过多模态数据集成和创新的模态平衡策略，有效地解码了来自低信器比EEG信号的视觉神经表征，为神经科学研究提供了新的解码方法。

Abstract: In this work, we propose an innovative framework that integrates EEG, image,
and text data, aiming to decode visual neural representations from low
signal-to-noise ratio EEG signals. Specifically, we introduce text modality to
enhance the semantic correspondence between EEG signals and visual content.
With the explicit semantic labels provided by text, image and EEG features of
the same category can be more closely aligned with the corresponding text
representations in a shared multimodal space. To fully utilize pre-trained
visual and textual representations, we propose an adapter module that
alleviates the instability of high-dimensional representation while
facilitating the alignment and fusion of cross-modal features. Additionally, to
alleviate the imbalance in multimodal feature contributions introduced by the
textual representations, we propose a Modal Consistency Dynamic Balance (MCDB)
strategy that dynamically adjusts the contribution weights of each modality. We
further propose a stochastic perturbation regularization (SPR) term to enhance
the generalization ability of semantic perturbation-based models by introducing
dynamic Gaussian noise in the modality optimization process. The evaluation
results on the ThingsEEG dataset show that our method surpasses previous
state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by
2.0\% and 4.7\% respectively.

</details>


### [51] [Joint Training of Image Generator and Detector for Road Defect Detection](https://arxiv.org/abs/2509.03465)
*Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: JTGD是一种用于道路缺陷检测的联合训练方法，通过联合训练生成器和检测器，在边缘设备上实现高效检测，无需集成方法或测试时增强


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上道路缺陷检测的内存和计算资源限制问题，避免使用计算密集的集成方法和测试时增强

Method: 提出JTGD方法，联合训练图像生成器和检测器，使用双重判别器确保合成缺陷补丁和整体图像的真实性，采用CLIP-based FID损失提升图像质量

Result: 在RDD2022基准测试中超越最先进方法，参数量减少80%以上，更适合边缘设备部署

Conclusion: JTGD通过联合训练生成高质量难样本，实现了在资源受限边缘设备上的高效道路缺陷检测

Abstract: Road defect detection is important for road authorities to reduce the vehicle
damage caused by road defects. Considering the practical scenarios where the
defect detectors are typically deployed on edge devices with limited memory and
computational resource, we aim at performing road defect detection without
using ensemble-based methods or test-time augmentation (TTA). To this end, we
propose to Jointly Train the image Generator and Detector for road defect
detection (dubbed as JTGD). We design the dual discriminators for the
generative model to enforce both the synthesized defect patches and overall
images to look plausible. The synthesized image quality is improved by our
proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in
JTGD is trained jointly with the detector to encourage the generative model to
synthesize harder examples for the detector. Since harder synthesized images of
better quality caused by the aforesaid design are used in the data
augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road
defect detection benchmark across various countries under the condition of no
ensemble and TTA. JTGD only uses less than 20% of the number of parameters
compared with the competing baseline, which makes it more suitable for
deployment on edge devices in practice.

</details>


### [52] [Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA](https://arxiv.org/abs/2509.03494)
*Yahya Benmahane,Mohammed El Hassouni*

Main category: cs.CV

TL;DR: 提出一种基于像素空间视觉提示的参数高效NR-IQA方法，仅训练60万参数，在多个数据集上达到与全微调方法相当的性能


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在无参考图像质量评估任务中全微调参数过多、计算成本高的问题，探索像素空间视觉提示在低层视觉任务中的应用

Method: 通过像素空间优化的视觉提示与图像相加，输入到冻结的mPLUG-Owl2模型中，配合文本查询进行质量评估，仅训练视觉提示参数

Result: 在KADID-10k、KonIQ-10k和AGIQA-3k数据集上表现优异，KADID-10k上SRCC达到0.93，与全微调方法和专业NR-IQA模型竞争力相当

Conclusion: 首次将像素空间视觉提示应用于NR-IQA任务，证明了参数高效方法在低层视觉任务中的有效性，为MLLM的高效适配提供了新思路

Abstract: In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .

</details>


### [53] [OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation](https://arxiv.org/abs/2509.03498)
*Han Li,Xinyu Peng,Yaoming Wang,Zelin Peng,Xin Chen,Rongxiang Weng,Jingang Wang,Xunliang Cai,Wenrui Dai,Hongkai Xiong*

Main category: cs.CV

TL;DR: OneCAT是一个统一的多模态模型，采用纯解码器Transformer架构，整合了理解、生成和编辑功能，无需外部视觉组件，通过MoE结构和多尺度自回归机制实现高效高性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型通常需要外部视觉组件（如ViT或视觉分词器），导致推理效率低下，特别是在高分辨率输入时。需要一种更简洁高效的架构来实现真正的统一多模态智能。

Method: 提出纯解码器Transformer架构，采用模态特定的MoE结构，使用单一自回归目标训练，支持动态分辨率。在LLM中引入多尺度视觉自回归机制，大幅减少解码步骤。

Result: 在多项基准测试中超越了现有的开源统一多模态模型，在多模态生成、编辑和理解任务上均达到最先进性能，同时显著提升了推理效率。

Conclusion: 纯自回归建模可以作为统一多模态智能的充分而优雅的基础，OneCAT为多模态模型设计提供了新的方向和性能标准。

Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.

</details>


### [54] [DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video](https://arxiv.org/abs/2509.03499)
*Kevin Barnard,Elaine Liu,Kristine Walz,Brian Schlining,Nancy Jacobsen Stout,Lonny Lundsten*

Main category: cs.CV

TL;DR: 海洋深海视频多目标跟踪性能评测基准数据集开发，包含四段深海视频序列，使用HOTA指标评估多个检测模型和跟踪器的综合性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决深海视频多目标跟踪缺乏公开评测标准的问题，提供可量化的模型性能评估方法，以促进模型开发和性能优化。

Method: 开发了包含四段深海视频序列的新驱基准数据集，使用Higher Order Tracking Accuracy (HOTA)指标综合评估检测、定位和关联准确性，对多个检测模型和跟踪器进行性能测试。

Result: 成功建立了首个公开的深海视频多目标跟踪基准数据集，提供了完整的评测流程和工具，包括基准数据、工作流程文档和Python示例笔记本。

Conclusion: 该研究为深海视频分析领域提供了重要的评测标准和工具，有助于推动多目标跟踪技术在海洋科学中的应用和发展。

Abstract: Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.

</details>


### [55] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: Strefer是一个合成指令数据生成框架，旨在为视频大语言模型提供时空参考和推理能力，通过伪标注密集时空元数据来增强模型对空间和时间参考的理解。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在细粒度时空推理方面存在不足，特别是在处理基于时间的事件参考和手势提示的空间锚定时表现不佳，需要提升AI伴侣在动态真实环境中的参考解析能力。

Method: 使用数据引擎伪标注时间密集的细粒度视频元数据，以结构化方式捕获丰富的空间和时间信息，包括主体、对象、位置掩码、动作描述和时间线，生成多样化的指令调优数据。

Result: 实验评估显示，使用Strefer生成数据训练的模型在需要空间和时间消歧的任务上优于基线模型，并展现出增强的时空感知推理能力。

Conclusion: Strefer为感知基础的指令调优视频大语言模型建立了新基础，无需使用专有模型、昂贵的人工标注或大量新视频标注，即可有效提升模型的时空参考能力。

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


### [56] [A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting](https://arxiv.org/abs/2509.03510)
*Abbas Zohrevand,Javad Sadri,Zahra Imani*

Main category: cs.CV

TL;DR: 这篇论文开发了一个关于遗传对手写影响的综合数据库，包含210个家庭的手写样本，用于研究手写的遗传性和家庭关系影响。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门研究手写遗传效应的数据库，需要建立一个综合性的数据集来回答手写是否存在遗传性组分、是否可以遗传以及家庭关系如何影响手写风格等问题。

Method: 收集210个家庭（包括祖父母、父母、叔伯、姑姊、兄弟姐妹、堂表兄弟、姑侄等）的手写样本，包括数字、字母、形状和自由段落。使用特别设计的表格收集数据，并记录所有写作者的家庭关系。

Result: 通过对家庭成员手写特征的比较和研究，发现了他们手写特征和写作风格之间的相似性。建立了一个免费开放的数据库。

Conclusion: 该数据库为模式识别社区提供了一个重要资源，将为研究遗传和家庭关系对手写的影响开启新的研究方向。

Abstract: This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.

</details>


### [57] [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](https://arxiv.org/abs/2509.03516)
*Ouxiang Li,Yuan Wang,Xinting Hu,Huijuan Huang,Rui Chen,Jiarong Ou,Xin Tao,Pengfei Wan,Fuli Feng*

Main category: cs.CV

TL;DR: T2I-CoReBench是一个全面的文本到图像生成基准测试，通过12维评估分类法评估模型的组合和推理能力，包含1080个挑战性提示和约13500个检查问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估文本到图像生成模型的组合和推理能力方面存在局限性，无法全面评估复杂场景和高密度组合，需要更全面的评估框架。

Method: 基于场景图元素（实例、属性、关系）构建组合能力评估，基于哲学推理框架（演绎、归纳、溯因）构建推理能力评估，创建高密度组合和多步推理的提示。

Result: 实验测试27个当前T2I模型显示，在复杂高密度场景中组合能力仍然有限，推理能力更是关键瓶颈，所有模型都难以从提示中推断隐含元素。

Conclusion: T2I-CoReBench提供了一个全面复杂的基准测试，揭示了当前T2I模型在组合和推理能力方面的局限性，特别是推理能力需要显著改进。

Abstract: Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour](https://arxiv.org/abs/2509.02727)
*Guillaume Gagné-Labelle,Vassil Atanassov,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 提出一种用于四足机器人动态运动场景的通用强化学习算法，性能媲美专家混合方法，但训练所需智能体数量减少75%


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂地形中具有优势，但传统建模方法难以处理滑倒和绊倒等问题，而强化学习通过试错能够实现最优控制

Method: 开发通用强化学习算法，训练四足智能体在动态运动场景中的策略，分析关键组件和成功因素

Result: 学习到的策略性能与最先进的专家混合方法相当，训练时仅需25%的智能体数量

Conclusion: 强化学习为四足机器人 locomotion 控制提供了实用解决方案，通用策略能够有效处理复杂运动场景

Abstract: Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.

</details>


### [59] [The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI](https://arxiv.org/abs/2509.02749)
*Giorgia Buracchio,Ariele Callegari,Massimo Donini,Cristina Gena,Antonio Lieto,Alberto Lillo,Claudio Mattutino,Alessandro Mazzei,Linda Pigureddu,Manuel Striani,Fabiana Vernero*

Main category: cs.RO

TL;DR: 情感对齐对机器人说服力和用户沟通风格无影响，但显著影响用户对机器人心理状态归因和共情感知


<details>
  <summary>Details</summary>
Motivation: 研究自适应情感对齐作为共情沟通前提在人类-机器人交互中的作用，探索情感对齐对话对机器人说服力、用户沟通风格和心理状态归因的影响

Method: 使用NAO机器人进行实验，42名参与者分为两组：中性沟通组和情感自适应响应组，比较两种条件下的效果

Result: 情感对齐不影响用户沟通风格和说服效果，但显著增强了用户对机器人心理状态的归因和共情感知

Conclusion: 情感对齐虽然不能提升机器人的说服力或改变用户沟通方式，但能有效增强用户对机器人心理能力和共情能力的感知

Abstract: The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy

</details>


### [60] [A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality](https://arxiv.org/abs/2509.02760)
*Maximilian Neidhardt,Ludwig Bosse,Vidas Raudonis,Kristina Allgoewer,Axel Heinemann,Benjamin Ondruschka,Alexander Schlaefer*

Main category: cs.RO

TL;DR: 虚拟现实数字双生技术用于机器人治理后小使使检查，实现远程规划和控制，提高精确性和安全性


<details>
  <summary>Details</summary>
Motivation: 传统开放式小使使检查对医生感染风险高，需要一种更安全、减少伤害的方法来进行治理后检查

Method: 使用虚拟现实数字双生设备，实现全远程机器人治理后小使使检查的规划和控制，对三种交互方法进行可用性研究

Result: 进行了132次针插入，离轴针指放置误差为5.30±3.25mm，组织样本成功获取并经历史病理学验证

Conclusion: 系统作为一种有前景、精确且低风险的替代方案，用户报告针指放置方法非常直观

Abstract: Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.

</details>


### [61] [Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers](https://arxiv.org/abs/2509.02808)
*Isaac Ronald Ward,Mark Paral,Kristopher Riordan,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 该论文提出了一种在大型地下环境中自主控制四旋翼飞行器的方法，通过训练基于标准化流的先验环境模型来监测飞行器是否处于训练分布之外，并据此在基于学习的控制器和安全控制器之间切换，实现了快速完成任务和避免碰撞的双重目标。


<details>
  <summary>Details</summary>
Motivation: 基于学习的控制器在自主控制四旋翼飞行器方面具有吸引力，但它们在训练时未遇到的环境（分布外环境）中泛化能力较差。为了在大型地下环境中实现安全有效的自主控制，需要解决学习控制器在分布外环境中的性能下降问题。

Method: 训练一个基于标准化流的环境先验模型，该模型能够量化四旋翼飞行器在任何给定时间点与训练分布的偏离程度。使用这个度量作为运行时监控器，当检测到足够大的分布外偏离时，在基于学习的控制器和安全控制器之间进行切换。

Result: 在基于DARPA地下挑战赛真实点云数据构建的模拟3D洞穴环境中进行点对点导航任务测试。实验结果表明，组合控制器同时具备了基于学习控制器的活跃性（快速完成任务）和安全控制器的安全性（避免碰撞）。

Conclusion: 通过结合基于标准化流的环境先验监测和控制器切换机制，成功实现了在分布外环境中既保持学习控制器的效率又确保安全性的四旋翼自主控制方案，为地下环境中的自主飞行提供了有效的解决方案。

Abstract: Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).

</details>


### [62] [Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization](https://arxiv.org/abs/2509.02815)
*Nico Bohlinger,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一个统一的运动控制策略，通过在50种不同腿式机器人上进行训练，结合改进的URMAv2架构和性能导向课程学习，实现了对百万级形态变化的控制，并在真实人形和四足机器人上实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要为每个特定机器人设计单独控制策略的问题，探索单一通用策略控制多种不同形态腿式机器人的可能性。

Method: 采用改进的URMAv2（本体感知架构）结合性能导向的极端本体随机化课程学习，在50种腿式机器人数据集上进行训练。

Result: 成功训练出能够控制百万级形态变化的通用策略，在未见过的真实人形和四足机器人上实现了零样本迁移控制。

Conclusion: 该方法证明了单一通用运动控制策略可以适应广泛的不同机器人形态，为机器人通用控制提供了新的解决方案。

Abstract: We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.

</details>


### [63] [Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms](https://arxiv.org/abs/2509.02870)
*Harsh Muriki,Hong Ray Teo,Ved Sengupta,Ai-Ping Hu*

Main category: cs.RO

TL;DR: 使用自定义FarmBot和新算法进行草莓花机器人掌控位估计，准确率80%花机检测率和7.7度平均位置误差


<details>
  <summary>Details</summary>
Motivation: 利用低成本农业机器人平台实现植物表型分析，特别是为机器人掌控提供准确的草莓花位置估测

Method: 采用自定义摄像头绘制三维点云模型，通过正交轴占位格网生成六个视角图像，使用2D物体检测确定花机位置并转换到3D空间，最后用三种几何形状拟合花机位置

Result: 算法成功检测约80%的花机，平均位置误差仅7.7度，超过了以前的机器人掌控结果

Conclusion: 该方法为小规模城市农场提供了一种可行的植物表型分析方案，能够满足机器人掌控的精度要求，代码已开源

Abstract: The small scale of urban farms and the commercial availability of low-cost
robots (such as the FarmBot) that automate simple tending tasks enable an
accessible platform for plant phenotyping. We have used a FarmBot with a custom
camera end-effector to estimate strawberry plant flower pose (for robotic
pollination) from acquired 3D point cloud models. We describe a novel algorithm
that translates individual occupancy grids along orthogonal axes of a point
cloud to obtain 2D images corresponding to the six viewpoints. For each image,
2D object detection models for flowers are used to identify 2D bounding boxes
which can be converted into the 3D space to extract flower point clouds. Pose
estimation is performed by fitting three shapes (superellipsoids, paraboloids
and planes) to the flower point clouds and compared with manually labeled
ground truth. Our method successfully finds approximately 80% of flowers
scanned using our customized FarmBot platform and has a mean flower pose error
of 7.7 degrees, which is sufficient for robotic pollination and rivals previous
results. All code will be made available at
https://github.com/harshmuriki/flowerPose.git.

</details>


### [64] [Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model](https://arxiv.org/abs/2509.02876)
*Hongrui Yu,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 通过大语言模型和标准化技能学习框架，解决建筑机器人多任务技能转移问题，减少重复编程工作量


<details>
  <summary>Details</summary>
Motivation: 建筑行业机器人应用遍存在技能转移困难，需要重复编程场景理解、路径规划等模块，影响机器人普及

Method: 使用大语言模型(LLM)、标准化模块化层次建模方法和BIM-机器人语义数据管道，通过群众源自然语言指令教学

Result: 在工业级机器手上进行长时间窗干壁安装实验，实现了最小化编程工作量和高质量的多任务重编程

Conclusion: 提出的技能标准化方案和LLM基础层次技能学习框架有效解决了建筑机器人多任务技能转移问题

Abstract: The quasi-repetitive nature of construction work and the resulting lack of
generalizability in programming construction robots presents persistent
challenges to the broad adoption of robots in the construction industry. Robots
cannot achieve generalist capabilities as skills learnt from one domain cannot
readily transfer to another work domain or be directly used to perform a
different set of tasks. Human workers have to arduously reprogram their
scene-understanding, path-planning, and manipulation components to enable the
robots to perform alternate work tasks. The methods presented in this paper
resolve a significant proportion of such reprogramming workload by proposing a
generalizable learning architecture that directly teaches robots versatile
task-performance skills through crowdsourced online natural language
instructions. A Large Language Model (LLM), a standardized and modularized
hierarchical modeling approach, and Building Information Modeling-Robot sematic
data pipeline are developed to address the multi-task skill transfer problem.
The proposed skill standardization scheme and LLM-based hierarchical skill
learning framework were tested with a long-horizon drywall installation
experiment using a full-scale industrial robotic manipulator. The resulting
robot task learning scheme achieves multi-task reprogramming with minimal
effort and high quality.

</details>


### [65] [IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments](https://arxiv.org/abs/2509.02972)
*Haolan Zhang,Thanh Nguyen Canh,Chenghao Li,Ruidong Yang,Yonghoon Ji,Nak Young Chong*

Main category: cs.RO

TL;DR: 这篇论文提出了一种特征感知机制来解决动态SLAM中特征不足的问题，通过在必要时才激活线性特征，减少计算开销和低质量特征的引入。


<details>
  <summary>Details</summary>
Motivation: 传统动态SLAM系统在移除动态特征后导致特征不足，而现有方法不分情况地持续添加特征会导致计算费用增加和性能下降。

Method: 提出特征感知机制，评估当前特征是否足够，以决定是否激活线性特征支持。在后续处理中，引入的线性特征只用于跟踪和位估，但排除在全局优化之外。

Result: 在TUM数据集上的实验显示，方法在ATE和RPE指标上显著收改了ORB-SLAM3基线，且表现优于其他动态SLAM和多特征方法。

Conclusion: 该特征感知机制能够在保持稳健性能的同时显著降低计算复杂度，并减少低质量特征和噪声的影响。

Abstract: Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.

</details>


### [66] [DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features](https://arxiv.org/abs/2509.02983)
*Jinghe Yang,Minh-Quan Le,Mingming Gong,Ye Pu*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的水下视觉导航策略DUViN，通过知识迁移的深度特征实现未知环境中的4自由度运动控制，无需预建地图即可避障和保持安全高度。


<details>
  <summary>Details</summary>
Motivation: 水下自主导航面临传感能力有限和水下环境建图困难等挑战，同时大规模水下导航数据集收集困难，需要解决从空中到水下环境的领域迁移问题。

Method: 采用两阶段训练框架：首先在空域数据集上使用预训练深度特征提取器训练扩散视觉导航策略；然后在深度估计任务上重新训练提取器，并将其集成到已训练的导航策略中。

Result: 在模拟和真实水下环境中的实验证明了该方法的有效性和泛化能力。

Conclusion: DUViN方法通过知识迁移策略成功实现了从空中到水下环境的领域适应，为水下视觉导航提供了一种有效的端到端解决方案。

Abstract: Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.

</details>


### [67] [CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning](https://arxiv.org/abs/2509.02986)
*Rankun Li,Hao Wang,Qi Li,Zhuo Han,Yifei Chu,Linqi Ye,Wende Xie,Wenlong Liao*

Main category: cs.RO

TL;DR: 这篇论文提出了一种接触触发的盲升桌框架(CTBC)，用于提升轮式双足机器人在复杂环境中的障碍迈过能力


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人在平坦地形上具有高速移动优势，但在复杂环境(如楼梯)上的性能远差于传统的足式机器人，需要找到解决方案

Method: 提出CTBC框架：检测到轮子-障碍接触时，触发腿部抬起动作来过障碍，利用强导向的前向飞轨迹使机器人快速掌握灵活的腿部抬起技能

Result: 在LimX Dynamics的轮式双足机器人Tron1上进行了实验验证，真实世界测试表明Tron1可靠地升起超过其轮子半径的障碍，仅使用自感反馈

Conclusion: 该方法显著提升了轮式双足机器人在非结构化地形上的通过能力，为轮式双足机器人的机动性提升提供了有效的解决方案

Abstract: In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.

</details>


### [68] [Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression](https://arxiv.org/abs/2509.03012)
*Uddeshya Upadhyay*

Main category: cs.RO

TL;DR: 提出UT³框架，通过不确定性感知的自监督任务选择性应用测试时训练，在保持性能的同时显著降低推理时间，适用于资源受限的实时系统。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在域偏移下泛化能力差，现有测试时训练方法虽然能适应新测试分布，但推理时间大幅增加，无法满足实时机器人应用的延迟要求。

Method: 提出不确定性感知的自监督任务，利用量化不确定性选择性应用训练，提供连续设置让用户控制训练频率，减少不必要的计算开销。

Result: 在单目深度估计任务上验证，推理时间显著减少，性能与标准测试时训练相当，适合实时应用。

Conclusion: UT³框架在保持域适应性能的同时大幅降低计算开销，为资源受限的实时系统提供了可行的测试时训练解决方案。

Abstract: Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.

</details>


### [69] [Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage](https://arxiv.org/abs/2509.03119)
*Yash Vyas,Matteo Bottin*

Main category: cs.RO

TL;DR: 基于闭链平面五条链组的力平衡操纶器设计，包括2-DOF平面版本Forbal-2和5-DOF空间版本Forbal-5，通过力平衡设计显著降低关节扭矩和反力矩


<details>
  <summary>Details</summary>
Motivation: 设计力平衡操纶器以降低关节扭矩和反力矩，提高精度，适用于需要毫米级精度的应用场景

Method: 基于闭链平面五条链组架构，进行几何、运动学和动力学设计，满足力平衡条件并最大化工作空间，导出逆运动学方程

Result: 力平衡配置对比反重平衡和不平衡配置：平均反力矩降低66%，平均关节扭矩降低79%（Forbal-2）和84%（Forbal-5），位置误差显著减小

Conclusion: 力平衡操纶器设计能够有效降低关节扭矩和反力矩，适合于需要高精度精度的应用，实验结果验证了设计的有效性

Abstract: A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles.
  We validate the improvements from force balancing the manipulator through
comparative experiments with counter mass balanced and unbalanced
configurations. The results show how the balanced configuration yields a
reduction in the average reaction moments of up to 66\%, a reduction of average
joint torques of up to 79\%, as well as a noticeable reduction in position
error for Forbal-2. For Forbal-5, which has a higher end effector payload mass,
the joint torques are reduced up to 84\% for the balanced configuration.
Experimental results validate that the balanced manipulator design is suitable
for applications where the reduction of joint torques and reaction
forces/moments helps achieve millimeter level precision.

</details>


### [70] [Efficient Active Training for Deep LiDAR Odometry](https://arxiv.org/abs/2509.03211)
*Beibei Zhou,Zhiyuan Zhang,Zhenbo Song,Jianhui Guo,Hui Kong*

Main category: cs.RO

TL;DR: 主动训练框架通过精选数据提升深度LiDAR测程模型的效率和稳健性，仅需51%数据即达到全数据集训练效果


<details>
  <summary>Details</summary>
Motivation: 解决深度LiDAR测程模型需要大量多样化训练数据来适应不同环境的效率问题

Method: 采用初始训练集选择(ITSS)和主动增量选择(AIS)两种策略，通过轨迹分析和预测不一致性来精选训练样本

Result: 在多种数据集和天气条件下验证有效，仅需52%序列数据即可达到全数据集训练的性能

Conclusion: 该主动训练框架能够优化训练过程，为更灵活可靠的LiDAR测程系统奠定基础

Abstract: Robust and efficient deep LiDAR odometry models are crucial for accurate
localization and 3D reconstruction, but typically require extensive and diverse
training data to adapt to diverse environments, leading to inefficiencies. To
tackle this, we introduce an active training framework designed to selectively
extract training data from diverse environments, thereby reducing the training
load and enhancing model generalization. Our framework is based on two key
strategies: Initial Training Set Selection (ITSS) and Active Incremental
Selection (AIS). ITSS begins by breaking down motion sequences from general
weather into nodes and edges for detailed trajectory analysis, prioritizing
diverse sequences to form a rich initial training dataset for training the base
model. For complex sequences that are difficult to analyze, especially under
challenging snowy weather conditions, AIS uses scene reconstruction and
prediction inconsistency to iteratively select training samples, refining the
model to handle a wide range of real-world scenarios. Experiments across
datasets and weather conditions validate our approach's effectiveness. Notably,
our method matches the performance of full-dataset training with just 52\% of
the sequence volume, demonstrating the training efficiency and robustness of
our active training paradigm. By optimizing the training process, our approach
sets the stage for more agile and reliable LiDAR odometry systems, capable of
navigating diverse environmental conditions with greater precision.

</details>


### [71] [The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation](https://arxiv.org/abs/2509.03222)
*Sophia Bianchi Moyen,Rickmer Krohn,Sophie Lueth,Kay Pompetzki,Jan Peters,Vignesh Prasad,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 比较两种机器人控制范式（耦合与解耦）和两种视觉反馈机制（VR与传统屏幕）在移动操作任务中的表现，发现VR会增加任务时间和认知负荷，耦合控制与解耦控制的工作负荷相当，但耦合控制获得的数据在模仿学习中表现更好


<details>
  <summary>Details</summary>
Motivation: 开发直观的遥操作界面对于移动操作机器人至关重要，需要在高强度数据收集中减少操作员负担并保持数据质量，特别是在需要全身协调的长期任务中

Method: 系统评估两种机器人控制范式（耦合体现与解耦体现）和两种视觉反馈机制（沉浸式VR与传统屏幕可视化），在复杂的多阶段任务序列中进行测试

Result: 使用VR作为反馈方式会增加任务完成时间、认知负荷和感知努力；耦合操作与导航的用户工作负荷与解耦方式相当；初步实验表明耦合遥操作获得的数据在模仿学习中表现更好

Conclusion: 研究为收集高质量、高维度的移动操作数据提供了有价值的见解，强调了以人为中心的遥操作界面设计的重要性

Abstract: Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/

</details>


### [72] [Exploring persuasive Interactions with generative social robots: An experimental framework](https://arxiv.org/abs/2509.03231)
*Stephan Vonschallen,Larissa Julia Corina Finsler,Theresa Schmiedel,Friederike Eyssel*

Main category: cs.RO

TL;DR: 研究通过实验框架评估集成生成式AI的社交机器人的说服能力，发现其效果受上下文和沟通细节影响，参与者对机器人的科技限制和个性化需求有更高期待


<details>
  <summary>Details</summary>
Motivation: 研究集成生成式AI的社交机器人的说服能力，以了解其如何影响人类决策，以及不同外观和自我知识对效果的影响

Method: 设计实验框架测试决策行为，进行小规模试验，变化机器人外观和自我知识，使用定性分析评估交互质量、说服效果和沟通策略

Result: 参与者对交互感受正面，认为机器人能干、友好、支持性强，但也指出响应延迟、语音识别错误等限制。说服效果高度依赖上下文，社交角色和个性化论据很重要

Conclusion: 生成式社交机器人可以影响用户决策，但效果受沟通细节和上下文相关性影响，需要更个性化、上下文感知的论据和更清晰的社交角色设计

Abstract: Integrating generative AI such as large language models into social robots
has improved their ability to engage in natural, human-like communication. This
study presents a method to examine their persuasive capabilities. We designed
an experimental framework focused on decision making and tested it in a pilot
that varied robot appearance and self-knowledge. Using qualitative analysis, we
evaluated interaction quality, persuasion effectiveness, and the robot's
communicative strategies. Participants generally experienced the interaction
positively, describing the robot as competent, friendly, and supportive, while
noting practical limits such as delayed responses and occasional
speech-recognition errors. Persuasiveness was highly context dependent and
shaped by robot behavior: participants responded well to polite, reasoned
suggestions and expressive gestures, but emphasized the need for more
personalized, context-aware arguments and clearer social roles. These findings
suggest that generative social robots can influence user decisions, but their
effectiveness depends on communicative nuance and contextual relevance. We
propose refinements to the framework to further study persuasive dynamics
between robots and human users.

</details>


### [73] [Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control](https://arxiv.org/abs/2509.03238)
*Martin Goubej,Lauria Clarke,Martin Hrabačka,David Tolar*

Main category: cs.RO

TL;DR: 对Rafael Lozano-Hemmer的互动艺术装置进行硬件和控制算法升级，通过数学建模和输入整形方法有效抑制振动，提升系统性能和互动响应。


<details>
  <summary>Details</summary>
Motivation: 原装置存在振荡动力学问题，导致扭转和摆锤式振动，限制了旋转速度和互动响应性，需要进行技术升级。

Method: 开发详细的飞带系统数学模型，采用凸优化问题形式的输入整形方法来抑制振动，升级硬件和运动控制算法。

Result: 实验结果显示系统性能和观众互动性得到显著改善，实现了更平滑快速的带子运动。

Conclusion: 这项工作展示了机器人技术、控制工程和互动艺术的融合，为大型动态装置实时运动控制和振动抑制提供了新的解决方案。

Abstract: This paper presents a comprehensive refurbishment of the interactive robotic
art installation Standards and Double Standards by Rafael Lozano-Hemmer. The
installation features an array of belts suspended from the ceiling, each
actuated by stepper motors and dynamically oriented by a vision-based tracking
system that follows the movements of exhibition visitors. The original system
was limited by oscillatory dynamics, resulting in torsional and pendulum-like
vibrations that constrained rotational speed and reduced interactive
responsiveness. To address these challenges, the refurbishment involved
significant upgrades to both hardware and motion control algorithms. A detailed
mathematical model of the flying belt system was developed to accurately
capture its dynamic behavior, providing a foundation for advanced control
design. An input shaping method, formulated as a convex optimization problem,
was implemented to effectively suppress vibrations, enabling smoother and
faster belt movements. Experimental results demonstrate substantial
improvements in system performance and audience interaction. This work
exemplifies the integration of robotics, control engineering, and interactive
art, offering new solutions to technical challenges in real-time motion control
and vibration damping for large-scale kinetic installations.

</details>


### [74] [Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety](https://arxiv.org/abs/2509.03261)
*Elias Fontanari,Gianni Lunardi,Matteo Saveriano,Andrea Del Prete*

Main category: cs.RO

TL;DR: 通过并行计算多个MPC问题，在不同时间步设置安全集约束，提升非线性系统的安全性和性能


<details>
  <summary>Details</summary>
Motivation: 确保限制条件满足对于安全关键系统至关重要，但非线性系统的约束满足具有挑战性

Method: 使用并行计算同时求解多个MPC问题，每个问题在预测越其不同时间步设置安全集约束，然后根据用户定义标准选择最佳解

Result: 通过3关节机器手模拟验证，仅使用4核计算就能在安全性和性能方面实现显著改善

Conclusion: 并行计算方法能够有效提升限制系统的安全性和控制性能，为安全关键系统提供了可靠的控制方案

Abstract: Ensuring constraint satisfaction is a key requirement for safety-critical
systems, which include most robotic platforms. For example, constraints can be
used for modeling joint position/velocity/torque limits and collision
avoidance. Constrained systems are often controlled using Model Predictive
Control, because of its ability to naturally handle constraints, relying on
numerical optimization. However, ensuring constraint satisfaction is
challenging for nonlinear systems/constraints. A well-known tool to make
controllers safe is the so-called control-invariant set (a.k.a. safe set). In
our previous work, we have shown that safety can be improved by letting the
safe-set constraint recede along the MPC horizon. In this paper, we push that
idea further by exploiting parallel computation to improve safety. We solve
several MPC problems at the same time, where each problem instantiates the
safe-set constraint at a different time step along the horizon. Finally, the
controller can select the best solution according to some user-defined
criteria. We validated this idea through extensive simulations with a 3-joint
robotic arm, showing that significant improvements can be achieved in terms of
safety and performance, even using as little as 4 computational cores.

</details>


### [75] [Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management](https://arxiv.org/abs/2509.03436)
*Md Mhamud Hussen Sifat,Md Maruf,Md Rokunuzzaman*

Main category: cs.RO

TL;DR: 这篇论文探讨了基于物联网的护士机器人系统，该系统能够在大流行病情况下自动进行健康检查和药物管理，以降低感染风险并改善病人结果。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行增强了对辅助医疗的机器人需求，通过自动化检查和药物管理来降低感染风险，同时提高医疗系统的可持续性和盈利能力。

Method: 研究设计了一个基于物联网的护士机器人系统，该系统能够评估病人健康状态并根据情况采取相应行动。系统支持自动化医疗辅助和基于自定义命令的监督控制。

Result: 研究展示了一个测试案例，评估了系统在药物管理、健康状态监测和生命周期考虑方面的性能表现。

Conclusion: 这种机器人护士系统在大流行环境中有效降低了感染风险，改善了病人结果，为医疗系统的可持续发展提供了有力支持。

Abstract: The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.

</details>


### [76] [Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena](https://arxiv.org/abs/2509.03500)
*Itai Zilberstein,Alberto Candela,Steve Chien*

Main category: cs.RO

TL;DR: 通过结合前瞻卫星图像中火山烟柿检测与自主轨迹规划，实现了高分辨率传感器对动态科学现象的自主追踪观测，使观测效用提升了一个数量级。


<details>
  <summary>Details</summary>
Motivation: 利用边缘计算能力在卫星上实时进行计算机视觉和机器学习，以捕捉稀有、瞬态和精确的动态科学现象测量。

Method: 开发了一个自动化工作流，结合前瞻卫星图像中动态事件检测与自主轨迹规划。采用传统机器学习算法和卷积神经网络进行分类，并设计了多种轨迹规划算法来跟踪烟柿形态特征。

Result: 通过模拟实验显示，高分辨率传感器的效用回报与基准方法相比提升了一个数量级，同时保持了高效的运行时间。

Conclusion: 该自动化工作流能够有效地结合动态事件检测与自主轨迹规划，为观测火山烟柿等动态科学现象提供了高效的解决方案，显著提高了观测效果。

Abstract: Advancements in onboard computing mean remote sensing agents can employ
state-of-the-art computer vision and machine learning at the edge. These
capabilities can be leveraged to unlock new rare, transient, and pinpoint
measurements of dynamic science phenomena. In this paper, we present an
automated workflow that synthesizes the detection of these dynamic events in
look-ahead satellite imagery with autonomous trajectory planning for a
follow-up high-resolution sensor to obtain pinpoint measurements. We apply this
workflow to the use case of observing volcanic plumes. We analyze
classification approaches including traditional machine learning algorithms and
convolutional neural networks. We present several trajectory planning
algorithms that track the morphological features of a plume and integrate these
algorithms with the classifiers. We show through simulation an order of
magnitude increase in the utility return of the high-resolution instrument
compared to baselines while maintaining efficient runtimes.

</details>


### [77] [Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories](https://arxiv.org/abs/2509.03515)
*Yanlin Zhang,Sungyong Chung,Nachuan Li,Dana Monzer,Hani S. Mahmassani,Samer H. Hamdar,Alireza Talebpour*

Main category: cs.RO

TL;DR: 研究发现Waymo开放运动数据集(WOMD)无法准确反映真实自动驾驶车辆的行为动态，特别是在短车头时距和紧急减速方面存在低估，可能导致行为模型系统性低估驾驶风险。


<details>
  <summary>Details</summary>
Motivation: 验证Waymo开放运动数据集(WOMD)对自动驾驶车辆行为分析的准确性，因为该数据集存在专有后处理、缺乏误差量化以及轨迹分段等问题。

Method: 使用独立收集的凤凰城L4级自动驾驶自然数据集(PHX)进行比较分析，涵盖信号交叉口排队、跟车和换道三种典型场景。采用人工提取航拍视频车头时距、SIMEX方法处理测量误差、DTW距离量化行为差异。

Result: 所有场景分析一致显示PHX数据的行为超出WOMD的行为范围，WOMD低估了短车头时距和紧急减速行为。

Conclusion: 仅基于WOMD校准的行为模型可能系统性低估自然驾驶的变异性、风险和复杂性，使用WOMD进行行为建模时需要谨慎并需用独立数据进行验证。

Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for
data-driven modeling of autonomous vehicles (AVs) behavior. However, its
validity for behavioral analysis remains uncertain due to proprietary
post-processing, the absence of error quantification, and the segmentation of
trajectories into 20-second clips. This study examines whether WOMD accurately
captures the dynamics and interactions observed in real-world AV operations.
Leveraging an independently collected naturalistic dataset from Level 4 AV
operations in Phoenix, Arizona (PHX), we perform comparative analyses across
three representative urban driving scenarios: discharging at signalized
intersections, car-following, and lane-changing behaviors. For the discharging
analysis, headways are manually extracted from aerial video to ensure
negligible measurement error. For the car-following and lane-changing cases, we
apply the Simulation-Extrapolation (SIMEX) method to account for empirically
estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to
quantify behavioral differences. Results across all scenarios consistently show
that behavior in PHX falls outside the behavioral envelope of WOMD. Notably,
WOMD underrepresents short headways and abrupt decelerations. These findings
suggest that behavioral models calibrated solely on WOMD may systematically
underestimate the variability, risk, and complexity of naturalistic driving.
Caution is therefore warranted when using WOMD for behavior modeling without
proper validation against independently collected data.

</details>


### [78] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 这是一份关于基于大视觉-语言模型的视觉-语言-动作模型在机器人操纵领域的系统性调研报告，提供了架构分类、技术集成、特征综述和未来方向


<details>
  <summary>Details</summary>
Motivation: 传统规则基础方法在非结构化环境中缺乏扩展性和通用性，VLA模型成为变革性范式，需要系统性的分类和评估

Method: 通过系统性调研方法，定义了两种主要架构范式：单一架构模型（包括单系统和双系统）和层次架构模型，并深入分析了与各个先进领域的集成

Result: 提供了完整的分类体系、架构特征综述、数据集和测试标准，解决了现有分类不一致问题，减轻了研究分散化

Conclusion: 该调研报告整合了最新进展，填补了大视觉-语言模型与机器人操纵交叉领域的研究空白，并指明了内存机制、4D感知、高效适应等未来研究方向

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: 提出Lifecycle(LC)原则，通过状态记忆机制解决长期神经元失活导致的训练不稳定问题，在恢复神经元时使用历史有效参数而非随机初始化，从而提升泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法如Dropout只进行临时神经元变化，本文研究长期神经元失活的更强正则化形式，但发现这会带来严重的训练不稳定问题。

Method: 提出LC原则，核心创新是状态记忆机制。当神经元被重新激活时，不是使用随机权重初始化，而是恢复到上次已知的有效参数状态，从而保留已学知识并避免破坏性的优化冲击。

Result: 理论分析表明LC原则能够平滑损失景观，引导优化趋向更平坦的最小值（与更好泛化相关）。图像分类基准实验证明该方法提高了泛化能力和鲁棒性。消融研究确认状态记忆是实现这些收益的关键。

Conclusion: LC原则通过状态记忆机制有效解决了长期神经元失活带来的训练不稳定问题，在保持正则化效果的同时显著提升了模型的泛化性能和鲁棒性。

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [80] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 提出基于期望最大化(EM)的隐变量建模方法，结合多智能体强化学习(MARL)实现无人机协同野生动物保护，在伊朗豹栖息地巡逻任务中表现优于PPO和DDPG算法。


<details>
  <summary>Details</summary>
Motivation: 解决濒危野生动物非法盗猎问题，特别是在广阔且部分可观测环境中需要实时响应的挑战。传统方法在不确定环境下探索和协调能力有限。

Method: 采用期望最大化(EM)隐变量建模方法，通过隐变量建模隐藏环境因素和智能体间动态关系，结合多智能体强化学习实现无人机协同决策。

Result: 在10架无人机巡逻伊朗豹栖息地的仿真实验中，该方法在检测精度、适应性和策略收敛性方面均优于PPO和DDPG等标准算法。

Conclusion: EM推理与MARL结合能显著提升复杂高风险保护场景中的分散决策能力，为野生动物保护提供了有效的技术解决方案。

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [81] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: 该论文提出了一种组别敏感的阈值检测方法，通过为不同人口组设置不同的决策阈值，在类别不平衡问题上显著提升了平衡精度和最差组表现，效果超越传统的合成数据增帽方法。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习中的核心挑战，传统解决方案常带来新的问题。需要找到更简单、可解释性强且效果更好的方法来处理不同人口组间的性能差异。

Method: 采用组别敏感的阈值检测方法，为不同的人口组设置不同的决策阈值，优化平衡精度和最差组平衡精度之间的底特雷效率前沿。这种方法与单一阈值方法形成对比，同时评估了与合成数据增帽方法的结合效果。

Result: 通过对七种模型家族（线性、树基、实例基、提升方法等）的涉广实验，组别特定阈值方法比SMOTE和CT-GAN增帽模型获得1.5-4%更高的平衡精度，同时提升了最差组平衡精度。将组别阈值应用于合成数据增帽方法并没有显著收益，说明这些方法存在根本性重复。

Conclusion: 组别敏感的阈值检测提供了一种更简单、更可解释且更有效的类别不平衡解决方案。该方法能够实现细粒度的组级性能控制，在保持整体平衡精度的同时改善最差组表现，是传统合成数据增帽方法的优质替代方案。

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [82] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: 提出了DPO-PRO算法，一种基于直接偏好优化的鲁棒微调方法，用于公共卫生领域的序列资源分配问题奖励函数设计，相比现有方法更不保守且计算成本更低


<details>
  <summary>Details</summary>
Motivation: 解决公共卫生领域中复杂模糊目标和有限数据可用性带来的对齐挑战，需要设计能够处理偏好分布不确定性的鲁棒奖励函数

Method: 基于直接偏好优化(DPO)，采用轻量级分布鲁棒优化(DRO)公式来考虑偏好分布的不确定性

Result: 在真实世界母婴移动健康项目和标准对齐基准测试中，相比现有DPO变体显著提高了对噪声偏好信号的鲁棒性，且推理成本显著降低

Conclusion: DPO-PRO方法在保持性能的同时提供了更好的鲁棒性和计算效率，适用于数据有限的复杂决策场景

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [83] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 改进的策略梯度方法ACPG，通过固定ETF结构作为动作选择层目标，促进动作冲击现象的形成，提升训练效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 分析策略网络的表征结构，发现在某些约束下会出现类似神经冲击的动作冲击现象，这种结构能最大化动作间的分离度

Method: 提出ACPG方法，在动作选择层固定一个简单星等角紧凑框架（ETF）作为目标配置，促使策略网络产生理想的冲击结构

Result: 在多个OpenAI Gym环境中证明，ACPG可以集成到任何离散策略梯度方法中，更快速、更稳健地提升奖励表现

Conclusion: 固定ETF结构作为动作选择层的目标配置，可以有效促进动作冲击现象皅形成，从而提高策略梯度方法的训练效率和性能

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


### [84] [Mentality: A Mamba-based Approach towards Foundation Models for EEG](https://arxiv.org/abs/2509.02746)
*Saarang Panchavati,Corey Arnold,William Speier*

Main category: cs.LG

TL;DR: 使用Mamba选择性状态空间模型作为基础模型，通过自监督重建和癫痫检测任务，在EEG数据分析中取得了0.72的AUROC性能，为开发临床适用的EEG基础模型迈出重要一步。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有噪声大、高维和非线性的特点，传统机器学习方法难以捕捉其复杂的时空动态特性，需要更先进的序列建模方法来提升神经疾病诊断的自动化分析能力。

Method: 采用Mamba选择性状态空间模型，在大规模包含癫痫和非癫痫EEG记录的数据集上进行训练，先通过自监督重建任务学习特征表示，然后进行癫痫检测任务。

Result: 在保留测试集上实现了0.72的AUROC性能，证明了该模型在EEG癫痫检测方面的有效性。

Conclusion: 这项工作展示了基础模型在EEG分析中的潜力，为开发大规模、临床适用的EEG基础模型奠定了重要基础，有望推动神经疾病诊断的自动化发展。

Abstract: This work explores the potential of foundation models, specifically a
Mamba-based selective state space model, for enhancing EEG analysis in
neurological disorder diagnosis. EEG, crucial for diagnosing conditions like
epilepsy, presents significant challenges due to its noisy, high-dimensional,
and nonlinear nature. Traditional machine learning methods have made advances
in automating EEG analysis but often fail to capture its complex
spatio-temporal dynamics. Recent advances in deep learning, particularly in
sequence modeling, offer new avenues for creating more generalized and
expressive models capable of handling such complexities. By training a
Mamba-based model on a large dataset containing seizure and non-seizure EEG
recordings through a self-supervised reconstruction task followed by a seizure
detection task, we demonstrate the model's effectiveness, achieving an AUROC of
0.72 on a held-out test set. This approach marks a significant step toward
developing large-scale, clinically applicable foundation models for EEG data
analysis.

</details>


### [85] [LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference](https://arxiv.org/abs/2509.02753)
*Krishna Teja Chitty-Venkata,Sandeep Madireddy,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: LExI是一种数据无关的MoE模型优化技术，通过自适应调整每层激活专家数量来提升推理效率，相比传统剪枝方法在保持精度的同时显著提高吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有MoE剪枝方法主要减少内存占用但无法显著提升GPU推理性能，且固定专家激活策略导致冗余计算和次优性能

Method: LExI利用模型权重估计每层相对重要性，无需训练数据即可自适应确定每层最优激活专家数量

Result: 在语言和视觉MoE基准测试中，LExI显著优于传统剪枝方法，Qwen1.5-MoE在H100 GPU上实现相同吞吐量且精度提升10%

Conclusion: LExI提供了一种有效的后训练优化方案，通过层间专家数量自适应分配来提升MoE模型的推理效率

Abstract: Mixture-of-Experts (MoE) models scale efficiently by activating only a subset
of experts per token, offering a computationally sparse alternative to dense
architectures. While prior post-training optimizations, such as inter- and
intra-expert pruning, reduce memory usage they provide limited gains in
inference-time compute efficiency. Moreover, existing MoE architectures
typically activate a fixed number of experts uniformly across all layers,
resulting in redundant computation and suboptimal performance. In this work, we
first demonstrate that MoE pruning strategies improve only the memory footprint
but do not significantly improve inference performance on GPU using optimized
frameworks such as vLLM. To address this, we introduce LExI, a data-free
optimization technique that determines the optimal number of active experts per
layer in a pretrained MoE model. LExI leverages only the model weights to
estimate the relative importance of each layer and adaptively assigns the
number of active experts accordingly per layer. Experiments on state-of-the-art
language and vision MoE benchmarks demonstrate that LExI significantly
outperforms traditional MoE pruning approaches in terms of inference efficiency
with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves
the same throughput on Nvidia H100 GPU with 10% better accuracy than
traditional expert pruning.

</details>


### [86] [The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface](https://arxiv.org/abs/2509.02783)
*Arnab Mazumder,Javier E. Santos,Noah Hobbs,Mohamed Mehana,Daniel O'Malley*

Main category: cs.LG

TL;DR: Transparent Earth是一个基于Transformer的架构，用于从稀疏度、分辨率和模态各异的异构数据集中重建地下属性，支持任意模态数量的扩展和上下文学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决从不同类型观测数据（如应力角度、地幔温度、构造板块类型等）重建地下属性的挑战，这些数据在稀疏性、分辨率和模态上存在差异，需要一个能够处理多模态数据并支持灵活扩展的模型。

Method: 采用Transformer架构，结合位置编码和模态编码（通过文本嵌入模型从每个模态的描述中派生），支持任意数量的模态，包括八种模态（方向角度、分类类别、连续属性如温度和厚度），实现上下文学习能力。

Result: 在验证数据上，预测应力角度的误差减少了三倍以上，模型具有可扩展性，参数增加时性能提升。

Conclusion: Transparent Earth是地球地下属性的初步基础模型，旨在预测地球上任何地方的地下属性，具有可扩展性和改进的性能。

Abstract: We present the Transparent Earth, a transformer-based architecture for
reconstructing subsurface properties from heterogeneous datasets that vary in
sparsity, resolution, and modality, where each modality represents a distinct
type of observation (e.g., stress angle, mantle temperature, tectonic plate
type). The model incorporates positional encodings of observations together
with modality encodings, derived from a text embedding model applied to a
description of each modality. This design enables the model to scale to an
arbitrary number of modalities, making it straightforward to add new ones not
considered in the initial design. We currently include eight modalities
spanning directional angles, categorical classes, and continuous properties
such as temperature and thickness. These capabilities support in-context
learning, enabling the model to generate predictions either with no inputs or
with an arbitrary number of additional observations from any subset of
modalities. On validation data, this reduces errors in predicting stress angle
by more than a factor of three. The proposed architecture is scalable and
demonstrates improved performance with increased parameters. Together, these
advances make the Transparent Earth an initial foundation model for the Earth's
subsurface that ultimately aims to predict any subsurface property anywhere on
Earth.

</details>


### [87] [Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity](https://arxiv.org/abs/2509.02792)
*Alejandro Rodriguez Dominguez,Muhammad Shahzad,Xia Hong*

Main category: cs.LG

TL;DR: 提出结构化基函数网络，通过Bregman散度统一多假设预测和集成学习，解决预测不确定性问题，提供可调节的偏差-方差-多样性权衡机制。


<details>
  <summary>Details</summary>
Motivation: 现有预测不确定性方法存在局限：多假设预测缺乏原则性聚合，集成学习难以捕捉结构化歧义，缺乏与损失几何一致的统一框架。

Method: 使用Bregman散度诱导的质心聚合连接多假设预测和集成学习，支持闭式最小二乘估计器和基于梯度的通用目标优化，提供可调多样性机制。

Result: 实验验证了该方法在多假设泛化与损失感知集成聚合之间的关系，并在深度学习预测器上研究了复杂度-容量-多样性权衡。

Conclusion: 结构化基函数网络提供了一个统一的框架，能够同时处理回归和分类问题，通过几何对齐的预测和可调节的多样性机制，有效解决了预测不确定性问题。

Abstract: Existing approaches to predictive uncertainty rely either on multi-hypothesis
prediction, which promotes diversity but lacks principled aggregation, or on
ensemble learning, which improves accuracy but rarely captures the structured
ambiguity. This implicitly means that a unified framework consistent with the
loss geometry remains absent. The Structured Basis Function Network addresses
this gap by linking multi-hypothesis prediction and ensembling through
centroidal aggregation induced by Bregman divergences. The formulation applies
across regression and classification by aligning predictions with the geometry
of the loss, and supports both a closed-form least-squares estimator and a
gradient-based procedure for general objectives. A tunable diversity mechanism
provides parametric control of the bias-variance-diversity trade-off,
connecting multi-hypothesis generalisation with loss-aware ensemble
aggregation. Experiments validate this relation and use the mechanism to study
the complexity-capacity-diversity trade-off across datasets of increasing
difficulty with deep-learning predictors.

</details>


### [88] [Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks](https://arxiv.org/abs/2509.02803)
*Howard Dai,Nyambura Njenga,Benjamin Whitsett,Catherine Ma,Darwin Deng,Sara de Ángel,Alexandre Van Tassel,Siddharth Viswanath,Ryan Pellico,Ian Adelstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出通过归纳学习拉普拉斯特征向量来预训练图神经网络的新框架，解决传统MPNN因网络深度增加而难以捕获全局图结构的问题。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络(MPNN)随着网络深度增加容易出现过平滑问题，难以捕获全局和区域图结构。图拉普拉斯矩阵的低频特征向量编码了全局信息，通过预训练GNN预测这些特征向量可以促使网络自然学习大规模结构模式。

Method: 提出自监督预训练框架，通过归纳学习图拉普拉斯特征向量来预训练GNN。该方法不依赖特定领域任务，而是基于图结构，可应用于所有图数据集，在任务特定数据稀疏时还可使用合成特征。

Result: 实验表明，通过该框架预训练的模型在各种基于图结构的任务上优于基线模型。

Conclusion: 该框架提供了一种灵活有效的GNN预训练方法，能够更好地学习图的大规模结构模式，提升模型在结构相关任务上的性能。

Abstract: We propose a novel framework for pre-training Graph Neural Networks (GNNs) by
inductively learning Laplacian eigenvectors. Traditional Message Passing Neural
Networks (MPNNs) often struggle to capture global and regional graph structure
due to over-smoothing risk as network depth increases. Because the
low-frequency eigenvectors of the graph Laplacian matrix encode global
information, pre-training GNNs to predict these eigenvectors encourages the
network to naturally learn large-scale structural patterns over each graph.
Empirically, we show that models pre-trained via our framework outperform
baseline models on a variety of graph structure-based tasks. While most
existing pre-training methods focus on domain-specific tasks like node or edge
feature reconstruction, our self-supervised pre-training framework is
structure-based and highly flexible. Eigenvector-learning can be applied to all
graph-based datasets, and can be used with synthetic features when
task-specific data is sparse.

</details>


### [89] [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
*Trang Nguyen,Jackson Michaels,Madalina Fiterau,David Jensen*

Main category: cs.LG

TL;DR: 本文研究发现视觉语言模型(VLMs)中的冲突检测和冲突解决是可分离的机制，在LLaVA-OV-7B模型的中间层存在线性可解码的冲突信号，注意力模式在不同网络阶段出现分化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型中冲突检测与冲突解决机制分离的挑战，以提升模型在复杂多模态场景下的可解释性和鲁棒性。

Method: 使用线性探针进行监督度量分析，以及基于组的注意力模式分析，对LLaVA-OV-7B模型进行机制性研究。

Result: 发现模型中间层存在线性可解码的冲突信号，冲突检测和解决的注意力模式在网络不同阶段出现分化，证实了检测与解决是功能上不同的机制。

Conclusion: 这种机制分离为实现更可操作的可解释性和针对性干预提供了可能，有助于提升模型在挑战性多模态环境中的鲁棒性。

Abstract: This paper highlights the challenge of decomposing conflict detection from
conflict resolution in Vision-Language Models (VLMs) and presents potential
approaches, including using a supervised metric via linear probes and
group-based attention pattern analysis. We conduct a mechanistic investigation
of LLaVA-OV-7B, a state-of-the-art VLM that exhibits diverse resolution
behaviors when faced with conflicting multimodal inputs. Our results show that
a linearly decodable conflict signal emerges in the model's intermediate layers
and that attention patterns associated with conflict detection and resolution
diverge at different stages of the network. These findings support the
hypothesis that detection and resolution are functionally distinct mechanisms.
We discuss how such decomposition enables more actionable interpretability and
targeted interventions for improving model robustness in challenging multimodal
settings.

</details>


### [90] [Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs](https://arxiv.org/abs/2509.02820)
*Naman Deep Singh,Maximilian Müller,Francesco Croce,Matthias Hein*

Main category: cs.LG

TL;DR: JensUn是一种新的大语言模型遗忘方法，使用Jensen-Shannon散度作为训练目标，在遗忘特定信息的同时保持模型性能，并提出了更精确的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型遗忘方法在彻底评估时效果不佳，需要更有效的方法来安全删除私有数据或有害知识，同时需要更精确的评估标准。

Method: 提出JensUn方法，利用Jensen-Shannon散度作为训练目标来处理遗忘集和保留集，实现更稳定有效的遗忘动态；引入LKF数据集和基于LLM的语义评估代替ROUGE分数。

Result: JensUn在实验中实现了更好的遗忘-效用平衡，对良性再学习表现出强韧性；新的评估框架显示许多现有方法效果不如预期。

Conclusion: Jensen-Shannon散度是有效的遗忘训练目标，需要更严格的评估标准来准确衡量遗忘方法的有效性，JensUn方法在遗忘性能和模型保持方面表现优异。

Abstract: Unlearning in large language models (LLMs) involves precisely removing
specific information from a pre-trained model. This is crucial to ensure safety
of LLMs by deleting private data or harmful knowledge acquired during
pre-training. However, existing unlearning methods often fall short when
subjected to thorough evaluation. To overcome this, we introduce JensUn, where
we leverage the Jensen-Shannon Divergence as the training objective for both
forget and retain sets for more stable and effective unlearning dynamics
compared to commonly used loss functions. In extensive experiments, JensUn
achieves better forget-utility trade-off than competing methods, and even
demonstrates strong resilience to benign relearning. Additionally, for a
precise unlearning evaluation, we introduce LKF, a curated dataset of
lesser-known facts that provides a realistic unlearning scenario. Finally, to
comprehensively test unlearning methods, we propose (i) employing an LLM as
semantic judge instead of the standard ROUGE score, and (ii) using worst-case
unlearning evaluation over various paraphrases and input formats. Our improved
evaluation framework reveals that many existing methods are less effective than
previously thought.

</details>


### [91] [Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction](https://arxiv.org/abs/2509.02826)
*Towhidul Islam,Md Sumon Ali*

Main category: cs.LG

TL;DR: 这篇论文比较了混合多数投票咆集成堆叠两种集成学习方法在肥胖风险预测中的性能，发现集成堆叠方法在复杂数据分布下表现更优。


<details>
  <summary>Details</summary>
Motivation: 肥胖是一个全球急需解决的健康问题，机器学习在早期风险预测中展现出潜力，但对于集成技术（特别是混合多数投票咆集成堆叠）的比较评估仍有限。

Method: 使用两个数据集评估三种集成模型：多数硬投票、权重硬投票咆堆叠（以多层感知机为元分类器）。分析9个机器学习算法的50个超参数配置，选择前三个模型作为基学习器。预处理步骤包括数据集平衡咆异常值检测，以准确率咆F1分数评估模型性能。

Result: 在数据集1上，权重硬投票咆堆叠表现相似（准确率0.920304，F1分数0.920070），超过多数硬投票。在数据集2上，堆叠方法表现最优（准确率0.989837，F1分数0.989825），超过多数硬投票（准确率0.981707，F1分数0.981675）咆权重硬投票。

Conclusion: 集成堆叠方法在复杂数据分布下具有更强的预测能力，而混合多数投票仍是一个健壮的备选方案，为健康预测模型选择提供了指导。

Abstract: Obesity is a critical global health issue driven by dietary, physiological,
and environmental factors, and is strongly associated with chronic diseases
such as diabetes, cardiovascular disorders, and cancer. Machine learning has
emerged as a promising approach for early obesity risk prediction, yet a
comparative evaluation of ensemble techniques -- particularly hybrid majority
voting and ensemble stacking -- remains limited. This study aims to compare
hybrid majority voting and ensemble stacking methods for obesity risk
prediction, identifying which approach delivers higher accuracy and efficiency.
The analysis seeks to highlight the complementary strengths of these ensemble
techniques in guiding better predictive model selection for healthcare
applications. Two datasets were utilized to evaluate three ensemble models:
Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer
Perceptron as meta-classifier). A pool of nine Machine Learning (ML)
algorithms, evaluated across a total of 50 hyperparameter configurations, was
analyzed to identify the top three models to serve as base learners for the
ensemble methods. Preprocessing steps involved dataset balancing, and outlier
detection, and model performance was evaluated using Accuracy and F1-Score. On
Dataset-1, weighted hard voting and stacking achieved nearly identical
performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard
voting. On Dataset-2, stacking demonstrated superior results (Accuracy:
0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,
F1: 0.981675) and weighted hard voting, which showed the lowest performance.
The findings confirm that ensemble stacking provides stronger predictive
capability, particularly for complex data distributions, while hybrid majority
voting remains a robust alternative.

</details>


### [92] [Conformal Prediction for Time-series Forecasting with Change Points](https://arxiv.org/abs/2509.02844)
*Sophia Sun,Rose Yu*

Main category: cs.LG

TL;DR: 提出了一种新的CPTC算法，用于处理具有突变点的时间序列数据，通过结合状态预测模型和在线保形预测来建模非平稳时间序列中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的保形预测方法在处理具有突变点的时间序列数据时存在困难，无法有效应对数据生成过程中的突然变化。

Method: CPTC算法整合了状态预测模型和在线保形预测，能够预测基础状态并建模非平稳时间序列中的不确定性。

Result: 在6个合成和真实数据集上的实验表明，CPTC相比最先进的基线方法具有更好的有效性和适应性。

Conclusion: CPTC算法在最小假设条件下证明了其有效性，能够有效处理具有突变点的时间序列不确定性量化问题。

Abstract: Conformal prediction has been explored as a general and efficient way to
provide uncertainty quantification for time series. However, current methods
struggle to handle time series data with change points - sudden shifts in the
underlying data-generating process. In this paper, we propose a novel Conformal
Prediction for Time-series with Change points (CPTC) algorithm, addressing this
gap by integrating a model to predict the underlying state with online
conformal prediction to model uncertainties in non-stationary time series. We
prove CPTC's validity and improved adaptivity in the time series setting under
minimum assumptions, and demonstrate CPTC's practical effectiveness on 6
synthetic and real-world datasets, showing improved validity and adaptivity
compared to state-of-the-art baselines.

</details>


### [93] [Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm](https://arxiv.org/abs/2509.02846)
*Siddharth Mansingh,James Amarel,Ragib Arnab,Arvind Mohan,Kamaljeet Singh,Gerd J. Kunde,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Nathan A. Debarledeben,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.LG

TL;DR: 基于大语言模型的"思考"策略，提出首个偏微方程测试时计算框架，通过测试阶段利用计算资源提升预测准确性，减少训练数据和模型规模要求。


<details>
  <summary>Details</summary>
Motivation: 现有偏微方程基础模型受限于预训练数据集，自回归滚动性能不佳，特别是在分布外情况下，且计算和训练数据需求高，限制了在关键应用中的使用。

Method: 采用两种奖励模型评估随机基础模型的预测结果，检验其空间-时间一致性，在测试阶段利用计算资源进行调整和优化。

Result: 在PDEGym质点流动方程模拟中，TTC框架相比标准非适应性自回归推理，能够获得更准确的预测结果。

Conclusion: 该TTC框枵为偏微方程建模提供了基础性的推理算法方向，有望通过建立基于强化学习的方法，改变物理和工程领域的计算工作流程。

Abstract: Partial Differential Equations (PDEs) are the bedrock for modern
computational sciences and engineering, and inherently computationally
expensive. While PDE foundation models have shown much promise for simulating
such complex spatio-temporal phenomena, existing models remain constrained by
the pretraining datasets and struggle with auto-regressive rollout performance,
especially in out-of-distribution (OOD) cases. Furthermore, they have
significant compute and training data requirements which hamper their use in
many critical applications. Inspired by recent advances in ``thinking"
strategies used in large language models (LLMs), we introduce the first
test-time computing (TTC) strategy for PDEs that utilizes computational
resources during inference to achieve more accurate predictions with fewer
training samples and smaller models. We accomplish this with two types of
reward models that evaluate predictions of a stochastic based model for
spatio-temporal consistency. We demonstrate this method on compressible
Euler-equation simulations from the PDEGym benchmark and show that TTC captures
improved predictions relative to standard non-adaptive auto-regressive
inference. This TTC framework marks a foundational step towards more advanced
reasoning algorithms or PDE modeling, inluding building
reinforcement-learning-based approaches, potentially transforming computational
workflows in physics and engineering.

</details>


### [94] [Power Grid Control with Graph-Based Distributed Reinforcement Learning](https://arxiv.org/abs/2509.02861)
*Carlo Fabrizio,Gianvito Losapio,Marco Mussi,Alberto Maria Metelli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出基于图神经网络的分布式强化学习框架，用于实时可扩展的电网管理，通过分解动作和观测空间，结合模仿学习和奖励塑形，在Grid2Op环境中优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 可再生能源集成和电网规模扩大对传统控制系统提出挑战，需要更动态和分布式的控制策略。

Method: 采用分层架构：底层分布式代理控制单个电力线路，高层管理器协调；使用GNN编码网络拓扑信息；集成模仿学习和基于电位的奖励塑形来加速收敛。

Result: 在Grid2Op仿真环境中 consistently 优于标准基线方法，计算效率显著高于基于仿真的专家方法。

Conclusion: 该分布式强化学习框架为大规模电网管理提供了有效的解决方案，具有良好的性能和计算效率。

Abstract: The necessary integration of renewable energy sources, combined with the
expanding scale of power networks, presents significant challenges in
controlling modern power grids. Traditional control systems, which are human
and optimization-based, struggle to adapt and to scale in such an evolving
context, motivating the exploration of more dynamic and distributed control
strategies. This work advances a graph-based distributed reinforcement learning
framework for real-time, scalable grid management. The proposed architecture
consists of a network of distributed low-level agents acting on individual
power lines and coordinated by a high-level manager agent. A Graph Neural
Network (GNN) is employed to encode the network's topological information
within the single low-level agent's observation. To accelerate convergence and
enhance learning stability, the framework integrates imitation learning and
potential-based reward shaping. In contrast to conventional decentralized
approaches that decompose only the action space while relying on global
observations, this method also decomposes the observation space. Each low-level
agent acts based on a structured and informative local view of the environment
constructed through the GNN. Experiments on the Grid2Op simulation environment
show the effectiveness of the approach, which consistently outperforms the
standard baseline commonly adopted in the field. Additionally, the proposed
model proves to be much more computationally efficient than the
simulation-based Expert method.

</details>


### [95] [Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)](https://arxiv.org/abs/2509.02863)
*Vikas Kashtriya,Pardeep Singh*

Main category: cs.LG

TL;DR: QI-SMOTE是一种基于量子原理的新型数据增强技术，通过量子演化和分层纠缠生成合成样本，有效解决医学机器学习中的类别不平衡问题，显著提升多种分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 医学机器学习中存在严重的类别不平衡问题，少数类样本不足导致模型偏见和预测性能下降，需要开发更有效的数据增强方法来改善模型泛化能力。

Method: 提出量子启发的SMOTE方法(QI-SMOTE)，利用量子演化和分层纠缠原理生成合成实例，保持复杂数据结构，与传统过采样方法相比能产生更平衡和信息的训练数据。

Result: 在MIMIC-III和MIMIC-IV数据集上的实验表明，QI-SMOTE在准确率、F1分数、G-Mean和AUC-ROC等指标上显著优于传统过采样方法，特别提升了集成方法、核方法和深度学习方法的效果。

Conclusion: QI-SMOTE通过整合量子启发变换到机器学习流程中，不仅缓解了类别不平衡问题，还增强了医学诊断和决策中预测模型的鲁棒性和可靠性，展示了量子启发重采样技术在推进最先进机器学习方法中的潜力。

Abstract: Class imbalance remains a critical challenge in machine learning (ML),
particularly in the medical domain, where underrepresented minority classes
lead to biased models and reduced predictive performance. This study introduces
Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that
enhances the performance of ML classifiers, including Random Forest (RF),
Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors
(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum
principles such as quantum evolution and layered entanglement. Unlike
conventional oversampling methods, QI-SMOTE generates synthetic instances that
preserve complex data structures, improving model generalization and
classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV
datasets, using mortality detection as a benchmark task due to their clinical
significance and inherent class imbalance. We compare our method against
traditional oversampling techniques, including Borderline-SMOTE, ADASYN,
SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as
Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE
significantly improves the effectiveness of ensemble methods (RF, GB, ADA),
kernel-based models (SVM), and deep learning approaches by producing more
informative and balanced training data. By integrating quantum-inspired
transformations into the ML pipeline, QI-SMOTE not only mitigates class
imbalance but also enhances the robustness and reliability of predictive models
in medical diagnostics and decision-making. This study highlights the potential
of quantum-inspired resampling techniques in advancing state-of-the-art ML
methodologies.

</details>


### [96] [Improving Generative Methods for Causal Evaluation via Simulation-Based Inference](https://arxiv.org/abs/2509.02892)
*Pracheta Amaranath,Vinitra Muralikrishnan,Amit Sharma,David D. Jensen*

Main category: cs.LG

TL;DR: SBICE是一个基于模拟推理的因果评估框架，通过建模生成参数的不确定性并推断其后验分布，生成更符合源数据分布的合成数据集，提高因果估计器评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法需要用户提供点估计而非分布估计，且参数固定无法改进，无法表达参数不确定性，导致估计器比较不可靠。

Method: 利用模拟推理技术，将生成参数建模为不确定的，并基于源数据集推断其后验分布，识别能产生与源数据分布紧密对齐的合成数据集的参数配置。

Result: 实证结果表明SBICE通过生成更真实的数据集，提高了估计器评估的可靠性，支持在不确定性下进行稳健和数据一致的因果基准测试。

Conclusion: SBICE框架为因果评估提供了更可靠的方法，能够处理参数不确定性并生成更真实的合成数据，改善因果估计器的比较和基准测试。

Abstract: Generating synthetic datasets that accurately reflect real-world
observational data is critical for evaluating causal estimators, but remains a
challenging task. Existing generative methods offer a solution by producing
synthetic datasets anchored in the observed data (source data) while allowing
variation in key parameters such as the treatment effect and amount of
confounding bias. However, existing methods typically require users to provide
point estimates of such parameters (rather than distributions) and fixed
estimates (rather than estimates that can be improved with reference to the
source data). This denies users the ability to express uncertainty over
parameter values and removes the potential for posterior inference, potentially
leading to unreliable estimator comparisons. We introduce simulation-based
inference for causal evaluation (SBICE), a framework that models generative
parameters as uncertain and infers their posterior distribution given a source
dataset. Leveraging techniques in simulation-based inference, SBICE identifies
parameter configurations that produce synthetic datasets closely aligned with
the source data distribution. Empirical results demonstrate that SBICE improves
the reliability of estimator evaluations by generating more realistic datasets,
which supports a robust and data-consistent approach to causal benchmarking
under uncertainty.

</details>


### [97] [VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills](https://arxiv.org/abs/2509.02930)
*Erik M. Lintunen*

Main category: cs.LG

TL;DR: 本文提出了VendiRL框架，使用生态学中的Vendi Score指标来评估和优化强化学习中的技能多样性，解决了传统方法中多样性定义不一致和评估困难的问题。


<details>
  <summary>Details</summary>
Motivation: 自监督强化学习中，技能多样性学习面临两个主要挑战：高维特征空间中相关特征难以识别，以及多样性定义不统一导致评估结果难以比较和多种多样性形式未被探索。

Method: 采用生态学中的Vendi Score作为多样性度量指标，允许用户指定任意形式的多样性。基于此提出了VendiRL统一框架，通过不同的相似性函数来激励不同形式的技能多样性。

Result: VendiRL能够促进技能评估，并在新的丰富交互环境中支持多种多样性形式的技能预训练。

Conclusion: Vendi Score提供了一个灵活的多样性评估框架，VendiRL能够统一地学习多样化的技能集合，为未来任务准备提供了更好的基础。

Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.

</details>


### [98] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 通过地震信号检测大象脚步的分析框架，采用CCW事件检测技术和SVM分类器，在自然环境中达到有140米检测范围和70-99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决人象冲突问题，克服现有方案依赖人工分类而无法实现实时检测的限制，为资源受限环境设计高效分析框架。

Method: 提出专门为大象脚步检测设计的Contextually Customized Windowing (CCW)事件检测技术，与STA/LTA方法进行对比。使用支持向量机(SVM)配合径向基函数(RBF)核进行分类，并通过可解释AI进行特征影响分析。

Result: 在受控环境中最大检测范围155.6米，自然环境中140米。SVM分类器在不同环境下准确率分别为：受控环境99%、自然生境73%、人象冲突环境70%。零点交叉数和DTW对齐成本是最重要特征。

Conclusion: 该框架能够在资源受限环境中高效检测大象脚步，CCW技术和SVM分类器表现优异，为实时人象冲突解决方案提供了可行技术支撑。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [99] [Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning](https://arxiv.org/abs/2509.03030)
*Zida Wu,Mathieu Lauriere,Matthieu Geist,Olivier Pietquin,Ankur Mehta*

Main category: cs.LG

TL;DR: 提出一种高效的深度强化学习算法，用于在平均场博弈中寻找纳什均衡，无需依赖平均或历史采样，适用于未知初始分布和共同噪声场景。


<details>
  <summary>Details</summary>
Motivation: 平均场博弈是研究大规模多智能体系统的强大框架，但在初始分布未知或存在共同噪声时，学习纳什均衡仍然具有挑战性。现有方法依赖平均或历史采样，限制了算法的适应性和鲁棒性。

Method: 基于Munchausen RL和在线镜像下降的深度强化学习算法，能够实现与种群相关的纳什均衡，无需依赖平均或历史采样。

Result: 在七个典型示例上的数值实验表明，该算法相比最先进算法（特别是基于虚拟游戏的DRL版本）具有更优越的收敛特性，在共同噪声存在时表现出鲁棒性和适应性。

Conclusion: 该算法为平均场博弈中的纳什均衡学习提供了高效解决方案，特别适用于初始分布未知和存在共同噪声的复杂场景，展现出良好的收敛性能和鲁棒性。

Abstract: Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.

</details>


### [100] [A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers](https://arxiv.org/abs/2509.02923)
*Kunal Kumar,Muhammad Ashad Kabir,Luke Donnan,Sayed Ahmed*

Main category: cs.LG

TL;DR: 这篇论文通过系统评估甲尿病足濁痕防治鞋靴处方案，提出了一个五部分的临床决策支持框架，重点关注数据集、混合模型、可解释性和结果评估。


<details>
  <summary>Details</summary>
Motivation: 甲尿病足濁痕防治鞋靴处方案存在特征选择不一、个性化限制和评估方法分散的问题，需要统一的决策支持框架来提高临床效果。

Method: 对45项研究进行叙事性综述（包括12个指南/协议，25个知识基础系统，8个机器学习应用），从知识类型、决策逻辑、评估方法和技术支撑进行主题分析。

Result: 指南重视身底压力阈值，知识基础系统采用规则和传感器逻辑，ML模型计算准确性高但可解释性和临床验证不足。评估方法仍然分散不统一。

Conclusion: 提出五部分CDSS框架：最小数据集、混合架构、结构化特征输出、连续验证和临床流程集成，以支持可扩展、症人为中心的甲尿病足濁痕管理。

Abstract: Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by
lowering plantar pressure (PP), yet prescription decisions remain fragmented:
feature selection varies, personalization is limited, and evaluation practices
differ. We performed a narrative review of 45 studies (12 guidelines/protocols,
25 knowledge-based systems, 8 machine-learning applications) published to Aug
2025. We thematically analyzed knowledge type, decision logic, evaluation
methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200
kPa or >=25--30\% reduction) but rarely yield actionable, feature-level
outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating
PP monitoring, adherence tracking, and usability testing. ML work introduces
predictive, optimization, and generative models with high computational
accuracy but limited explainability and clinical validation. Evaluation remains
fragmented: protocols prioritize biomechanical tests; knowledge-based systems
assess usability/adherence; ML studies focus on technical accuracy with weak
linkage to long-term outcomes. From this synthesis we propose a five-part CDSS
framework: (1) a minimum viable dataset; (2) a hybrid architecture combining
rules, optimization, and explainable ML; (3) structured feature-level outputs;
(4) continuous validation and evaluation; and (5) integration with clinical and
telehealth workflows. This framework aims to enable scalable, patient-centered
CDSSs for DFU care; prioritizing interoperable datasets, explainable models,
and outcome-focused evaluation will be key to clinical adoption.

</details>


### [101] [PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials](https://arxiv.org/abs/2509.02927)
*Shih-Peng Huang,Nontawat Charoenphakdee,Yuta Tsuboi,Yong-Bin Zhuang,Wenwen Li*

Main category: cs.LG

TL;DR: 本文提出了一种简单高效的后处理不确定性量化方法PDRL，利用训练好的图神经网络描述子估计预测殊差，以免去集成方法的高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 集成方法虽然是不确定性量化的金标准，但计算成本过高。现有的替代方法如Monte Carlo dropout和deep kernel learning既不能应用于已训练模型，又可能影响预测准确性。

Method: 提出PDRL方法，利用训练好的图神经网络演化演化演化描述子来估计殊差值，将模型预测与真实值的差异作为不确定性的代理指标。研究了PDRL的多种变体。

Result: 对PDRL的多种变体进行了评测，并与现有的不确定性量化方法进行了对比分析。

Conclusion: PDRL提供了一种简单高效的后处理方案，能够在保持预测准确性的同时，免去集成方法的高计算成本，为机器学习间原子潜力的不确定性量化提供了更实用的解决方案。

Abstract: Ensemble method is considered the gold standard for uncertainty
quantification (UQ) for machine learning interatomic potentials (MLIPs).
However, their high computational cost can limit its practicality. Alternative
techniques, such as Monte Carlo dropout and deep kernel learning, have been
proposed to improve computational efficiency; however, some of these methods
cannot be applied to already trained models and may affect the prediction
accuracy. In this paper, we propose a simple and efficient post-hoc framework
for UQ that leverages the descriptor of a trained graph neural network
potential to estimate residual errors. We refer to this method as post-hoc
descriptor-based residual-based learning (PDRL). PDRL models the discrepancy
between MLIP predictions and ground truth values, allowing these residuals to
act as proxies for prediction uncertainty. We explore multiple variants of PDRL
and benchmark them against established UQ methods, evaluating both their
effectiveness and limitations.

</details>


### [102] [AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting](https://arxiv.org/abs/2509.02967)
*Chen Zeng,Tiehang Xu,Qiao Wang*

Main category: cs.LG

TL;DR: 提出了AR-KAN混合模型，结合KAN网络处理静态非线性部分和预训练AR组件处理记忆信息，在72%的真实数据集上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在信号频谱分析中面临挑战，特别是处理非周期信号叠加问题时表现不佳，传统ARIMA模型在处理非周期函数时甚至优于大多数神经网络包括大语言模型

Method: 使用Autoregressive-Weight-Enhanced AR-KAN混合模型，基于Universal Myopic Mapping Theorem，用Kolmogorov-Arnold Network处理静态非线性部分，通过预训练的AR组件引入记忆机制，保留有用信息并消除冗余

Result: 实验数据显示AR-KAN在72%的真实世界数据集上提供了优越的结果

Conclusion: AR-KAN模型成功解决了传统神经网络在处理非周期信号叠加问题时的局限性，通过结合KAN和AR组件的优势，在频谱分析任务中取得了显著改进

Abstract: Conventional neural networks frequently face challenges in spectral analysis
of signals. To address this challenge, Fourier neural networks (FNNs) and
similar approaches integrate components of Fourier series into the structure of
neural networks. Nonetheless, a significant hurdle is often overlooked: the
superposition of periodic signals does not necessarily result in a periodic
signal. For example, when forecasting almost periodic functions composed of
signals with incommensurate frequencies, traditional models such as
Autoregressive Integrated Moving Average (ARIMA) frequently outperform most
neural networks including large language models (LLMs). To tackle this goal, we
propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the
benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply
a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include
memory through a pre-trained AR component, which can be explained to retain the
most useful information while eliminating redundancy. Experimental data
indicates that AR-KAN delivers superior results on $72\%$ of real-world
datasets.

</details>


### [103] [Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation](https://arxiv.org/abs/2509.02970)
*Kaoru Otsuka,Yuki Takezawa,Makoto Yamada*

Main category: cs.LG

TL;DR: 提出了D-Byz-SGDM方法，通过延迟动量聚合解决联邦学习中部分参与场景下的拜占庭攻击问题，在理论和实验上都取得了良好效果


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒联邦学习方法假设全客户端参与，但在实际通信约束和客户端可用性限制下，部分参与是常态。当采样客户端包含拜占庭多数时，现有方法会立即失效

Method: 引入延迟动量聚合原则：服务器聚合最近接收的非参与客户端梯度与活跃客户端的新鲜动量。开发D-Byz-SGDM优化器实现这一原则

Result: 建立了收敛保证，恢复了全参与结果并匹配部分参与设置的基本下界。深度学习实验验证了理论发现，在各种拜占庭攻击下表现出稳定鲁棒的训练效果

Conclusion: D-Byz-SGDM方法有效解决了部分参与联邦学习中的拜占庭攻击问题，提供了理论保证和实际可行性

Abstract: Federated Learning (FL) allows distributed model training across multiple
clients while preserving data privacy, but it remains vulnerable to Byzantine
clients that exhibit malicious behavior. While existing Byzantine-robust FL
methods provide strong convergence guarantees (e.g., to a stationary point in
expectation) under Byzantine attacks, they typically assume full client
participation, which is unrealistic due to communication constraints and client
availability. Under partial participation, existing methods fail immediately
after the sampled clients contain a Byzantine majority, creating a fundamental
challenge for sparse communication. First, we introduce delayed momentum
aggregation, a novel principle where the server aggregates the most recently
received gradients from non-participating clients alongside fresh momentum from
active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with
Momentum) implements this delayed momentum aggregation principle for
Byzantine-robust FL with partial participation. Then, we establish convergence
guarantees that recover previous full participation results and match the
fundamental lower bounds we prove for the partial participation setting.
Experiments on deep learning tasks validated our theoretical findings, showing
stable and robust training under various Byzantine attacks.

</details>


### [104] [AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates](https://arxiv.org/abs/2509.02981)
*Minxin Zhang,Yuxuan Liu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: AdaGO算法结合了正交化动量更新和AdaGrad自适应步长，在保持正交更新方向的同时通过累积梯度范数自适应调整步长，在理论和实验上都优于Muon和Adam。


<details>
  <summary>Details</summary>
Motivation: Muon优化器通过正交化动量更新权重矩阵，在大语言模型训练中表现优异，但缺乏确定学习率的方法。而AdaGrad作为广泛使用的自适应方法，通过累积历史梯度来缩放步长。需要结合两者的优势。

Method: 提出AdaGO算法，将基于范数的AdaGrad型步长与正交化更新方向相结合。保持更新方向的正交性（可解释为谱下降方向），同时通过累积梯度范数缩放方向来适应优化景观。实现只需对Muon进行最小修改，增加一个标量变量（累积平方梯度范数）。

Result: 在非凸函数的随机和确定性设置下建立了最优理论收敛速率。在CIFAR-10分类和函数回归任务上的实证结果表明，AdaGO优于Muon和Adam。

Conclusion: AdaGO成功结合了正交化更新和自适应步长的优势，在计算和内存效率高的前提下，实现了更好的优化性能，为大规模语言模型训练提供了有效的优化方案。

Abstract: The recently proposed Muon optimizer updates weight matrices via
orthogonalized momentum and has demonstrated strong empirical success in large
language model training. However, it remains unclear how to determine the
learning rates for such orthogonalized updates. AdaGrad, by contrast, is a
widely used adaptive method that scales stochastic gradients by accumulated
past gradients. We propose a new algorithm, AdaGO, which combines a norm-based
AdaGrad-type stepsize with an orthogonalized update direction, bringing
together the benefits of both approaches. Unlike other adaptive variants of
Muon, AdaGO preserves the orthogonality of the update direction, which can be
interpreted as a spectral descent direction, while adapting the stepsizes to
the optimization landscape by scaling the direction with accumulated past
gradient norms. The implementation of AdaGO requires only minimal modification
to Muon, with a single additional scalar variable, the accumulated squared
gradient norms, to be computed, making it computationally and memory efficient.
Optimal theoretical convergence rates are established for nonconvex functions
in both stochastic and deterministic settings under standard smoothness and
unbiased bounded-variance noise assumptions. Empirical results on CIFAR-10
classification and function regression demonstrate that AdaGO outperforms Muon
and Adam.

</details>


### [105] [StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails](https://arxiv.org/abs/2509.02982)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 提出了一种流式、无源测试时自适应方法，结合熵最小化、批归一化统计刷新和安全机制，用于睡眠分期模型的在线适应，在未见过的生理或记录条件下保持性能。


<details>
  <summary>Details</summary>
Motivation: 睡眠分期模型在面对未见过的患者生理特征或记录条件时性能会下降，需要一种实用的在线自适应方法，无需源数据或患者校准，适合设备端或床边使用。

Method: 结合熵最小化(Tent)和批归一化统计刷新，采用两个安全机制：熵门控在不确定窗口暂停适应，EMA重置机制防止漂移。模型无关，无需源数据。

Result: 在Sleep-EDF Expanded数据集上，使用单导联EEG，相比冻结基线模型在秒级延迟和最小内存占用下获得一致性能提升，报告了各阶段指标和Cohen's k值。

Conclusion: 该方法提供了一种实用、高效的测试时自适应方案，能够有效应对睡眠分期模型在部署时的领域偏移问题，适合实时临床应用。

Abstract: Sleep staging models often degrade when deployed on patients with unseen
physiology or recording conditions. We propose a streaming, source-free
test-time adaptation (TTA) recipe that combines entropy minimization (Tent)
with Batch-Norm statistic refresh and two safety rails: an entropy gate to
pause adaptation on uncertain windows and an EMA-based reset to reel back
drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s
epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline
at seconds-level latency and minimal memory, reporting per-stage metrics and
Cohen's k. The method is model-agnostic, requires no source data or patient
calibration, and is practical for on-device or bedside use.

</details>


### [106] [Multimodal learning of melt pool dynamics in laser powder bed fusion](https://arxiv.org/abs/2509.03029)
*Satyajit Mojumder,Pallock Halder,Tiana Tonge*

Main category: cs.LG

TL;DR: 多模态数据融合方法通过结合高份边X射数据和低成本吸收率数据，提高湿床动力学预测精度，后通过迁移学习仅使用吸收率数据即可进行准确预测


<details>
  <summary>Details</summary>
Motivation: 解决高速X射成像成本高且实用性低，而低成本光电二极管吸收率数据噪声大、单独使用预测准确性低的问题

Method: 采用多模态学习框架，结合CNN处理X射空间特征和RNN处理吸收率时序特征，通过迁移学习优化仅使用吸收率数据的RNN模型

Result: 多模态训练显著提高了预测准确性，训练完成后可仅使用吸收率数据进行准确预测，避免了昂贵的X射成像

Conclusion: 该多模态融合方法能够实现成本效益高的实时监控，在增材制造领域具有广泛的应用前景

Abstract: While multiple sensors are used for real-time monitoring in additive
manufacturing, not all provide practical or reliable process insights. For
example, high-speed X-ray imaging offers valuable spatial information about
subsurface melt pool behavior but is costly and impractical for most industrial
settings. In contrast, absorptivity data from low-cost photodiodes correlate
with melt pool dynamics but is often too noisy for accurate prediction when
used alone. In this paper, we propose a multimodal data fusion approach for
predicting melt pool dynamics by combining high-fidelity X-ray data with
low-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.
Our multimodal learning framework integrates convolutional neural networks
(CNNs) for spatial feature extraction from X-ray data with recurrent neural
networks (RNNs) for temporal feature extraction from absorptivity signals,
using an early fusion strategy. The multimodal model is further used as a
transfer learning model to fine-tune the RNN model that can predict melt pool
dynamics only with absorptivity, with greater accuracy compared to the
multimodal model. Results show that training with both modalities significantly
improves prediction accuracy compared to using either modality alone.
Furthermore, once trained, the model can infer melt pool characteristics using
only absorptivity data, eliminating the need for expensive X-ray imaging. This
multimodal fusion approach enables cost-effective, real-time monitoring and has
broad applicability in additive manufacturing.

</details>


### [107] [Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models](https://arxiv.org/abs/2509.03036)
*Bilge Taskin,Wenxiong Xie,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 利用预训练大语言模型(LLMs)来自动化物理知识融入符号回归(SR)，提高方程发现的准确性和健壃性


<details>
  <summary>Details</summary>
Motivation: 当前的物理知识融入符号回归(PiSR)方法需要专业的形式化和手工特征工程，限制了普通研究人员的使用。需要一种更易于访问的方法来自动化域知识的融入

Method: 将预训练的LLMs(Falcon、Mistral、LLama 2)集成到SR算法(DEAP、gplearn、PySR)的损失函数中，添加LLM对SR生成方程的评估项来融入域知识

Result: 在三种物理动力学问题(固体回收、简谐运动、电磁波)上，LLM集成一致地提高了从数据重构物理动力学的性能，增强了SR模型对噪声和复杂性的健壃性

Conclusion: 利用LLMs的上下文理解能力可以有效地自动化物理知识的融入过程，提高符号回归的效果，且提示工程的质量对性能有显著影响

Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated
scientific discovery, enabling the derivation of governing equations from
experimental data. A growing body of work illustrates the promise of
integrating domain knowledge into the SR to improve the discovered equation's
generality and usefulness. Physics-informed SR (PiSR) addresses this by
incorporating domain knowledge, but current methods often require specialized
formulations and manual feature engineering, limiting their adaptability only
to domain experts. In this study, we leverage pre-trained Large Language Models
(LLMs) to facilitate knowledge integration in PiSR. By harnessing the
contextual understanding of LLMs trained on vast scientific literature, we aim
to automate the incorporation of domain knowledge, reducing the need for manual
intervention and making the process more accessible to a broader range of
scientific problems. Namely, the LLM is integrated into the SR's loss function,
adding a term of the LLM's evaluation of the SR's produced equation. We
extensively evaluate our method using three SR algorithms (DEAP, gplearn, and
PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three
physical dynamics (dropping ball, simple harmonic motion, and electromagnetic
wave). The results demonstrate that LLM integration consistently improves the
reconstruction of physical dynamics from data, enhancing the robustness of SR
models to noise and complexity. We further explore the impact of prompt
engineering, finding that more informative prompts significantly improve
performance.

</details>


### [108] [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)
*Xinzhe Zheng,Zhen-Qun Yang,Haoran Xie,S. Joe Qin,Arlene Chen,Fangzhen Lin*

Main category: cs.LG

TL;DR: 一种新的二进制量化方法，通过动态选择最优子矩阵和适应性分组策略，在平均1.007位的压缩比下保持了高模型质量，性能超过之前的二进制量化方法并与4位量化方法竞争。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要大量内存和计算资源，二进制量化可以大幅减少存储和推理成本，但传统的二进制量化方法导致显著的性能下降。

Method: 提出了一种专门为二进制量化设计的新题优化目标，以及三种实现该目标的算法。通过动态识别最优的非结构化子矩阵，采用适应性分组策略来改进块量化。

Result: 平均位长仅1.007位，LLaMA 3.2 3B模型的混淆度从原始的7.81降至8.23，显著超过之前的BiLLM方法（混淆度为123.90）。压缩过程高效，单核CPU仅需14秒完成量化，整个过程在100分钟内完成。

Conclusion: 该方法在极端的二进制量化下仍能保持高模型质量，性能与4位量化方法相当，但压缩比更高，具有强并行性能，为大模型部署提供了高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of Natural Language Processing (NLP) tasks, but require
substantial memory and computational resources. Binary quantization, which
compresses model weights from 16-bit Brain Float to 1-bit representations in
{-1, 1}, offers significant reductions in storage and inference costs. However,
such aggressive quantization often leads to notable performance degradation
compared to more conservative 4-bit quantization methods. In this research, we
propose a novel optimization objective tailored for binary quantization, along
with three algorithms designed to realize it effectively. Our method enhances
blocked quantization by dynamically identifying optimal unstructured
sub-matrices through adaptive grouping strategies. Experimental results
demonstrate that our approach achieves an average bit length of just 1.007
bits, while maintaining high model quality. Specifically, our quantized LLaMA
3.2 3B model attains a perplexity of 8.23, remarkably close to the original
7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.
Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ
in both performance and efficiency. The compression process is highly
efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights
on a single CPU core, with the entire process completing in under 100 minutes
and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan

</details>


### [109] [Discrete Functional Geometry of ReLU Networks via ReLU Transition Graphs](https://arxiv.org/abs/2509.03056)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文扩展了ReLU转移图(RTG)框架，提出了一个全面的图论模型来分析深度ReLU网络。该模型将线性激活区域表示为节点，通过ReLU激活翻转连接相邻区域，揭示了网络的离散几何结构和泛化特性。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解深度ReLU网络的功能行为和泛化机制，需要建立一个统一的框架来分析网络的离散几何结构。传统方法难以捕捉ReLU网络的区域划分和连接特性。

Method: 构建ReLU转移图(RTG)模型，其中节点代表线性激活区域，边连接仅差一个ReLU激活翻转的相邻区域。通过理论分析随机初始化RTG的扩展性、度分布和谱特性，并进行实证验证。

Result: 理论证明RTG在随机初始化时具有强扩展性、二项式度分布和紧密控制泛化的谱特性。实证显示区域熵在过参数化下饱和，谱间隙与泛化相关，相邻区域间的KL散度反映功能平滑性。

Conclusion: 该工作提供了一个通过离散功能几何视角分析ReLU网络的统一框架，为理解、诊断和改进泛化性能提供了新工具，揭示了网络结构特性与泛化能力之间的深刻联系。

Abstract: We extend the ReLU Transition Graph (RTG) framework into a comprehensive
graph-theoretic model for understanding deep ReLU networks. In this model, each
node represents a linear activation region, and edges connect regions that
differ by a single ReLU activation flip, forming a discrete geometric structure
over the network's functional behavior. We prove that RTGs at random
initialization exhibit strong expansion, binomial degree distributions, and
spectral properties that tightly govern generalization. These structural
insights enable new bounds on capacity via region entropy and on generalization
via spectral gap and edge-wise KL divergence. Empirically, we construct RTGs
for small networks, measure their smoothness and connectivity properties, and
validate theoretical predictions. Our results show that region entropy
saturates under overparameterization, spectral gap correlates with
generalization, and KL divergence across adjacent regions reflects functional
smoothness. This work provides a unified framework for analyzing ReLU networks
through the lens of discrete functional geometry, offering new tools to
understand, diagnose, and improve generalization.

</details>


### [110] [Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers](https://arxiv.org/abs/2509.03059)
*Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li*

Main category: cs.LG

TL;DR: Loong项目是一个开源框架，通过合成数据生成和代码验证来扩展RLVR在多领域的推理能力培训，包含LoongBench数据集和LoongEnv生成环境。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在非数学/编程领域的推理能力培训挑战，因为这些领域缺乏高质量可验证数据集且人工监督成本高。

Method: 构建了两个核心组件：LoongBench（8,729个人工审核的多领域示例，包含可执行代码）和LoongEnv（模块化合成数据生成环境），形成一个代理-环境循环来支持强化学习。

Result: 在广泛的开源和专有LLM上对LoongBench进行了基准测试，评估领域覆盖范围和性能瓶颈，并对合成数据进行了正确性、难度和多样性分析。

Conclusion: Loong项目为多领域推理能力培训提供了一个可扩展的解决方案，通过合成数据生成和代码验证机制有效解决了高质量数据缺乏的问题。

Abstract: Recent advances in Large Language Models (LLMs) have shown that their
reasoning capabilities can be significantly improved through Reinforcement
Learning with Verifiable Reward (RLVR), particularly in domains like
mathematics and programming, where ground-truth correctness can be
automatically evaluated. However, extending this success to other
reasoning-intensive domains remains challenging due to the scarcity of
high-quality, verifiable datasets and the high cost of human supervision. In
this work, we introduce the Loong Project: an open-source framework for
scalable synthetic data generation and verification across a diverse range of
reasoning-intensive domains. The framework consists of two key components: (1)
LoongBench, a curated seed dataset containing 8,729 human-vetted examples
across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired
with executable code and rich metadata; and (2) LoongEnv, a modular synthetic
data generation environment that supports multiple prompting strategies to
produce new question-answer-code triples. Together, these components form an
agent-environment loop that enables reinforcement learning, where an LLM-based
agent is rewarded for generating Chain-of-Thought (CoT) solutions that align
with code-executed answers. Empirically, we benchmark LoongBench on a broad
suite of both open-source and proprietary LLMs to evaluate domain coverage and
reveal performance bottlenecks. In addition, we conduct a comprehensive
analysis of synthetic data generated by LoongEnv, examining correctness,
difficulty, and diversity. Code and documentation are available at
https://github.com/camel-ai/loong.

</details>


### [111] [LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization](https://arxiv.org/abs/2509.03110)
*Yunfei Teng,Sixin Zhang*

Main category: cs.LG

TL;DR: LSAM是一种新颖的优化器，在保持SAM泛化优势的同时提供更高的分布式大批次训练效率


<details>
  <summary>Details</summary>
Motivation: SAM在分布式大批次训练中存在效率低下的问题，需要一种既能保持泛化优势又能提高训练效率的优化方法

Method: LSAM将SAM的对抗步骤与异步分布式采样策略相结合，生成异步分布式采样方案，产生平滑的锐度感知损失景观进行优化

Result: LSAM消除了同步瓶颈，加速了大批次收敛，相比数据并行SAM获得了更高的最终准确率

Conclusion: LSAM成功解决了SAM在分布式训练中的效率问题，同时保持了其泛化优势

Abstract: While Sharpness-Aware Minimization (SAM) improves generalization in deep
neural networks by minimizing both loss and sharpness, it suffers from
inefficiency in distributed large-batch training. We present Landscape-Smoothed
SAM (LSAM), a novel optimizer that preserves SAM's generalization advantages
while offering superior efficiency. LSAM integrates SAM's adversarial steps
with an asynchronous distributed sampling strategy, generating an asynchronous
distributed sampling scheme, producing a smoothed sharpness-aware loss
landscape for optimization. This design eliminates synchronization bottlenecks,
accelerates large-batch convergence, and delivers higher final accuracy
compared to data-parallel SAM.

</details>


### [112] [A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning](https://arxiv.org/abs/2509.03118)
*Hankang Gu,Yuli Zhang,Chengming Wang,Ruiyuan Jiang,Ziheng Qiao,Pengfei Fan,Dongyao Jia*

Main category: cs.LG

TL;DR: 提出了一种名为DHCP的分层深度强化学习模型，用于交通信号控制，通过高层和低层代理分别分配主要方向和转向运动的绿灯时间，解决了传统"选择相位"和"切换"策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统DRL交通信号控制方法存在两个问题："选择相位"策略可能导致不可预测的相位序列影响驾驶安全，"切换"策略可能导致相位分配不公平和效率低下。需要一种既能保持可预测性又能灵活分配时间的方法。

Method: 提出Deep Hierarchical Cycle Planner (DHCP)分层模型：高层代理根据整体交通状态分配南北和东西方向的总周期时间，低层代理在每个主要方向内进一步分配直行和左转运动的持续时间。

Result: 在真实和合成的道路网络及交通流数据集上测试，DHCP模型在所有数据集上都优于基线方法，取得了最佳性能。

Conclusion: DHCP通过分层时间分配方法，在保持相位序列可预测性的同时实现了更灵活的持续时间分配，有效解决了传统DRL交通信号控制方法的局限性。

Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic
signal control (TSC) due to its ability to learn adaptive policies from complex
traffic environments. Within DRL-based TSC methods, two primary control
paradigms are ``choose phase" and ``switch" strategies. Although the agent in
the choose phase paradigm selects the next active phase adaptively, this
paradigm may result in unexpected phase sequences for drivers, disrupting their
anticipation and potentially compromising safety at intersections. Meanwhile,
the switch paradigm allows the agent to decide whether to switch to the next
predefined phase or extend the current phase. While this structure maintains a
more predictable order, it can lead to unfair and inefficient phase
allocations, as certain movements may be extended disproportionately while
others are neglected. In this paper, we propose a DRL model, named Deep
Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle
duration hierarchically. A high-level agent first determines the split of the
total cycle time between the North-South (NS) and East-West (EW) directions
based on the overall traffic state. Then, a low-level agent further divides the
allocated duration within each major direction between straight and left-turn
movements, enabling more flexible durations for the two movements. We test our
model on both real and synthetic road networks, along with multiple sets of
real and synthetic traffic flows. Empirical results show our model achieves the
best performance over all datasets against baselines.

</details>


### [113] [A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy](https://arxiv.org/abs/2509.03137)
*Li Yi,Qian Yang*

Main category: cs.LG

TL;DR: 使用人工智能框架结合数值谱模拟和深度学习，实现了无标准的自动化多核素液体闪烁谱分析


<details>
  <summary>Details</summary>
Motivation: 解决传统TDCR谱技在多核素分析中遇到的自动化程度有限和依赖混合物标准的问题，特别是当标准物质不易获得时

Method: 采用Geant4模拟生成β谱数据，结合统计检测器响应采样，设计了专门的神经网络结构，训练覆盖不同核素混比和潜氧场景的数据集

Result: 模型在活度比例(均值绝对误差=0.009)、检测效率(均值绝对误差=0.002)和谱图重建(结构相似性指数=0.9998)方面都达到高精度

Conclusion: 这种AI驱动方法具有强大的潜力，能够在无标准物质或需要快速现场分析的场景中实现自动化、安全符合要求的多核素分析，具有稳健的泛化能力、实时处理能力和工程可行性

Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is
widely adopted as a standard method for radionuclide quantification because of
its inherent advantages such as high precision, self-calibrating capability,
and independence from radioactive reference sources. However, multiradionuclide
analysis via TDCR faces the challenges of limited automation and reliance on
mixture-specific standards, which may not be easily available. Here, we present
an Artificial Intelligence (AI) framework that combines numerical spectral
simulation and deep learning for standard-free automated analysis. $\beta$
spectra for model training were generated using Geant4 simulations coupled with
statistically modeled detector response sampling. A tailored neural network
architecture, trained on this dataset covering various nuclei mix ratio and
quenching scenarios, enables autonomous resolution of individual radionuclide
activities and detecting efficiency through end-to-end learning paradigms. The
model delivers consistent high accuracy across tasks: activity proportions
(mean absolute error = 0.009), detection efficiencies (mean absolute error =
0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),
validating its physical plausibility for quenched $\beta$ spectroscopy. This
AI-driven methodology exhibits significant potential for automated
safety-compliant multiradionuclide analysis with robust generalization,
real-time processing capabilities, and engineering feasibility, particularly in
scenarios where reference materials are unavailable or rapid field analysis is
required.

</details>


### [114] [Rashomon in the Streets: Explanation Ambiguity in Scene Understanding](https://arxiv.org/abs/2509.03169)
*Helge Spieker,Jørn Eirik Betten,Arnaud Gotlieb,Nadjib Lazaar,Nassim Belmecheri*

Main category: cs.LG

TL;DR: 这篇论文首次实证量化了自动驾驶场景中可解释AI的Rashomon效应，发现不同模型对同一预测提供异穿解释，解释模糊性是问题的本质属性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等安全关键应用中，可解释AI的可靠性面临Rashomon效应的挑战，多个准确性相似的模型可能会提供分散的解释。

Method: 使用谨性可解释图(QXGs)作为符号场景表示，训练两种不同模型类别：可解释的基于对的梯度提升模型和复杂的图神经网络(GNNs)，并使用特征归因方法测量解释一致性。

Result: 结果显示了显著的解释不一致性，同一模型类内和不同模型类间都存在解释差异。

Conclusion: 解释模糊性是自动驾驶行为预测问题的内在本质属性，而不仅仅是模型建模的人工效应，这对可解释AI的可靠性提出了重要挑战。

Abstract: Explainable AI (XAI) is essential for validating and trusting models in
safety-critical applications like autonomous driving. However, the reliability
of XAI is challenged by the Rashomon effect, where multiple, equally accurate
models can offer divergent explanations for the same prediction. This paper
provides the first empirical quantification of this effect for the task of
action prediction in real-world driving scenes. Using Qualitative Explainable
Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two
distinct model classes: interpretable, pair-based gradient boosting models and
complex, graph-based Graph Neural Networks (GNNs). Using feature attribution
methods, we measure the agreement of explanations both within and between these
classes. Our results reveal significant explanation disagreement. Our findings
suggest that explanation ambiguity is an inherent property of the problem, not
just a modeling artifact.

</details>


### [115] [Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns](https://arxiv.org/abs/2509.03176)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 本文提出了一个无阈值评估框架(AUC-IoU)来解决归因方法评估中的阈值选择偏差问题，在皮肤影像数据上验证了其相比单阈值评估的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前归因方法评估存在阈值选择偏差，单一阈值选择会显著改变方法排名并削弱结论可靠性，需要开发无阈值的评估框架。

Method: 提出基于AUC-IoU的无阈值评估框架，通过计算交并比曲线下面积来捕捉全阈值范围内的归因质量，避免了单一阈值选择带来的偏差。

Result: 在皮肤影像数据上评估7种归因方法，发现单阈值指标产生矛盾结果，而无阈值评估能可靠区分方法性能。XRAI相比LIME提升31%，相比原始积分梯度提升204%。

Conclusion: 该无阈值框架消除了评估伪影，为医学影像及其他领域的归因方法提供了理论洞见和稳健比较的实践指导，建立了方法学标准。

Abstract: Attribution methods explain neural network predictions by identifying
influential input features, but their evaluation suffers from threshold
selection bias that can reverse method rankings and undermine conclusions.
Current protocols binarize attribution maps at single thresholds, where
threshold choice alone can alter rankings by over 200 percentage points. We
address this flaw with a threshold-free framework that computes Area Under the
Curve for Intersection over Union (AUC-IoU), capturing attribution quality
across the full threshold spectrum. Evaluating seven attribution methods on
dermatological imaging, we show single-threshold metrics yield contradictory
results, while threshold-free evaluation provides reliable differentiation.
XRAI achieves 31% improvement over LIME and 204% over vanilla Integrated
Gradients, with size-stratified analysis revealing performance variations up to
269% across lesion scales. These findings establish methodological standards
that eliminate evaluation artifacts and enable evidence-based method selection.
The threshold-free framework provides both theoretical insight into attribution
behavior and practical guidance for robust comparison in medical imaging and
beyond.

</details>


### [116] [Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025](https://arxiv.org/abs/2509.03191)
*Taiga Saito,Yu Otake,Stephen Wu*

Main category: cs.LG

TL;DR: TabPFN（基于transformer的表格数据基础模型）在岩土工程场地特征化问题中表现出色，在零训练、少样本的上下文学习设置下，相比传统分层贝叶斯模型具有更高的预测精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在岩土工程领域的应用潜力，解决传统方法在预测精度和计算效率方面的局限性，特别是在空间剪切强度预测和机械参数缺失值填补任务中。

Method: 使用TabPFN模型，采用零训练、少样本的上下文学习方法，无需超参数调优，并利用大型间接数据库（BID）提供额外上下文信息。

Result: 在基准问题1（空间剪切强度预测）中，TabPFN在预测精度上优于分层贝叶斯模型，且运行时间快一个数量级；在基准问题2（机械参数缺失值填补）中，TabPFN对所有目标参数都实现了更低的RMSE，但累积计算成本较高。

Conclusion: 这是表格基础模型在岩土工程建模中的首次成功应用，标志着概率场地特征化方法的潜在范式转变，展示了基础模型在该领域的巨大潜力。

Abstract: This paper presents a novel application of the Tabular Prior-Data Fitted
Network (TabPFN) - a transformer-based foundation model for tabular data - to
geotechnical site characterization problems defined in the GEOAI benchmark
BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the
spatial variation of undrained shear strength (su) across borehole depth
profiles, and (2) imputing missing mechanical parameters in a dense-site
dataset. We apply TabPFN in a zero-training, few-shot, in-context learning
setting - without hyper-parameter tuning - and provide it with additional
context from the big indirect database (BID). The study demonstrates that
TabPFN, as a general-purpose foundation model, achieved superior accuracy and
well-calibrated predictive distributions compared to a conventional
hierarchical Bayesian model (HBM) baseline, while also offering significant
gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),
TabPFN outperformed the HBM in prediction accuracy and delivered an
order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical
parameter imputation), TabPFN likewise achieved lower RMSE for all target
parameters with well-quantified uncertainties, though its cumulative
computation cost was higher than HBM's due to its one-variable-at-a-time
inference. These results mark the first successful use of a tabular foundation
model in geotechnical modeling, suggesting a potential paradigm shift in
probabilistic site characterization.

</details>


### [117] [Exploring the Design Space of Fair Tree Learning Algorithms](https://arxiv.org/abs/2509.03204)
*Kiara Stempel,Mattia Cerrato,Stefan Kramer*

Main category: cs.LG

TL;DR: 这篇论文探讨了公平性决策树的三种设计方案，重点研究了前两种未被充分研究的方法，并在多个数据集上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 目前公平性决策树研究主要集中在前两种设计方案，而第三种和第二种的非贪心变体很少被研究。论文的动机是填补这一研究空白，展开公平性决策树的设计空间。

Method: 提出了两种新的公平性决策树构建方法：(1) 使用关于y的目标函数和关于s的约束条件；(2) 分别构建两棵树T_y和T_s，独立使用y和s的信息。并在多个数据集上进行实验分析。

Result: 实验结果展示了新提出的两种方法在保持预测性能的同时确保公平性的效果，为公平性决策树提供了更多的设计选择。

Conclusion: 论文扩展了公平性决策树的设计空间，实验证明了新提方法的可行性和效果，为该领域的研究提供了新的视角和方法。

Abstract: Decision trees have been studied extensively in the context of fairness,
aiming to maximize prediction performance while ensuring non-discrimination
against different groups. Techniques in this space usually focus on imposing
constraints at training time, constraining the search space so that solutions
which display unacceptable values of relevant metrics are not considered,
discarded, or discouraged. If we assume one target variable y and one sensitive
attribute s, the design space of tree learning algorithms can be spanned as
follows: (i) One can have one tree T that is built using an objective function
that is a function of y, s, and T. For instance, one can build a tree based on
the weighted information gain regarding y (maximizing) and s (minimizing). (ii)
The second option is to have one tree model T that uses an objective function
in y and T and a constraint on s and T. Here, s is no longer part of the
objective, but part of a constraint. This can be achieved greedily by aborting
a further split as soon as the condition that optimizes the objective in y
fails to satisfy the constraint on s. A simple way to explore other splits is
to backtrack during tree construction once a fairness constraint is violated.
(iii) The third option is to have two trees T_y and T_s, one for y and one for
s, such that the tree structure for y and s does not have to be shared. In this
way, information regarding y and regarding s can be used independently, without
having to constrain the choices in tree construction by the mutual information
between the two variables. Quite surprisingly, of the three options, only the
first one and the greedy variant of the second have been studied in the
literature so far. In this paper, we introduce the above two additional options
from that design space and characterize them experimentally on multiple
datasets.

</details>


### [118] [Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback](https://arxiv.org/abs/2509.03206)
*Zeqiang Zhang,Fabian Wurzberger,Gerrit Schmid,Sebastian Gottwald,Daniel A. Braun*

Main category: cs.LG

TL;DR: 提出了一种将对比学习整合到GCSL框架中的新模型，从成功和失败中学习，克服智能体初始偏见，实现更好的探索性和性能表现


<details>
  <summary>Details</summary>
Motivation: 解决GCSL框架的两个主要局限：1)仅从自身经验学习会加剧智能体固有偏见；2)重标注策略只关注成功结果，无法从错误中学习

Method: 在GCSL框架中集成对比学习原理，使智能体能够同时从成功和失败的经验中学习

Result: 算法克服了智能体初始偏见的限制，促进了更多的探索行为，有助于识别和采用有效策略，在各种挑战性环境中表现出优越性能

Conclusion: 通过整合对比学习到GCSL框架，成功解决了自模仿学习的局限性，实现了更全面的学习效果和更好的性能表现

Abstract: Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

</details>


### [119] [TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2509.03234)
*Yuxuan Gu,Wuyang Zhou,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: TeRA是一种新颖的参数高效微调方法，通过张量网络实现高秩权重更新，同时保持向量化方法的参数效率


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA风格适配器在高表达性高秩适配器和极致参数效率向量方法之间的权衡问题

Method: 使用Tucker-like张量网络参数化权重更新矩阵，冻结大型随机初始化因子并在层间共享，仅训练小的层特定缩放向量

Result: TeRA匹配甚至超越高秩适配器性能，同时所需可训练参数数量与向量化方法相当

Conclusion: TeRA成功实现了高秩权重更新与参数效率的平衡，理论和实验验证了其有效性

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have significantly reduced the number of trainable parameters needed in
fine-tuning large language models (LLMs). Subsequent developments of LoRA-style
adapters have diverged into two main directions: (1) enhancing model
expressivity with high-rank adapters, and (2) pushing for further parameter
reduction, as exemplified by vector-based methods. However, these approaches
present a trade-off, as achieving the expressivity of high-rank weight updates
typically comes at the cost of sacrificing the extreme parameter efficiency
offered by vector-based techniques. To address this issue, we propose a
vector-based random \underline{\textbf{Te}}nsor network for
high-\underline{\textbf{R}}ank \underline{\textbf{A}}daptation (TeRA), a novel
PEFT method that achieves high-rank weight updates while retaining the
parameter efficiency of vector-based PEFT adapters. This is achieved by
parameterizing the tensorized weight update matrix as a Tucker-like tensor
network (TN), in which large randomly initialized factors are frozen and shared
across layers, while only small layer-specific scaling vectors, formed by
entries in diagonal factor matrices, are trained. This design effectively
decouples the rank of the weight update matrix from the number of trainable
parameters. Comprehensive experiments demonstrate that TeRA matches or even
outperforms high-rank adapters, while requiring a trainable parameter count
similar to vector-based methods. Theoretical analysis and ablation studies
further validate the effectiveness of our approach.

</details>


### [120] [Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric](https://arxiv.org/abs/2509.03240)
*Harald Vilhelm Skat-Rørdam,Sneha Das,Kathrine Sofie Rasmussen,Nicole Nadine Lønfeldt,Line Clemmensen*

Main category: cs.LG

TL;DR: 本文提出了一种基于时间容差的窗口F1指标(F1$_w$)，用于评估时间序列中的事件检测性能，解决了传统指标在真实世界不平衡数据集中的评估偏差问题。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备压力监测等应用中，地面真值通常标注为单点事件，但实际现象是渐进的、时间上分散的。标准指标如F1和点调整F1(F1$_{pa}$)在这种真实世界不平衡数据集中往往错误地表示模型性能。

Method: 引入窗口F1指标(F1$_w$)，该指标包含时间容差，能够在精确对齐不切实际的情况下对事件检测进行更稳健的评估。在三个生理数据集(两个野外数据集ADARP、Wrist Angel和一个实验数据集ROAD)上进行实证分析。

Result: F1$_w$揭示了传统指标无法发现的模型性能模式，其窗口大小可根据领域知识进行调整以避免高估。使用TimesFM的预测结果显示，只有时间容差指标在两个野外用例中显示出相对于随机和零基线的统计显著改进。

Conclusion: 这项工作解决了时间序列评估中的关键差距，并为医疗保健应用提供了实用指导，其中对时间精度的要求因上下文而异。

Abstract: Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.

</details>


### [121] [Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network](https://arxiv.org/abs/2509.03241)
*Pujitha Mamillapalli,Yoghitha Ramamoorthi,Abhinav Kumar,Tomoki Murakami,Tomoaki Ogawa,Yasushi Takatori*

Main category: cs.LG

TL;DR: 提出基于神经网络的可重构智能表面(RIS)元素分配方案，通过五层全连接神经网络和预处理技术降低计算复杂度，在提高系统吞吐量6.8%的同时增强可扩展性


<details>
  <summary>Details</summary>
Motivation: 传统迭代优化方法在RIS元素数量增加时计算复杂度呈指数增长，且难以生成监督学习训练标签，需要更高效的解决方案

Method: 使用五层全连接神经网络结合预处理技术，显著降低输入维度，减少计算复杂度并提高可扩展性

Result: 相比现有RIS元素分配方案，计算开销降低的同时系统吞吐量提高6.8%，性能更好且计算复杂度更低

Conclusion: 提出的神经网络解决方案在保持高性能的同时显著降低了计算复杂度，比迭代优化算法具有更好的可扩展性

Abstract: The increasing demand for high data rates and seamless connectivity in
wireless systems has sparked significant interest in reconfigurable intelligent
surfaces (RIS) and artificial intelligence-based wireless applications. RIS
typically comprises passive reflective antenna elements that control the
wireless propagation environment by adequately tuning the phase of the
reflective elements. The allocation of RIS elements to multipleuser equipment
(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a
joint optimization problem that optimizes the RIS phase configuration and
resource allocation under an $\alpha$-fair scheduling framework and propose an
efficient way of allocating RIS elements. Conventional iterative optimization
methods, however, suffer from exponentially increasing computational complexity
as the number of RIS elements increases and also complicate the generation of
training labels for supervised learning. To overcome these challenges, we
propose a five-layer fully connected neural network (FNN) combined with a
preprocessing technique to significantly reduce input dimensionality, lower
computational complexity, and enhance scalability. The simulation results show
that our proposed NN-based solution reduces computational overhead while
significantly improving system throughput by 6.8% compared to existing RIS
element allocation schemes. Furthermore, the proposed system achieves better
performance while reducing computational complexity, making it significantly
more scalable than the iterative optimization algorithms.

</details>


### [122] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: TopoMap是一种黑盒、模型无关的深度学习方法，通过降维和聚类创建输入特征空间的拓扑图，自动选择最优配置来区分不同故障特征，在突变测试中显著优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 现有DL测试方法主要关注特定故障特征而忽略其他特征区域，需要一种能够全面映射输入特征空间并有效分组故障输入的方法。

Method: 使用降维获得输入嵌入，然后进行聚类，通过DNN近似人类评估者自动选择最优的嵌入/聚类配置来生成拓扑图。

Result: TopoMap生成的拓扑图包含可区分且有意义的区域，在突变测试中比随机选择平均高出35%（可杀死突变体）和61%（不可杀死突变体）。

Conclusion: TopoMap提供了一种有效的方法来创建输入特征空间的拓扑图，能够更好地理解和测试DL模型的故障行为，在突变分析中表现出色。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [123] [FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization](https://arxiv.org/abs/2509.03244)
*Yiming Yao,Fei Liu,Liang Zhao,Xi Lin,Qingfu Zhang*

Main category: cs.LG

TL;DR: FoMEMO是一种基于基础模型的昂贵多目标优化新范式，通过大规模合成数据预训练实现对新问题的快速适应，无需重新训练模型


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要为每个新问题重新构建高斯过程代理模型或依赖大量领域实验预训练的问题，提高样本效率和泛化能力

Method: 建立基于任意领域轨迹和用户偏好的基础模型，通过预测偏好聚合后验实现快速上下文优化，使用数亿合成数据进行预训练

Result: 在合成基准和实际应用中表现出优越的泛化能力和竞争性性能

Conclusion: FoMEMO为昂贵多目标优化提供了新的解决方案，通过合成数据预训练的基础模型能够快速适应未知问题，具有很好的实用性

Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in
many real-world scenarios, where sample-efficiency is vital due to the limited
evaluations to recover the true Pareto front for decision making. Existing
works either involve rebuilding Gaussian process surrogates from scratch for
each objective in each new problem encountered, or rely on extensive past
domain experiments for pre-training deep learning models, making them hard to
generalize and impractical to cope with various emerging applications in the
real world. To address this issue, we propose a new paradigm named FoMEMO
(Foundation Models for Expensive Multi-objective Optimization), which enables
the establishment of a foundation model conditioned on any domain trajectory
and user preference, and facilitates fast in-context optimization based on the
predicted preference-wise aggregation posteriors. Rather than accessing
extensive domain experiments in the real world, we demonstrate that
pre-training the foundation model with a diverse set of hundreds of millions of
synthetic data can lead to superior adaptability to unknown problems, without
necessitating any subsequent model training or updates in the optimization
process. We evaluate our method across a variety of synthetic benchmarks and
real-word applications, and demonstrate its superior generality and competitive
performance compared to existing methods.

</details>


### [124] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 提出了一种称为"结构转移"的新颖演算方法，用于实现不同表示系统之间的表示转换，确保源表示和目标表示满足任何指定关系（如语义等价）。


<details>
  <summary>Details</summary>
Motivation: 表示选择对有效沟通和推理至关重要，但缺乏能够驱动表示转换和选择的表示系统无关技术。

Method: 利用模式编码表示系统知识，通过结构转移演算规则在源表示和目标表示之间建立关系，基于表示系统理论中的构造空间概念进行形式化。

Result: 结构转移是一个系统无关的演算方法，能够识别各种实际场景中的替代表示，适用于形式语言、几何图形、图表和非正式符号等多种表示系统。

Conclusion: 结构转移为解决表示转换问题提供了一个通用框架，能够在保持所需关系的前提下实现跨表示系统的表示生成。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [125] [HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling](https://arxiv.org/abs/2509.03260)
*Minjung Park,Gyuyeon Na,Soyoun Kim,Sunyoung Moon,HyeonJeong Cha,Sangmi Chai*

Main category: cs.LG

TL;DR: HyPV-LEAD是一个用于加密货币异常交易早期检测的框架，通过窗口-视野建模、峰谷采样和双曲嵌入技术，在比特币交易数据上实现了0.9624的PR-AUC性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币异常交易（如混币服务、欺诈转账、拉盘砸盘）对金融完整性构成日益增长的风险，但现有方法主要是事后检测，缺乏预防价值，且面临类别不平衡、时间波动性和复杂网络依赖等挑战。

Method: HyPV-LEAD框架包含三个创新：1）窗口-视野建模确保可操作的提前预警；2）峰谷采样缓解类别不平衡同时保持时间连续性；3）双曲嵌入捕捉区块链交易网络的层次和无标度特性。

Result: 在大规模比特币交易数据上的实证评估显示，HyPV-LEAD持续优于最先进的基线方法，实现了0.9624的PR-AUC，在精确率和召回率方面都有显著提升。消融研究证实各组件都提供互补效益。

Conclusion: HyPV-LEAD将异常检测从被动分类转变为主动预警，为实时风险管理、反洗钱合规和动态区块链环境中的金融安全建立了坚实基础。

Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent
transfers, and pump-and-dump operations -- pose escalating risks to financial
integrity but remain notoriously difficult to detect due to class imbalance,
temporal volatility, and complex network dependencies. Existing approaches are
predominantly model-centric and post hoc, flagging anomalies only after they
occur and thus offering limited preventive value. This paper introduces
HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a
data-driven early-warning framework that explicitly incorporates lead time into
anomaly detection. Unlike prior methods, HyPV-LEAD integrates three
innovations: (1) window-horizon modeling to guarantee actionable lead-time
alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while
preserving temporal continuity, and (3) hyperbolic embedding to capture the
hierarchical and scale-free properties of blockchain transaction networks.
Empirical evaluation on large-scale Bitcoin transaction data demonstrates that
HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a
PR-AUC of 0.9624 with significant gains in precision and recall. Ablation
studies further confirm that each component - PV sampling, hyperbolic
embedding, and structural-temporal modeling - provides complementary benefits,
with the full framework delivering the highest performance. By shifting anomaly
detection from reactive classification to proactive early-warning, HyPV-LEAD
establishes a robust foundation for real-time risk management, anti-money
laundering (AML) compliance, and financial security in dynamic blockchain
environments.

</details>


### [126] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 论文分析了MLPerf Training v4.1在BERT、Llama2 LoRA、RetinaNet和Stable Diffusion四个工作负载上的性能数据，发现存在优化性能、GPU使用率和效率之间关系的配置方案，找到了减少训练时间同时最大化效率的平衡点。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型训练面临效率挑战，虽然大量使用GPU可以显著加速训练，但对效率有负面影响，需要找到性能与效率的最佳平衡点。

Method: 对MLPerf Training v4.1基准测试中四个工作负载（BERT、Llama2 LoRA、RetinaNet、Stable Diffusion）的报告时间进行详细分析，研究不同配置下的性能表现。

Result: 发现了可以优化性能、GPU使用率和效率之间关系的配置方案，确定了能够减少训练时间同时最大化效率的平衡点。

Conclusion: 通过合理的配置选择，可以在大规模深度学习模型训练中实现性能与效率的最佳平衡，为实际部署提供指导。

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [127] [Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning](https://arxiv.org/abs/2509.03316)
*Fatemeh Azad,Zoran Bosnić,Matjaž Kukar*

Main category: cs.LG

TL;DR: 本文提出了一种新的元填补方法Meta-Imputation Balanced (MIB)，通过综合多个基础填补器的输出来更准确预测缺失值。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的缺失数据问题会降低模型性能和可靠性，特别是在生物信息学和临床机器学习领域。尽管有多种填补方法，但没有一种方法能在所有数据集和缺失机制下都表现良好。

Method: 提出Meta-Imputation Balanced (MIB)方法，通过在已知真实值的合成遮盖数据上训练，让系统学习根据每种方法的行为预测最适合的填补值。

Result: 该方法展示了集成学习在填补中的潜力，能够提高填补的准确性。

Conclusion: 这为实际机器学习系统中更稳健、模块化和可解释的预处理流程开启了新路径。

Abstract: Missing data represents a fundamental challenge in machine learning
applications, often reducing model performance and reliability. This problem is
particularly acute in fields like bioinformatics and clinical machine learning,
where datasets are frequently incomplete due to the nature of both data
generation and data collection. While numerous imputation methods exist, from
simple statistical techniques to advanced deep learning models, no single
method consistently performs well across diverse datasets and missingness
mechanisms. This paper proposes a novel Meta-Imputation approach that learns to
combine the outputs of multiple base imputers to predict missing values more
accurately. By training the proposed method called Meta-Imputation Balanced
(MIB) on synthetically masked data with known ground truth, the system learns
to predict the most suitable imputed value based on the behavior of each
method. Our work highlights the potential of ensemble learning in imputation
and paves the way for more robust, modular, and interpretable preprocessing
pipelines in real-world machine learning systems.

</details>


### [128] [EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms](https://arxiv.org/abs/2509.03335)
*Leizhen Wang,Peibo Duan,Hao Wang,Yue Wang,Jian Xu,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: EvolveSignal使用大型语言模型自动发现新的交通信号控制算法，通过程序合成和进化搜索优化，在交通仿真中表现优于传统Webster方法，平均延迟减少20.1%，停车次数减少47.1%。


<details>
  <summary>Details</summary>
Motivation: 固定时间交通信号控制虽然成本低、稳定且可解释，但依赖手工公式和人工调整，在异构或拥堵条件下效果不佳，需要自动化算法设计方法。

Method: 将问题表述为程序合成，候选算法表示为具有固定输入输出结构的Python函数，通过外部评估（交通仿真器）和进化搜索进行迭代优化。

Result: 在信号化交叉口实验中，发现的算法优于Webster基线，平均延迟减少20.1%，平均停车次数减少47.1%。消融和增量分析显示调整周期长度边界、纳入右转需求和重新分配绿灯时间等修改具有实际意义。

Conclusion: 这项工作通过利用AI进行交通信号控制算法设计，开辟了新的研究方向，将程序合成与交通工程相结合。

Abstract: In traffic engineering, the fixed-time traffic signal control remains widely
used for its low cost, stability, and interpretability. However, its design
depends on hand-crafted formulas (e.g., Webster) and manual re-timing by
engineers to adapt to demand changes, which is labor-intensive and often yields
suboptimal results under heterogeneous or congested conditions. This paper
introduces the EvolveSignal, a large language models (LLMs) powered coding
agent to automatically discover new traffic signal control algorithms. We
formulate the problem as program synthesis, where candidate algorithms are
represented as Python functions with fixed input-output structures, and
iteratively optimized through external evaluations (e.g., a traffic simulator)
and evolutionary search. Experiments on a signalized intersection demonstrate
that the discovered algorithms outperform Webster's baseline, reducing average
delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and
incremental analyses reveal that EvolveSignal modifications-such as adjusting
cycle length bounds, incorporating right-turn demand, and rescaling green
allocations-can offer practically meaningful insights for traffic engineers.
This work opens a new research direction by leveraging AI for algorithm design
in traffic signal control, bridging program synthesis with transportation
engineering.

</details>


### [129] [Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems](https://arxiv.org/abs/2509.03340)
*Fleur Hendriks,Ondřej Rokoš,Martin Doškář,Marc G. D. Geers,Vlado Menkovski*

Main category: cs.LG

TL;DR: 提出基于流匹配的生成框架，用于建模分岔现象中的多模态概率分布，通过等变建模保持系统对称性，在多种系统中验证了优于非概率和变分方法的性能


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统中的分岔现象会产生多个共存稳定解，但确定性机器学习模型难以捕捉这种多重性，往往对解进行平均而无法表示低对称性结果

Method: 基于流匹配的生成框架，引入对称匹配策略，在群操作下对齐预测和目标输出，通过等变建模保持系统对称性

Result: 在从玩具模型到复杂物理问题（如屈曲梁和Allen-Cahn方程）的一系列系统中验证，流匹配方法在捕捉多模态分布和对称破缺分岔方面显著优于非概率和变分方法

Conclusion: 为高维系统中的多稳态建模提供了一个原则性且可扩展的解决方案，能够直接采样多个有效解同时保持系统对称性

Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple
coexisting stable solutions, particularly in the presence of symmetry breaking.
Deterministic machine learning models struggle to capture this multiplicity,
averaging over solutions and failing to represent lower-symmetry outcomes. In
this work, we propose a generative framework based on flow matching to model
the full probability distribution over bifurcation outcomes. Our method enables
direct sampling of multiple valid solutions while preserving system symmetries
through equivariant modeling. We introduce a symmetric matching strategy that
aligns predicted and target outputs under group actions, allowing accurate
learning in equivariant settings. We validate our approach on a range of
systems, from toy models to complex physical problems such as buckling beams
and the Allen-Cahn equation. Our results demonstrate that flow matching
significantly outperforms non-probabilistic and variational methods in
capturing multimodal distributions and symmetry-breaking bifurcations, offering
a principled and scalable solution for modeling multistability in
high-dimensional systems.

</details>


### [130] [On the MIA Vulnerability Gap Between Private GANs and Diffusion Models](https://arxiv.org/abs/2509.03341)
*Ilana Sebag,Jean-Yves Franceschi,Alain Rakotomamonjy,Alexandre Allauzen,Jamal Atif*

Main category: cs.LG

TL;DR: 本文首次对差分隐私生成模型（GANs和扩散模型）的隐私风险进行了统一的理论和实证分析，发现GANs在抵抗成员推理攻击方面具有结构优势。


<details>
  <summary>Details</summary>
Motivation: GANs和扩散模型虽然都可以在差分隐私下训练以保护敏感数据，但它们对成员推理攻击（MIA）的敏感性尚未得到充分理解，而MIA是数据机密性的主要威胁。

Method: 通过基于稳定性的理论分析比较模型对数据扰动的敏感性，并使用标准化的MIA流程进行全面的实证研究，评估不同数据集和隐私预算下的隐私泄露情况。

Result: 研究结果显示GANs表现出显著较低的隐私泄露风险，即使在强差分隐私机制下，GANs也比扩散模型具有明显的隐私鲁棒性优势。

Conclusion: 模型类型本身对隐私泄露具有关键影响，GANs在抵抗成员推理攻击方面具有结构优势，这为选择隐私保护的生成模型提供了重要指导。

Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as
leading approaches for high-quality image synthesis. While both can be trained
under differential privacy (DP) to protect sensitive data, their sensitivity to
membership inference attacks (MIAs), a key threat to data confidentiality,
remains poorly understood. In this work, we present the first unified
theoretical and empirical analysis of the privacy risks faced by differentially
private generative models. We begin by showing, through a stability-based
analysis, that GANs exhibit fundamentally lower sensitivity to data
perturbations than diffusion models, suggesting a structural advantage in
resisting MIAs. We then validate this insight with a comprehensive empirical
study using a standardized MIA pipeline to evaluate privacy leakage across
datasets and privacy budgets. Our results consistently reveal a marked privacy
robustness gap in favor of GANs, even in strong DP regimes, highlighting that
model type alone can critically shape privacy leakage.

</details>


### [131] [epiGPTope: A machine learning-based epitope generator and classifier](https://arxiv.org/abs/2509.03351)
*Natalia Flechas Manrique,Alberto Martínez,Elena López-Martínez,Luc Andrea,Román Orus,Aitor Manteca,Aitziber L. Cortajarena,Llorenç Espinosa-Portalés*

Main category: cs.LG

TL;DR: 提出了epiGPTope模型，这是一个基于蛋白质数据预训练并在线性表位上微调的大型语言模型，能够直接生成新型表位样序列，结合统计分类器预测表位来源，用于表位发现和合成表位库构建。


<details>
  <summary>Details</summary>
Motivation: 表位是免疫疗法、疫苗和诊断开发的核心，但合成表位库的理性设计面临组合序列空间巨大的挑战（20^n种组合），即使使用高通量实验技术也难以筛选测试。

Method: 开发epiGPTope语言模型，在蛋白质数据上预训练后在线性表位数据上微调，直接生成表位样序列；训练统计分类器预测表位的细菌或病毒来源；仅使用线性表位的一级氨基酸序列，无需几何框架或手工特征。

Result: 模型生成的序列具有与已知表位相似的统计特性；生成方法可用于准备表位候选序列库；分类器能有效预测表位来源，缩小候选库范围。

Conclusion: 生成模型与预测模型的结合有助于表位发现，该方法可加速合成表位的生成和筛选，降低成本，在生物技术开发中具有重要应用价值。

Abstract: Epitopes are short antigenic peptide sequences which are recognized by
antibodies or immune cell receptors. These are central to the development of
immunotherapies, vaccines, and diagnostics. However, the rational design of
synthetic epitope libraries is challenging due to the large combinatorial
sequence space, $20^n$ combinations for linear epitopes of n amino acids,
making screening and testing unfeasible, even with high throughput experimental
techniques. In this study, we present a large language model, epiGPTope,
pre-trained on protein data and specifically fine-tuned on linear epitopes,
which for the first time can directly generate novel epitope-like sequences,
which are found to possess statistical properties analogous to the ones of
known epitopes. This generative approach can be used to prepare libraries of
epitope candidate sequences. We further train statistical classifiers to
predict whether an epitope sequence is of bacterial or viral origin, thus
narrowing the candidate library and increasing the likelihood of identifying
specific epitopes. We propose that such combination of generative and
predictive models can be of assistance in epitope discovery. The approach uses
only primary amino acid sequences of linear epitopes, bypassing the need for a
geometric framework or hand-crafted features of the sequences. By developing a
method to create biologically feasible sequences, we anticipate faster and more
cost-effective generation and screening of synthetic epitopes, with relevant
applications in the development of new biotechnologies.

</details>


### [132] [Fair Resource Allocation for Fleet Intelligence](https://arxiv.org/abs/2509.03353)
*Oguzhan Baser,Kaan Kale,Po-han Li,Sandeep Chinchali*

Main category: cs.LG

TL;DR: Fair-Synergy是一个开源算法框架，通过利用智能体准确性与系统资源之间的凹关系，在多智能体系统中实现公平资源分配，在推理和学习任务中分别比基准方法提升25%和11%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统资源分配方法往往忽视智能体不同的计算能力和复杂操作环境，导致资源分配效率低下且不公平，需要一种能够确保多智能体智能系统中公平资源分配的新方法。

Method: 开发了Fair-Synergy算法框架，扩展传统分配方法以涵盖由模型参数、训练数据量和任务复杂性定义的多维机器学习效用景观，利用智能体准确性与资源之间的凹关系进行公平分配。

Result: 在BERT、VGG16、MobileNet、ResNets等先进视觉和语言模型上，使用MNIST、CIFAR-10、CIFAR-100、BDD和GLUE数据集进行评估，Fair-Synergy在多智能体推理中比标准基准提升25%，在多智能体学习中提升11%。

Conclusion: Fair-Synergy有效解决了多智能体系统中的公平资源分配问题，提供了对公平性水平如何影响最弱势、最优势和平均智能体的深入见解，为公平的群体智能提供了实用框架。

Abstract: Resource allocation is crucial for the performance optimization of
cloud-assisted multi-agent intelligence. Traditional methods often overlook
agents' diverse computational capabilities and complex operating environments,
leading to inefficient and unfair resource distribution. To address this, we
open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave
relationship between the agents' accuracy and the system resources to ensure
fair resource allocation across fleet intelligence. We extend traditional
allocation approaches to encompass a multidimensional machine learning utility
landscape defined by model parameters, training data volume, and task
complexity. We evaluate Fair-Synergy with advanced vision and language models
such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,
CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy
outperforms standard benchmarks by up to 25% in multi-agent inference and 11%
in multi-agent learning settings. Also, we explore how the level of fairness
affects the least advantaged, most advantaged, and average agents, providing
insights for equitable fleet intelligence.

</details>


### [133] [Some patterns of sleep quality and Daylight Saving Time across countries: a predictive and exploratory analysis](https://arxiv.org/abs/2509.03358)
*Bhanu Sharma,Eugene Pinsky*

Main category: cs.LG

TL;DR: 研究分析61个国家的平均睡眠时长，发现夏令时制对睡眠的影响受纬度调节：低纬度区夏令时国家睡眠更短，高纬度区更长


<details>
  <summary>Details</summary>
Motivation: 探索夏令时制对不同地理位置国家睡眠时长的影响

Method: 对61个国家的睡眠数据进行统计相关分析，按夏令时实施情况分组，并考虑纬度因素

Result: 夏令时国家平均睡眠时长更长，但纬度调节效应显著：低纬度区夏令时国家睡眠更短，高纬度区更长

Conclusion: 夏令时对睡眠的影响受到地理位置的显著调节，需要根据纬度考虑其效果

Abstract: In this study we analyzed average sleep durations across 61 countries to
examine the impact of Daylight Saving Time (DST) practices. Key metrics
influencing sleep were identified, and statistical correlation analysis was
applied to explore relationships among these factors. Countries were grouped
based on DST observance, and visualizations compared sleep patterns between DST
and non-DST regions. Results show that, on average, countries observing DST
tend to report longer sleep durations than those that do not. A more detailed
pattern emerged when accounting for latitude: at lower latitudes, DST-observing
countries reported shorter sleep durations compared to non-DST countries, while
at higher latitudes, DST-observing countries reported longer average sleep
durations. These findings suggest that the influence of DST on sleep may be
moderated by geographical location.

</details>


### [134] [The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex](https://arxiv.org/abs/2509.03365)
*Paul-Gauthier Noé,Andreas Nautsch,Driss Matrouf,Pierre-Michel Bousquet,Jean-François Bonastre*

Main category: cs.LG

TL;DR: 该论文将似然函数校准从二元假设扩展到多元假设，使用Aitchison几何和等距对数比变换，提出了多类情况下的校准定义、幂等性和分布约束，并应用于机器学习提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统概率预测校准研究广泛，但似然函数校准主要局限于二元假设情况。需要将校准概念扩展到多假设场景，以提升机器学习方法的可解释性和可靠性。

Method: 使用Aitchison几何和等距对数比变换，将二元情况下的对数似然比概念扩展到多元假设，恢复贝叶斯规则的向量形式，定义多类校准和幂等性。

Result: 成功将校准概念扩展到多假设情况，提出了多类似然函数的校准定义和约束条件，并在非线性判别分析中实现了校准的似然函数。

Conclusion: 该工作为多假设情况下的似然函数校准提供了理论框架，提高了机器学习方法的可解释性和可靠性，主要贡献是概念性的扩展。

Abstract: While calibration of probabilistic predictions has been widely studied, this
paper rather addresses calibration of likelihood functions. This has been
discussed, especially in biometrics, in cases with only two exhaustive and
mutually exclusive hypotheses (classes) where likelihood functions can be
written as log-likelihood-ratios (LLRs). After defining calibration for LLRs
and its connection with the concept of weight-of-evidence, we present the
idempotence property and its associated constraint on the distribution of the
LLRs. Although these results have been known for decades, they have been
limited to the binary case. Here, we extend them to cases with more than two
hypotheses by using the Aitchison geometry of the simplex, which allows us to
recover, in a vector form, the additive form of the Bayes' rule; extending
therefore the LLR and the weight-of-evidence to any number of hypotheses.
Especially, we extend the definition of calibration, the idempotence, and the
constraint on the distribution of likelihood functions to this multiple
hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio
transformed likelihood function. This work is mainly conceptual, but we still
provide one application to machine learning by presenting a non-linear
discriminant analysis where the discriminant components form a calibrated
likelihood function over the classes, improving therefore the interpretability
and the reliability of the method.

</details>


### [135] [Cluster and then Embed: A Modular Approach for Visualization](https://arxiv.org/abs/2509.03373)
*Elizabeth Coda,Ery Arias-Castro,Gal Mishne*

Main category: cs.LG

TL;DR: 提出了一种模块化的降维方法，先聚类再嵌入然后对齐，相比t-SNE和UMAP能更好地保持全局几何结构且更透明


<details>
  <summary>Details</summary>
Motivation: t-SNE和UMAP等降维方法在可视化聚类数据时虽然能很好分离簇并保留局部信息，但往往会扭曲数据的全局几何结构

Method: 采用三步法：先对数据进行聚类，然后分别嵌入每个簇，最后对齐所有簇以获得全局嵌入

Result: 在多个合成和真实数据集上验证，该方法与现有方法竞争力相当，但透明度更高

Conclusion: 模块化的聚类-嵌入-对齐方法提供了一种更透明且能更好保持全局结构的降维替代方案

Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods
for visualizing data with a potential (latent) clustered structure. They are
known to group data points at the same time as they embed them, resulting in
visualizations with well-separated clusters that preserve local information
well. However, t-SNE and UMAP also tend to distort the global geometry of the
underlying data. We propose a more transparent, modular approach consisting of
first clustering the data, then embedding each cluster, and finally aligning
the clusters to obtain a global embedding. We demonstrate this approach on
several synthetic and real-world datasets and show that it is competitive with
existing methods, while being much more transparent.

</details>


### [136] [Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2509.03393)
*Taisiya Khakharova,Lucas Sakizloglou,Leen Lambers*

Main category: cs.LG

TL;DR: 这篇论文探索了使用图神经网络学习来处理流血性住血症治疗中的治疗决策问题，通过异构图表征患者数据并与强化学习策略结合，证明了图基方法的潜力和表征学习的复杂性。


<details>
  <summary>Details</summary>
Motivation: 流血性住血症治疗中非常难确定静脉液体和升压药的最佳用量。虽然自动化强化学习方法已有成效，但以往研究依赖关系数据。考虑到现代医疗数据的复杂性，使用图数据结构可能提供更自然和有效的方法。

Method: 将MIMIC-III数据集中的患者数据模型化为时间演化的异构图，使用GraphSAGE和GATv2两种图神经网络架构学习患者状态表征。采用表征学习与策略学习解耦的方法，编码器与预测下一患者状态的解码器联合训练生成潜在状态表征，然后使用dBCQ算法进行策略学习。

Result: 实验评估结果证实了图基方法的潜力，同时也显示了在这个领域进行表征学习的复杂性。

Conclusion: 这项研究证明了使用图神经网络表征患者数据并与强化学习策略结合在流血性住血症治疗中的可行性和潜力，为医疗决策支持系统提供了新的研究方向。

Abstract: Sepsis is a serious, life-threatening condition. When treating sepsis, it is
challenging to determine the correct amount of intravenous fluids and
vasopressors for a given patient. While automated reinforcement learning
(RL)-based methods have been used to support these decisions with promising
results, previous studies have relied on relational data. Given the complexity
of modern healthcare data, representing data as a graph may provide a more
natural and effective approach. This study models patient data from the
well-known MIMIC-III dataset as a heterogeneous graph that evolves over time.
Subsequently, we explore two Graph Neural Network architectures - GraphSAGE and
GATv2 - for learning patient state representations, adopting the approach of
decoupling representation learning from policy learning. The encoders are
trained to produce latent state representations, jointly with decoders that
predict the next patient state. These representations are then used for policy
learning with the dBCQ algorithm. The results of our experimental evaluation
confirm the potential of a graph-based approach, while highlighting the
complexity of representation learning in this domain.

</details>


### [137] [Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training](https://arxiv.org/abs/2509.03403)
*Chenlu Ye,Zhou Yu,Ziji Zhang,Hao Chen,Narayanan Sadagopan,Jing Huang,Tong Zhang,Anurag Beniwal*

Main category: cs.LG

TL;DR: PROF是一种结合过程奖励模型(PRM)和结果奖励模型(ORM)的数据处理方法，通过一致性驱动的样本选择来解决数学推理任务中奖励模型的粒度问题，显著提升推理准确性和中间步骤质量。


<details>
  <summary>Details</summary>
Motivation: 现有的结果奖励模型(ORM)过于粗粒度，无法区分正确答案中的错误推理或错误答案中的有效推理，而过程奖励模型(PRM)虽然提供细粒度指导但存在不准确性和奖励攻击问题。

Method: 提出PROF方法，通过一致性驱动的样本选择策略，保留具有较高平均过程值的正确响应和较低平均过程值的错误响应，同时保持正负训练样本平衡，而不是简单地在目标函数中混合PRM和ORM。

Result: 实验表明该方法相比混合方法持续提升最终准确率超过4%，同时增强了中间推理步骤的质量。

Conclusion: PROF有效解决了数学推理任务中奖励模型的粒度困境，通过协调噪声细粒度过程奖励和准确粗粒度结果奖励，实现了推理能力的显著提升。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a
predominant paradigm for mathematical reasoning tasks, offering stable
improvements in reasoning ability. However, Outcome Reward Models (ORMs) in
RLVR are too coarse-grained to distinguish flawed reasoning within correct
answers or valid reasoning within incorrect answers. This lack of granularity
introduces noisy and misleading gradients significantly and hinders further
progress in reasoning process quality. While Process Reward Models (PRMs) offer
fine-grained guidance for intermediate steps, they frequently suffer from
inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an
effective data process curation method that harmonizes noisy, fine-grained
process rewards with accurate, coarse-grained outcome rewards. Rather than
naively blending PRM and ORM in the objective function
(arXiv:archive/2506.18896), PROF leverages their complementary strengths
through consistency-driven sample selection. Our approach retains correct
responses with higher averaged process values and incorrect responses with
lower averaged process values, while maintaining positive/negative training
sample balance. Extensive experiments demonstrate that our method not only
consistently improves the final accuracy over $4\%$ compared to the blending
approaches, but also strengthens the quality of intermediate reasoning steps.
Codes and training recipes are available at https://github.com/Chenluye99/PROF.

</details>


### [138] [Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study](https://arxiv.org/abs/2509.03417)
*Spyros Rigas,Dhruv Verma,Georgios Alexandridis,Yixuan Wang*

Main category: cs.LG

TL;DR: 本文研究了Kolmogorov-Arnold Networks (KANs)的初始化策略，提出了理论驱动的LeCun和Glorot初始化方法以及经验性的幂律初始化方法，在多个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: KANs作为一种新兴的神经网络架构，虽然已在科学和机器学习任务中成功应用，但其初始化策略尚未得到充分研究，这限制了其性能的进一步提升。

Method: 提出了两种理论驱动的初始化方案（基于LeCun和Glorot理论）和一个经验性的幂律初始化家族，通过大规模网格搜索在函数拟合、PDE前向求解等基准上进行评估，并结合神经正切核分析训练动态。

Result: 研究发现Glorot启发的初始化在参数丰富的模型中显著优于基线，而幂律初始化在所有任务和不同规模架构上都取得了最佳性能。

Conclusion: 本文为KANs提供了有效的初始化策略，特别是幂律初始化方法表现出色，所有代码和数据均已公开，有助于推动KANs的进一步研究和应用。

Abstract: Kolmogorov-Arnold Networks (KANs) are a recently introduced neural
architecture that replace fixed nonlinearities with trainable activation
functions, offering enhanced flexibility and interpretability. While KANs have
been applied successfully across scientific and machine learning tasks, their
initialization strategies remain largely unexplored. In this work, we study
initialization schemes for spline-based KANs, proposing two theory-driven
approaches inspired by LeCun and Glorot, as well as an empirical power-law
family with tunable exponents. Our evaluation combines large-scale grid
searches on function fitting and forward PDE benchmarks, an analysis of
training dynamics through the lens of the Neural Tangent Kernel, and
evaluations on a subset of the Feynman dataset. Our findings indicate that the
Glorot-inspired initialization significantly outperforms the baseline in
parameter-rich models, while power-law initialization achieves the strongest
performance overall, both across tasks and for architectures of varying size.
All code and data accompanying this manuscript are publicly available at
https://github.com/srigas/KAN_Initialization_Schemes.

</details>


### [139] [LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability](https://arxiv.org/abs/2509.03425)
*Phuc Pham,Viet Thanh Duy Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: LINKER是首个基于序列的模型，仅使用蛋白质序列和配体SMILES就能预测残基-功能基团相互作用类型，无需3D结构输入


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖3D结构输入或基于距离的接触标签，限制了应用范围和生物学相关性

Method: 采用结构监督注意力机制，通过功能基团基序提取从3D复合物获得交互标签，将配体结构抽象为功能基团

Result: 在LP-PDBBind基准测试中，基于功能基团抽象的结构监督能产生与真实生化注释高度一致的交互预测

Conclusion: LINKER仅需序列级输入即可实现大规模应用，特别适用于结构数据不可用的情况

Abstract: Accurate identification of interactions between protein residues and ligand
functional groups is essential to understand molecular recognition and guide
rational drug design. Existing deep learning approaches for protein-ligand
interpretability often rely on 3D structural input or use distance-based
contact labels, limiting both their applicability and biological relevance. We
introduce LINKER, the first sequence-based model to predict residue-functional
group interactions in terms of biologically defined interaction types, using
only protein sequences and the ligand SMILES as input. LINKER is trained with
structure-supervised attention, where interaction labels are derived from 3D
protein-ligand complexes via functional group-based motif extraction. By
abstracting ligand structures into functional groups, the model focuses on
chemically meaningful substructures while predicting interaction types rather
than mere spatial proximity. Crucially, LINKER requires only sequence-level
input at inference time, enabling large-scale application in settings where
structural data is unavailable. Experiments on the LP-PDBBind benchmark
demonstrate that structure-informed supervision over functional group
abstractions yields interaction predictions closely aligned with ground-truth
biochemical annotations.

</details>


### [140] [Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects](https://arxiv.org/abs/2509.03446)
*Niteesh Midlagajni,Constantin A. Rothkopf*

Main category: cs.LG

TL;DR: 基于图神经网络的液体动力学模拟框架，能够处理液体与动态空间柱体的复杂交互，并具有良好的过送性和控制应用能力


<details>
  <summary>Details</summary>
Motivation: 解决现有数据驱动方法在模拟液体与动态空间柱体复杂交互时的局限性，特别是在复杂表面几何和主动操控场景中

Method: 使用GNN框架，将粒子表示为图节点，采用边界体积层次算法(BVH)处理粒子-物体碰撞，支持复杂表面几何的交互

Result: 模型能准确捕捉动态设置中的流体行为，在未见对象和新操作任务(如搅拌、撇取)中体现良好过送性，并能通过梯度优化方法解决控制任务

Conclusion: 该GNN框架为液体动力学模拟提供了一种有效方法，特别在处理液体与动态空间柱体的复杂交互方面显示出优势，具有广阔的应用前景

Abstract: Simulating particle dynamics with high fidelity is crucial for solving
real-world interaction and control tasks involving liquids in design, graphics,
and robotics. Recently, data-driven approaches, particularly those based on
graph neural networks (GNNs), have shown progress in tackling such problems.
However, these approaches are often limited to learning fluid behavior in
static free-fall environments or simple manipulation settings involving
primitive objects, often overlooking complex interactions with dynamically
moving kinematic rigid bodies. Here, we propose a GNN-based framework designed
from the ground up to learn the dynamics of liquids under rigid body
interactions and active manipulations, where particles are represented as graph
nodes and particle-object collisions are handled using surface representations
with the bounding volume hierarchy (BVH) algorithm. This approach enables the
network to model complex interactions between liquid particles and intricate
surface geometries. Our model accurately captures fluid behavior in dynamic
settings and can also function as a simulator in static free-fall environments.
Despite being trained on a single-object manipulation task of pouring, our
model generalizes effectively to environments with unseen objects and novel
manipulation tasks such as stirring and scooping. Finally, we show that the
learned dynamics can be leveraged to solve control and manipulation tasks using
gradient-based optimization methods.

</details>


### [141] [Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning](https://arxiv.org/abs/2509.03477)
*Duy A. Nguyen,Abhi Kamboj,Minh N. Do*

Main category: cs.LG

TL;DR: Robult是一个可扩展的多模态学习框架，通过信息论方法处理缺失模态和有限标注数据问题，包含PU对比损失和潜在重构损失两个核心目标，在多种数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中缺失模态和有限标注数据的关键挑战，提升鲁棒性和实用性。

Method: 采用软正未标注(PU)对比损失最大化任务相关特征对齐，以及潜在重构损失保留模态特定信息，模块化设计确保推理时对不完整模态的韧性。

Result: 在多样化数据集上的实验验证表明，Robult在半监督学习和缺失模态场景下均优于现有方法。

Conclusion: Robult框架轻量且可扩展，能够无缝集成到现有架构中，适用于实际多模态应用场景。

Abstract: Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.

</details>


### [142] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: DP-SGD中的量化会导致显著的准确度下降，QPQuant通过动态层选择和损失敏感性估计有效减少量化方差，在保持DP保护的前提下提升训练效率和准确度


<details>
  <summary>Details</summary>
Motivation: 量化技术虽然能大幅降低训练时间和能耗，但在差分隐私SGD(DP-SGD)中会导致更严重的准确度下降，这是由于噪声注入放大了量化方差

Method: 提出QPQuant动态量化框架，包含：1)每个迭代轮换选择量化层的概率采样；2)使用差分隐私损失敏感性估计器识别对模型质量影响最小的层进行量化

Result: 在ResNet18、ResNet50和DenseNet121上的实验评估显示，QPQuant较静态量化基线持续优化，实现近Pareto最优的准确度-计算权衡，低精度硬件上理论吞吐量提升2.21倍，验证准确度下降不超2%

Conclusion: QPQuant通过动态层选择机制有效解决了DP-SGD中量化导致的准确度下降问题，在保持差分隐私保护的同时显著提升了训练效率和模型性能

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


### [143] [Geometric Foundations of Tuning without Forgetting in Neural ODEs](https://arxiv.org/abs/2509.03474)
*Erkan Bayram,Mohamed-Ali Belabbas,Tamer Başar*

Main category: cs.LG

TL;DR: 本文证明了Tuning without Forgetting (TwF)方法中参数子空间在非奇异控制下构成有限余维Banach子流形，并刻画了其切空间，为TwF在顺序训练中精确保持映射提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 为先前提出的TwF方法提供严格的数学理论基础，证明其在顺序训练中能够精确保持已学习样本的端点映射，而不仅仅是一阶近似意义下的保持。

Method: 通过数学分析证明在非奇异控制条件下，TwF方法中的参数子空间构成有限余维Banach子流形，并精确刻画该子流形的切空间结构。

Result: 证明了TwF对应的参数子空间确实形成Banach子流形，其切空间特征表明TwF方法可以实现控制函数沿着该子流形切空间的连续变形，从而精确保持映射关系。

Conclusion: 该研究为TwF方法提供了坚实的数学基础，表明该方法能够在顺序训练过程中精确保持先前学习到的映射关系，超越了最初提出时的一阶近似保证。

Abstract: In our earlier work, we introduced the principle of Tuning without Forgetting
(TwF) for sequential training of neural ODEs, where training samples are added
iteratively and parameters are updated within the subspace of control functions
that preserves the end-point mapping at previously learned samples on the
manifold of output labels in the first-order approximation sense. In this
letter, we prove that this parameter subspace forms a Banach submanifold of
finite codimension under nonsingular controls, and we characterize its tangent
space. This reveals that TwF corresponds to a continuation/deformation of the
control function along the tangent space of this Banach submanifold, providing
a theoretical foundation for its mapping-preserving (not forgetting) during the
sequential training exactly, beyond first-order approximation.

</details>


### [144] [SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](https://arxiv.org/abs/2509.03487)
*Jigang Fan,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang,Zaixi Zhang*

Main category: cs.LG

TL;DR: SafeProtein是首个针对蛋白质基础模型的红队测试框架，通过多模态提示工程和启发式束搜索系统性地测试模型安全性，在ESM3等先进模型上达到70%的攻击成功率，揭示了潜在的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动了蛋白质基础模型的快速发展，但这些模型缺乏系统性的红队测试，存在被滥用于生成具有生物安全风险蛋白质的严重隐患。

Method: 结合多模态提示工程和启发式束搜索，设计红队测试方法并构建SafeProtein-Bench基准数据集和评估协议。

Result: 在先进的蛋白质基础模型上实现了持续越狱（ESM3攻击成功率高达70%），揭示了当前模型存在的潜在生物安全风险。

Conclusion: SafeProtein框架成功暴露了蛋白质基础模型的安全漏洞，为前沿模型开发稳健的安全防护技术提供了重要见解。

Abstract: Proteins play crucial roles in almost all biological processes. The
advancement of deep learning has greatly accelerated the development of protein
foundation models, leading to significant successes in protein understanding
and design. However, the lack of systematic red-teaming for these models has
raised serious concerns about their potential misuse, such as generating
proteins with biological safety risks. This paper introduces SafeProtein, the
first red-teaming framework designed for protein foundation models to the best
of our knowledge. SafeProtein combines multimodal prompt engineering and
heuristic beam search to systematically design red-teaming methods and conduct
tests on protein foundation models. We also curated SafeProtein-Bench, which
includes a manually constructed red-teaming benchmark dataset and a
comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks
on state-of-the-art protein foundation models (up to 70% attack success rate
for ESM3), revealing potential biological safety risks in current protein
foundation models and providing insights for the development of robust security
protection technologies for frontier models. The codes will be made publicly
available at https://github.com/jigang-fan/SafeProtein.

</details>


### [145] [On Entropy Control in LLM-RL Algorithms](https://arxiv.org/abs/2509.03493)
*Han Shen*

Main category: cs.LG

TL;DR: AEnt是一种针对LLM-RL训练的新型熵控制方法，通过钳位熵奖励和自动调整系数来解决传统熵正则化在大语言模型强化学习中的失效问题。


<details>
  <summary>Details</summary>
Motivation: 传统熵正则化在机器人控制和游戏RL中有效，但在LLM-RL训练中效果不佳，主要原因是LLM响应空间极大且最优输出稀疏。

Method: 提出AEnt方法：使用钳位熵奖励，在更小的标记空间上定义重归一化策略来计算熵，并自动调整熵系数来控制熵诱导偏差。

Result: 在数学推理任务中，AEnt在不同基础模型和数据集上均优于基线方法，在多个基准测试中表现一致优异。

Conclusion: AEnt通过改进的熵控制机制有效解决了LLM-RL训练中的探索问题，为大规模语言模型的强化学习提供了更有效的熵管理方案。

Abstract: For RL algorithms, appropriate entropy control is crucial to their
effectiveness. To control the policy entropy, a commonly used method is entropy
regularization, which is adopted in various popular RL algorithms including
PPO, SAC and A3C. Although entropy regularization proves effective in robotic
and games RL conventionally, studies found that it gives weak to no gains in
LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL
setting. Specifically, we first argue that the conventional entropy
regularization suffers from the LLM's extremely large response space and the
sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy
control method that utilizes a new clamped entropy bonus with an automatically
adjusted coefficient. The clamped entropy is evaluated with the re-normalized
policy defined on certain smaller token space, which encourages exploration
within a more compact response set. In addition, the algorithm automatically
adjusts entropy coefficient according to the clamped entropy value, effectively
controlling the entropy-induced bias while leveraging the entropy's benefits.
AEnt is tested in math-reasoning tasks under different base models and
datasets, and it is observed that AEnt outperforms the baselines consistently
across multiple benchmarks.

</details>


### [146] [Invariant Features for Global Crop Type Classification](https://arxiv.org/abs/2509.03497)
*Xin-Yi Tong,Sherrie Wang*

Main category: cs.LG

TL;DR: 该研究构建了全球作物数据集CropGlobe，提出CropNet模型和时序数据增强方法，发现Sentinel-2的2D中值时序特征在跨区域作物分类中具有最强的地理不变性。


<details>
  <summary>Details</summary>
Motivation: 解决全球尺度作物分类中因地理空间转移导致的性能下降问题，提高遥感作物分类的跨区域泛化能力。

Method: 构建包含30万个样本的全球作物数据集CropGlobe；比较不同遥感特征的迁移性；设计轻量级CNN模型CropNet；使用时序数据增强方法模拟跨区域物候变化。

Result: Sentinel-2的2D中值时序特征在所有迁移场景中表现出最强的地理不变性；数据增强进一步提高了模型鲁棒性，特别是在训练数据多样性有限时。

Conclusion: 识别了更具不变性的特征表示，为全球多样化地区的可扩展、低成本作物类型应用提供了有前景的路径。

Abstract: Accurately obtaining crop type and its spatial distribution at a global scale
is critical for food security, agricultural policy-making, and sustainable
development. Remote sensing offers an efficient solution for large-scale crop
classification, but the limited availability of reliable ground samples in many
regions constrains applicability across geographic areas. To address
performance declines under geospatial shifts, this study identifies remote
sensing features that are invariant to geographic variation and proposes
strategies to enhance cross-regional generalization. We construct CropGlobe, a
global crop type dataset with 300,000 pixel-level samples from eight countries
across five continents, covering six major food and industrial crops (corn,
soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,
CropGlobe enables a systematic evaluation under cross-country, cross-continent,
and cross-hemisphere transfer. We compare the transferability of temporal
multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic
coefficients) and hyperspectral features (from EMIT). To improve generalization
under spectral and phenological shifts, we design CropNet, a lightweight and
robust CNN tailored for pixel-level crop classification, coupled with temporal
data augmentation (time shift, time scale, and magnitude warping) that
simulates realistic cross-regional phenology. Experiments show that 2D median
temporal features from Sentinel-2 consistently exhibit the strongest invariance
across all transfer scenarios, and augmentation further improves robustness,
particularly when training data diversity is limited. Overall, the work
identifies more invariant feature representations that enhance geographic
transferability and suggests a promising path toward scalable, low-cost crop
type applications across globally diverse regions.

</details>


### [147] [Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients](https://arxiv.org/abs/2509.03503)
*Gwen Legate,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了ZOWarmUp算法，一种联邦零阶优化器，允许低资源边缘设备参与联邦学习训练，解决了内存和通信限制导致的设备排除问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，由于内存和通信限制，许多边缘设备无法参与训练，导致数据不可访问和系统偏差增加。需要一种方法让低资源设备也能参与训练。

Method: 基于MeZO零阶方法，开发了ZOWarmUp算法，利用不同客户端能力和方差减少技术，通过随机种子而非完整梯度进行通信，实现从随机初始化的零阶训练。

Result: 实验表明ZOWarmUp在各种数据集和模型架构下都具有鲁棒性，能够访问更多样化的数据，改善训练结果。

Conclusion: ZOWarmUp为高比例低资源边缘设备的系统提供了有效的解决方案，提高了数据访问量和多样性，从而提升联邦学习性能。

Abstract: Federated learning enables collaborative model training across numerous edge
devices without requiring participants to share data; however, memory and
communication constraints on these edge devices may preclude their
participation in training. We consider a setting in which a subset of edge
devices are below a critical memory or communication threshold required to
conduct model updates. Under typical federated optimization algorithms, these
devices are excluded from training which renders their data inaccessible and
increases system induced bias. We are inspired by MeZO, a zeroth-order method
used for memory-efficient fine-tuning. The increased variance inherent to
zeroth-order gradient approximations has relegated previous zeroth-order
optimizers exclusively to the domain of fine tuning; a limitation we seek to
correct. We devise a federated, memory-efficient zeroth-order optimizer,
ZOWarmUp that permits zeroth-order training from a random initialization.
ZOWarmUp leverages differing client capabilities and careful variance reduction
techniques to facilitate participation of under-represented, low-resource
clients in model training. Like other federated zeroth-order methods, ZOWarmUp
eliminates the need for edge devices to transmit their full gradients to the
server and instead relies on only a small set of random seeds, rendering the
up-link communication cost negligible. We present experiments using various
datasets and model architectures to show that ZOWarmUp is a robust algorithm
that can can be applied under a wide variety of circumstances. For systems with
a high proportion of edge devices that would otherwise be excluded from
training, this algorithm provides access to a greater volume and diversity of
data, thus improving training outcomes.

</details>


### [148] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: LimiX是首个大型结构化数据基础模型，通过联合分布建模和查询条件预测，统一处理多种表格任务，在10个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 实现通用智能需要语言、物理世界和结构化数据的互补基础模型，当前缺乏专门处理结构化数据的基础模型

Method: 采用掩码联合分布建模和情景条件目标，通过查询条件预测支持训练免费推理适应

Result: 在分类、回归、缺失值填补和数据生成等任务中 consistently 超越梯度提升树、深度表格网络和自动化集成方法

Conclusion: LimiX展示了单一模型处理多样化表格任务的强大能力，为结构化数据基础模型开辟了新方向

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


### [149] [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
*Haoran Huan,Mihir Prabhudesai,Mengning Wu,Shantanu Jaiswal,Deepak Pathak*

Main category: cs.LG

TL;DR: 这篇论文系统研究了大语言模型的说谎行为，区分了说谎与幻觉的不同，通过机制解释技术揭示了欺骗行为的神经机制，并探索了说谎与任务性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在实际应用中的自主性增强，关于其可靠性的担忧日益增长。虽然幻觉现象已得到广泛研究，但LLM主观说谎的现象仍然被忽视。本研究旨在系统地调查LLM的说谎行为，以提高对AI伦理风险的认知。

Method: 采用机制解释技术，包括logit lens分析、因果干预和对比激活导航等方法，识别和控制欺骗行为。研究真实世界说谎场景，并引入行为导航向量来精细操控说谎倾向性。

Result: 揭示了LLM欺骗行为的神经机制，并发现了说谎与终端任务性能之间的Pareto最优边界，即在某些情况下不诚实可以提高目标优化效果。

Conclusion: 研究结果为AI伦理讨论做出了贡献，揭示了在高风险环境中部署LLM的风险，并提供了潜在的安全保障措施。这项工作强调了对LLM说谎行为进行系统研究和控制的重要性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/

</details>

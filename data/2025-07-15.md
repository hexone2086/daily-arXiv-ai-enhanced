<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.RO](#cs.RO) [Total: 45]
- [cs.LG](#cs.LG) [Total: 161]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 论文提出V2-VLNCE（多视角VLNCE）和VIL（视角不变学习），通过对比学习和师生框架提升导航策略对视角变化的鲁棒性，并在多个数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有导航策略对视角变化敏感，限制了在真实环境中的应用。

Method: 提出VIL，结合对比学习和师生框架，优化Waypoint Predictor Module，实现端到端训练。

Result: 在V2-VLNCE上性能提升8-15%，在标准VLNCE和RxR-CE数据集上也表现优异。

Conclusion: VIL是一种即插即用的后训练方法，能提升多视角和标准视角下的导航性能。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [2] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种基于面部生物特征异常模式的深度学习检测方法，用于识别深度伪造视频。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术易被用于欺诈、诈骗和政治虚假信息，亟需有效的检测手段。

Method: 利用面部生物特征中的异常模式，开发了一种新型法医机器学习技术。

Result: 在大规模深度伪造数据集上评估了该技术，并测试了其对视频篡改的鲁棒性和泛化能力。

Conclusion: 该方法能有效检测深度伪造视频，并对未知生成器具有一定泛化能力。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [3] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据且任务无关的方法，用于减少视觉语言模型（如CLIP）中的隐式偏差。它通过生成场景描述和对比式去偏损失来最小化虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常因训练数据中的偏差而产生预测偏差，PRISM旨在无需外部数据或预定义偏差类别的情况下解决这一问题。

Method: PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）使用对比式去偏损失学习投影，最小化虚假相关性并保持图像与文本嵌入的对齐。

Result: PRISM在Waterbirds和CelebA数据集上优于现有去偏方法。

Conclusion: PRISM提供了一种有效且通用的去偏解决方案，无需额外数据或预定义偏差类别。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [4] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: HMR-ViT结合时空与运动学信息，通过Vision Transformer提升人体网格恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有HMR方法仅利用时空或运动学信息，未结合两者，限制了性能。

Method: 构建时空-运动学特征图像，使用CRM优化特征排列，Vision Transformer编码，回归网络推断SMPL参数。

Result: 在3DPW和Human3.6M数据集上表现优异。

Conclusion: HMR-ViT通过结合时空与运动学信息，显著提升了HMR任务的性能。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [5] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 结合NeRF和MPM框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 在直接测量不可行的情况下，通过视觉观测逆向分析颗粒材料的特性。

Method: 生成合成实验数据，用NeRF重建3D几何，MPM模拟材料点位置，通过贝叶斯优化最小化图像损失估计摩擦角。

Result: 摩擦角估计误差在2度以内，验证了方法的有效性。

Conclusion: 该方法为颗粒材料特性表征提供了可行的解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA框架通过结合多阶段数据验证策略和人类专业知识，提升多模态基础模型生成标签的质量，从而改善开放词汇图像分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注数据量而非质量，且缺乏全面验证手段，导致多模态基础模型生成的标签质量未得到充分研究。

Method: 提出VISTA框架，整合多阶段数据验证策略与人类专业知识，识别并修正标签中的隐藏问题。

Result: 在基准数据集和专家评审中，VISTA从定量和定性角度均表现出有效性。

Conclusion: VISTA显著提升了标签质量，为多模态模型的性能改进提供了新思路。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个Python工具包，用于构建模块化的脑部病变图像分析流程，简化开发并支持多模态图像处理。


<details>
  <summary>Details</summary>
Motivation: 为临床和科研提供高效、低认知负担的脑部病变图像分析工具。

Method: 基于Pythonic原则设计，包含预处理模块（配准、去颅骨等）和BraTS算法（模态合成、病变修复等），并提供性能评估工具。

Result: 支持脑部病变（如胶质瘤、转移瘤等）的分析，并可扩展至其他生物医学图像应用。

Conclusion: BrainLesion Suite是一个灵活且功能强大的工具，适用于脑部病变图像分析，且开源可用。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于改善类别不平衡扩散模型中尾部类别图像的多样性，同时保持头部类别的保真度和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决类别不平衡数据中尾部类别图像合成多样性不足的问题，避免模式崩溃。

Method: 引入无监督InfoNCE损失和MSE损失，通过对比学习和条件-无条件对齐增强尾部类别多样性。

Result: 在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他方法。

Conclusion: 对比学习框架简单有效，显著提升了类别不平衡扩散模型的性能。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文探讨了当前大语言模型和多模态扩展在视频理解中的局限性，提出了‘无限视频理解’作为未来研究方向，旨在解决长时间视频处理中的计算、内存和连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前模型在处理长时间视频时面临计算、内存和连贯性挑战，需要新的研究方向以突破这些限制。

Method: 通过分析现有技术（如Video-XL-2、HoPE、VideoRoPE++等）的不足，提出‘无限视频理解’作为未来目标，并探讨相关研究方向。

Result: 指出了实现无限视频理解的核心挑战，如流式架构、持久内存机制、层次化表示等。

Conclusion: 无限视频理解是一个雄心勃勃但必要的目标，将为多媒体和AI研究提供新的方向和创新动力。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight通过利用注意力稀疏性优化视觉语言模型推理，减少FLOPs 32%-41%，精度变化在±2%内。


<details>
  <summary>Details</summary>
Motivation: 视觉数据增加提示长度和注意力计算的二次复杂度，导致预填充时间延长。

Method: 分析注意力模式，提出训练无关的注意力稀疏掩码方法BlindSight。

Result: 在Qwen2-VL等模型上，FLOPs减少32%-41%，精度变化小。

Conclusion: BlindSight有效优化推理效率，适用于多图像理解任务。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 本文系统回顾了定量遥感反演方法的发展，从物理模型到机器学习，再到基础模型，并比较了各范式的假设、应用场景和局限性。


<details>
  <summary>Details</summary>
Motivation: 随着遥感系统和人工智能的发展，传统物理模型逐渐被数据驱动和基础模型方法取代，本文旨在总结这一演变并展望未来发展方向。

Method: 通过系统综述，比较物理模型（如PROSPECT）、机器学习方法（如深度学习）和基础模型（如SatMAE）的优缺点。

Result: 强调了基础模型在自监督预训练、多模态集成和跨任务适应方面的进展，但也指出了物理可解释性、领域泛化等挑战。

Conclusion: 展望了下一代基础模型的发展方向，强调统一建模能力、跨领域泛化和物理可解释性。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 论文提出了一种无需微调的零样本光流提取方法，通过扰动生成视频模型并追踪其传播，显著提升了光流提取性能。


<details>
  <summary>Details</summary>
Motivation: 受大型通用模型的启发，探索是否可以通过提示冻结的自监督视频模型（仅用于未来帧预测）来输出光流，而无需微调。

Method: 基于Counterfactual World Model范式，提出KL-tracing方法，通过扰动第一帧并计算预测分布的KL散度来提取光流。

Result: 在TAP-Vid DAVIS和Kubric数据集上，方法分别实现了16.6%和4.7%的相对改进，优于现有技术。

Conclusion: 研究表明，通过可控生成视频模型的因果提示是一种可扩展且高效的光流提取替代方案。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种名为MI CAM的后验视觉解释方法，通过激活映射和互信息权重生成显著性可视化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制及其推理原因变得至关重要。

Method: MI CAM通过计算特征图与输入图像的互信息来加权，生成线性组合的显著性可视化，并通过反事实分析验证因果解释。

Result: MI CAM在定性和定量指标上均优于现有方法，提供了无偏的模型推理解释。

Conclusion: MI CAM是一种有效的视觉解释方法，能够清晰展示模型推理过程，性能优于现有技术。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo利用放射科医生的眼动视频序列提升大型视觉语言模型（LVLM）在胸片分析和报告生成中的性能，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了眼动的顺序信息，而RadEyeVideo通过整合眼动视频序列，捕捉时空动态，以提升模型性能。

Method: 提出RadEyeVideo方法，将眼动数据作为视频序列输入LVLM，评估其在胸片报告生成和疾病诊断中的效果。

Result: 使用眼动视频后，模型性能在报告生成任务中提升24.6%，在两项任务中平均提升15.2%，甚至超越专业医学LVLM。

Conclusion: RadEyeVideo展示了专家知识（眼动信息）与LVLM结合可显著提升通用模型在临床任务中的能力，为医学图像分析提供可扩展的人本方法。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用Stable Diffusion（SD）模型增强3D点云的自监督学习，通过点云引导去噪图像并提取SD特征，提升3D表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型受限于小规模数据集，而SD模型在大规模数据上训练，具备更强能力，可用于改进3D自监督学习。

Method: 提出PointSD框架，将SD模型的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并提取SD特征对齐3D主干网络。

Result: 实验表明，SD模型能有效提升点云自监督学习性能。

Conclusion: PointSD成功利用SD模型增强3D表示学习，为点云任务提供新思路。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 本文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），解决了传统方法在推理阶段的误差累积问题，并提升了实时性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理阶段因缺乏真实数据导致误差累积，而扩散模型因迭代性质难以实时应用。本文旨在结合两者优势，提升SLP的生成质量和实时性。

Method: 采用混合自回归和扩散模型的方法，设计了多尺度姿态表示模块和置信感知因果注意力机制，以捕捉细节并动态引导生成。

Result: 在PHOENIX14T和How2Sign数据集上的实验表明，该方法在生成质量和实时流效率上均表现优异。

Conclusion: 混合方法有效结合了自回归和扩散模型的优势，显著提升了SLP的生成质量和实时性能。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准测试RoHOI，并引入了一种新的语义感知掩蔽渐进学习策略（SAMPL）以提高模型在现实环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在干净数据集上表现良好，但在现实世界中的噪声、遮挡和环境变化下性能显著下降，亟需提高模型的鲁棒性。

Method: 提出了RoHOI基准测试，包含20种基于HICO-DET和V-COCO数据集的干扰类型，并设计了SAMPL策略，通过动态调整模型优化来增强鲁棒性。

Result: 实验表明，SAMPL策略显著提升了模型在干扰条件下的表现，优于现有方法。

Conclusion: RoHOI基准和SAMPL策略为HOI检测的鲁棒性研究提供了新标准，相关资源将公开共享。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MG-CLIP的方法，通过分析CLIP模型中的模态间隙变化，改进其在类增量学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用CLIP模型进行持续学习时，现有方法忽略了模态间隙的关键作用，而模态间隙反映了预训练知识的保留程度。

Method: 提出MG-CLIP方法，通过保持模态间隙以减少遗忘，并通过补偿模态间隙增强对新数据的适应能力。

Result: 在多个基准测试中，MG-CLIP表现优于现有方法，且无需额外的回放数据。

Conclusion: 模态间隙为持续学习提供了新的视角，MG-CLIP方法简单有效。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen是一个新的文本-运动数据集，包含高质量的运动捕捉数据和详细的文本标注，支持长期运动生成研究。MoMask++模型通过多尺度标记序列和改进的生成掩码变换器，在性能上达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成运动的方法受限于短文本或通用提示，缺乏细粒度控制和泛化能力。SnapMoGen旨在解决这一问题。

Method: 引入SnapMoGen数据集，包含20K运动片段和122K详细文本描述。提出MoMask++模型，将运动转化为多尺度标记序列，并使用单一生成掩码变换器生成所有标记。

Result: MoMask++在HumanML3D和SnapMoGen基准测试中达到最新性能，并能处理用户输入的随意提示。

Conclusion: SnapMoGen和MoMask++为文本生成运动提供了更高质量的数据集和模型，支持长期运动生成和细粒度控制。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出了一种基于大型语言模型的姿态估计框架，通过非线性MLP视觉语言连接器提升定位精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统姿态估计方法依赖关键点先验，泛化能力有限；语言引导方法（如LocLLM）虽能零样本泛化，但其线性投影器无法捕捉复杂空间-文本交互。

Method: PoseLLM采用非线性MLP（两层+GELU激活）作为视觉语言连接器，增强视觉块与文本关键点描述的融合。

Result: 在COCO验证集上达到77.8 AP，优于LocLLM（+0.4 AP），并在Human-Art和MPII上保持强零样本泛化能力。

Conclusion: 非线性连接器显著提升定位精度且不牺牲泛化能力，推动了语言引导姿态估计的进展。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 论文提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化（intra-scene和inter-scene）和采用编码器-解码器架构，实现了在自动驾驶系统中对3D场景演化的高效预测和生成。


<details>
  <summary>Details</summary>
Motivation: 解决复杂3D场景的高效标记化问题，以提升自动驾驶系统对极端场景的处理能力。

Method: 提出双标记器设计（intra-scene和inter-scene），结合多尺度残差量化和残差聚合时间依赖，采用编码器-解码器架构实现高效控制。

Result: 在4D占用预测任务中，mIoU和IoU分别提升25.1%和36.9%，训练内存仅需2.9 GB，推理速度达37.0 FPS。

Conclusion: $I^{2}$-World在性能和效率上均优于现有方法，为自动驾驶场景生成提供了高效解决方案。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [22] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 论文提出了一种名为稳定分数蒸馏（SSD）的框架，用于改进文本引导的图像和3D编辑，解决了现有方法在稳定性、空间控制和编辑强度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Delta Denoising Score依赖复杂的辅助结构，导致优化信号冲突和局部编辑不精确。SSD旨在通过简化框架提升编辑过程的稳定性和对齐性。

Method: SSD通过固定一个分类器到源提示，利用Classifier-Free Guidance（CFG）方程实现跨提示对齐，并引入空文本分支稳定优化。还包括提示增强分支以提升编辑强度。

Result: SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得最先进结果，收敛更快且复杂度更低。

Conclusion: SSD为文本引导编辑提供了高效、稳定的解决方案，同时保持了内容的连贯性和编辑的精确性。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [23] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的RGB与深度信息融合方法，通过对比学习提升样本效率，并采用课程学习实现sim2real迁移。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景变化具有鲁棒性且包含3D空间细节，但现有方法在泛化能力上不足。

Method: 分别用CNN处理RGB和深度信息，再通过视觉Transformer融合特征；设计了对比学习方案和课程学习策略。

Result: 方法提升了泛化能力和样本效率，支持sim2real迁移。

Conclusion: 提出的多模态融合和训练策略有效提升了视觉任务的性能。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [24] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文研究了Few-Shot Class-Incremental Learning (FSCIL)中提示池方法的性能下降问题，并提出了一种新的空间提示方法LGSP-Prompt，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: FSCIL面临数据稀缺和增量学习的双重挑战，现有提示池方法在此场景下性能下降，需要新的解决方案。

Method: 提出LGSP-Prompt方法，将提示学习从令牌维度转向空间维度，结合局部空间特征和全局频域表示生成空间提示。

Result: 实验表明，LGSP-Prompt在多个FSCIL基准测试中达到最先进性能，显著优于现有方法。

Conclusion: LGSP-Prompt通过空间维度的提示设计，有效解决了FSCIL中的性能下降问题，为未来研究提供了新思路。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [25] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文提出MCA-LLaVA方法，通过改进RoPE的长时衰减问题，缓解多模态对齐偏差，减少LVLMs中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）中幻觉问题严重，多模态特征对齐不足是主要原因之一。研究发现RoPE的长时衰减导致图像对齐偏差，影响模型性能。

Method: 提出基于曼哈顿距离的MCA-LLaVA方法，将一维序列顺序与二维空间位置结合，改进RoPE的长时衰减为多方向空间衰减。

Result: 实验表明MCA-LLaVA在幻觉和通用基准测试中表现优异，有效缓解了图像对齐偏差。

Conclusion: MCA-LLaVA通过改进位置编码方法，显著提升了多模态对齐效果，减少了幻觉现象。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [26] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的方法，通过层次特征聚合和循环时间精化，解决了动态场景图中空间细节和时间依赖的问题，并在新数据集AeroEye-v1.0上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解在自动驾驶、监控等领域需求迫切，但现有方法难以同时捕捉细粒度空间细节和长时间依赖。

Method: 提出THYME方法，结合层次特征聚合和循环时间精化，建模多尺度空间上下文并保证时间一致性。

Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法，提升了场景理解能力。

Conclusion: THYME方法有效解决了动态场景图中的关键问题，为复杂场景理解提供了新工具。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [27] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 通过视频分析表面波的传播特性，推断材料厚度和刚度的方法。


<details>
  <summary>Details</summary>
Motivation: 表面波的传播包含材料内部物理特性的信息，可用于无创检测和健康监测。

Method: 从视频中提取色散关系，并通过基于物理的优化问题求解最佳厚度和刚度参数。

Result: 在模拟和真实数据中验证，结果与真实测量值高度一致。

Conclusion: 该方法为家庭健康监测提供了概念验证，并适用于人机交互等领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [28] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Expert-CFG的专家参与框架，用于在不额外训练的情况下提升医学视觉语言模型的临床对齐性，通过不确定性估计和专家指导优化输出。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型存在概率不确定性，可能产生错误或未验证的响应，这对医学应用有严重影响。现有方法依赖训练且成本高，临床对齐不足。

Method: 提出Expert-CFG框架，包括不确定性估计、检索相关参考文献辅助专家标注关键术语，并通过分类器自由指导优化模型输出。

Result: 在三个医学视觉问答基准测试中，Expert-CFG（4.2B参数）优于现有13B参数模型，且仅需有限专家标注。

Conclusion: Expert-CFG展示了在资源有限环境中临床应用的可行性，无需额外训练即可提升模型性能。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [29] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升泛化能力，并设计了异常评分算法。同时，合成数据集KITTI-AR用于验证算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D检测模型在开放道路中对罕见异常物体的误检或漏检问题，提升模型的泛化能力和异常检测能力。

Method: 提出S3AD算法，解耦2D和3D训练策略，设计基于前景置信度的异常评分算法。合成KITTI-AR数据集，包含新类别以验证算法。

Result: 实验验证了算法和数据集的有效性，提升了3D异常检测的泛化能力。

Conclusion: S3AD算法和KITTI-AR数据集为3D异常检测提供了有效解决方案，适用于开放道路场景。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [30] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出一种新颖的球形采样方法，用于全景图像，直接利用现有的二维预训练模型，减少失真并提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏百万级规模的全景图像数据集，现有任务依赖二维预训练模型，但这些模型无法处理全景图像的失真和不连续性，影响性能。

Method: 采用基于预训练模型权重的球形离散采样方法，减少失真并获得良好的初始训练值；将该方法应用于全景图像分割，利用球形模型特征作为特定通道注意力的掩码。

Result: 在常用室内数据集Stanford2D3D上取得了良好效果。

Conclusion: 提出的球形采样方法有效解决了全景图像处理中的失真问题，并提升了任务性能。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [31] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 论文提出了一种在线长时点跟踪方法Track-On，利用视觉基础模型和Transformer架构，在无需未来帧信息的情况下实现高效跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有长时点跟踪方法多为离线模式，依赖未来帧信息，而实际应用（如流媒体视频和具身AI）需要在线因果预测，因此需开发一种无需未来信息的跟踪方法。

Method: 结合视觉基础模型的空间特征，提出Track-On模型，基于Transformer架构，将每个跟踪点作为查询逐帧处理视频。

Result: Track-On在七个公开基准测试中达到最新技术水平，验证了无需未来帧信息的长时跟踪可行性。

Conclusion: Track-On通过结合视觉基础模型和Transformer，成功实现了在线长时点跟踪，为实际应用提供了高效解决方案。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [32] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM是一个统一框架，通过Fisher信息惩罚（FIP）和置信度对齐惩罚（CMP）解决基础模型在视觉分类和医学分割任务中的分布偏移和置信度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如CLIP和SAM）在低样本迁移学习中表现优异，但在部署时面临分布偏移和置信度不匹配的挑战，现有解决方案多为领域特定。

Method: StaRFM引入FIP减少嵌入中的协变量偏移，并通过CMP校准分割任务中的不确定性。理论分析表明FIP通过Fisher-Rao范数控制泛化，CMP通过Brier分数优化最小化校准误差。

Result: 在19个视觉数据集上，StaRFM提升3.5%准确率并降低28% ECE；在医学分割任务中达到84.7% DSC和4.8mm HD95；跨域性能差距降低40%。

Conclusion: StaRFM是一个即插即用的框架，无需大幅架构调整即可与基础模型集成，显著提升性能。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [33] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成先验的方法，利用Stable Diffusion从单视角的自我中心图像重建可动画的虚拟形象，解决了遮挡和身体比例失真的问题。


<details>
  <summary>Details</summary>
Motivation: 为了在虚拟现实中准确复制人的身体、服装和动作，需要从自我中心视角（第一人称）捕捉和传输动作。然而，这种视角存在遮挡和身体比例失真的问题，目前缺乏基于生成先验的方法。

Method: 基于Stable Diffusion，结合ControlNet，提出了一种从遮挡的俯视图像生成逼真正面视图的流程，并将其输入到图像到动作模型中。

Result: 该方法减少了训练负担并提高了泛化能力，首次实现了从自我中心输入重建可动画的虚拟形象。

Conclusion: 该方法为更易用和泛化的远程呈现系统铺平了道路。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [34] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的框架PPJudge，用于评估绘画过程的动态性，并引入了首个大规模数据集PPAD。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注静态图像，忽略了绘画过程的动态性和多阶段性。

Method: 提出PPJudge模型，结合时间感知位置编码和混合专家架构，并构建PPAD数据集。

Result: 实验表明，PPJudge在准确性、鲁棒性和与人类判断的一致性上优于现有基线。

Conclusion: 该框架为计算创造力和艺术教育提供了新视角。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [35] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net通过注意力引导的上下文去偏方法，结合因果干预模块和新型卷积编码器，有效解决了情感识别中的上下文偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法容易受到上下文偏差的影响（如将特定背景与情感标签错误关联），需要更鲁棒的解决方案。

Method: 提出AGCD-Net模型，集成Hybrid ConvNeXt编码器和AG-CIM模块，通过因果干预和注意力机制消除上下文偏差。

Result: 在CAER-S数据集上表现优异，达到最先进性能。

Conclusion: AGCD-Net证明了因果去偏在复杂场景下对情感识别的重要性。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [36] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: AAHR框架通过动态聚类原型对比学习和多模态关系矩阵，解决了图像-文本匹配中的语义模糊和高阶关联问题，显著提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高阶关联和语义模糊（如软正负样本）时表现不足，且未能充分利用训练批次中的邻域关系。

Method: AAHR结合动态聚类原型对比学习、全局和局部特征提取、自适应聚合网络、GNN增强语义交互以及动量对比学习。

Result: 在Flickr30K、MSCOCO和ECCV Caption数据集上，AAHR优于现有方法，提升了匹配准确性和效率。

Conclusion: AAHR通过多策略融合有效解决了语义模糊和高阶关联问题，为图像-文本匹配提供了新思路。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [37] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于视觉标记化的无注释手语翻译方法，通过分段减少输入序列长度，显著降低计算需求，并在性能上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有无注释手语翻译方法因模型复杂性和高计算需求而难以扩展的问题。

Method: 采用分段感知的视觉标记化框架，将连续视频转换为离散的视觉标记，并结合对比对齐目标和双重监督策略。

Result: 在PHOENIX14T基准测试中性能显著提升，序列长度减少50%，内存使用降低2.67倍。

Conclusion: 该方法在性能和可扩展性上均优于现有技术，验证了标记化与对齐策略的潜力。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [38] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 论文提出跨知识蒸馏（CKD）方法，通过语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题，提升SNN在DVS数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注事件数据有限和SNN架构不成熟，SNN性能不如ANN。为提升SNN在DVS数据上的表现，探索利用RGB数据和ANN进行知识蒸馏。

Method: 提出CKD方法，结合语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题。

Result: 在主流神经形态数据集（如N-Caltech101和CEP-DVS）上验证，性能优于当前最优方法。

Conclusion: CKD方法有效解决了跨模态和跨架构挑战，显著提升了SNN性能。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [39] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一个强化学习框架，用于提升多模态大语言模型（MLLMs）在医疗领域的置信度校准，同时提高任务准确性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在安全关键场景（如医疗）中的部署受到提示设计敏感性和高置信度错误响应的限制，需要改进其置信度校准以提高可靠性。

Method: 通过训练一个轻量级LLM生成上下文感知的辅助提示，指导下游任务MLLM生成置信度更准确的响应。

Result: 在PMC-VQA基准测试中达到最先进的医学视觉问答性能，并展示了零样本泛化能力。

Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs可信度方面的潜力，特别是在安全关键领域。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [40] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的盲运动去模糊（BMD）框架，通过预训练的GAN生成器和初始化器优化模糊核的初始估计，解决了传统方法对初始核敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统深度先验方法在盲运动去模糊中因优化过程的高度非凸性对初始模糊核极度敏感，限制了性能。

Method: 预训练一个基于GAN的核生成器来编码核先验分布，并设计一个核初始化器提供高质量的初始估计，从而约束解在紧凑的核流形内。

Result: 该方法在挑战性基准数据集上实现了最先进的性能，并能轻松集成到现有BMD方法中。

Conclusion: 提出的框架显著缓解了初始核敏感性问题，提升了盲运动去模糊的性能，且适用于非均匀运动去模糊。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [41] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出了一种语义感知的定位框架，通过联合估计深度和语义光线，结合结构-语义概率体积进行预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有楼层平面图定位技术主要关注深度结构线索，忽略了其中的丰富语义信息。

Method: 采用粗到细的方式构建概率体积，先稀疏采样生成低分辨率概率体积，再在高概率区域密集采样以细化预测。

Result: 在两个标准基准测试中显著优于现有方法，召回率显著提升，且能轻松整合额外元数据（如房间标签）以提高准确性和效率。

Conclusion: 语义信息的引入显著提升了楼层平面图定位的性能，展示了框架的灵活性和扩展性。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [42] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet利用深度信息增强手术阶段识别，结合RGB和深度数据，提出几何感知框架，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别在智能辅助系统中至关重要，但RGB图像的高视觉相似性和缺乏结构信息带来挑战，深度信息可提供几何线索。

Method: 提出Geo-RepNet，基于RepVGG框架，整合RGB和深度信息，包含DGPG模块提取几何先验和GEMA模块进行几何增强的多尺度注意力。

Result: 在真实ESD数据集上，Geo-RepNet实现最优性能，保持鲁棒性和高效计算。

Conclusion: 深度信息显著提升手术阶段识别性能，Geo-RepNet为复杂手术场景提供了有效解决方案。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [43] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，在少样本图像分类中表现优异，优于CNN原型网络，并在多个基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 利用Vision Transformers（ViTs）在少样本图像分类中的潜力，提升原型网络的性能。

Method: 将ViT-Small集成到原型网络中，通过平均支持样本的类别条件令牌嵌入构建鲁棒原型。

Result: 在5-shot设置下，ViT-ProtoNet在多个基准测试中表现优于CNN原型网络，提升3.2%准确率，并展示出更好的特征可分性。

Conclusion: ViT-ProtoNet是一种强大且灵活的少样本分类方法，为基于Transformer的元学习设定了新基准。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [44] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAA*的新方法，通过引入路径角度自由度（PAF）改进A*算法，以提升路径平滑性和相似性。


<details>
  <summary>Details</summary>
Motivation: 路径平滑性在专家演示的路径模仿学习中常被忽视，作者希望通过自适应路径平滑性提升路径相似性。

Method: 结合PAF的DAA*方法，通过联合优化路径缩短和平滑（分别对应启发式距离和PAF）来改进路径最优性。

Result: 在7个数据集上的实验表明，DAA*在路径相似性和长度上显著优于现有方法，如神经A*和TransPath。

Conclusion: DAA*在路径最优性和搜索效率之间存在轻微权衡，但在路径相似性和平滑性上表现优异。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [45] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: ALPHA是首个全面的RGBA基准测试，通过alpha混合适应标准RGB指标。ALPHAVAE是一种端到端的RGBA VAE，扩展了预训练的RGB VAE，在少量数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 透明或分层内容（RGBA图像）的生成缺乏大规模基准测试，限制了研究进展。

Method: 提出ALPHA基准和ALPHAVAE模型，结合alpha混合重建、补丁级保真度、感知一致性和双KL约束。

Result: ALPHAVAE在8K图像上训练，PSNR提升4.9 dB，SSIM提升3.2%，优于LayerDiffuse。

Conclusion: ALPHAVAE在透明图像生成中表现优异，代码和模型已开源。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [46] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 论文介绍了ProactiveBench，首个评估多模态对话系统主动交互能力的基准，并提出了PAUC指标以更准确地评估系统表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的深入，用户期望系统能更主动地交互，例如在视频播放时实时决定多轮回应时机。

Method: 提出ProactiveBench基准和PAUC指标，考虑模型回应的时序动态性，并通过用户研究验证其有效性。

Result: PAUC与传统指标相比更符合人类偏好，能更准确地评估主动交互场景中的用户体验。

Conclusion: PAUC为主动交互场景提供了更可靠的评估方法，推动了多模态对话系统的发展。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [47] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整类间混淆损失的音频-视频预训练编码器（DICCAE），通过细粒度的类别级对齐和认知对比，提升模型对相似活动的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视频预训练方法仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。

Method: 提出DICCAE编码器，动态调整类间混淆损失，并结合音频、视频及其融合的新训练框架；采用聚类引导的自监督预训练策略解决数据稀缺问题。

Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近SOTA性能。

Conclusion: DICCAE通过模块化设计和动态混淆损失，显著提升了音频-视频表示的质量和区分能力。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [48] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个用于3D多模态大语言模型（MLLM）的视觉令牌修剪框架，通过全局注意力预测和样本自适应修剪技术，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs在实际部署中因计算效率低下而面临挑战，主要原因是处理过多的对象中心视觉令牌。

Method: 提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应修剪（SAP）两项创新技术。

Result: 在五个基准测试中验证了Fast3D的有效性，尤其是在高修剪比例下表现优异。

Conclusion: Fast3D在不修改目标模型参数的情况下，显著提升了3D MLLMs的计算效率。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [49] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级神经网络方法，用于实时检测和描述天体表面特征，解决了传统方法计算量大和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的光度测量方法在计算能力和数据泛化性上存在局限，而新兴的基于学习的方法又因计算需求和数据稀缺难以在航天器上实时运行。

Method: 采用轻量级神经网络架构，结合改进的领域适应方法和注意力对齐机制，实现实时天体特征检测与描述。

Result: 提出的系统在性能上优于现有技术，适用于航天器实时任务。

Conclusion: 该方法为航天器自主导航和科学任务提供了高效解决方案。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [50] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 简单编码器架构结合预训练在交通异常检测中表现优异，超越复杂方法。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂架构是否必要，发现预训练能提升简单模型的性能。

Method: 使用Video ViTs作为编码器，研究预训练对性能的影响。

Result: 强预训练使简单模型性能超越复杂方法，自监督MVM效果最佳。

Conclusion: 预训练是关键，简单架构也能实现高效、可扩展的TAD。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [51] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个新颖的3D点云实例分割框架，结合了注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D点云实例分割和多模态理解中的局限性，提出一种统一且高效的解决方案。

Method: 构建分层特征提取器增强几何结构建模，通过对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。

Result: 相比Mask3D和ULIP等方法，SegVec3D在实例分割和多模态理解上表现更优，且需要更少的监督。

Conclusion: SegVec3D为3D点云实例分割和多模态理解提供了一种高效且实用的解决方案。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [52] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）的图像分类系统，用于自动化检测和分类八种常见作物病害，并通过移动平台为农民提供实时诊断和治疗建议。


<details>
  <summary>Details</summary>
Motivation: 作物病害对农业产量和全球粮食安全构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。

Method: 研究采用深度学习流程：从大型标记数据集中获取图像，通过调整大小、归一化和增强进行预处理，使用TensorFlow和Keras的Sequential API训练模型。CNN架构包括三个卷积层、ReLU激活、最大池化、展平和全连接层，最后通过softmax输出进行多类分类。

Result: 系统在训练数据上达到约90%的准确率，验证准确率约为60%，表明存在轻微过拟合。模型还集成了治疗建议模块，为检测到的病害提供农药或杀菌剂干预建议。

Conclusion: 该研究为精准农业提供了一个可扩展且易于使用的工具，通过结合深度学习和实际农艺支持，展示了CNN在作物健康监测和全球粮食生产中的潜力。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [53] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为LifelongPR的持续学习框架，用于解决点云地点识别（PCPR）中的灾难性遗忘问题，通过动态样本选择和提示学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 点云地点识别在自动驾驶等领域至关重要，但现有模型在适应新环境时会出现灾难性遗忘，导致性能下降。

Method: 提出动态样本选择方法和基于提示学习的持续学习框架，结合两阶段训练策略。

Result: 在公开和自采数据集上验证，性能显著优于现有方法，mIR@1提升6.50%，mR@1提升7.96%，F值降低8.95%。

Conclusion: LifelongPR框架有效解决了PCPR中的持续学习问题，提升了模型的适应性和实用性。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [54] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI技术，适用于资源有限的小型研究团队。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱数据量大、标注复杂、环境多变，且现有ML/AI工具难以整合到资源有限的研究中。

Method: 开发了一种低资源流程，支持数据本地处理，包括传输、推理和评估，并针对小型团队优化。

Result: 提供了一种实用解决方案，帮助研究者高效处理和分析大量相机陷阱数据。

Conclusion: 该流程为资源有限的研究团队提供了可行的数据处理方法，提升了数据利用效率。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [55] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 提出了一种结合半监督联邦学习、室内定位导航和视觉识别的跌倒检测框架，整体准确率达99.99%，兼顾高效与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 老龄化加剧导致跌倒风险增加，及时检测可降低医疗成本与恢复时间，同时需解决隐私问题。

Method: 采用SF2D（半监督联邦学习）、室内定位导航系统和视觉识别系统，结合可穿戴设备、边缘设备和机器人。

Result: SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，综合准确率达99.99%。

Conclusion: 该框架高效可靠且保护隐私，适用于老年人跌倒检测。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [56] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的多人物运动预测模型，通过简化时空交互，结合轻量级双分支设计和跨级别交互块，显著降低了计算成本并达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决多人物运动预测中个体运动与交互依赖的复杂性及高计算成本问题。

Method: 设计轻量级双分支学习局部和全局表示，引入跨级别交互块整合时空表示，并显式嵌入空间人际距离。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上实现最优性能，同时显著降低计算成本。

Conclusion: 提出的方法高效且性能优越，为多人物运动预测提供了新思路。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [57] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过双级知识对齐和任务置信度引导的适配器混合，提升了持续学习模型对误导任务ID的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调（PEFT）方法中因独立训练子模块导致特征子空间不对齐，从而产生模糊决策的问题。

Method: 提出双级知识对齐（DKA）和任务置信度引导的适配器混合（TC-MoA）两种创新方法。

Result: 实验表明CKAA优于现有PEFT-based CL方法。

Conclusion: CKAA通过特征对齐和自适应知识聚合，显著提升了模型对误导任务ID的鲁棒性。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [58] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net结合掩码图像建模和知识蒸馏技术，在双曲空间中高效训练模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉和语义概念通常具有层次结构，现有方法MERU在双曲空间中表现良好，但训练效率有待提升。

Method: 提出HMID-Net，在双曲空间中整合掩码图像建模和知识蒸馏技术，并设计专用蒸馏损失函数。

Result: 实验表明，该方法在图像分类和检索任务中显著优于MERU和CLIP等模型。

Conclusion: 双曲空间中的MIM和知识蒸馏技术能高效捕捉层次结构，性能媲美欧式空间。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [59] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频基准测试，旨在评估大型视觉语言模型（LVLM）是否能真正理解视频内容，而不仅仅是静态图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试类似图像基准测试，模型仅需扫描关键帧即可回答问题，无法评估LVLM是否具备深度时间推理能力。

Method: GLIMPSE包含3,269个视频和4,342个视觉中心问题，涵盖11个类别，所有问题需观看完整视频并推理。

Result: 人类评估准确率达94.82%，但最佳LVLM（GPT-o3）仅达66.43%，显示LVLM仍难以超越表面推理。

Conclusion: GLIMPSE揭示了LVLM在真正理解视频内容方面的局限性，为未来研究提供了方向。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [60] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出了一种自适应的张量正则化网络（SDTN）和轻量级网络（TRN），用于高光谱图像分类，解决了高维数据、冗余和标记样本不足的问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类在精准农业中至关重要，但传统方法难以处理高维数据、冗余和标记样本不足的问题。

Method: 结合张量分解和正则化机制动态调整张量秩，并通过轻量级网络提取多尺度特征。

Result: 在PaviaU数据集上表现出更高的分类精度和更少的模型参数。

Conclusion: 该方法适合资源受限环境下的实时部署。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [61] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权（CER）和多样性驱动的分布校准（DDC）解决现有缓存方法在分布偏移下的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在零样本任务中表现优异，但在无标注数据时对分布偏移的适应能力不足，测试时适应（TTA）方法应运而生。缓存方法虽有效，但面临熵不可靠和决策边界不灵活的问题。

Method: ReTA结合CER（利用一致性约束优化熵权重）和DDC（通过高斯分布建模文本嵌入以调整决策边界），提升缓存质量和预测准确性。

Result: 实验表明，ReTA在真实分布偏移场景下显著优于现有方法。

Conclusion: ReTA通过双重策略解决了缓存方法的可靠性问题，为VLM的测试时适应提供了更鲁棒的解决方案。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [62] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT团队提出了一种用于微手势在线识别的新方法，结合数据增强和时空注意力机制，显著提升了性能，并在IJCAI 2025 MiGA挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 微手势在线识别任务具有挑战性，需精确定位时间位置并区分类别，且微手势差异较大，传统方法难以胜任。

Method: 采用手工数据增强和时空注意力机制，提升模型对微手势的分类和定位能力。

Result: F1分数达到38.03，比之前最优方法提升了37.9%，在比赛中排名第一。

Conclusion: 该方法在微手势识别任务中表现出色，为未来研究提供了有效解决方案。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [63] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种后训练激活剪枝方法，通过移除冗余空间激活并恢复维度，提升VMamba等SSM模型的吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: VMamba等基于SSM的视觉主干模型在四向扫描中存在空间冗余，限制了性能。

Method: 提出QuarterMap方法，通过剪枝冗余激活和最近邻上采样恢复维度，无需重新训练。

Result: 在ImageNet-1K上，吞吐量提升11%，准确率下降小于0.9%；在ADE20K和MedMamba上也表现优异。

Conclusion: QuarterMap是一种即插即用的部署效率工具，适用于SSM模型，且不影响迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [64] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB提出了一种基于Schrödinger Bridge的无配对去雾框架，利用最优传输理论直接连接雾图和清晰图的分布，并通过细节保留正则化和提示学习提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法因生成器的传输映射能力有限，效果受限。

Method: 采用Schrödinger Bridge框架，结合最优传输理论和细节保留正则化，并引入提示学习利用CLIP模型。

Result: 在多个真实数据集上表现优越。

Conclusion: DehazeSB通过优化传输映射和细节保留，显著提升了无配对去雾的效果。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [65] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct是一种多模态大语言模型，通过内容感知的标记化策略和显式布局建模，显著提升了密集文档的理解能力，并在KIE任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在密集文档上表现不佳，且视觉标记化方法因图像尺寸导致计算冗余和内存效率低下。

Method: VDInstruct采用内容感知的标记化策略，根据文档复杂度生成标记，并结合三阶段训练范式。

Result: 模型在KIE基准测试中达到SOTA，图像标记数量减少约3.6倍，零样本评估中F1分数超越基线+5.5点。

Conclusion: 内容感知标记化与显式布局建模为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [66] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种动态RPCA网络（DRPCA-Net），通过结合稀疏先验和深度学习，提高了红外小目标检测的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在红外小目标检测中过于复杂且缺乏可解释性，忽略了目标固有的稀疏性先验。

Method: 提出DRPCA-Net，结合动态展开机制和轻量级超网络，自适应生成参数，并设计了动态残差组模块以优化背景建模。

Result: 在多个公开红外数据集上，DRPCA-Net显著优于现有方法。

Conclusion: DRPCA-Net通过动态建模和稀疏先验，实现了高效且鲁棒的红外小目标检测。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [67] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——Sequential CSIST Unmixing，旨在从高密度红外小目标群中检测所有目标，并通过DeRefNet模型和SeqCSIST数据集解决了这一挑战。


<details>
  <summary>Details</summary>
Motivation: 由于光学镜头焦距和红外探测器分辨率的限制，远距离红外小目标群在图像中常表现为混合斑点，现有方法难以精确检测。

Method: 提出了Deformable Refinement Network (DeRefNet)，包含Temporal Deformable Feature Alignment (TDFA)模块，用于自适应帧间信息聚合。

Result: 在SeqCSIST数据集上，方法优于现有技术，mAP指标提升5.3%。

Conclusion: 论文首次在多帧范式中解决CSIST Unmixing任务，并提供了公开数据集和工具包以促进研究。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [68] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 提出了一种分段架构EHPE，通过局部提取指尖和手腕关节，减少误差累积，提升手部姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视指尖和手腕关节的重要性，导致远端关节误差累积，影响整体姿态估计质量。

Method: EHPE分为两个阶段：TW-stage提取指尖和手腕关节，PG-stage通过双分支交互网络优化其余关节位置。

Result: 在两个广泛使用的基准测试中，EHPE达到了最先进的性能。

Conclusion: EHPE通过分段架构显著提升了手部姿态估计的精度，解决了误差累积问题。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [69] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文首次全面调查了Segment Anything Model（SAM）及其变体中的提示工程技术，系统梳理了该领域的方法、应用和挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM通过基于提示的方法革新了图像分割，但提示工程的关键作用尚未被充分研究。本文旨在填补这一空白。

Method: 系统整理并分析了SAM中提示工程技术的发展，包括基础方法、实际应用和关键挑战。

Result: 研究发现提示工程已从简单的几何输入发展为多模态方法，支持SAM在医学影像和遥感等领域的应用。

Conclusion: 本文为理解和发展分割基础模型中的提示工程提供了结构化框架，并指出了未来的研究方向。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [70] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个交互式艺术字体系统，结合扩散模型和大语言模型，支持局部编辑、迭代优化和多语言输入，显著提升了艺术字体生成的交互性。


<details>
  <summary>Details</summary>
Motivation: 传统艺术字体设计依赖手工操作，现有生成模型在交互性、局部编辑和多字符组合方面存在局限。

Method: WordCraft采用无训练的局部注意力机制和噪声混合技术，结合大语言模型解析用户提示。

Result: 系统能够高质量生成单字符和多字符的艺术字体，支持多语言和多样化用户需求。

Conclusion: WordCraft提升了艺术字体生成的交互性和创造性，为设计师提供了更多可能性。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [71] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一个新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，提升生成控制性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在视觉控制、多模态输入平衡和复杂多模态图像生成训练需求方面的局限性。

Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段。

Result: 在DreamBench++基准测试中表现优异，优于基线模型，图像重建保真度高，任务适应性强。

Conclusion: MENTOR在模型规模、训练资源有限的情况下仍能高效生成高质量多模态图像。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [72] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2是一种无需训练的视频对象分割策略，通过上下文感知和遮挡弹性记忆模型，显著提升了SAM2在复杂手术视频中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 手术视频分割对提升手术质量和患者结果至关重要，但SAM2在复杂手术视频中因贪婪选择内存设计表现不佳。

Method: 提出MA-SAM2，结合上下文感知和遮挡弹性记忆模型，采用多目标、单循环、单提示推理。

Result: 在EndoVis2017和EndoVis2018数据集上，MA-SAM2性能分别提升4.36%和6.1%。

Conclusion: MA-SAM2在无需额外参数或训练的情况下，显著提升了手术视频分割的鲁棒性和准确性。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [73] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一个基于扩散模型的文本生成图像模型，性能优于主流模型，但缺乏官方技术文档。


<details>
  <summary>Details</summary>
Motivation: 通过逆向工程解析FLUX的架构，以支持未来研究和开发。

Method: 从源代码中逆向工程FLUX的架构。

Result: 生成了非官方的技术报告，解析了FLUX的架构。

Conclusion: 该报告为FLUX的进一步研究提供了基础，但未获官方认可。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [74] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former通过优化密集令牌处理的计算分配，提出四种关键改进，解决了交互式分割中速度与质量的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割方法在密集令牌处理上速度慢，而稀疏令牌方法（如SAM）牺牲了分割质量。Inter2Former旨在平衡速度与精度。

Method: 1. 动态提示嵌入（DPE）减少背景令牌开销；2. 动态混合注意力（DHA）优化边界与非边界区域计算；3. 混合专家（HMoE）并行处理；4. 动态局部上采样（DLU）精细化处理。

Result: 实验表明，Inter2Former在高精度基准测试中实现了SOTA性能，并在CPU设备上高效运行。

Conclusion: Inter2Former通过自适应计算策略，显著提升了交互式分割的速度与质量，适用于实际应用。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [75] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR方法通过动态对齐图像和文本特征，提出了一种改进的无监督适应方法，显著提升了细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度分类中无法捕捉动态的类别差异，或计算成本过高。

Method: FAIR通过Class Description Anchors动态对齐局部图像特征和语言嵌入，并使用Learned Alignment Score优化自训练。

Result: 在13个细粒度数据集上，FAIR比现有方法平均提升2.78%。

Conclusion: FAIR通过动态跨模态交互和伪标签优化，显著提升了无监督适应性能。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [76] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA是一个基于区域引导的少样本异常图像-掩码对生成框架，利用预训练的潜在扩散模型生成真实、多样且语义对齐的异常样本。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常样本稀缺，现有方法在定位和分类任务中效果受限，且现有异常合成方法存在低真实性、掩码对齐不准确和泛化性差的问题。

Method: GAA通过局部概念分解联合建模异常语义特征和空间信息，自适应多轮异常聚类细化语义表示，区域引导掩码生成确保对齐，并引入低质量样本过滤模块。

Result: 在MVTec AD和LOCO数据集上，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。

Conclusion: GAA通过区域引导和少样本生成，显著提升了异常合成的真实性和下游任务性能。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [77] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MaxViT的人工智能框架，用于CT扫描图像的多类中风分类（缺血性、出血性和无中风），并通过数据增强和XAI技术提高了模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其是在紧急情况下。

Method: 采用MaxViT作为主要深度学习模型，结合数据增强和XAI技术（如Grad-CAM++），提升分类性能和模型透明度。

Result: MaxViT模型在增强数据训练下达到98.00%的准确率和F1分数，优于其他模型和基线方法。

Conclusion: 该研究为临床提供了一种可信赖的AI辅助诊断工具，有助于早期中风检测和紧急诊断，挽救更多生命。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [78] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 论文评估了AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，探讨了解缠技术对减少偏见的有效性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查成本高且难以普及，AI算法提供了可扩展的解决方案，但公平性和泛化性仍是问题。

Method: 使用mBRSET数据集，训练了ConvNeXt V2、DINOv2和Swin V2三种模型预测DR和敏感属性（如年龄和性别），并评估公平性及解缠技术效果。

Result: 所有模型在DR预测中表现优异（最高94% AUROC），但公平性评估显示年龄组间存在10% AUROC差距。解缠技术对模型效果影响不一。

Conclusion: 解缠技术在医学影像AI中效果复杂，需重视公平性以确保医疗解决方案的公正性和可靠性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [79] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一个新颖的眼部分割框架，通过贝叶斯不确定性学习解决运动模糊、眼睑遮挡和域差距问题，显著提升了分割和注视估计的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动模糊、眼睑遮挡和域差距情况下表现不佳，影响了AR/VR中的人机交互体验。

Method: 设计了一个基于贝叶斯不确定性学习的眼部分割框架，显式建模不确定性，并通过后验学习量化分割不确定性。

Result: EyeSeg在MIoU、E1、F1和ACC等指标上优于现有方法，尤其在运动模糊和跨域挑战下表现突出。

Conclusion: EyeSeg通过不确定性感知的分割框架，显著提升了眼部分割和注视估计的鲁棒性和准确性。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [80] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose是一种基于WiFi的深度学习框架，用于精确连续的人体姿态估计，通过双流时空注意力架构和速度建模分支提升性能，在智能家居场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: WiFi姿态估计因其穿透性和隐私优势成为非视觉替代方案，但现有方法在精细运动捕捉和准确性上存在不足。

Method: 提出ViSTA-Former双流架构，分别捕捉时间依赖和关节结构关系，并集成速度建模分支增强细微运动敏感性。

Result: 在自建数据集上PCK@50准确率达92.2%，优于现有方法8.3%，在公共MMFi数据集上也验证了其鲁棒性。

Conclusion: VST-Pose为室内连续运动分析提供了可靠且隐私保护的解决方案。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [81] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于高分辨率数字高程模型（DEM）的绝对全局映射，实现了从30米到30厘米的100倍分辨率提升。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程估计对水文、城市形态和生态系统研究至关重要，但现有方法受限于放大因子或缺乏全局高程背景。

Method: 利用低分辨率SRTM高程数据作为提示，结合高分辨率NAIP RGB图像，通过视觉变换器编码器和LiDAR DEM进行微调，实现DEM估计、填补和更新。

Result: 在三个美国景观中验证，分辨率提升100倍，MAE低于5米，比SRTM提升18%，适用于水文和环境研究。

Conclusion: 该框架具有鲁棒性和可扩展性，适用于大区域应用，代码和模型已公开。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [82] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 论文提出自动生成多学科科学实验评论的任务，构建了首个相关数据集ExpInstruct，并开发了检索增强模型ExpStar，显著优于现有大型多模态模型。


<details>
  <summary>Details</summary>
Motivation: 解决人工教师准备实验评论耗时且依赖专业知识的问题，探索大型多模态模型在生成细粒度实验评论方面的潜力。

Method: 构建ExpInstruct数据集（7K+评论，21学科），提出检索增强模型ExpStar，自适应访问和利用外部知识。

Result: ExpStar在实验中显著优于14种领先的大型多模态模型，验证了数据集和模型的有效性。

Conclusion: ExpStar在AI辅助科学实验教学方面具有重要潜力，为自动生成实验评论提供了新方向。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [83] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文系统分类并比较了令牌压缩技术，评估了其在标准和紧凑型ViT架构上的表现，发现这些方法在紧凑型设计中效果较差。


<details>
  <summary>Details</summary>
Motivation: 解决令牌压缩技术在ViT加速中的两大空白：缺乏统一分类和比较，以及其在紧凑型ViT上的有效性未验证。

Method: 提出首个系统分类和比较研究，评估代表性技术在标准和紧凑型ViT架构上的表现。

Result: 令牌压缩方法在通用ViT中有效，但在紧凑型设计中表现不佳。

Conclusion: 研究为未来适应紧凑型Transformer的令牌优化技术提供了方向，适用于边缘AI和AI代理应用。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [84] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 论文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过调整优化顺序和使用线性化模型，解决了传统VSD方法收敛慢和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统变分分数蒸馏（VSD）方法在实践中存在收敛慢和不稳定的问题，论文旨在通过改进优化顺序和引入线性化模型提升生成质量。

Method: 论文提出$L^2$-VSD方法，通过调整优化顺序使分数模型提前感知3D状态，并使用线性化模型避免过拟合。

Result: 实验表明$L^2$-VSD在生成质量上优于现有方法，并能无缝集成到其他VSD框架中。

Conclusion: $L^2$-VSD通过改进优化策略和线性化模型，显著提升了文本到3D生成的效率和稳定性。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [85] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出了一种高效混合（几何和图像）方法，用于计算碎片对的最优对齐，无需假设其形状、尺寸或图像内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理现实几何特性的碎片时表现不佳，或依赖碎片形状限制。

Method: 结合几何和图像信息，提出新的兼容性计算方法，并引入新的碎片数据集和侵蚀模型。

Result: 在RePAIR 2D数据集上展示了最先进的邻域级精度和召回率。

Conclusion: 该方法显著提升了兼容性计算性能，适用于考古拼图等实际应用。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [86] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine提出了一种改进的负标签细化框架，用于零样本OOD检测，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提升了检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误判问题，主要由于负标签可能是分布内标签的子类别或专有名词，且无法有效处理图像匹配多个标签的情况。

Method: NegRefine通过过滤机制排除负标签集中的子类别和专有名词，并引入多匹配感知评分函数动态调整多个标签的贡献。

Result: 在ImageNet-1K等大规模基准测试中，NegRefine表现出更鲁棒的OOD检测性能。

Conclusion: NegRefine通过改进负标签选择和评分机制，显著提升了零样本OOD检测的准确性。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [87] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident是一个大规模视觉语言基准，用于评估多模态大语言模型在涉及弱势道路使用者的高风险交通场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中弱势道路用户（VRUs）安全问题，缺乏标准化基准评估多模态大语言模型在复杂安全关键场景中的推理能力。

Method: 提出VRU-Accident基准，包含1K真实事故视频、6K多选题对和1K密集场景描述，评估17种先进模型。

Result: 模型在视觉属性上表现良好，但在事故原因、类型和可预防性推理方面存在显著挑战。

Conclusion: VRU-Accident填补了评估多模态大语言模型在VRU安全场景中的空白，揭示了模型在复杂推理任务中的不足。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [88] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 论文探讨了人类和深度学习模型在识别3D形状时的表现差异，发现视觉变换器模型更接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型是否形成与人类相似的3D形状表征。

Method: 通过实验比较人类和两种深度学习模型（DGCNN和点变换器）在点密度、对象方向和局部几何结构变化下的表现。

Result: 点变换器模型在解释人类表现上优于卷积模型，因其支持3D形状的层次抽象。

Conclusion: 点变换器模型在3D形状表征上更接近人类视觉机制。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [89] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 该论文综述了基于多模态大语言模型（MLLMs）的视觉丰富文档理解（VRDU）的最新进展，包括特征编码与融合方法、训练范式及数据集，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档理解（VRDU）的需求推动了自动处理复杂文档的研究，多模态大语言模型（MLLMs）在此领域展现出潜力。

Method: 论文总结了MLLM在VRDU中的三大核心组件：特征编码与融合方法、训练范式（如预训练和指令调优）及数据集。

Result: 综述了MLLM在VRDU中的应用，强调了其在特征融合和训练策略上的进展。

Conclusion: 未来需提升VRDU系统的效率、泛化性和鲁棒性，并探索更多应用方向。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [90] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 论文介绍了SpeakerVid-5M数据集，首个为音频-视觉双模态交互虚拟人生成设计的大规模高质量数据集，包含超过520万视频片段，并提供了基准模型和评估指标。


<details>
  <summary>Details</summary>
Motivation: 大规模模型的快速发展推动了数字人领域的突破，但音频-视觉双模态交互虚拟人仍是一个挑战，需要高质量数据集支持研究。

Method: 构建SpeakerVid-5M数据集，按交互类型（对话分支、单分支、倾听分支和多轮分支）和数据质量（预训练子集和高质量SFT子集）分类，并提供AR基准模型和评估指标VidChatBench。

Result: 数据集包含520万视频片段，总计8743小时，覆盖多种交互类型，并提供了基准模型和测试数据。

Conclusion: SpeakerVid-5M为音频-视觉双模态交互虚拟人研究提供了重要资源，数据集和代码将公开，推动未来研究。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [91] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本信息进行推理，忽视了视觉模态的重要性，而人类在推理时会自然结合视觉内容。

Method: 构建了Video-Text Interleaved Benchmark (ViTIB)，并提出了ViTCoT范式，结合视觉和文本信息进行推理。

Result: 实验表明，ViTCoT显著优于传统仅文本的CoT范式，并激活了更多MLLMs的神经元值。

Conclusion: ViTCoT为视频理解领域提供了一种更直观且认知对齐的推理方法。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [92] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 论文提出了一种4D人体解析框架，解决了现有方法依赖封闭数据集和推理时间长的问题，通过减少推理时间并引入开放词汇能力。


<details>
  <summary>Details</summary>
Motivation: 动态3D人体表示在虚拟和扩展现实应用中日益重要，但现有的人体部分分割方法受限于封闭数据集和长推理时间，限制了其适用性。

Method: 基于最先进的开放词汇3D人体解析技术，提出三个关键创新：1）采用基于掩码的视频对象跟踪建立时空对应；2）设计掩码验证模块管理新目标识别和跟踪失败；3）提出4D掩码融合模块，结合记忆条件注意力和对数均衡。

Result: 实验证明该方法在4D人体解析任务中有效且灵活，相比之前的方法加速高达93.3%。

Conclusion: 该框架显著提升了4D人体解析的效率和灵活性，适用于开放词汇场景。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [93] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS框架通过因果引导的对抗方法生成反事实解释，避免虚假因素的干扰，提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。

Method: 提出CECAS框架，结合因果视角生成反事实解释，避免虚假因素的扰动。

Result: 在多个基准数据集上优于现有方法，实现有效性、稀疏性、邻近性和真实性的平衡。

Conclusion: CECAS通过因果引导显著提升反事实解释的质量和实用性。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [94] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过先学习光谱模式再估计映射，解决了RGB到HSI转换的高维挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到HSI的映射，忽略了低维到高维转换的固有挑战。

Method: MCGA采用两阶段方法：1) 多尺度VQ-VAE学习光谱模式，提取混合码本；2) 通过查询码本特征优化映射。引入灰度感知注意力和量化自注意力提升重建质量。

Result: 实验表明MCGA达到了最先进的性能。

Conclusion: MCGA通过物理驱动的注意力机制和测试时适应策略，实现了轻量高效的HSI重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [95] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 论文提出了一种严格旋转等变的单阶段检测器MessDet，通过改进网络结构和多分支头设计，在低参数量下实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 解决航空图像中物体方向任意性带来的旋转等变性问题，探索严格旋转等变对检测性能的影响。

Method: 实现严格旋转等变的骨干和颈部网络，提出多分支头设计以减少参数并提高精度。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上达到最优性能，且参数量极低。

Conclusion: 严格旋转等变对航空图像检测性能有显著提升，多分支头设计有效平衡了参数和精度。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [96] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD利用多模态理解和扩散模型，通过自然语言指令快速生成可编辑的多模态图层，解决了传统方法缺乏创造力和可编辑性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统图形设计方法缺乏创造力和智能性，且现有扩散模型生成的文件不可编辑，文本渲染效果差，无法满足自动化设计需求。

Method: IGD结合参数化渲染和图像资产生成，利用MLLM的多模态理解能力预测属性、排序和布局，并通过扩散模型生成图像内容。

Result: 实验结果表明，IGD在复杂图形设计任务中具有可扩展性和扩展性，提供了新的解决方案。

Conclusion: IGD为图形设计提供了一种高效、可编辑且智能的新方法。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [97] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 论文提出Crucial-Diff框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成方法生成的样本重复或简单，无法针对下游模型的弱点提供关键信息。

Method: Crucial-Diff框架包含SAFE模块（统一特征提取器）和WASM模块（基于下游模型反馈生成难检测样本），两者结合生成高质量训练数据。

Result: 在MVTec上达到83.63%的像素级AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff能有效生成多样且高质量的训练数据，显著提升下游任务的性能。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [98] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文评估了GPT-4o-mini和Gemini 2.0 Flash在时尚产品属性识别任务中的零样本性能，发现Gemini 2.0 Flash表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在细粒度时尚属性识别中的潜力，以提升电商产品目录的组织和客户发现体验。

Method: 使用DeepFashion-MultiModal数据集，仅以图像为输入，评估模型在18个时尚属性类别中的表现。

Result: Gemini 2.0 Flash的宏F1得分为56.79%，优于GPT-4o-mini的43.28%。

Conclusion: Gemini 2.0 Flash在零样本任务中表现最佳，但需领域微调以进一步提升性能，为时尚AI研究奠定基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [99] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出了一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在极低电子剂量下实现原子级分辨率的电子显微镜成像。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜在光束敏感材料（如蛋白质和二维材料）中的应用受到辐射损伤的限制，需要突破传统电子显微镜的电子剂量限制。

Method: 通过融合多个低分辨率、亚像素位移的图像，并利用CNN增强重建，开发了一种双路径注意力引导网络，用于4D-STEM。

Result: 在极低剂量条件下，实现了与传统ptychography相当的空间分辨率，适用于非晶、半晶和晶体光束敏感样品。

Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [100] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，通过结合Dempster-Shafer证据理论和变分Dirichlet分布，提升了不确定性和多视图融合的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖KL散度估计不确定性，忽略了模态间的领域差距，导致多视图集成和决策的可靠性不足。

Method: KPHD-Net利用变分Dirichlet分布表示类别概率分布，结合Hölder散度和Dempster-Shafer证据理论，并通过Kalman滤波器增强融合结果的可靠性。

Result: 实验表明，KPHD-Net在分类和聚类任务中的准确性、鲁棒性和可靠性均优于现有方法。

Conclusion: KPHD-Net通过改进的不确定性估计和多视图融合方法，显著提升了多视图学习的性能。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [101] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 论文分析了潜在扩散模型（LDMs）中自编码器的关键属性，提出了一种新型自编码器VMAE，显著提升了图像生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在图像生成方面潜力巨大，但自编码器的理想属性和优化设计尚未充分探索。

Method: 提出了Variational Masked AutoEncoders（VMAEs），结合了Masked AutoEncoder的层次特征，并将其集成到LDM框架中，形成LDMAEs。

Result: 实验表明，LDMAEs显著提升了图像生成质量和计算效率。

Conclusion: VMAEs通过同时满足潜在平滑性、感知压缩质量和重建质量三个关键属性，优化了LDMs的性能。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [102] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 3DGAA是一种基于3D高斯散射的对抗攻击框架，通过联合优化几何和外观属性，生成物理上可实现且鲁棒的对抗物体，显著降低目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理攻击在平衡物理真实性和攻击鲁棒性方面存在不足，需要一种更全面的方法。

Method: 利用3D高斯散射的14维参数化，联合优化几何（形状、尺度、旋转）和外观（颜色、透明度）属性，并引入物理过滤和增强模块。

Result: 在虚拟和物理实验中，3DGAA将检测mAP从87.21%降至7.38%，显著优于现有方法，并具有高迁移性。

Conclusion: 3DGAA为评估自动驾驶感知系统安全性提供了实用的对抗攻击框架。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [103] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 该研究利用多壳扩散MRI数据和视觉变换器深度学习框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测。


<details>
  <summary>Details</summary>
Motivation: 通过多壳扩散MRI数据的微观结构信息，结合深度学习技术，提高阿尔茨海默病和淀粉样蛋白积累的早期诊断准确性。

Method: 采用Swin Transformer模型对多壳扩散MRI数据进行分类，提取DTI和NODDI关键指标，并通过低秩适应技术优化模型。

Result: 在区分认知正常与阿尔茨海默病痴呆时，平衡准确率达95.2%；淀粉样蛋白检测的平衡准确率为77.2%（轻度认知障碍/阿尔茨海默病痴呆）和67.9%（认知正常）。

Conclusion: 研究表明，扩散MRI和变换器架构在阿尔茨海默病早期检测和淀粉样蛋白病理学中有潜力，支持数据有限的生物医学诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [104] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 论文综述了反无人机跟踪技术的现状与挑战，整理了公开数据集，分析了近年来的视觉和视觉融合算法，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的快速发展和广泛应用，反无人机跟踪在公共安全、边境巡逻等复杂环境中的重要性日益凸显。

Method: 回顾反无人机检测与跟踪技术的特点与挑战，整理公开数据集，分析近年来的视觉和视觉融合算法。

Result: 总结了当前主流技术和数据集，为研究者提供了解决相关挑战的资源。

Conclusion: 论文为反无人机跟踪领域的未来发展提供了有价值的见解和研究方向。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [105] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种名为I-BSC的方法，通过加权求和同质条纹图像而非相位帧，解决了P-BSC的高计算开销和误差累积问题，显著提升了动态3D扫描的性能。


<details>
  <summary>Details</summary>
Motivation: PSP在动态测量中因物体运动导致误差，现有方法P-BSC虽有效但计算复杂且误差累积。

Method: 提出I-BSC，加权求和同质条纹图像，仅计算一次反正切函数，降低计算复杂度。

Result: I-BSC在减少运动误差和计算效率上优于现有方法，实现高像素-深度-时间分辨率的3D重建。

Conclusion: I-BSC显著提升了动态3D扫描的性能，计算效率更高，误差收敛更快。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [106] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 论文重新评估了两种CLIP变体（ResNet和ViT）是否表现出类似人类的bouba-kiki效应，发现模型表现不一致且与人类认知差距显著。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉-语言模型（VLMs）是否以类似人类的方式整合跨模态信息，特别是bouba-kiki效应。

Method: 使用基于提示的概率评估和Grad-CAM视觉注意力分析，对比人类实验数据。

Result: 模型未一致表现bouba-kiki效应，ResNet偏好圆形但整体表现不足，与人类认知差距明显。

Conclusion: VLMs在跨模态概念理解和人类直觉对齐方面存在局限性。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [107] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: 论文提出了一种名为Hyma的新方法，通过超网络技术优化多模态基础模型中的单模态模型选择和连接器训练，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练连接器模块以对齐单模态模型时计算成本高，且单模态模型选择复杂，亟需高效解决方案。

Method: 利用超网络的参数预测能力，为多种单模态模型组合联合训练连接器模块，实现高效模型选择和训练。

Result: 实验表明，Hyma将最优单模态模型对的搜索成本降低10倍，同时性能与网格搜索相当。

Conclusion: Hyma是一种高效的多模态模型对齐方法，显著降低了计算负担，适用于大规模应用。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [108] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种选择性优化框架，结合低分辨率图像的反向传播（BP-low）和高分辨率图像的零阶优化（ZO-high），以实现内存高效且高质量的文本到图像扩散模型个性化。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上适应文本到图像扩散模型时的内存效率和隐私保护问题。

Method: 通过时间步感知的概率函数动态选择BP-low或ZO-high优化策略，结合两者的优势。

Result: 实验表明，该方法在显著降低内存消耗的同时保持高性能，支持高质量设备端个性化。

Conclusion: 该框架实现了内存高效、高质量的模型个性化，适用于资源受限的边缘设备。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [109] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo是一种新型多模态Transformer，用于漫画书页面流分割（PSS），在视觉和多模态版本中均优于传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 漫画书页面流分割是自动化内容理解的关键任务，为角色分析、故事索引等下游任务提供基础。

Method: 提出CoSMo模型，构建了20,800页的标注数据集，并在视觉和多模态版本中验证其性能。

Result: CoSMo在F1-Macro、Panoptic Quality等指标上显著优于传统模型，视觉特征主导宏观结构，多模态有助于解决模糊问题。

Conclusion: CoSMo为漫画书分析设立了新标准，推动了可扩展的自动化分析。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [110] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 提出一种轻量级机器学习方法，通过分析家禽粪便图像检测疾病，实现高准确率且低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖易受传染病影响，需低成本、高效的疾病检测方法。

Method: 采用多颜色空间特征提取和多种描述符，结合PCA和XGBoost降维，使用ANN分类器。

Result: 模型准确率达95.85%，无需GPU，执行时间短，资源消耗低。

Conclusion: 该方法为低资源农业环境提供了一种高效、可解释的疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [111] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一种新颖的前馈模型，能够在一秒内从单目视频合成4D动态新视角。它通过高斯基元的像素对齐网格表示动态3D场景，并显式监督其随时间变化的运动。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以统一建模动态场景的外观、几何和运动，MoVieS旨在填补这一空白，实现新视角合成、重建和3D点跟踪的统一框架。

Method: MoVieS使用像素对齐的高斯基元网格表示动态3D场景，并显式监督其运动，结合大规模数据集训练，减少对任务特定监督的依赖。

Result: MoVieS在多个任务中表现出高效性和有效性，性能竞争性强，速度提升显著。

Conclusion: MoVieS通过统一建模动态场景的外观、几何和运动，支持多种零样本应用，为动态场景分析提供了高效解决方案。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [112] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于小波变换的频率域调节机制，通过分别调整低频和高频子带，显著改善了扩散模型的生成质量，并提供了对曝光偏差的鲁棒解决方案。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现出色，但受曝光偏差影响较大。作者发现预测噪声图像的能量在扩散过程中降低，且低频和高频子带的能量减少模式不同，这导致了网络重建的干净数据与真实数据之间的幅度变化。

Method: 引入了一种基于小波变换的频率域调节机制，分别调整低频和高频子带，并对曝光偏差在子带中的影响进行了更准确的分析。该方法无需训练，即插即用。

Result: 该方法显著提升了多种扩散模型的生成质量，并为不同模型架构下的曝光偏差提供了鲁棒解决方案。

Conclusion: 通过频率域调节机制，论文有效解决了扩散模型中的曝光偏差问题，提升了生成质量，且方法通用性强。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [113] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer的两阶段迁移学习策略，显著提升了遥感图像水体分割的精度，解决了域偏移和小样本问题。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像水体分割中域偏移和小样本的挑战，特别是在复杂地形和光谱特征的地区。

Method: 采用两阶段迁移学习策略：先在多样源域训练基础模型，再在目标域微调。

Result: 目标域的水体分割IoU从25.50%提升至64.84%。

Conclusion: 该策略有效解决了域差异导致的性能下降，为数据稀缺且环境独特的遥感场景提供了高精度信息提取的技术范例。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [114] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP通过双分支训练、区域提示和分层特征对齐模块，解决了CLIP在长文本任务中的输入限制问题，并在长短文本检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CLIP在短文本任务中表现良好，但在长文本输入（>77 token）任务中受限，因此需要改进以适应长文本任务。

Method: 提出FIX-CLIP，包括双分支训练、多区域提示和分层特征对齐模块，并利用合成数据训练。

Result: FIX-CLIP在长短文本检索任务中达到最优性能，并在扩散模型中表现出色。

Conclusion: FIX-CLIP有效解决了CLIP的长文本输入问题，并在多任务中表现优异。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [115] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出了一种多摄像头多目标跟踪框架，通过轨迹和外观特征实现跨视角的全局身份一致性分配。


<details>
  <summary>Details</summary>
Motivation: 解决多摄像头环境下目标跟踪中全局身份一致性的问题。

Method: 采用BoT-SORT单摄像头跟踪，结合轨迹特征匹配初始化全局ID，后续通过优先全局匹配策略更新ID，并使用3D位置进行空间验证。

Result: 实现了跨视角的全局身份一致性分配。

Conclusion: 该框架在多摄像头多目标跟踪中有效确保了身份一致性。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [116] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 提出了一种新的半监督全景分割方法DEARLi，通过结合两个基础模型，显著提升了识别和定位能力，在少量标注数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决像素级标注成本高的问题，探索基础模型在半监督分割中的有效利用。

Method: 结合无监督掩码变换器一致性和CLIP特征的零样本分类增强识别，通过SAM伪标签进行类无关解码器预热增强定位。

Result: 在ADE20K数据集上仅用158张标注图像，实现了29.9 PQ和38.9 mIoU，性能优于现有方法且GPU内存需求更低。

Conclusion: DEARLi方法在半监督场景下表现卓越，为小样本分割任务提供了高效解决方案。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [117] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 该论文研究了现代点跟踪方法在超声心动图中的潜力，通过改进训练策略和提出轻量级网络，显著提高了运动估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂的超声心动图运动估计中存在局限性，而现代点跟踪方法在该领域的应用尚未充分探索。

Method: 通过分析心脏运动偏差，改进了训练策略并引入定制化增强方法，同时提出了一种轻量级网络。

Result: 实验表明，改进后的方法显著提升了性能，例如EchoTracker在位置准确性和轨迹误差上分别提高了60.7%和61.5%。

Conclusion: 尽管某些点跟踪模型表现不佳，但改进后的方法在临床评估中显示出更好的可重复性，更接近专家验证的工具。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [118] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过循环优化内部状态，显著提升了模型在噪声条件下的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈连接优化感知，而人工神经网络多为前馈结构。本文旨在通过反馈机制提升模型的鲁棒性和适应性。

Method: 在U-Net架构中引入反馈循环，结合软最大投影和指数衰减操作确保稳定性，并在合成分割任务中验证效果。

Result: 反馈模型在噪声条件下表现更优，仅需两个训练样本即可超越随机性能，而前馈模型需至少四个样本。

Conclusion: 反馈机制增强了模型的鲁棒性和数据效率，为更接近生物启发的神经网络架构提供了方向。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [119] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard是一种基于AI的视频系统，用于实时自动评估混凝土流动性，替代传统耗时且不一致的人工坍落度测试。


<details>
  <summary>Details</summary>
Motivation: 传统坍落度测试方法手动、耗时且结果不一致，无法满足实时监测需求。

Method: 开发了SlumpGuard系统，通过AI分析混凝土流动视频，实现全批次自动检测。

Result: 系统在实际部署中表现出高效性和准确性，提升了质量控制。

Conclusion: SlumpGuard是混凝土质量保证的实用解决方案。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [120] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术解决了合成数据与真实数据之间的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和手动标注的高成本，合成数据成为预训练模型的流行选择，但合成数据与真实数据之间的领域差距（如光照、颜色和视角差异）限制了预训练-微调范式的效果。

Method: 提出了一种统一的文本人物检索流程，包含图像级别的域感知扩散（DaD）和区域级别的多粒度关系对齐（MRA）。DaD用于迁移图像分布，MRA通过建立视觉区域与描述性句子之间的对应关系进行精细对齐。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果。

Conclusion: 双级别域适应方法有效解决了合成数据与真实数据之间的领域差距问题，显著提升了文本人物检索的性能。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [121] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP框架通过两阶段方法提升MLLM在高分辨率图像上的性能，无需训练且任务无关。


<details>
  <summary>Details</summary>
Motivation: MLLM在高分辨率图像上表现不佳，因固定分辨率训练导致泛化能力差，而降低分辨率又损失细节。

Method: 提出ECP框架：先通过降采样图像预测候选区域，再基于候选区域进行最终预测。

Result: 在4K GUI grounding和4K、8K MLLM感知任务中，分别提升21.3%、5.8%、5.2%。

Conclusion: ECP有效解决了MLLM在高分辨率图像上的性能问题，同时保留了细节。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [122] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化提升多模态学习性能，证明模态依赖比例与方差比例成反比时性能最优。


<details>
  <summary>Details</summary>
Motivation: 现有方法认为多模态学习性能不佳是由于模态间学习不平衡，但本文认为不平衡学习反而是最优设置。

Method: 提出ARL策略，通过辅助正则化器计算模态预测方差，并基于方差比例重新加权优化，同时引入预测偏差最小化泛化误差。

Result: 在多个数据集上的实验验证了ARL的有效性和通用性。

Conclusion: ARL无需额外参数，独立于多模态模型结构和融合方法，显著提升了多模态学习性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [123] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 该研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证情绪表达是否真的如Ekman假设的那样具有普遍性，还是受到种族背景的影响。

Method: 构建了一个跨文化微表情数据库，并通过算法标注种族标签，进行单一种族与多种族的对比研究。

Result: 研究发现种族背景对情绪表达有显著影响，并提出了一个考虑种族差异的微表情识别框架。

Conclusion: 结论指出情绪普遍性假设存在过度泛化，种族背景应在情绪表达分析中被纳入考量。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [124] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文揭示了多模态学习中模态编码器与融合模块的优化冲突，提出解耦梯度学习（DGL）框架以提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中性能不如单模态学习，现有方法未能解释主导模态表现不佳的原因。

Method: 提出DGL框架，通过截断多模态损失梯度并用单模态损失梯度替代，消除梯度干扰。

Result: 实验证明DGL在多模态、任务和框架中有效且通用。

Conclusion: DGL解决了多模态学习中的优化冲突，提升了性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [125] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，通过训练LoRA层实现高效个性化人体图像生成，无需推理时微调，显著提升了生成图像的保真度和一致性。


<details>
  <summary>Details</summary>
Motivation: 个性化人体图像生成面临计算成本高和实时性差的问题，现有方法需要推理时微调或大规模数据集训练。

Method: 采用LoRA层训练，结合衣柜条件和空间参考，引入选择性主题区域损失，减少信息丢失。

Result: 实验表明，该方法在保真度和一致性上显著优于现有技术，支持少样本训练和单模型推理。

Conclusion: Wardrobe Polyptych LoRA为个性化人体图像生成提供了一种高效且高质量的解决方案。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [126] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO通过噪声优化和联合训练框架改进了Reflow，显著提升了单步和少步生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: Reflow在训练过程中通过构建噪声与图像的确定性耦合来改善生成图像质量，但存在分布差距问题，导致无法快速生成高质量图像。

Method: 提出VRFNO框架，结合编码器和神经速度场，引入历史速度项和噪声优化技术，优化耦合并减少误差。

Result: 在合成和真实数据集上，VRFNO显著优于Reflow，在单步和少步生成任务中达到最优性能。

Conclusion: VRFNO有效解决了Reflow的局限性，为高质量图像生成提供了新方法。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [127] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) 是一种新颖的密集预测方法，通过将输入提升到高维空间并使用高维网络处理，实现了高效且参数少的模型。


<details>
  <summary>Details</summary>
Motivation: 传统密集预测方法在性能和成本上存在局限，SL旨在通过维度提升解决这些问题。

Method: SL将2D图像提升到高维空间（如3D），并使用高维网络（如3D U-Net）处理，生成结构化输出。

Result: 在19个基准数据集上验证，SL在减少98%参数和降低推理成本的同时，性能与传统方法相当。

Conclusion: SL为密集预测任务提供了高效、准确且可靠的新范式。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [128] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 论文提出多用途数据集ProGait，支持视觉任务，优化假肢步态分析。


<details>
  <summary>Details</summary>
Motivation: 假肢步态分析对康复至关重要，但现有视觉方法难以准确检测假肢。

Method: 引入ProGait数据集，包含412个视频片段，支持多种视觉任务，并提供基准模型。

Result: 基准模型在假肢特定任务中表现优于预训练模型。

Conclusion: ProGait数据集填补了假肢步态分析的空白，提升了模型泛化能力。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [129] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD利用基础模型生成合成OOD数据，通过迭代修复和噪声调整增强CLIP模型的边界区分能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有预训练视觉语言模型在处理接近InD数据的OOD样本时的误分类问题。

Method: 利用扩散模型和MLLMs生成合成OOD数据，通过迭代修复和噪声调整优化样本，并微调CLIP模型。

Result: 在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%。

Conclusion: SynOOD通过合成边界对齐的OOD数据，显著提升了模型的OOD检测性能。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [130] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了AI生成图像检测（AID）在现实世界中的挑战，提出了ITW-SM数据集，并分析了影响AID性能的四个关键因素，最终实现了26.87%的AUC提升。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的快速发展，AI生成图像的质量已足以欺骗人类，但现有AID模型在现实场景中表现不佳，亟需改进。

Method: 引入ITW-SM数据集，系统评估了骨干架构、训练数据组成、预处理策略和数据增强组合对AID性能的影响。

Result: 通过优化关键因素，AID模型在现实条件下的平均AUC提升了26.87%。

Conclusion: 研究揭示了AID模型在现实世界中的局限性，并提供了改进方向，为未来研究奠定了基础。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [131] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究探讨了风格迁移在语义分割中减少纹理偏差和增强鲁棒性的效果。


<details>
  <summary>Details</summary>
Motivation: 探索风格迁移是否能在语义分割中减少纹理偏差并提高模型鲁棒性。

Method: 通过Voronoi细胞生成随机区域进行风格迁移，训练语义分割DNN以减少纹理依赖。

Result: 风格迁移显著减少纹理偏差，提高对图像损坏和对抗攻击的鲁棒性。

Conclusion: 风格迁移在语义分割中具有普适性，能有效提升模型性能。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [132] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为Kaleidoscopic Background Attack（KBA）的方法，通过使用多对称性背景纹理攻击相机位姿估计模型。


<details>
  <summary>Details</summary>
Motivation: 在稀疏输入的物体中心场景中，背景纹理对相机位姿估计的准确性有显著影响。

Method: 利用多对称性纹理构造背景，并提出投影方向一致性损失优化纹理。

Result: 实验表明，优化的对抗性背景能有效攻击多种相机位姿估计模型。

Conclusion: KBA方法能显著提升攻击效果，对位姿估计模型构成威胁。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [133] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer提出了一种基于聚类的动态视觉令牌生成方法，通过语义而非空间位置分配令牌，提升了特征表示效果。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在视觉任务中采用均匀网格令牌，忽略了图像区域的语义信息，导致特征表示不理想。

Method: FTCFormer引入聚类下采样模块，结合DPC-FKNN、SCS和Cmerge策略，动态生成语义驱动的令牌。

Result: 在32个数据集上验证，FTCFormer在图像分类任务中表现优于基线，最高提升1.43%。

Conclusion: FTCFormer通过语义驱动的令牌分配，显著提升了视觉任务的性能。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [134] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种利用高质量参考图像作为视觉提示的面部视频恢复方法，通过解耦交叉注意力机制和反馈学习策略，显著提升了身份一致性和恢复质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法在严重退化情况下难以保留细粒度的身份特征，导致恢复结果缺乏个体特性。

Method: IP-FVR采用参考图像提供身份条件，结合解耦交叉注意力机制、反馈学习和指数混合策略，确保帧内和帧间身份一致性。

Result: 实验表明，IP-FVR在合成和真实数据集上均优于现有方法，恢复质量和身份保持效果显著。

Conclusion: IP-FVR在面部视频恢复中表现出巨大潜力，适用于实际应用。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [135] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM是一种专为面部图像理解设计的多模态大语言模型，通过ChatGPT生成的弱监督数据集FairFaceGPT训练，提升了面部任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力，尤其是面部结构、表情、情感和人口统计特征的详细理解。

Method: 提出弱监督流程，利用ChatGPT生成基于FairFace数据集的问答对（FairFaceGPT），训练FaceLLM模型。

Result: FaceLLM在多种面部任务中表现优异，达到最先进水平。

Conclusion: FaceLLM展示了语言模型合成监督在领域专用MLLMs中的潜力，为可信赖、以人为中心的多模态AI系统奠定了基础。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [136] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新的视觉封装方法，通过结合视觉概念判别器和时间焦点校准器，解决了视频MLLMs中的语义模糊和时间不一致问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 线性投影器在视频MLLMs中导致语义模糊和时间不一致，而重采样器结构虽有望解决但尚未实现有效方案。

Method: DisCo包含视觉概念判别器（VCD）和时间焦点校准器（TFC），分别确保语义清晰和时间一致性。

Result: 实验表明DisCo在多种视频理解基准上优于现有方法，同时提高了令牌效率。

Conclusion: DisCo为视频MLLMs提供了一种高效且性能优越的视觉封装解决方案。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [137] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统手语翻译依赖昂贵且不完整的注释，本文旨在开发一种无需注释的高效方法。

Method: 采用双视觉编码器进行对比预训练，下游任务中融合视觉特征输入编码器-解码器模型。

Result: 在Phoenix-2014T基准测试中，性能优于单流变体，并取得无注释方法的最高BLEU-4分数。

Conclusion: 双编码器框架在无注释手语翻译中表现优异，验证了对比预训练的有效性。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [138] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为IMD的框架，通过预训练的扩散模型解决图像特征匹配中的对齐问题，显著提升了多实例场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入基础模型进行特征匹配时忽略了单图像理解与跨图像理解之间的不对齐问题，导致性能下降。

Method: IMD框架包含两部分：1) 使用生成式扩散模型捕捉实例级细节；2) 提出跨图像交互提示模块促进图像对间的双向信息交互。

Result: IMD在常用基准测试中达到新SOTA，并在多实例基准IMIM上提升12%。

Conclusion: IMD有效解决了基础模型在特征匹配中的不对齐问题，显著提升了多实例场景下的性能。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [139] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示指导扩散模型的量化，降低计算复杂度并提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在资源受限环境中的高计算复杂度限制了其应用，现有量化方法未充分利用输入条件（如文本提示）。

Method: 提出QLIP方法，通过文本提示动态选择每层每时间步的比特精度，并可与其他量化方法结合。

Result: 实验表明QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。

Conclusion: QLIP为扩散模型量化提供了高效且灵活的解决方案，尤其适用于资源受限场景。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [140] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet是一种多头部特征引导的语义分割架构，旨在提高平面图中墙壁分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 改进平面图中墙壁分割的泛化能力。

Method: 采用U-Net分割主干，结合多头部专用特征提取器提取领域特定特征图，注入U-Net潜在空间以指导分割过程。特征提取器通过编码器-解码器结构训练，预测墙壁宽度并生成压缩的潜在表示。

Result: 实验表明，注入特征后性能优于普通U-Net，验证了方法的有效性。

Conclusion: FGSSNet通过特征引导显著提升了墙壁分割性能。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [141] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于随机图模型的文本适配器（VRGAdapter），通过顶点随机知识图（VRKG）建模类别多样性和类间关系，结合不确定性引导的多分支融合（UMF）提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性文本适配器无法充分捕捉类别描述的多样性和类间关系，限制了知识迁移的效果。

Method: 利用VRKG建模类别多样性和类间关系，通过概率消息传播学习上下文感知分布表示，并采用重参数化采样实现适配器学习。结合UMF动态集成多预训练模型。

Result: 在多个基准数据集上验证了方法的有效性。

Conclusion: VRGAdapter提供了一种更通用的适配器解决方案，显著提升了视觉学习任务的性能。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [142] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 论文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标检测（ZSD）方法主要针对粗粒度对象，而现实场景中常需处理细粒度对象（如不同鸟类、鱼类和花卉），其类间差异微小，难以区分。

Method: 提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。

Result: 在构建的FGZSD-Birds数据集上，MSHC方法优于现有ZSD模型。

Conclusion: FG-ZSD是一个重要且具有挑战性的问题，MSHC方法在细粒度检测任务中表现优异。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [143] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一种测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换来实现鲁棒感知，无需重新训练或架构更改。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖专用架构或预定义增强训练，限制了泛化能力，而FOCAL旨在通过数据驱动方式提升视觉感知的鲁棒性。

Method: FOCAL通过生成和优化候选变换，将其推向视觉上典型的“规范”视图，从而增强鲁棒性。

Result: 实验表明，FOCAL显著提升了CLIP和SAM在2D/3D旋转、光照变化和昼夜变化等挑战性变换中的鲁棒性。

Conclusion: FOCAL挑战了变换特定训练的必要性，提供了一种可扩展的鲁棒性实现路径。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [144] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的方法，显著提升了遥感图像分类的性能。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）在图像分类中偏向于纹理特征，而TDA能有效描述复杂数据的几何信息。结合两者可以弥补CNN的局限性。

Method: 设计了一个TDA特征工程流程，并将其与ResNet18模型结合，用于遥感图像分类。

Result: 在EuroSAT数据集上准确率达到99.33%，比ResNet18基线提升1.44%；在RESISC45数据集上提升1.82%。

Conclusion: TDA特征可以成功与深度学习模型结合，提升分类性能，扩展了TDA的应用范围。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [145] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度卷积神经网络的手写天城体字符识别方法，旨在解决天城体脚本数字化工具的不足，并在测试和训练中分别取得了96.36%和99.55%的准确率。


<details>
  <summary>Details</summary>
Motivation: 天城体是印度最古老的语言脚本之一，但缺乏合适的数字化工具。研究旨在通过自动化方法提取手写印地语字符，以节省时间并避免数据过时。

Method: 采用两层深度卷积神经网络，利用天城体手写字符数据集（DHCD）进行训练和测试，每类字符包含1700张图像。

Result: 测试准确率为96.36%，训练准确率为99.55%。

Conclusion: 该方法在天城体手写文本识别（DHTR）中表现出色，为相关应用提供了有效解决方案。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [146] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: CrisisLandMark是一个包含64.7万张Sentinel-1 SAR和Sentinel-2多光谱图像的大型语料库，配以结构化文本注释。CLOSP框架通过对比学习将光学和SAR图像对齐到统一嵌入空间，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统主要依赖RGB数据，未能充分利用其他传感器（如SAR和多光谱数据）的独特物理信息。

Method: 提出CLOSP框架，利用文本作为桥梁，将未配对的光学和SAR图像对齐到统一嵌入空间；并进一步提出GeoCLOSP，整合地理坐标以提升特定任务的性能。

Result: CLOSP在检索任务中nDGC提升54%，GeoCLOSP在位置依赖任务中表现更优。

Conclusion: 整合多传感器数据和地理上下文对充分发挥遥感档案潜力至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [147] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 探讨代数、数值计算与计算机视觉的交叉问题，聚焦于解决参数化代数方程组的多个实例，并应用于计算机视觉中的RanSaC模型。


<details>
  <summary>Details</summary>
Motivation: 解决参数化代数方程组的多个实例问题，尤其是在计算机视觉中RanSaC模型的应用需求。

Method: 概述过去5年多的工作，旨在衡量解决此类参数化系统的内在难度，并推动实际解决方案。

Result: 提出了衡量参数化系统求解难度的方法，并取得实际应用进展。

Conclusion: 该研究为代数、数值计算与计算机视觉的交叉领域提供了新的视角和实用工具。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [148] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SC-AGIQA的统一框架，通过文本-视觉语义约束来评估AI生成图像的质量，解决了现有方法在语义对齐和细节感知上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估AI生成图像质量时存在语义对齐和细节感知的不足，需要更全面的评估框架。

Method: SC-AGIQA框架包含两个核心模块：TSAM（文本辅助语义对齐模块）和FFDPM（频域细粒度退化感知模块），分别解决语义对齐和视觉失真问题。

Result: 在多个基准数据集上的实验表明，SC-AGIQA优于现有方法。

Conclusion: SC-AGIQA通过文本-视觉语义约束显著提升了AI生成图像质量的评估效果。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [149] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal是一种无需稀疏关键点标注即可从视频重建可动画3D动物的新框架，通过密集特征网络和分层对齐策略提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏语义关键点，获取成本高且不可靠，需改进。

Method: 提出密集特征网络将2D表示映射到SMAL参数，并结合分层对齐策略。

Result: 实验表明4D-Animal优于基线方法，生成高质量3D资产。

Conclusion: 该方法高效稳定，具有大规模应用潜力。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [150] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA是一个专为珊瑚礁分析设计的大规模视觉问答数据集，包含12,805张真实珊瑚图像和277,653个问题-答案对，旨在支持珊瑚礁生态监测和保护。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁是重要但脆弱的生态系统，需要持续监测。现有珊瑚图像分析依赖专业知识，而视觉问答（VQA）技术可提供用户友好的交互方式，但缺乏专用数据集。

Method: 开发了CoralVQA数据集，包含多维度问题-答案对，并通过半自动数据构建流程确保专业质量。

Result: 评估了多种先进的大规模视觉语言模型（LVLM），揭示了其局限性及改进机会。

Conclusion: CoralVQA为珊瑚礁图像中的视觉语言推理提供了基准，并为未来LVLM开发支持珊瑚保护奠定了基础。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [151] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于内容自适应卷积的Pansharpening方法，通过RAPConv和PAN-DFF模块提升空间细节提取和光谱保真度，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法在Pansharpening中因卷积核均匀应用而忽略局部内容变化，限制了性能。

Method: 提出RAPNet，采用RAPConv生成空间自适应卷积核，结合PAN-DFF模块的注意力机制平衡空间细节与光谱保真。

Result: 在公开数据集上，RAPNet在定量和定性评估中均优于现有方法，消融实验验证了自适应组件的有效性。

Conclusion: RAPNet通过自适应卷积和动态特征融合，显著提升了Pansharpening的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [152] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种新的盲人脸图像修复方法RefSTAR，通过参考图像的选择、转移和重建，解决了身份保持问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份保持上表现不佳，主要因为对纹理细节的特征引入不当。

Method: 构建了参考选择模块（RefSel），设计了特征融合范式，并提出了参考图像重建机制。

Result: 在多种骨干模型上表现出色，身份保持能力和参考特征转移质量更优。

Conclusion: RefSTAR方法在盲人脸图像修复中具有显著优势，代码和数据集已开源。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [153] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc是一种基于检索的新方法，联合预测图像的拍摄时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，优于现有时间预测方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间戳预测与地理定位的相互依赖问题，支持元数据校正、检索和数字取证等应用。

Method: 采用图像、时间和位置的独立编码器，在共享特征空间中对齐嵌入，提出基于循环环形表面的时间度量学习目标。

Result: GT-Loc在时间预测上超越现有方法，即使使用真实地理位置输入，同时在地理定位任务中表现优异。

Conclusion: GT-Loc通过联合优化时间与地理定位，实现了高效的时间预测和地理定位，并支持组合和基于文本的图像检索。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [154] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出了一种基于置信度的自蒸馏方法，用于改进结肠镜检查中的息肉分割，减少资源消耗并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在息肉检测和分割中表现优异，但参数量大、易过拟合且泛化能力差，知识蒸馏方法又资源密集。

Method: 提出动态置信系数，仅利用前次迭代数据存储计算损失，无需额外计算或内存。

Result: 方法在息肉分割任务中优于现有技术，且在多临床中心数据集上泛化良好。

Conclusion: 该方法高效且性能优越，代码将公开。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [155] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 论文提出了一个全面的视网膜异常检测基准，解决了现有方法在数据、算法和评估上的局限性，并提出了NFM-DRA方法，结合解耦表示和记忆机制，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视网膜异常检测对疾病筛查至关重要，但缺乏公开的基准限制了方法的发展。现有方法存在数据简单、测试集饱和和泛化评估不足等问题。

Method: 提出了一个综合的视网膜异常检测基准，并基于解耦异常表示（DRA）提出了NFM-DRA方法，结合正常特征记忆机制。

Result: NFM-DRA方法在性能上优于现有方法，但对某些未见异常仍存在性能下降。

Conclusion: 该基准和方法为视网膜异常检测提供了更全面的评估框架，NFM-DRA成为新的SOTA方法。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [156] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文比较了多视角Transformer中相机几何条件的三种方法，并提出了一种新的相对位置编码（PRoPE），在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多视角计算机视觉任务中，相机几何关系对3D感知至关重要，但现有方法未能充分利用这些关系。

Method: 比较了三种相机条件方法：token级射线图编码、attention级相对位姿编码，以及提出的PRoPE（投影位置编码）。

Result: PRoPE在多种任务（如新视角合成、立体深度估计）中表现优异，尤其在泛化性和模型规模扩展时效果显著。

Conclusion: PRoPE能有效捕捉相机几何关系，提升多视角Transformer性能，适用于多种任务和场景。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [157] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 利用高分辨率地球观测数据和深度学习技术，首次绘制了莫桑比克全国2100万个农田边界，准确率达93%，揭示了小农农业系统的复杂性和多样性。


<details>
  <summary>Details</summary>
Motivation: 科学政策设计需要更深入理解小农农业系统的基本特性，如农田空间分布和地块大小，但现有数据不足。

Method: 结合高分辨率（1.5米）地球观测数据和深度迁移学习，以最小参考数据需求实现全国范围农田边界提取。

Result: 成果包括莫桑比克2023年2100万个农田边界数据集，准确区分农田与非农田（93%），并发现农田碎片化区域。地块大小中位数为0.16公顷，83%小于0.5公顷。

Conclusion: 地块大小是农业社会经济与环境结果（如粮食生产、生计、森林砍伐）的关键指标，研究为可持续农业政策提供了重要数据支持。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [158] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ是一种高效框架，利用预训练的VAE快速训练VQ-VAE，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决高压缩率VQ-VAE训练计算成本高的问题。

Method: 通过控制量化噪声在VAE容忍范围内，结合通道多组量化和后矫正器。

Result: 在ImageNet上压缩至512个token，重建质量保持竞争力（rFID=1.06），训练成本降低两个数量级。

Conclusion: ReVQ在效率和重建质量之间取得优越平衡。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [159] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文介绍了EmRACE-3K数据集，用于评估和改进视觉语言模型在具身环境中的推理能力，展示了现有模型的局限性及其改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在被动任务中表现良好，但在需要在线交互和主动场景理解的具身环境中表现有限。

Method: 构建EmRACE-3K数据集，包含3000多个语言指导任务，并基于此数据集对Qwen2.5-VL-7B模型进行监督学习和强化学习微调。

Result: 零样本设置下所有模型成功率低于20%，微调后模型在探索、动态空间语义推理和多阶段目标执行方面显著提升。

Conclusion: EmRACE-3K数据集有效推动了具身推理能力的发展，揭示了当前模型的局限性及改进方向。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [160] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出了一种完全自监督的方法，利用DINOv2框架从无标签的相机陷阱视频中学习黑猩猩面部嵌入，无需身份标签即可实现高性能的开放集重识别。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物监测中手动识别个体的瓶颈问题，探索自监督学习在生物多样性监测中的潜力。

Method: 基于DINOv2框架，利用自动提取的面部图像训练Vision Transformers，完全无需标签数据。

Result: 在Bossou等挑战性基准测试中，性能超过有监督基线方法。

Conclusion: 自监督学习在生物多样性监测中具有巨大潜力，为可扩展、非侵入性种群研究提供了新途径。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


### [161] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 提出了一种基于自适应多尺度融合的多光谱点云分类方法，解决了稀疏标注、地物尺度差异和长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法在室外数据集上表现不佳，主要问题包括稀疏标注、地物尺度差异和长尾分布。

Method: 采用网格平衡采样策略生成训练样本，提出多尺度特征融合模块和自适应混合损失模块。

Result: 在三个多光谱点云数据集上验证了方法的有效性，优于现有方法。

Conclusion: 该方法显著提升了多光谱点云分类性能，尤其在稀疏标注和长尾分布场景下。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: OTAS提出了一种开放词汇标记对齐方法，用于户外分割，直接从预训练视觉模型的输出标记中提取语义结构，无需场景微调，支持开放词汇查询，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于对象的分割方法在户外环境中因语义模糊和边界不清晰而表现不佳，需要一种更有效的方法。

Method: OTAS通过聚类单视图和多视图中的语义相似结构，并将其与语言对齐，构建几何一致的特征场。

Result: 在多个数据集上，OTAS在2D和3D分割任务中均优于现有方法，最高提升151% IoU。

Conclusion: OTAS适用于机器人应用，代码将公开。

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [163] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: AirScape是首个为六自由度空中智能体设计的世界模型，通过视觉输入和运动意图预测未来观测序列。


<details>
  <summary>Details</summary>
Motivation: 解决机器人预测自身运动意图在三维空间中结果的基本问题，探索更通用的空间想象能力。

Method: 构建包含11k视频-意图对的数据集，开发两阶段训练计划，将无空间知识的基础模型训练为可控的世界模型。

Result: AirScape能够基于当前视觉输入和运动意图预测未来观测序列。

Conclusion: AirScape为六自由度空中智能体提供了可控的世界模型，符合物理时空约束。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [164] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP是一个端到端框架，通过众包车辆数据生成高精度城市级矢量化地图，显著降低成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR制图成本高且慢，单车感知方法在恶劣条件下缺乏准确性和鲁棒性。

Method: 使用Trip-Aware Transformer架构直接融合多车、多时态地图元素，结合分层匹配和多目标损失。

Result: 在大规模多城市数据集上验证，性能优于单车基线，手动标注成本降低90%。

Conclusion: EGC-VMAP为城市级地图提供了一种可扩展、经济高效的解决方案。

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [165] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的相机-LiDAR融合框架，利用智能路边单元（IRUs）生成高清语义地图，并发布了RS-seq数据集，展示了多模态方法在语义分割上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统车辆方法在复杂交叉口的高清语义地图生成中存在遮挡和视角限制的问题，需要更高效的多模态解决方案。

Method: 采用两阶段融合框架，结合相机高分辨率纹理和LiDAR精确几何数据，通过RS-seq数据集进行验证。

Result: 多模态方法在RS-seq数据集上的mIoU比单模态方法分别提高了4%（图像）和18%（点云）。

Conclusion: 研究为基于IRU的高清语义地图生成提供了基准方法和数据集，推动了基础设施辅助自动驾驶系统的发展。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [166] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: 论文探讨了如何通过强化学习和模仿学习提升机器人多指手的灵巧操作能力，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 人类灵巧操作能力是物理智能的巅峰，但机器人实现类似能力面临根本性挑战。本文旨在通过改进学习方法解决这些问题。

Method: 采用结构化探索的强化学习方法，并结合基于采样的规划和视觉触觉人类示范的模仿学习技术。

Result: 提出了一种高效的强化学习框架，显著提升了多指手的灵巧操作能力。

Conclusion: 通过直接解决根本性限制，本文方法为机器人实现人类水平的灵巧操作提供了有效途径。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [167] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: 提出了一种结合包装策略、结构稳定性验证和启发式规划的新框架，解决了在线装箱问题中结构稳定性和安全重新配置的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在体积利用率上表现优异，但无法保证装箱的结构稳定性，且缺乏安全重新配置机制。

Method: 引入了负载可承受凸多边形（LBCP）概念和稳定重新规划（SRP）模块，分别用于高效验证稳定装载位置和规划安全重新配置。

Result: 实验表明LBCP验证高效且通用，SRP在节省重排成本方面表现优越。

Conclusion: 该方法为实际工业和物流应用提供了鲁棒且实用的自动化装箱解决方案。

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [168] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: Tactile-VLA框架通过融合视觉、语言、动作和触觉感知，结合混合力-位置控制器和推理模块，实现了在接触丰富任务中的精确物理交互和自适应策略调整。


<details>
  <summary>Details</summary>
Motivation: 提升Vision-Language-Action（VLA）模型在物理交互中的表现，尤其是在需要精细力控制的接触丰富场景中。

Method: 提出Tactile-VLA框架，结合混合力-位置控制器和基于触觉反馈的推理模块。

Result: 实验证明Tactile-VLA在触觉感知指令跟随、触觉相关常识利用和自适应触觉推理方面具有有效性和泛化能力。

Conclusion: VLM的先验知识已包含物理交互的语义理解，通过少量演示连接触觉传感器，可实现零样本泛化。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [169] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: 提出了一种新颖的多步接触式机器人操作任务生成方法，通过符号和物理验证确保任务可解性，适用于强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作任务中多步、接触式任务的生成问题，确保任务逻辑和物理可解性。

Method: 输入原子动作、对象和空间谓词，通过符号和物理验证生成可解任务序列。

Result: 生成了数百万个独特的可解多步任务，适用于强化学习训练。

Conclusion: 该方法能高效生成可解任务，为机器人操作任务提供了丰富的训练数据。

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [170] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 提出了一种无需标靶的多LiDAR外参校准框架，通过LiDAR束调整和迭代优化实现高精度校准，适用于非重叠视场场景。


<details>
  <summary>Details</summary>
Motivation: 多LiDAR系统的外参校准对3D地图重建至关重要，传统方法依赖重叠视场或人工标注，限制了适用性。

Method: 结合LiDAR束调整（LBA）和自适应加权机制，构建参考点云图并联合优化外参，减少累积误差和异常值影响。

Result: 在仿真和真实场景中表现优异，平移误差5 mm，旋转误差0.2°，初始误差容忍度0.4 m/30°。

Conclusion: 该方法无需基础设施或人工调参，开源且优于现有技术，适用于复杂场景。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [171] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: HZ-MP是一种基于混合Zonotope的运动规划器，通过分解无障碍空间和低维面采样，解决了非凸自由空间中的路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 非凸自由空间中的路径规划问题由于NP难特性，传统MILP方法难以高效解决，因此需要一种新的方法。

Method: HZ-MP通过分解无障碍空间，利用低维面采样和启发式引导，专注于有希望的过渡区域进行探索。

Result: HZ-MP在有限时间内收敛到接近最优的轨迹，适用于高维复杂场景，并具有概率完备性和渐近最优性。

Conclusion: HZ-MP提供了一种高效且可扩展的路径规划方法，优于现有方法如AIT*和EIT*。

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [172] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 提出了一种名为RMRP的轻量级线性参数化地图构建方法，结合随机映射和随机投影，解决了复杂环境中自主导航的计算负担和感知问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模复杂环境中移动机器人自主导航的计算负担、传感器遮挡和不规则地形挑战，缺乏感知感知策略的问题。

Method: 通过RMRP方法构建轻量级线性参数化地图，结合高维映射和稀疏随机投影降维，提出RPATR框架，分别针对无人机和地面机器人优化路径规划和轨迹优化。

Result: 在多种场景中验证，展示了在时间、内存和准确性上的优越映射性能，支持高速无人机和地面机器人的高效安全导航。

Conclusion: RMRP和RPATR框架有效解决了复杂环境中的导航问题，代码将开源以促进社区合作。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [173] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种名为C-ZUPT的方法，用于空中导航和控制，通过定义不确定性阈值识别准静态平衡，显著减少惯性漂移和控制能耗。


<details>
  <summary>Details</summary>
Motivation: 卫星定位和相机在许多环境中可用性有限，导致依赖惯性传感器时精度快速下降，需要替代更新源来提供精确修正。

Method: 引入C-ZUPT方法，通过不确定性阈值识别准静态平衡，为估计滤波器提供精确速度更新。

Result: 验证表明，C-ZUPT显著减少惯性漂移和控制能耗，增强导航稳定性。

Conclusion: C-ZUPT方法有效减少滤波器发散，提升能源效率，适用于资源受限的空中系统。

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [174] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种基于约束马尔可夫决策过程（CMDP）的方法，用于在机器人任务中平衡任务性能和风格模仿质量，通过自适应调整拉格朗日乘数实现选择性模仿。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格模仿时往往牺牲任务性能，且依赖与任务目标高度一致的专家演示，而实际演示通常不完整或不现实。

Method: 将问题建模为CMDP，优化风格模仿目标的同时通过约束保持任务性能，引入自适应拉格朗日乘数选择性模仿演示。

Result: 在多种机器人平台和任务中验证了方法，实现了稳健的任务性能和高保真风格学习，ANYmal-D硬件上机械能耗降低14.5%，步态更敏捷。

Conclusion: 该方法有效解决了风格模仿与任务性能的平衡问题，展示了实际应用中的优势。

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [175] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 结合能量扩散模型与人工势场的运动规划框架，用于复杂环境中的实时轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决追逃问题，提出一种无需完整几何表示的高效规划方法。

Method: 利用点云处理障碍物信息，结合无分类器引导训练和局部势场采样增强避障能力。

Result: 在动态场景中生成初始轨迹并通过势场优化，在部分可观测的追逃场景中表现良好。

Conclusion: 该框架在复杂环境中实现了高效的实时轨迹规划。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [176] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: 论文通过数据驱动方法分析多旋翼飞行器的下洗效应，提出优化编队和控制的物理策略。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器在近距离飞行时因复杂的空气动力学相互作用（如下洗效应）导致性能下降，传统保守策略限制了其应用范围。

Method: 使用力和力矩测量以及粒子图像测速（PIV）技术，量化单旋翼和多旋翼配置中的下洗效应。

Result: 数据揭示了飞行器间的相互作用，为优化编队和控制提供了物理依据。

Conclusion: 研究为多旋翼系统的协调、编队优化和鲁棒性提升提供了新方法。

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [177] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: 该论文提出了一种基于数据驱动的无人机建模软件，利用低成本传感器和传感器融合技术，通过处理IMU和GPS数据，实现高精度的无人机方向和位置可视化。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）作为多功能平台，需要精确建模以支持开发测试。低成本传感器和高效数据处理是解决这一需求的关键。

Method: 使用IMU数据（四元数表示避免万向节锁问题）和GPS数据（结合加速度计以平衡更新频率和稳定性），通过数据过滤和传感器融合技术提高数据质量。

Result: 软件能够高精度、流畅地实时渲染无人机的方向和位置。

Conclusion: 该数据驱动建模软件有效提升了无人机建模的准确性和实时性。

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [178] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 论文提出了一种名为mmE-Loc的高精度、低延迟地面定位系统，结合事件相机和毫米波雷达，用于无人机精确降落。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机的采样频率低于毫米波雷达，限制了系统吞吐量，因此需要一种更高效的传感器融合方法。

Method: 采用事件相机与毫米波雷达结合，提出两个创新模块：一致性指导的协作跟踪和图引导的自适应联合优化。

Result: 实验表明，mmE-Loc在精度和延迟上显著优于现有方法。

Conclusion: mmE-Loc系统为无人机精确降落提供了高效解决方案。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [179] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: 论文介绍了TruckV2X数据集，首个以卡车为中心的大规模协同感知数据集，用于解决卡车感知中的盲区和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 卡车自动驾驶面临独特的感知挑战，如盲区和动态拖车运动，现有数据集缺乏针对重型车辆的多智能体配置。

Method: 提出TruckV2X数据集，包含多模态传感（LiDAR和摄像头）和多智能体协作（牵引车、拖车、CAV和RSU）。

Result: 数据集为开发协同感知系统提供了基础，提升了遮挡处理能力，并加速了多智能体自动驾驶卡车系统的部署。

Conclusion: TruckV2X填补了重型车辆协同感知数据集的空白，为未来研究提供了重要资源。

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [180] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: Plan-MAE是一个基于掩码自编码器的预测与规划统一预训练框架，通过三项任务融合场景理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模仿学习，忽视场景理解对整体轨迹规划的重要性。

Method: 提出Plan-MAE框架，通过掩码重建道路网络、代理轨迹和导航路线三项任务学习场景理解，并加入局部子规划任务对齐车辆动态与安全约束。

Result: 在大规模数据集上，Plan-MAE在规划指标上大幅领先现有方法。

Conclusion: Plan-MAE可作为基于学习的运动规划器的重要预训练步骤。

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [181] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 论文研究了利用脉冲神经网络（SNN）处理激光雷达数据实现机器人导航和避障，并探讨了神经元膜泄漏对SNN精度的影响。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在神经形态硬件中具有高精度、低内存和计算复杂度的优势，适合用于资源有限的自主机器人应用。

Method: 搭建了配备激光雷达的机器人平台，收集标注数据集，并研究了神经元膜泄漏常数对SNN精度的影响。

Result: 通过调整泄漏常数，SNN的机器人控制精度可与非脉冲卷积神经网络（CNN）媲美。

Conclusion: 论文首次聚焦神经元膜泄漏对SNN处理激光雷达数据的重要性，并公开了数据集以促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [182] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: 提出了一种基于i2LQR的统一规划控制策略IteraOptiRacing，用于自动驾驶赛车环境中与其他赛车竞争，优化圈速并避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶赛车环境中，需要一种能够同时优化圈速并避免与其他动态赛车碰撞的策略。

Method: 基于i2LQR的迭代线性二次调节器，利用历史数据迭代优化轨迹，实现多车避障和时间成本优化。

Result: 仿真结果表明，该策略在随机生成的动态赛车场景中优于现有方法，实现了无碰撞且时间最优的轨迹生成。

Conclusion: IteraOptiRacing策略在计算负担低、适合并行计算的情况下，能够实时运行，显著提升自动驾驶赛车的性能。

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [183] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 该论文提出了一种基于蚂蚁视觉归巢行为的生物启发式方法，首次在真实世界中实现了一种侧向化蘑菇体（MB）架构，用于紧凑型自主车辆的视觉归巢。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁通过极少的感官输入和少量学习行走实现稳健的视觉归巢，这启发了自主导航的生物模拟解决方案。

Method: 使用侧向化MB架构，通过角路径积分（PI）信号对全景视图进行分类，分为“目标在左”和“目标在右”记忆库，并在真实环境中进行验证。

Result: 通过四个实验验证了方法的有效性，包括模拟、真实世界归巢、随机行走归巢和精确停止行为，系统在资源受限设备上高效运行。

Conclusion: 该方法提供了一种生物学基础、资源高效的自主视觉归巢解决方案，模仿了蚂蚁的精确归巢行为。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [184] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: 论文提出了一种结合轨迹规划、多模态预测和主动探测的统一框架，用于增强不确定性下的决策能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的导航需要处理其他代理行为的不确定性，传统方法难以应对多模态预测的不确定性。

Method: 开发了一种新的风险度量，通过混合模型整合多模态预测不确定性，并结合主动探测机制减少预测模糊性。

Result: 在MetaDrive仿真环境中验证了框架的有效性，能够成功处理复杂交通场景中的不确定性预测。

Conclusion: 该框架在多模态不确定性下表现出鲁棒性，适用于现实世界的自主导航挑战。

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [185] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: 论文提出了一种名为MRMEL的新框架，用于解决自动驾驶车辆在多样化交通场景中的拉格朗日交通控制问题，通过混合专家学习和残差修正提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆作为移动执行器在交通流控制中具有潜力，但传统固定执行器（如交通信号）无法适应多样化交通场景，亟需一种通用且鲁棒的控制策略。

Method: MRMEL框架结合残差强化学习和混合专家学习，动态选择最优名义策略并学习残差修正，以应对多样化交通场景。

Result: 在真实交通场景的案例研究中，MRMEL实现了比基线方法额外4%-9%的车辆排放减少。

Conclusion: MRMEL为拉格朗日交通控制提供了一种有效且通用的解决方案，显著提升了自动驾驶车辆在复杂交通环境中的性能。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [186] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: AdvGrasp框架从物理角度攻击机器人抓取系统，通过形变物体降低抓取性能。


<details>
  <summary>Details</summary>
Motivation: 研究机器人抓取的对抗攻击，以评估和改进系统的鲁棒性。

Method: AdvGrasp通过形变物体增加重力扭矩和降低稳定性，攻击抓取能力。

Result: 实验验证AdvGrasp有效，实际应用表现稳健。

Conclusion: AdvGrasp为机器人抓取的对抗攻击提供了新视角，具有实用价值。

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [187] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: 提出了一种新方法，通过混合优化算法自动生成具有特定拓扑特性的谐波势场路径，适用于复杂工作空间。


<details>
  <summary>Details</summary>
Motivation: 现有谐波势场方法难以定制路径的拓扑特性，限制了其在复杂任务中的应用。

Method: 采用混合优化算法，搜索路径的同伦类，选择树状结构，并通过投影梯度下降优化权重参数。

Result: 在复杂工作空间中成功生成了具有定制拓扑特性的安全路径，并通过仿真和硬件实验验证。

Conclusion: 该方法有效解决了谐波势场路径的拓扑定制问题，适用于复杂场景。

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [188] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: Octopi-1.5是一个新型的视觉-触觉-语言模型，支持多部位触觉信号处理和检索增强生成（RAG），通过手持触觉接口TMI实现交互，展示了触觉推理和实时学习能力。


<details>
  <summary>Details</summary>
Motivation: 触觉对人类和机器人至关重要，尤其在灵巧操作、材料识别和视觉遮挡场景中。Octopi-1.5旨在提升触觉模型的性能和应用范围。

Method: Octopi-1.5采用多部位触觉信号处理和RAG模块，结合GelSight和TAC-02触觉传感器，通过手持接口TMI实现交互。

Result: 模型能够完成触觉推理任务（如物体识别和处理建议），并实时学习新物体。

Conclusion: Octopi-1.5展示了视觉-触觉-语言模型的进展，但仍存在局限性，代码和设计文件已开源。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [189] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的水下探索与检查自主解决方案，集成到定制的水下机器人Ariel中，通过多相机视觉-惯性状态估计和学习型机器人速度预测方法，增强了在视觉退化条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决水下环境中视觉退化对自主探索和检查任务的挑战，提升机器人在复杂视觉条件下的性能。

Method: 采用折射感知的多相机视觉-惯性状态估计方法，并结合学习型机器人速度预测，集成自主探索和视觉检查解决方案。

Result: 在潜艇干船坞的实地测试中，系统在挑战性视觉条件下表现出状态估计的鲁棒性和路径规划技术的通用性。

Conclusion: 提出的解决方案在水下环境中实现了类似空中无人机的自主性，展示了其在实际应用中的潜力。

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [190] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: 该论文提出了一种结合深度强化学习和进化策略的方法，用于优化欠驱动机器人的控制策略。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在处理复杂控制问题时表现优异，但某些情况下需要进一步优化策略以实现最佳性能和鲁棒性。

Method: 首先使用SAC算法训练RL代理，随后通过SNES进化策略进行零阶优化，直接针对原始评分进行微调。

Result: 实验表明，该方法显著提升了代理性能，同时保持了高鲁棒性，控制器表现优于基线。

Conclusion: 进化微调方法有效提升了深度强化学习策略的性能，适用于复杂任务。

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [191] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: MP-RBFN是一种基于径向基函数网络的新方法，用于高效学习自动驾驶中的运动基元，结合了采样方法的高性能和优化方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法计算成本高，而采样方法对轨迹几何形状有限制，需要一种兼顾高精度和高效的方法。

Method: MP-RBFN结合采样方法的高保真轨迹生成和车辆动力学的精确描述，通过径向基函数网络实现。

Result: 实验显示MP-RBFN在生成优化运动基元时比现有半解析方法准确七倍，且推理时间低。

Conclusion: MP-RBFN在运动规划中表现出色，已开源并集成到采样轨迹规划器中，具有实际应用价值。

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [192] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: 提出了一种轻量级深度学习手势识别系统，用于自然控制协作机器人，模型仅需1,103参数和22KB大小，准确率达93.5%。通过量化和剪枝优化后，模型缩小至7KB，并在UR5机器人上成功测试。


<details>
  <summary>Details</summary>
Motivation: 实现直接自然的人机交互，避免使用额外设备（如操纵杆、平板或可穿戴传感器）。

Method: 采用轻量级深度学习模型识别8种手势，并通过TensorFlow Lite进行量化和剪枝优化。

Result: 模型准确率为93.5%，优化后大小仅7KB，成功在UR5机器人上实时运行。

Conclusion: 极轻量级模型也能提供准确响应，为受限环境中的自然人机交互开辟新可能。

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [193] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 本文提出了一种信任感知的博弈论换道决策框架（TGLD），通过动态评估人类驾驶车辆的信任水平，优化自动驾驶车辆的换道策略，提高效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）需要与人类驾驶车辆（HVs）在异构交通环境中协同合作，但现有换道框架忽略了HVs的动态信任水平，导致行为预测不准确。

Method: 1. 构建多车辆联盟博弈模型，结合AVs的全合作行为和HVs的部分合作行为；2. 开发在线信任评估方法，动态估计HVs的信任水平；3. 考虑社会兼容性目标，最小化对周围车辆的干扰。

Result: 实验验证表明，TGLD框架能根据HVs的信任水平和驾驶风格调整策略，显著提高换道效率、安全性及交互透明度。

Conclusion: TGLD框架通过信任机制实现了自适应和透明的AV-HV交互，为异构交通环境中的协同驾驶提供了有效解决方案。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [194] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种改进无迹卡尔曼滤波中sigma点传播的方法，提高了滤波精度和导航性能。


<details>
  <summary>Details</summary>
Motivation: 无迹卡尔曼滤波在导航应用中广泛使用，但其sigma点传播对滤波稳定性至关重要，需要改进以提高精度。

Method: 通过非线性动态模型传播sigma点，改进导航误差状态向量的预测。

Result: 实验表明，该方法提高了滤波精度和导航性能。

Conclusion: 提出的方法有效提升了无迹卡尔曼滤波的性能，适用于实际导航应用。

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [195] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: 该论文综述了基础模型（如LLMs和VLMs）在机器人领域的应用，强调其在语义理解、推理和多模态泛化方面的优势，同时指出实际应用中的瓶颈和挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型如何推动机器人技术的变革，特别是在感知、规划、控制和交互方面的进展。

Method: 通过结构化综述，分类应用场景（如仿真设计、开放世界执行等），并评估系统级策略的可行性。

Result: 总结了基础模型在机器人领域的潜力，但也揭示了局限性（如数据不足、安全风险等）。

Conclusion: 提出未来研究方向，旨在通过更鲁棒、可解释和具身化的模型，弥合语义推理与物理智能的差距。

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [196] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出了一种无需关节扭矩传感器的人形机器人全身扭矩控制框架，结合物理信息神经网络（PINNs）和无迹卡尔曼滤波（UKF），提高了扭矩跟踪精度和能量效率。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在无关节扭矩传感器情况下的扭矩控制问题，提升动态环境中的适应性和稳定性。

Method: 使用PINNs建模摩擦，UKF估计关节扭矩，并在实时扭矩控制架构中整合两者。

Result: 在ergoCub人形机器人上验证了扭矩跟踪精度、能量效率和抗干扰能力的提升，优于现有RNEA方法。

Conclusion: 该方法为无传感器扭矩控制提供了可扩展且实用的解决方案，适用于动态环境。

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [197] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: 开发了一种用于软连续臂（SCAs）的仿真框架，结合视频跟踪系统进行实验测试和控制设计。


<details>
  <summary>Details</summary>
Motivation: 软连续臂（SCAs）因其机械顺应性在多种应用中具有潜力，但其非线性行为难以控制，限制了实际应用。

Method: 开发了一个模块化组装纤维增强弹性体外壳（FREEs）的仿真框架，并整合视频跟踪系统进行实验和控制设计。

Result: 提出了一个可用于实验测试和控制设计的仿真框架。

Conclusion: 该框架为软连续臂的实际应用提供了潜在解决方案。

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [198] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER是一个概率框架，通过双阶段估计（导航和操作）推断人类意图，显著提升了机器人在移动操作任务中的意图推断能力。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作效率，避免因意图推断不准确导致的人类控制受限或冲突。

Method: GUIDER采用双阶段框架：导航阶段结合控制器速度和占用网格生成Synergy Map；操作阶段结合U2Net、FastSAM和几何抓取可行性测试，实时更新对象概率。

Result: 在25次试验中，导航稳定性达93-100%，操作稳定性达94-100%，显著优于基线方法。

Conclusion: GUIDER在导航和操作阶段均显著提升了意图推断能力，验证了其双阶段框架的有效性。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [199] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: 提出了一种强化学习框架，显式结合闭链动力学，显著提升了双足机器人的运动控制鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法将闭链简化为串行模型，导致仿真到现实的迁移效果不佳，无法捕捉关节耦合等关键特性。

Method: 采用对称感知损失函数、对抗训练和网络正则化，结合闭链动力学设计RL框架。

Result: 实验表明，该方法在多样化地形上实现稳定运动，显著优于基于简化模型的方法。

Conclusion: 显式结合闭链动力学的RL框架能有效提升双足机器人控制的鲁棒性和性能。

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [200] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: REACT框架通过实时几何模型和路径规划，避免水下缆绳缠绕，提升复杂结构检查效率。


<details>
  <summary>Details</summary>
Motivation: 解决水下缆绳缠绕问题，提升水下车辆检查复杂结构的效率和安全性。

Method: 使用基于几何的缆绳模型（SDF）和实时路径规划策略，避免缠绕。

Result: 仿真和实验显示，REACT实现全覆盖且任务时间缩短20%，避免缠绕。

Conclusion: REACT显著提升水下检查任务的安全性和效率，优于传统方法。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [201] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: 提出了一种名为PIRL的新方法，结合大型语言模型（LLM）的零样本推理能力和好奇心驱动的强化学习（RL），用于无人机视觉覆盖路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法依赖环境特定的奖励函数，缺乏语义适应性，因此需要一种更灵活的方法。

Method: PIRL利用GPT-3.5的语义反馈动态调整PPO算法的奖励函数，指导无人机的位置和相机调整。

Result: PIRL在OpenAI Gym和Webots中表现优于多种基线方法，视觉覆盖率分别提高14%和27%，电池效率提高25%，冗余降低18%。

Conclusion: LLM引导的奖励塑造在复杂空间探索任务中有效，为将自然语言先验整合到机器人RL提供了新方向。

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [202] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出了一种基于CADMM算法的轨迹优化框架，通过并行计算解决大规模长轨迹优化问题，显著提升了效率和光滑性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理大规模长轨迹时表现不佳，且并行计算在轨迹优化中的应用尚未充分探索。

Method: 使用CADMM算法将轨迹分解为多段并行求解，结合闭式解和数值解处理约束条件。

Result: 相比现有方法，每迭代时间降至O(1)，实验显示在百段轨迹上实现十倍加速，GPU部署支持千段并行。

Conclusion: 该框架在效率和光滑性上优于现有方法，适用于大规模轨迹优化，尤其在并行计算架构上表现优异。

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [203] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: 论文提出了一种将离散的占用网格地图转换为连续可微函数的方法，以解决MPC中障碍物避障约束的兼容性问题，并通过模糊逻辑重新表述约束，成功应用于仿真测试。


<details>
  <summary>Details</summary>
Motivation: 解决MPC在导航任务中因离散占用网格地图与连续可微函数不兼容而难以定义障碍物避障约束的问题。

Method: 将障碍物定义为多边形（半空间的交集），利用模糊逻辑将包含逻辑运算符的约束重新表述为不等式约束，使其兼容标准MPC。

Result: 提出的MPC轨迹规划器在仿真中成功测试，验证了方法的有效性。

Conclusion: 该方法不仅适用于导航任务，还可扩展至其他需要逻辑或语言约束的MPC应用。

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [204] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的运动估计器，融合视觉、惯性和毫米波雷达数据，提升恶劣环境下的里程估计精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在动态环境因素（如天气）下性能下降的问题。

Method: 利用传感器融合技术，动态调整各传感器贡献，雷达补偿视觉传感器在低能见度下的不足。

Result: 在Boreas数据集上的实验表明，模型在清晰和恶劣环境下均表现出鲁棒性和有效性。

Conclusion: 雷达在不同天气条件下的鲁棒性使其成为姿态估计系统的关键组件，尤其在视觉传感器性能下降时。

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [205] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过多轮对话和场景感知实现自适应驾驶辅助。


<details>
  <summary>Details</summary>
Motivation: 当前ADAS系统缺乏对场景上下文的理解和自然语言交互能力，限制了其在动态环境中的灵活性。

Method: SC-ADAS整合大型语言模型、视觉到文本解释和结构化功能调用，支持基于视觉和传感器上下文的多轮对话。

Result: 在CARLA模拟器中实现，系统展示了场景感知、对话和多轮交互的可行性，但也存在延迟和令牌增长等权衡。

Conclusion: SC-ADAS证明了结合对话推理、场景感知和模块化ADAS控制的可行性，为下一代智能驾驶辅助提供了方向。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [206] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: MP1是一种用于机器人操作的新方法，结合MeanFlow范式生成动作轨迹，避免了扩散模型的慢速采样和Flow-based方法的架构限制，提高了精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中生成模型在采样速度和架构限制之间的权衡问题。

Method: 使用MeanFlow Identity直接学习区间平均速度，避免一致性约束，并引入Dispersive Loss提升泛化能力。

Result: 在Adroit和Meta-World基准测试中，MP1的平均任务成功率优于DP3和FlowPolicy，推理速度快19倍。

Conclusion: MP1在机器人操作中实现了高效、精确的动作轨迹生成，适用于少样本学习和实际场景。

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [207] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: Recurrent Expansion (RE) 是一种新的学习范式，超越了传统的机器学习和深度学习，通过分析模型自身的行为实现自我迭代改进。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习专注于静态数据表示，而 RE 提出从模型行为的动态演变中学习，以实现更智能、自适应的系统。

Method: RE 通过多次映射数据并分析内部表示与性能信号（如损失）实现自我改进，扩展为 Multiverse RE (MVRE) 和 Heterogeneous MVRE (HMVRE)，进一步引入 Sc-HMVRE 以适应实际部署。

Result: RE 框架实现了行为感知、自我演化的深度学习系统，为可扩展、自适应的智能模型奠定了基础。

Conclusion: RE 标志着深度学习从静态表示学习转向行为感知的自我演化系统，为未来智能模型的发展提供了新方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [208] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 提出了一种基于可解释人工智能（XAI）的三模冗余（TMR）方法，通过梯度技术计算参数重要性，选择性保护关键权重，显著提升DNN的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，DNN的可靠性至关重要。传统TMR方法开销大，需选择性应用，因此需要高效的选择标准。

Method: 利用低成本的梯度XAI技术（LRP）计算参数重要性分数，选择性应用TMR保护关键权重。

Result: 在VGG16和AlexNet模型上测试，该方法在10-4误码率下显著提升AlexNet可靠性60%，且开销与现有方法相当。

Conclusion: 基于XAI的TMR方法能高效提升DNN可靠性，适用于安全关键领域。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [209] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和人机交互的决策支持系统，帮助印度卡纳塔克邦的农民应对市场和气候波动，并通过语音界面解决文盲问题。


<details>
  <summary>Details</summary>
Motivation: 农民面临市场和气候的双重波动，同时因文盲问题被排除在数字革命之外，亟需一种包容性解决方案。

Method: 系统结合随机森林分类器评估农艺适宜性，LSTM网络预测市场价格，并通过本地语言的语音界面提供服务。

Result: 随机森林模型准确率达98.5%，LSTM模型预测误差低，系统显著提升农民经济抗风险能力。

Conclusion: 该研究为边缘化农民社区提供了可扩展且高效的数据驱动解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [210] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA是一种广泛用于微调大语言模型的技术，通过引入少量可训练的低秩权重矩阵减少参数更新量，但速度提升不一致。本文分析其性能并提出更高效的微调方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在不同模型架构和训练设置中速度提升不一致，作者希望探究其性能限制因素并改进。

Method: 全面分析LoRA性能，提出更高效的微调方法，并进行实证评估。

Result: 新方法在性能相当或更优的同时，提供更一致的训练速度提升。

Conclusion: 为资源受限的LLM微调提供了实用指南和优化方法。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [211] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的神经网络（PINN）框架，用于模拟二维平流-扩散方程控制的污染物扩散，解决了传统数值方法在大规模动态海洋域中的复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理大规模动态海洋域中的污染物传输时面临复杂性和规模上的困难，需要一种更高效且物理一致的方法。

Method: 采用PINN框架，通过将物理定律嵌入神经网络训练过程，并结合噪声合成数据（通过有限差分法生成），使用混合损失函数（包括PDE残差、边界/初始条件一致性及加权数据拟合项）。

Result: 模型能够实现物理一致的预测，并处理非线性动态及边界/初始条件的约束，展示了在大规模模拟中的可扩展性和灵活性。

Conclusion: PINN框架为污染物扩散模拟提供了一种高效且物理一致的替代方案，优于传统数值方法。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [212] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer神经网络的新方法，用于检测洗钱行为，通过对比学习生成时间序列表示，并结合Benjamini-Hochberg程序控制假阳性率。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题，减少对领域专家监督的依赖，并提高检测准确性。

Method: 1. 通过对比学习无标签学习时间序列表示；2. 利用这些表示生成洗钱评分；3. 采用双阈值和BH程序控制假阳性率。

Result: Transformer能有效捕捉洗钱模式，检测非欺诈者和欺诈者能力优于基于规则或LSTM的方法，同时控制假阳性率。

Conclusion: 新方法在洗钱检测中表现出色，优于传统方法，且无需过多专家监督。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [213] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI压缩方法应用于Llama 3.1 8B模型，显著减少计算资源并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估CompactifAI压缩方法在大型语言模型中的效率和准确性。

Method: 使用Codecarbon和Ragas框架分别评估能耗和准确性，并与完整模型对比。

Result: 压缩模型显著减少资源消耗且保持准确性。

Conclusion: CompactifAI使模型更高效、可扩展且成本效益更高。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [214] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [215] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的Transferability Aware Transformer（TAT）方法，利用阿尔茨海默病（AD）数据增强路易体痴呆（LBD）的诊断，解决了数据稀缺和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: LBD是一种常见但研究不足的痴呆症，诊断面临数据稀缺和领域偏移的挑战，而AD数据丰富但存在领域差异。

Method: 使用结构MRI提取的结构连接性（SC）作为训练数据，通过TAT自适应分配权重，减少领域偏移。

Result: 实验证明TAT能有效提升LBD诊断准确性。

Conclusion: TAT为数据稀缺和领域偏移条件下的罕见疾病诊断提供了新框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [216] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的新型零成本神经架构搜索（NAS）代理方法，通过加权响应相关性快速评估架构性能，显著提升了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有零成本NAS代理方法在有效性、稳定性和通用性上不足，需要更高效的训练免费评估方法。

Method: 利用不同输入样本的响应相关性矩阵计算代理分数，衡量架构的表达能力和泛化能力。

Result: WRCor及其投票代理在评估和架构搜索中表现优于现有方法，能在4 GPU小时内发现ImageNet-1k上测试误差22.1%的架构。

Conclusion: WRCor是一种高效、稳定的零成本NAS代理方法，显著提升了架构搜索的效率和性能。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [217] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS是一个通信高效的联邦推荐系统框架，通过动作共享策略减少通信开销，同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FedRecs）面临高通信开销和低训练效率的问题，现有方法因压缩模型参数导致性能下降。

Method: FedRAS采用动作共享策略，将梯度聚类为有限的动作进行通信，而非直接压缩嵌入矩阵，并引入自适应聚类机制以适应异构环境。

Result: 实验表明，FedRAS能减少96.88%的通信负载，且不影响推荐性能。

Conclusion: FedRAS有效解决了通信开销和训练效率问题，适用于异构环境。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [218] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M是一个基于联邦学习和大语言模型的隐私保护框架，用于下一位置预测，通过本地保留用户数据和高效外积机制实现高精度和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决下一位置预测中的隐私问题，同时保持高准确性和低资源需求。

Method: 采用联邦学习框架，结合大语言模型和高效外积机制，保留用户数据本地化。

Result: 在多个数据集上取得SOT结果（如Gowalla Acc@1: 12.55），参数减少45.6%，内存使用降低52.7%。

Conclusion: FLLL3M在隐私保护和性能优化方面表现出色，适用于下一位置预测任务。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [219] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种动态自适应扇出优化采样器（DAFOS），通过动态调整扇出和优先处理重要节点，显著提升了GNN的训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN中均匀邻居采样和静态扇出设置导致的扩展性和效率限制。

Method: DAFOS基于节点度动态调整扇出，优先处理结构重要节点，并集成早停机制。

Result: 在多个基准数据集上，DAFOS实现了显著的训练速度提升和F1分数提高。

Conclusion: DAFOS是一种高效且可扩展的大规模GNN训练解决方案。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [220] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 本文提出了AMLAS-RL框架，通过迭代过程为强化学习（RL）系统生成安全保证论证，解决了RL在安全关键应用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习（ML）在信息物理系统（CPS）中的广泛应用，RL因其处理复杂动态环境的能力而受到青睐，但其安全性问题尚未得到系统解决。

Method: 作者将AMLAS方法（原用于监督学习）调整为AMLAS-RL框架，通过迭代过程为RL系统生成安全保证论证。

Result: 通过一个轮式车辆避障到达目标的案例，展示了AMLAS-RL的有效性。

Conclusion: AMLAS-RL为RL在安全关键应用中的系统化安全保证提供了可行框架。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [221] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 比较时间序列基础模型（TSFMs）与传统方法在共形预测中的表现，发现TSFMs在数据有限时更可靠且校准更稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在时间序列共形预测中的潜力，特别是在数据受限的情况下。

Method: 比较TSFMs与统计模型和梯度提升在共形预测设置中的性能。

Result: TSFMs在数据有限时提供更可靠的预测区间，且校准过程更稳定。

Conclusion: 基础模型能显著提升时间序列共形预测的可靠性，尤其在数据受限时。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [222] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 论文提出了一种名为e-Profits的新评估指标，用于客户关系管理中的流失预测模型，该指标基于客户价值、保留概率和干预成本，优于传统指标如AUC和F1-score。


<details>
  <summary>Details</summary>
Motivation: 传统指标如AUC和F1-score无法反映财务结果，可能导致战略决策失误，因此需要一种更贴合业务需求的评估方法。

Method: e-Profits利用Kaplan-Meier生存分析估计个性化保留率，支持细粒度的客户级评估，并与六种分类器在电信数据集上进行了对比。

Result: e-Profits改变了模型排名，揭示了传统指标忽略的财务优势，并为高价值客户提供了投资回报最大化的细分洞察。

Conclusion: e-Profits是一种易于理解的工具，适用于业务场景中的模型评估，特别适合注重利润驱动的营销和分析团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [223] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 本文提出了图神经网络（GNN）在求解偏微分方程（PDE）时所需消息传递迭代次数的严格下界，显著减少了超参数调优的需求。


<details>
  <summary>Details</summary>
Motivation: 通过将问题的物理特性与GNN的消息传递需求联系起来，减少GNN在求解PDE时的超参数调优负担。

Method: 针对三类基本PDE（双曲型、抛物型和椭圆型），推导了消息传递迭代次数的下界，研究了物理常数、时空离散化与GNN消息传递机制的关系。

Result: 当迭代次数低于下界时，信息无法有效传播，导致解质量差；满足下界时，GNN能准确捕捉现象，得到高精度解。

Conclusion: 提出的下界在四个方程示例中验证了其严格性，为GNN在PDE求解中的应用提供了理论支持。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [224] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究了数据偏见对算法歧视的影响，提出了数据偏见档案（DBP）作为检测和缓解偏见的工具。


<details>
  <summary>Details</summary>
Motivation: 数据偏见是算法歧视的主要驱动因素，但研究不足，阻碍了检测和缓解偏见的实践发展。

Method: 分析了三种常见数据偏见的单独和联合影响，开发了DBP作为检测工具。

Result: 发现弱势群体在训练集中的代表性不足对歧视的影响较小，而代理和标签偏见的组合更为关键。

Conclusion: DBP为系统性记录偏见信号提供了初步框架，有助于预测歧视风险和评估公平干预措施。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [225] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 论文探讨了如何开发可扩展的AI建议系统，以提供高质量反馈优化假设和实验设计。研究发现，小型模型结合压缩文献库和结构化推理框架，能在ICLR 2025测试集上表现优于通用语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究快速发展，但缺乏可扩展的建议系统来提供高质量反馈，因此研究如何开发此类系统。

Method: 研究模型大小、上下文长度、置信度估计和结构化推理等因素，开发小型模型结合压缩文献库和推理框架。

Result: 小型模型在ICLR 2025测试集上表现优于通用模型，高置信度预测的接受率超过90%。

Conclusion: 结构化推理框架和小型模型结合能显著提升假设生成和实验设计的质量与效率。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [226] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的旅行需求建模框架，通过整合人口合成、活动生成、位置分配和大规模微观交通模拟，实现了数据驱动、可扩展且可迁移的模型。在洛杉矶的验证中，模型性能与传统活动基模型相当，但成本更低、扩展性更强。


<details>
  <summary>Details</summary>
Motivation: 传统活动基模型（ABMs）依赖简化规则和假设，开发成本高且难以跨区域适应。本文旨在通过数据驱动的方法解决这些问题。

Method: 提出了一种生成式、数据驱动的框架，整合人口合成、协调活动生成、位置分配和微观交通模拟。

Result: 模型在洛杉矶验证中表现优异，OD矩阵余弦相似度为0.97，VMT的JSD为0.006，MAPE为9.8%。与真实数据对比，交通速度和流量的JSD为0.001，MAPE为6.11%。

Conclusion: 该框架显著降低了建模成本并提高了可扩展性，同时保持了与传统ABMs相当的性能，适用于跨区域应用。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [227] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出了一种基于CLIP模型的语义通信框架，无需训练即可提取数据语义，并通过强化学习优化模型和资源分配，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于神经网络的语义通信需要联合训练的问题，同时优化无线网络中的噪声和资源限制。

Method: 使用CLIP模型提取语义信息，结合PPO强化学习算法优化模型架构和频谱资源分配。

Result: 仿真显示，收敛速度提升40%，累积奖励提高4倍。

Conclusion: 该方法在语义通信中具有高效性和鲁棒性，适用于无线网络环境。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [228] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet是一种卷积神经网络模型，用于通过脑电图（EEG）及时识别有害脑活动，在多个类别中表现出高准确性和与人类专家相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在脑电图识别中存在评分者间差异、资源限制和泛化能力差的问题，限制了其在脑疾病诊断和治疗中的应用。

Method: 开发并验证了VIPEEGNet模型，使用了两个独立的数据集（1950名患者和1532名患者），模型基于卷积神经网络。

Result: VIPEEGNet在二元分类和多分类任务中表现出高准确性（AUROC高达0.972），性能与人类专家相当，外部验证表现优异。

Conclusion: VIPEEGNet是一种高效且参数较少的模型，在脑电图识别中具有潜在临床应用价值。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [229] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 论文提出了一种名为ODIA的新方法，通过在线用户交互数据加速LLM的函数调用，显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: LLM的函数调用延迟高，影响用户体验，需要一种高效的方法来优化。

Method: 利用在线用户数据识别“简单查询”，并通过知识蒸馏将大模型的能力迁移到小模型上。

Result: 响应延迟降低了45%（预期）和78%（中位数），小模型能处理60%的流量且精度损失可忽略。

Conclusion: ODIA方法无需过多人工干预，能持续优化，适用于生产环境。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [230] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿蒙特卡洛（HMC）采样的深度神经网络（DNN）最后一层概率方法（LL-HMC），以降低计算成本并适用于大规模数据。通过实验比较，LL-HMC在分类和异常检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: HMC虽为不确定性估计的金标准，但其计算成本限制了在大规模数据和大型DNN中的应用。LL-HMC通过仅对最后一层进行HMC采样，解决了这一问题。

Method: 提出LL-HMC方法，仅对DNN最后一层进行HMC采样，减少计算量。在三个真实视频数据集上比较了五种最后一层概率深度学习方法。

Result: LL-HMC在分类和异常检测中表现优异，但额外采样参数仅对异常检测有提升，多链或起始位置未带来一致改进。

Conclusion: LL-HMC是一种高效且性能优越的最后一层概率方法，适用于计算资源有限的场景。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [231] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 论文提出了一种名为Fair-FLIP的后处理方法，用于减少深度伪造检测中的偏见，同时保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测方法存在对种族和性别等人口属性的偏见，威胁公众信任。

Method: 提出Fair-FLIP方法，通过重新加权模型最后一层的输入来减少子群差异。

Result: Fair-FLIP将公平性指标提升高达30%，同时基线准确率仅降低0.25%。

Conclusion: Fair-FLIP在保持检测性能的同时有效减少了偏见。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [232] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 论文研究了无需Lipschitz平滑假设的随机洗牌梯度方法，提出了新的步长策略，证明了其在非凸、强凸和非强凸情况下的收敛性，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的随机洗牌梯度方法大多依赖Lipschitz平滑条件，但许多机器学习模型不满足此条件。论文旨在填补这一理论空白，扩展方法的适用性。

Method: 提出了新的步长策略，分析了随机洗牌梯度方法在非凸、强凸和非强凸情况下的收敛性，支持随机洗牌和任意洗牌方案。

Result: 在不依赖Lipschitz平滑条件下，算法仍能收敛，并匹配当前最佳收敛速率。数值实验验证了其实际性能。

Conclusion: 论文扩展了随机洗牌梯度方法的理论适用范围，为实际应用提供了更灵活的工具。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [233] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 论文提出了一种基于反向离散化和近端映射的扩散模型（ProxDM），相比传统得分匹配方法，理论和实践上均有优势。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖得分（梯度）进行采样，但存在效率和精度问题。本文探索通过反向离散化和近端映射改进。

Method: 使用近端映射替代得分，提出ProxDM模型，并基于近端匹配理论学习对数密度的近端算子。

Result: 理论证明ProxDM仅需$\widetilde{O}(d/\sqrt{\varepsilon})$步即可生成$\varepsilon$-精确分布；实验显示其收敛速度显著快于传统方法。

Conclusion: ProxDM在理论和实践中均优于传统得分匹配方法，为高维数据生成提供了更高效的解决方案。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [234] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维建模提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 提高跨平台广告推荐的准确性，捕捉用户兴趣迁移的潜在路径。

Method: 利用GNN对用户行为数据、广告内容和平台特征进行多维建模，调整超参数优化模型性能。

Result: 在三个平台的数据集上测试，Platform B的AUC值达到0.937，表现最佳；Platform A和C因广告标签分布不均，精度和召回率略有下降。

Conclusion: 通过优化超参数，模型在异构数据中的适应性和鲁棒性得到提升。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [235] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文理论分析了离散扩散模型中的无分类器引导（CFG）技术，发现早期高引导会降低生成质量，而晚期引导影响更大。作者提出了一种改进的CFG机制，通过简单代码调整提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型中CFG引导时间表的作用，解释现有实现的不完美性，并提出改进方法。

Method: 理论分析CFG在离散扩散中的表现，提出一种平滑数据分布与初始分布间传输的新机制。

Result: 实验证明新方法在ImageNet和QM9数据集上有效提升了样本质量。

Conclusion: 改进的CFG机制通过简单调整解决了现有实现的问题，显著提升了离散扩散模型的生成质量。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [236] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench是一个大规模AB-FEP数据集，用于机器学习开发，专注于人类雌激素受体α（ERα），包含8,770个复合物结构，并验证了ML方法的性能。


<details>
  <summary>Details</summary>
Motivation: 填补机器学习在蛋白质-配体结合亲和力预测中数据不足的空白，同时结合物理方法的准确性和机器学习的高效性。

Method: 提出ToxBench数据集，包含AB-FEP计算的结合自由能，并开发DualBind模型，采用双损失框架学习结合能函数。

Result: DualBind在基准测试中表现优异，展示了ML以较低计算成本近似AB-FEP的潜力。

Conclusion: ToxBench和DualBind为药物发现中的高效准确预测提供了新工具。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [237] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs通过物理方程直接训练神经网络，成功模拟了湍流，无需传统计算网格或训练数据，并验证了关键流统计数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统湍流模拟需要巨大的计算资源，而PINNs提供了一种基于物理方程的连续、无网格解决方案。

Method: 结合自适应网络架构、因果训练和先进优化方法，直接学习流体方程的解。

Result: PINNs准确再现了能量谱、动能、涡度和雷诺应力等关键流统计数据。

Conclusion: 神经方程求解器能处理复杂混沌系统，为超越传统计算限制的连续湍流建模开辟了新途径。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [238] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs结合机制模拟与神经网络，提升科学建模的灵活性和可解释性，在预测和推断任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决科学建模中机制模型可解释性差与机器学习模型灵活性不足的问题。

Method: 使用机制模拟作为神经网络的训练数据，预训练SGNNs于多样化模拟数据。

Result: SGNNs在COVID-19预测、化学产量预测等任务中表现优异，并实现新的机制可解释性。

Conclusion: SGNNs统一科学理论与深度学习，为科学建模提供新范式。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [239] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种系统框架，通过表示指导改进扩散模型，引入两种新策略增强表示对齐，实验显示性能提升和训练加速。


<details>
  <summary>Details</summary>
Motivation: 通过将扩散模型的内部表示与预训练模型对齐，提高生成质量。

Method: 提出两种策略：1) 将目标表示与示例配对学习联合模型；2) 设计平衡表示学习和数据生成的最优训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现优异，训练速度显著提升（如ImageNet 256×256任务中训练速度提升23.3倍）。

Conclusion: 表示指导能有效提升扩散模型的性能和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [240] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 论文探讨了模型排行榜作为大规模分发中毒模型的潜在渠道，提出了TrojanClimb框架，展示了其在多种模态中的有效性，揭示了机器学习生态系统的重大漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索对手如何通过模型排行榜大规模分发中毒模型，填补了现有研究的空白。

Method: 提出了TrojanClimb框架，通过在保持排行榜竞争力的同时注入恶意行为，验证了其在文本嵌入、文本生成、文本转语音和文本转图像四种模态中的有效性。

Result: 结果表明，对手可以在排行榜上获得高排名，同时嵌入任意有害功能（如后门或偏见注入）。

Conclusion: 结论指出机器学习生态系统存在重大漏洞，亟需重新设计排行榜评估机制以检测恶意模型，并警示社区避免使用未经验证的模型。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [241] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 提出了一种自监督深度学习模型，从多模态信号（EEG、ECG和呼吸信号）中提取有意义模式，用于预测心血管疾病（CVD）风险。


<details>
  <summary>Details</summary>
Motivation: 通过多模态信号分析，提升心血管疾病风险评估的准确性和个性化。

Method: 开发自监督深度学习模型，训练于4,398名参与者数据，通过对比有无CVD结果的嵌入生成投影分数，并在独立队列（1,093人）中验证。

Result: 投影分数揭示了临床有意义的模式，结合Framingham风险评分显著提升预测性能（AUC 0.607-0.965）。

Conclusion: 该框架可直接从PSG数据生成个性化CVD风险评分，有望整合到临床实践中。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [242] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 利用人类注视建模增强RLHF，通过注视感知奖励模型和基于注视的稀疏奖励分布，实现更快收敛并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLHF计算成本高，探索利用人类注视信号提升效率。

Method: 提出两种方法：注视感知奖励模型和基于注视的稀疏奖励分布。

Result: 实验显示注视增强的RLHF收敛更快，性能保持或略有提升。

Conclusion: 人类注视是政策优化的有价值信号，可提升RLHF效率。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [243] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 论文分析了当前大型语言模型（LLM）推理系统评估方法的常见缺陷，提出了识别和避免这些缺陷的框架，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在根本性缺陷，导致性能特征被掩盖，阻碍科学进步。

Method: 通过系统分析近期系统，识别了三个关键维度的常见评估反模式：基线公平性、评估设置和指标设计。

Result: 揭示了这些反模式如何导致误导性结论，并提出了一个全面的检查清单框架。

Conclusion: 该研究为LLM推理系统的评估方法奠定了严谨基础，促进了可重复的结果和有意义的比较。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [244] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出一种分布式预训练新方法，通过训练小型结构化子网络降低内存需求，减少节点间通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型预训练中内存需求高和节点间通信成本大的问题。

Method: 采用两种子网络构建策略，避免节点间激活通信，保持带宽需求与标准数据并行方案相当或更低。

Result: 随机块丢弃技术表现优于宽度子网络构建，内存使用减少20-40%且性能无损失。

Conclusion: 该方法在减少内存和通信成本方面具有潜力，尤其适用于大规模模型训练。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [245] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归MDN（R-MDN）层，用于在持续学习中消除混杂变量的影响，通过递归最小二乘法动态调整特征表示，从而减少预测偏差和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会导致虚假相关性和预测偏差，现有方法如MDN在持续学习中难以动态适应变化的混杂变量分布。

Method: 提出R-MDN层，集成到深度学习架构中，通过递归最小二乘法动态更新内部模型状态，适应数据和混杂变量的变化分布。

Result: 实验表明，R-MDN在静态学习和持续学习阶段均能促进公平预测，减少因混杂变量变化导致的灾难性遗忘。

Conclusion: R-MDN是一种有效的方法，能够在持续学习中动态消除混杂变量的影响，提升模型的公平性和稳定性。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [246] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 论文提出了一种名为“行为探索”的方法，通过训练长上下文生成模型，使自主代理能够快速在线探索和适应环境，模仿专家行为并实现目标导向的探索。


<details>
  <summary>Details</summary>
Motivation: 现有算法依赖随机探索和缓慢的梯度更新，无法像人类一样快速适应新环境。论文旨在赋予自主代理类似人类的快速探索和适应能力。

Method: 利用专家演示数据集，训练一个长上下文生成模型，预测专家行为，并结合上下文和探索性度量，实现目标导向的探索和快速适应。

Result: 在模拟和真实机器人任务中验证了方法的有效性，展示了其学习适应性和探索性行为的能力。

Conclusion: 行为探索方法为自主代理提供了快速在线适应和目标导向探索的能力，接近人类水平。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [247] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 提出一种改进高斯概率生成模型效率的框架，通过识别数据快速高斯化的步骤，用闭式高斯近似替代剩余轨迹，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 高斯概率生成模型的计算成本高，限制了实际部署，需提升效率。

Method: 识别数据快速高斯化的步骤，用闭式高斯近似替代剩余轨迹，避免冗余计算。

Result: 实验证明在多种数据模态下，样本质量和计算效率均有显著提升。

Conclusion: 该方法在保持学习动态完整性的同时，显著提升了生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [248] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 论文研究了在连续状态和动作动态系统中模仿专家演示者的问题，提出了两种最小干预方法（动作分块和噪声注入）来缓解复合误差问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在物理环境（如自动驾驶和机器人学习）中因复合误差问题而复杂化，需要更先进的策略参数化或数据增强方法。

Method: 提出了两种干预方法：对于开环稳定系统采用“动作分块”，对于可能不稳定的系统采用“噪声注入”。

Result: 这些方法在理论和实践中均能有效缓解复合误差问题，且与现有机器人学习实践一致。

Conclusion: 研究结合了控制理论和强化学习的工具，揭示了单独考虑任一领域时未出现的新问题。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [249] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 论文提出了一种结合队列理论和注意力模型（QT-SimAM）的新方法，用于高精度预测航班延误，并在不同网络中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 航班延误对航空业造成重大经济和运营影响，需要一种精确且通用的预测模型来改善乘客体验和减少收入损失。

Method: 提出QT-SimAM模型，结合队列理论和简单注意力机制，使用美国运输统计局和EUROCONTROL数据进行验证。

Result: 在美国数据集上准确率为0.927，F1分数为0.932；在欧盟数据集上准确率为0.826，F1分数为0.791。

Conclusion: QT-SimAM是一种高效的端到端方法，能够高精度预测航班延误，有助于减少乘客焦虑和优化运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [250] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 论文提出了一种基于广义投影贝尔曼误差（GPBE）的多步信用分配方法，改进了深度强化学习中的梯度TD方法，并在实验中优于PPO和StreamQ。


<details>
  <summary>Details</summary>
Motivation: 现有方法如半梯度TD虽简单高效但易发散，而GTD方法虽有强收敛性但未广泛应用于深度RL。GPBE的引入为非线性函数近似提供了高效支持，但仅限于单步方法，限制了信用分配效率。

Method: 扩展GPBE目标以支持基于λ-回报的多步信用分配，提出了三种梯度优化方法，包括与经验回放兼容的前向视图和与流式算法兼容的后向视图。

Result: 在MuJoCo和MinAtar环境中，新算法表现优于PPO和StreamQ。

Conclusion: 多步信用分配的GPBE扩展显著提升了深度RL的稳定性和效率。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [251] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 论文提出了一种基于隐式神经信号表示的连续时间向量信号低秩分解方法，统一了PCA和ICA问题，并通过对比函数实现信号统计特性学习。


<details>
  <summary>Details</summary>
Motivation: 扩展低秩分解（如PCA和ICA）到连续时间向量信号，解决传统方法无法处理点云和不规则采样信号的问题。

Method: 利用连续时间随机过程建模信号，通过神经网络的对比函数项统一PCA和ICA，学习信号分解的统计特性。

Result: 实现了对点云和不规则采样信号的分解，传统方法无法处理此类数据。

Conclusion: 该方法为连续时间信号的低秩分解提供了统一框架，扩展了应用范围。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [252] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 论文提出DejaVu攻击，利用网络延迟导致多模态传感器时间不同步，显著降低感知任务性能，并提出防御方法AION。


<details>
  <summary>Details</summary>
Motivation: 多模态融合（MMF）在自动驾驶感知中至关重要，但其依赖严格时间同步，易受攻击。

Method: 提出DejaVu攻击，分析传感器任务敏感性差异；提出AION防御，通过跨模态时间一致性监测攻击。

Result: 攻击显著降低检测和跟踪性能；AION在多种数据集和模型上表现优异（AUROC 0.92-0.98）。

Conclusion: DejaVu揭示多模态时间同步漏洞，AION提供有效防御方案。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [253] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 论文提出了一种基于集合到集合（S2S）的推荐框架S2SRec2，用于在电商中推荐互补食材，解决了传统方法在预测多食材和忽略食材间关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 在电商中，顾客常因缺乏专业知识而无法完成完整食谱的食材搭配，传统方法仅预测单一食材且忽略多食材间关系。

Method: 提出S2SRec2框架，基于Set Transformer，通过多任务学习联合学习检索缺失食材和评估篮子完整性。

Result: 实验表明S2SRec2显著优于单目标基线方法，提升了购物体验和烹饪创意。

Conclusion: S2SRec2为电商中的食材推荐提供了有效解决方案，具有实际应用潜力。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [254] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文探讨了特征选项（eigenoptions）在无模型强化学习中的信用分配作用，发现预定义特征选项有助于探索和信用分配，而在线发现可能阻碍学习。同时提出了一种在非线性函数逼近下学习选项值的方法。


<details>
  <summary>Details</summary>
Motivation: 特征选项在强化学习中表现出强大的探索能力，但其在信用分配中的作用尚未充分研究。本文旨在填补这一空白。

Method: 在表格和像素网格世界中评估特征选项对信用分配的影响，并提出一种在深度强化学习中学习选项值的方法。

Result: 预定义特征选项有助于探索和信用分配，而在线发现可能因过度偏置经验而阻碍学习。

Conclusion: 特征选项在同时支持信用分配和探索方面具有潜力，但也存在复杂性。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [255] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 论文提出了一种结合图提示与权重剪枝的新框架GPAWP，旨在通过减少提示数量提升图神经网络的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在图任务中表现优异，但仍面临训练时间长、复杂关系捕捉困难等问题。图预训练和提示方法虽受关注，但此前研究忽略了图提示的优化潜力及其对模型稳定性和效率的影响。

Method: 提出GPAWP框架，结合图提示与权重剪枝，通过重要性评估函数确定正负权重，并采用分层剪枝去除负提示标签。

Result: 在三个基准数据集上的实验表明，GPAWP显著减少了节点分类任务的参数量，同时保持了竞争力。

Conclusion: GPAWP通过优化图提示，提升了GNN的性能与效率，为图任务提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [256] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer是一种基于Transformer的框架，用于准确高效地将用户访问归因于特定POI，解决了GPS不准确和POI高密度带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于GPS精度问题和城市环境中POI的高密度分布，传统的基于邻近性的POI归因方法往往不准确，需要更先进的解决方案。

Method: POIFormer利用Transformer的自注意力机制，联合建模空间邻近性、访问时间、POI语义、用户行为和群体行为模式等多维信号。

Result: 在真实世界移动数据集上的实验表明，POIFormer在空间噪声和高密度POI聚类环境中显著优于现有基线方法。

Conclusion: POIFormer提供了一种通用且实用的解决方案，适用于多种数据源和地理环境，无需依赖难以获取的数据层。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [257] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一个新颖的图框架，整合分子和生物医学知识，用于预测药物相互作用（DDI），优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图方法独立处理药物对，忽略了其复杂性和上下文依赖性，且难以整合生物网络和分子结构。

Method: MolecBioNet将药物对建模为统一实体，结合宏观生物相互作用和微观分子影响，使用图神经网络学习多尺度表示，并引入两种池化策略（CASPool和AGIPool）及互信息最小化正则化。

Result: 实验表明MolecBioNet在DDI预测中优于现有方法，消融研究和嵌入可视化验证了其优势。

Conclusion: MolecBioNet通过统一建模和多尺度知识整合，提供了更准确和可解释的DDI预测。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [258] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp是一种多层级联邦学习方法，针对机器人抓取任务中的非独立同分布数据问题，通过选择数据质量高的机器人训练初始模型，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在机器人抓取任务中面临非独立同分布数据导致的性能下降问题。

Method: 提出MTF-Grasp方法，利用数据质量高的机器人训练初始模型，再分发给其他机器人。

Result: 在Cornell和Jacquard数据集上，性能比传统联邦学习提升8%。

Conclusion: MTF-Grasp有效解决了非独立同分布数据问题，提升了机器人抓取任务的性能。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [259] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于在线世界模型的持续强化学习方法（CRL），通过规划解决任务序列中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习中，智能体在学习新任务时容易遗忘旧任务（灾难性遗忘），论文旨在解决这一问题。

Method: 使用在线学习的Follow-The-Leader浅层模型捕获世界动态，并通过模型预测控制规划任务。模型具有理论保证的遗憾边界。

Result: 在设计的Continual Bench环境中，该方法优于基于深度世界模型的基线，能持续学习新任务且不遗忘旧技能。

Conclusion: 在线世界模型规划方法有效解决了CRL中的灾难性遗忘问题，性能优于现有技术。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [260] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个完全由AI驱动的全球天气预报系统，从数据同化到中期预报仅需17秒，性能媲美传统数值天气预报系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预报依赖数值天气预报系统准备初始条件，耗时且效率低，XiChen旨在实现完全独立的AI驱动预报。

Method: 基于预训练的基础模型，XiChen通过微调实现数据同化和观测操作，并结合四维变分知识提升精度。

Result: XiChen的预报准确性与传统系统相当，预报领先时间超过8.25天。

Conclusion: XiChen展示了完全脱离数值天气预报系统的AI驱动天气预报的潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [261] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN模型用于生成超出历史记录的极端气候事件，揭示潜在风险，并分析其对脆弱地区的影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法捕捉超出历史记录的极端气候事件及其空间依赖性，低估了同步灾害的风险。

Method: 开发DeepX-GAN模型，结合知识引导的深度生成方法，模拟统计上合理但未见的极端事件。

Result: 模型揭示了中东和北非等脆弱地区的潜在风险，并预测未来气候变暖可能引发新的热点区域。

Conclusion: 需制定空间适应性政策，以应对未来可能出现的极端气候风险，而非仅依赖历史数据。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [262] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的初始点来加速条件生成，显著减少了生成过程所需的步骤。


<details>
  <summary>Details</summary>
Motivation: 传统的迭代生成模型（如扩散模型和流匹配）需要大量计算步骤，生成速度慢。本文旨在通过优化初始点来加速生成过程。

Method: 引入warm-start模型，预测一个条件化的先验分布N(mu, sigma)，替代传统的N(0, I)先验，从而减少生成步骤。通过条件归一化技巧，使其兼容任何标准生成模型。

Result: 在图像修复等任务中，仅需11次函数评估（1次用于warm start，10次用于生成），即可达到与1000步DDPM基线相当的效果。

Conclusion: warm-start模型显著提升了生成效率，且无需修改现有生成模型，可与其他高效采样技术结合使用。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [263] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 提出了一种构建性小波神经网络（CWNN），通过频率分析和动态增加基函数来提高计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统小波神经网络在构建精确基函数和计算成本方面存在挑战，限制了其应用。

Method: 引入频率估计器和基函数增加机制，基于主频成分选择初始小波，动态增加基函数以满足精度要求。

Result: 显著提高了计算效率，并在静态映射估计、时间序列分析等任务中展示了广泛适用性。

Conclusion: CWNN框架在多个应用中表现出高效性和实用性，代码已开源。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [264] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种新颖的方法，通过将语言模型中的推测解码技术应用于Transformer时间点过程（TPP）采样，显著加速采样速度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer TPP模型在实际应用中采样速度慢的问题。

Method: 利用较小的草稿模型生成多个候选事件，通过较大的目标模型并行验证，保持与自回归采样相同的输出分布。

Result: 在合成和真实数据集上，TPP-SD实现了2-6倍的加速，且采样分布与标准方法一致。

Conclusion: TPP-SD填补了强大Transformer TPP模型与快速序列采样需求之间的差距。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [265] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 论文提出两种轻量级模块（CKM和CSM），实现动态调整补丁大小，提升计算效率和预测稳定性，适用于多种PDE任务。


<details>
  <summary>Details</summary>
Motivation: 固定补丁大小限制了模型在生产环境中的灵活性和效率，需要一种无需重新训练即可动态调整的方法。

Method: 引入CKM和CSM模块，结合循环补丁大小策略，动态控制补丁大小，避免重新训练。

Result: 在2D和3D PDE任务中提升了预测稳定性和计算效率，且不损失精度。

Conclusion: 该框架首次实现补丁大小动态调整，为PDE任务提供了通用且高效的解决方案。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [266] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出了一种量化并利用不确定性的选择性插补框架，避免不可靠插补，实验证明其能减少误差并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域时间序列数据常因传感器断开而缺失，现有方法多忽略模型不确定性或无法估计，需填补这一空白。

Method: 引入通用框架，量化不确定性并选择性插补高置信值，避免不可靠结果。

Result: 在多种EHR数据集上实验表明，选择性插补减少误差，并提升24小时死亡率预测等下游任务性能。

Conclusion: 将不确定性纳入时间序列插补具有实际价值，能显著提升模型表现。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [267] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 本文提出了一种元自编码器（MAE）的概念，用于对多个自编码器（AE）进行编码和解码，适用于动态演化的多类别建模。


<details>
  <summary>Details</summary>
Motivation: 研究目标是捕捉动态演化类别（如物种）之间的共性和特性，为机器学习和生物学研究提供新工具。

Method: 通过构建元自编码器（MAE），学习多个自编码器的紧凑表示及其编码解码过程。

Result: 初步定义了MAE的概念，并提供了示例，展示了其在机器学习和生物学中的应用潜力。

Conclusion: MAE为动态演化类别的建模提供了新方法，未来研究将探索其在更广泛领域的应用。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [268] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种新的公平CCA方法，确保投影特征与敏感属性无关，从而在不影响准确性的情况下提升公平性。


<details>
  <summary>Details</summary>
Motivation: 随着公平性在机器学习中的重要性增加，现有公平CCA方法忽视了其对下游分类任务的影响，限制了应用范围。

Method: 提出了一种新颖的公平CCA方法，用于公平表示学习，确保投影特征独立于敏感属性。

Result: 在合成数据和ADNI真实数据上验证了方法，表明其能保持高相关性分析性能，同时提升分类任务的公平性。

Conclusion: 该方法为神经影像研究中需要无偏分析的任务提供了公平机器学习的解决方案。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [269] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 提出了一种动态调整架构的图神经网络NCGNs，通过噪声水平自适应调整信息传递范围和分辨率，显著提升了生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图生成模型在噪声水平变化时架构固定，限制了表达能力。

Method: 引入噪声条件图网络（NCGNs），提出动态信息传递（DMP）方法，根据噪声水平调整信息传递范围和分辨率。

Result: DMP在3D点云、时空转录组学和图像等多个领域表现优于固定架构模型。

Conclusion: NCGNs通过动态调整架构，显著提升了图生成模型的性能，具有广泛的应用潜力。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [270] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究了多头潜在注意力（MLA）对预训练中Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨MLA在压缩键/值记忆时如何影响Transformer的预训练容量，特别是旋转嵌入的应用方式。

Method: 使用Marchenko-Pastur（MP）诊断分析$W_{Q}W_{K}^\top$矩阵的频谱，比较标准多头注意力（MHA）和两种MLA变体。

Result: 发现容量瓶颈局部出现，旋转嵌入的解耦变体能防止频谱碎片化并保持表达能力。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，解耦设计能有效维持模型容量。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [271] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 提出了一种基于缩放定律的系统方法，用于确定目标领域的最佳数据混合比例，避免了传统试错法的高成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数据混合比例选择依赖试错法，在大规模预训练中不切实际，需要一种更高效的方法。

Method: 利用缩放定律预测模型在不同数据混合比例下的损失，并通过小规模训练估算参数，外推到更大规模和未见过的数据混合。

Result: 在LLM、NMM和LVM三种大规模预训练场景中验证了缩放定律的普适性和预测能力，并能推导出给定预算下的最优数据混合比例。

Conclusion: 该方法为数据混合比例的选择提供了理论依据，显著降低了大规模预训练的成本和复杂性。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [272] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 论文提出了一种名为对抗性激活修补的新方法，用于检测和缓解大型语言模型中的欺骗行为，通过实验验证了其有效性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在安全对齐后仍可能表现出隐蔽的欺骗行为，需要一种机制来检测和缓解此类问题。

Method: 采用对抗性激活修补框架，通过将欺骗性提示的激活修补到安全前向传递中，模拟漏洞并量化欺骗率。

Result: 实验显示，对抗性修补将欺骗性输出从0%提升至23.9%，并验证了多个假设。

Conclusion: 该研究为AI安全提供了新工具，并提出了未来研究方向，如多模态场景下的扩展和鲁棒微调。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [273] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 本文探讨了信息几何在深度学习模型压缩中的应用，强调通过定义低计算子流形和投影实现压缩，并指出信息差异在预训练模型压缩中的重要性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型参数数量激增，需在资源受限设备上部署，因此需要有效的压缩技术。

Method: 利用信息几何分析模型压缩方法，特别是算子分解，提出通过定义低计算子流形和投影实现压缩。

Result: 研究发现信息差异在预训练模型压缩中至关重要，但在微调场景下，瓶颈模型的可训练性更为重要。迭代奇异值阈值方法被证明有效。

Conclusion: 通过软秩减少的简单修改可提升固定压缩率下的性能，信息几何视角为模型压缩提供了新思路。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [274] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新型架构，通过结合扩张时间卷积和动态稀疏注意力机制，提升多元时间序列中的因果发现能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中的因果关系分析在金融和营销等领域至关重要，但传统方法难以处理复杂依赖和滞后效应。

Method: DyCAST-Net采用扩张时间卷积捕捉多尺度时间依赖，结合动态稀疏注意力机制消除虚假连接，并通过统计洗牌测试验证鲁棒性。

Result: 在金融和营销数据集上，DyCAST-Net表现优于TCDF、GCFormer和CausalFormer，能更精确估计因果延迟并减少误发现。

Conclusion: DyCAST-Net通过可解释的注意力热图揭示隐藏因果模式，适用于高维动态场景，具有广泛的应用潜力。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [275] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 论文研究了大型预训练变压器中的上下文学习（ICL）机制，发现其泛化能力受限于训练数据分布，且表现与OLS等学习算法不一致。


<details>
  <summary>Details</summary>
Motivation: 探索变压器在推理时如何实现上下文学习，揭示其机制与现有学习算法的差异。

Method: 通过合成线性回归任务和分布外泛化实验，分析变压器在ICL中的行为。

Result: 变压器在分布外数据上泛化能力差，且其行为与OLS等算法不一致；预训练数据的光谱特征与低损失相关。

Conclusion: ICL的机制并非简单实现OLS等算法，而是受预训练数据分布的光谱特征影响。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [276] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 本文提出了一种结合卷积神经网络（CNN）和计算热力学模型的方法，用于实时监测压水堆燃料棒的温度、应力和应变，以支持核电站的预测性维护策略。


<details>
  <summary>Details</summary>
Motivation: 核电站的预测性维护（PdM）策略对减少因组件故障导致的意外停机至关重要。本文旨在通过实时监测燃料棒状态，提升PdM工具的效能。

Method: 使用CNN架构和计算热力学模型，基于燃料棒包壳外表面的有限温度测量数据，预测温度、应力和应变分布。训练数据通过BISON和MOOSE-THM耦合模拟生成。

Result: CNN经过1000多轮训练未出现过拟合，能够高精度预测温度分布，并进一步用于计算燃料棒的应力和应变分布。

Conclusion: 该方法为核反应堆的实时监测和预测性维护提供了潜在工具，展示了CNN在复杂热力学问题中的应用前景。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [277] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新的傅里叶基映射（FBM）方法，通过整合时间-频率特征解决现有傅里叶方法的问题，并展示了其在多种神经网络模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于傅里叶变换的方法存在起始周期不一致、序列长度不一致等问题，且无法精确解释频率成分或忽略时间信息。

Method: 提出FBM方法，通过傅里叶基展开和映射整合时间-频率特征，并设计了FBM-L、FBM-NL、FBM-NP和FBM-S等模型变体。

Result: 在多种真实数据集上验证了FBM方法在长短期预测任务中的SOTA性能。

Conclusion: FBM方法有效解决了现有傅里叶方法的局限性，并通过时间-频率特征的整合提升了预测性能。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [278] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 论文提出了一种半监督回归模型，用于通过家庭传感器数据预测ALS患者的ALSFRS-R评分变化，比较了不同学习方法和插值技术的效果，发现适应性模型选择能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 临床监测ALS功能衰退的周期性评估可能遗漏关键变化，因此需要更连续的监测方法。

Method: 开发了半监督回归模型，比较了三种学习范式（个体批量学习、群体批量学习和增量微调迁移学习）及三种插值技术（线性、三次多项式、自注意力伪标签插值）。

Result: 迁移学习在ALSFRS-R子量表中表现最佳（28/32对比），自注意力插值在子量表模型中误差最低，线性插值在综合量表中更稳定。不同功能域表现出同质性和异质性特征。

Conclusion: 根据功能域的同质性-异质性特征匹配学习和插值技术，可提高ALS进展预测的准确性，未来可集成到传感器监测平台中。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [279] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina是一种基于部分潜在蛋白质表示的新型生成模型，用于直接生成全原子蛋白质结构和氨基酸序列，解决了侧链长度变化的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型难以直接生成全原子结构和氨基酸序列，尤其是在处理侧链长度变化时。

Method: 使用部分潜在表示：显式建模粗粒度主链结构，通过固定维度的残基潜在变量捕获序列和原子细节。采用流匹配建模联合分布。

Result: 在多个生成基准测试中表现优异，包括全原子共设计性、多样性和结构有效性，并能生成长达800个残基的蛋白质。

Conclusion: La-Proteina在原子级蛋白质设计任务中表现出卓越的性能和可扩展性。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [280] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出一种新的离散微分算子，通过范德蒙系数矩阵估计导数并局部表示连续平滑函数，解决了泰勒公式的维度灾难和误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在函数表示中具有重要作用，但在离散情况下存在维度灾难和导数计算中的误差传播问题。

Method: 利用截断泰勒级数导出的范德蒙系数矩阵，提出新的离散微分算子，同时计算所有低于采样点数的导数，减轻误差传播。

Result: 数学上建立了严格的误差界限，实验证明该方法在导数估计和函数表示上优于有限前向差分、三次样条和线性插值。

Conclusion: 该方法在视觉表示、特征提取、流体力学和跨媒体成像等领域具有广泛应用前景。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [281] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文分析了从两个非对称价值函数（QV-learning和AV-learning）进行引导的TD学习方法，发现AV-learning在控制任务中优于Q-learning，并提出了新算法RDQ，性能优于Dueling DQN。


<details>
  <summary>Details</summary>
Motivation: 探讨从两个非对称价值函数引导的方法是否比单一动作价值函数（如Q-learning）更有效，以及这些方法的理论合理性。

Method: 分析了QV-learning和AV-learning的收敛性和样本效率，并提出了新算法Regularized Dueling Q-learning (RDQ)。

Result: AV-learning在控制任务中优于Q-learning；RDQ在MinAtar基准测试中显著优于Dueling DQN。

Conclusion: AV-learning方法在控制任务中具有优势，新算法RDQ表现优异。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [282] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 该研究探讨了在不平衡数据集中评估XAI方法解释可靠性的重要性，并提出了一种针对少数类的评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的广泛应用和立法要求的增加，XAI方法的解释鲁棒性成为关键，尤其是在高风险场景中常见的不平衡数据集。

Method: 提出了一种基于流形生成邻居、解释聚合和一致性度量的少数类评估方法。

Result: 通过一个基于数值特征的表格数据集（霜冻事件）的案例展示了方法的有效性。

Conclusion: 研究为不平衡数据集中XAI解释的可靠性评估提供了初步见解，强调了少数类的重要性。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [283] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 论文介绍了一个用于社交媒体用户帖子中健康维度分类的数据集，涵盖六个关键方面，并评估了传统和先进的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过社交媒体用户生成内容评估健康维度，为个性化健康评估和心理健康早期干预提供支持。

Method: 开发了全面的注释框架，评估了传统机器学习和基于Transformer的模型，使用10折交叉验证和性能指标。

Result: 模型性能通过精确率、召回率和F1分数评估，并确保决策的透明性和可解释性。

Conclusion: 该数据集有助于区域特定健康评估，并推动个性化健康干预策略，同时遵循伦理规范公开数据集。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [284] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 论文提出DRAGD和DRAGDP攻击方法，利用联邦学习中遗忘前后的梯度差异重构被删除的数据，揭示了联邦遗忘中的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦遗忘虽然允许用户删除数据，但其过程中的梯度交换可能泄露敏感信息，引发新的隐私问题。

Method: 提出DRAGD攻击方法，利用梯度差异重构数据；进一步提出DRAGDP，结合公开数据提升重构精度。

Result: 实验表明，DRAGD和DRAGDP在数据重构上显著优于现有方法。

Conclusion: 研究揭示了联邦遗忘的隐私漏洞，并提出了实际解决方案，提升了联邦遗忘系统的安全性。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [285] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上部署Transformer网络的挑战。

Method: 采用两阶段优化过程，包括层内优化和层间优化，并结合自适应舍入技术减少压缩误差。

Result: 在图像分类、目标检测和实例分割任务中，性能提升高达15%。

Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于边缘设备。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [286] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 论文通过大规模多语言人类研究，发现人类偏好比21种先进LLM的响应更具多样性，提出负相关采样方法提升对齐方法性能，并开源了最大的多语言多轮偏好数据集Community Alignment。


<details>
  <summary>Details</summary>
Motivation: 解决LLM如何服务具有不同文化、政治等冲突偏好的用户，填补现有偏好数据集在多样性上的不足。

Method: 通过多国人类研究比较LLM响应与人类偏好的差异，提出负相关采样方法生成候选集，并构建多语言多轮偏好数据集。

Result: 人类偏好比LLM响应更具多样性；负相关采样显著提升对齐方法性能；开源了最大的多语言偏好数据集。

Conclusion: Community Alignment数据集有望提升LLM对全球多样化用户的服务效果。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [287] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究了在确定性加密数据上结合共形预测（CP）与监督学习的方法，以填补不确定性量化与隐私保护机器学习之间的空白。


<details>
  <summary>Details</summary>
Motivation: 旨在在加密数据上实现严格的共形预测，同时保护隐私。

Method: 使用AES加密的MNIST数据集，测试了基于p值和e值的共形预测方法。

Result: 在加密数据上训练的模型仍能提取有意义的结构，测试准确率为36.88%，显著高于随机猜测（9.56%）。e值CP的预测集覆盖率达60%以上。

Conclusion: 共形预测在加密数据中具有潜力，但也存在预测集紧凑性与可靠性之间的权衡。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [288] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究分布式学习问题，代理嵌入有向无环图（DAG）中，通过观察部分特征和父节点预测进行学习，探讨信息聚合的条件。


<details>
  <summary>Details</summary>
Motivation: 探索在分布式学习中，代理如何通过有限信息共享实现全局最优模型的学习。

Method: 代理按DAG拓扑顺序学习，利用自身观察特征和父节点预测作为额外特征训练模型。

Result: DAG深度是关键参数，信息聚合在足够长的路径上可实现，但某些分布下无法实现。

Conclusion: DAG深度决定信息聚合能力，实验验证了理论结果。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [289] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 该论文比较了生成式和判别式LSTM文本分类模型在边缘计算中的表现，重点研究了后训练量化（PTQ）对模型性能的影响，发现生成式模型对量化比特宽度和校准数据更敏感。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中的文本分类需要低延迟和高准确性，生成式分类器对噪声和分布外数据具有鲁棒性，但部署时面临计算和内存限制。PTQ是一种无需重新训练即可减小模型大小和计算成本的方法。

Method: 使用Brevitas量化库对生成式和判别式LSTM模型进行PTQ，评估不同比特宽度下的性能，并测试其在噪声输入条件下的鲁棒性。还研究了校准数据类别不平衡的影响。

Result: 判别式分类器在量化后仍保持鲁棒性，而生成式分类器对比特宽度、校准数据和输入噪声更敏感。类别不平衡的校准数据会导致生成式模型在低比特宽度下性能下降。

Conclusion: 校准数据在PTQ中起关键作用，生成式分类器在噪声条件下的表现受量化比特宽度和校准数据分布影响，这对边缘环境中的部署具有指导意义。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [290] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 论文介绍了一个开源框架，用于开发相关性核函数，特别关注用户自定义和核组合，以支持代理建模。通过引入频率感知元素，扩展了传统核函数，并验证了方法在多个实际案例中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统核函数（如基于指数的方法）在捕捉复杂机械行为和时频动态方面存在局限性，因此需要更灵活和多样化的核函数框架。

Method: 提出了一个开源框架（SMT 2.0），扩展了核函数范围（如指数平方正弦核和有理二次核），并支持核的组合与自定义。方法在正弦测试案例和实际应用（如CO2浓度预测）中进行了验证。

Result: 框架成功应用于多个案例，展示了其在复杂频率敏感领域的有效性，并提供了灵活的工具集。

Conclusion: 该框架为工程师和研究人员提供了强大的工具，支持未来在复杂领域的代理建模应用。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [291] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2是Earth Physics Transformer系列的最新AI模型，显著提升了地球系统预测性能，超越了前代EPT-1.5和主流AI天气模型，同时推出了概率预测模型EPT-2e，性能优于ECMWF ENS且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 提升地球系统预测的准确性和效率，超越现有AI和传统数值预测模型。

Method: 采用Transformer架构的EPT-2模型，并引入扰动集合模型EPT-2e进行概率预测。

Result: EPT-2在0-240小时预测范围内表现优异，EPT-2e在计算成本更低的情况下超越了ECMWF ENS。

Conclusion: EPT-2和EPT-2e为地球系统预测提供了更高效、更准确的解决方案。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [292] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 该论文探讨了如何利用高分辨率遥感数据和AI工具改进大范围精细生境分类，解决了现有地图在主题和空间分辨率上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于人类活动对生态系统的压力增加，需要高精度生境地图以支持有效的保护和恢复工作。

Method: 结合多光谱和合成孔径雷达影像，利用AI工具和分层生境命名法，采用集成机器学习方法解决类别不平衡问题。

Result: 分层命名法解决了分类模糊性，多源数据集成提升了分类性能，集成机器学习进一步提高了准确性。

Conclusion: 该框架可推广至其他地区，未来研究应关注动态生境的时间建模、生境分割和质量评估，并利用新一代遥感数据。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [293] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据直接学习物理定律的通用物理模拟基础AI模型，无需预先编码方程。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法（如PINNs和有限差分法）需要显式数学方程，限制了其泛化能力和发现潜力。

Method: 采用草图引导的扩散变换器方法，将模拟视为条件生成问题，利用空间边界条件合成物理准确的稳态解。

Result: 模型实现了边界到稳态的直接映射，SSIM > 0.8，且保持亚像素级边界精度，适用于多种物理领域。

Conclusion: 该工作标志着从AI加速物理到AI发现物理的范式转变，建立了首个真正通用的物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [294] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究探讨非等变卷积神经网络（CNN）是否通过旋转增强训练能达到等变图神经网络（GNN）的性能，并分析模型规模、数据集大小和训练时长对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于等变GNN的分子生成模型复杂且难以训练，研究旨在验证非等变CNN是否可通过学习实现类似性能。

Method: 提出一种损失分解方法，分离预测误差和等变误差，并在去噪、分子生成和性质预测任务中评估模型性能。

Result: 首次分析了生成任务中学习到的等变性，验证了非等变CNN的潜力。

Conclusion: 非等变CNN通过旋转增强训练可匹配等变模型的性能，为分子生成提供了更简单的替代方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [295] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 该论文提出了一种基于混合专家（MoE）的新方法，用于转录因子结合位点（TFBS）预测，结合多个预训练的CNN模型，并在分布内和分布外（OOD）数据上验证其性能。结果显示MoE模型表现优异，尤其在OOD场景下。同时，论文提出了一种新的解释性技术ShiftSmooth，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 转录因子结合位点（TFBS）预测对理解基因调控和生物过程至关重要，但目前方法在泛化性和解释性上存在不足。

Method: 采用混合专家（MoE）方法，整合多个预训练的CNN模型，并使用ShiftSmooth技术提升模型解释性。

Result: MoE模型在分布内和分布外数据上表现优异，ANOVA测试证实性能差异显著。ShiftSmooth在解释性分析中优于传统方法。

Conclusion: 该研究为TFBS预测提供了一种高效、泛化性强且可解释的解决方案，有望推动基因组生物学和转录调控的研究。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [296] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 提出了一种结合物理监督与时空学习的RGPD框架，用于RUL和SOH估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确估计RUL和SOH对PHM至关重要，现有方法需改进以适应工业应用。

Method: 结合GCRN、GATConv、SAC模块和Q学习，动态加权物理约束，优化时空学习。

Result: 在多个工业数据集上表现优于现有方法，具有强鲁棒性和预测准确性。

Conclusion: RGPD框架在RUL和SOH估计中表现出色，适用于多样化工业场景。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [297] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 该论文提出了一种支持早期退出的神经网络架构，用于单通道语音分离，并通过不确定性感知的概率框架动态调整计算资源，实现高性能和可解释的退出条件。


<details>
  <summary>Details</summary>
Motivation: 当前大多数语音分离架构计算和参数固定，无法适应不同设备的需求，限制了在嵌入式设备中的应用。

Method: 设计了一种支持早期退出的神经网络架构，并结合不确定性感知的概率框架，建模干净语音信号和误差方差，推导出基于信噪比的早期退出条件。

Result: 实验表明，单一早期退出模型在多种计算和参数预算下，性能与最先进模型相当。

Conclusion: 该框架实现了语音分离网络的动态计算扩展，同时保持高性能和可解释性。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [298] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 本文提出两种加速分子构象生成的训练和推理机制：SO(3)-平均流训练目标和基于回流与蒸馏的方法，实现高效且高质量的分子构象生成。


<details>
  <summary>Details</summary>
Motivation: 快速准确地生成分子构象对计算化学和药物发现任务至关重要，但现有扩散或流模型需要大量计算资源。

Method: 提出SO(3)-平均流训练目标以加速训练，并利用回流和蒸馏方法实现快速推理。

Result: SO(3)-平均流训练目标在生成质量上达到最优，回流与蒸馏方法支持高质量的单步生成。

Conclusion: 本文方法为基于流模型的高效分子构象生成提供了可行路径。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [299] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 论文提出两种互补方法加速分类导向的近似机器遗忘（AMU）：Blend（分布匹配数据集压缩）和A-AMU（损失中心加速方法），显著减少计算时间并保持模型效用和隐私。


<details>
  <summary>Details</summary>
Motivation: 当前AMU方法在处理保留数据集时计算开销大，且减少训练轮次仍具挑战性。

Method: 1. Blend：通过合并视觉相似图像减少保留数据集大小；2. A-AMU：通过增强遗忘目标加速收敛。

Result: 实验表明，该方法显著降低了端到端遗忘延迟，同时保持模型性能。

Conclusion: 首次系统性通过数据集压缩和损失函数设计提升遗忘效率。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [300] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn开发了STAR系统，结合LLM和GNN解决推荐系统中的冷启动、过滤气泡和偏见问题。


<details>
  <summary>Details</summary>
Motivation: LinkedIn在职业推荐系统中面临冷启动、过滤气泡和偏见等挑战，需要高效解决方案。

Method: STAR系统整合LLM（处理文本数据）和GNN（捕捉关系），结合自适应采样和版本管理等工业级范式。

Result: STAR提供了端到端的嵌入解决方案，实现了高性能推荐，并贡献了工业级嵌入构建方法。

Conclusion: STAR系统通过LLM和GNN的结合，有效解决了推荐系统中的核心问题，并提供了实用的部署见解。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [301] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种轻量级的图感知联邦学习方法，结合了FedAvg的简单性和图学习的关键思想，有效捕捉空间关系并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 在交通预测中，空间关系对性能至关重要，但标准联邦学习方法（如FedAvg）忽略了这一点，而现有的图学习方法计算开销大。

Method: 采用基于图连接性的邻域聚合原则，指导参数更新，加权客户端模型，而非训练完整模型。

Result: 在METR-LA和PEMS-BAY数据集上验证，性能优于标准基线和近期图联邦学习方法。

Conclusion: 该方法在保持计算效率的同时，有效捕捉空间关系，适用于交通预测任务。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [302] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 研究了神经网络在计算中的叠加现象，发现实际训练中找到的解决方案与理论构造不同，具有更高的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络是否能在实践中学习到计算高效的压缩计算电路，而非仅理论构造。

Method: 使用一个玩具模型（Universal-AND问题），限制隐藏维度以迫使模型寻找计算高效的电路。

Result: 训练过程发现了一种完全密集的简单解决方案，比理论构造更高效且具有鲁棒性。

Conclusion: 研究揭示了神经网络偏好的电路类型，为网络结构和可解释性提供了新见解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [303] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出一种结合动态时间规整（DTW）和神经网络的可训练模型，适应冷启动和丰富数据场景，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络依赖大量标注数据和缺乏可解释性的问题，同时克服DTW无法利用丰富数据的局限性。

Method: 提出动态长度缩短算法，将时间序列转换为原型，并将DTW递归关系转化为等效的循环神经网络。

Result: 在低资源场景显著优于现有方法，在丰富数据场景保持竞争力。

Conclusion: 该模型兼具DTW的可解释性和神经网络的可训练性，适应不同数据条件。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [304] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成式认知诊断范式，通过生成建模实现认知状态的归纳推理，避免了传统方法的高计算成本和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型需要昂贵的重新训练来适应新学习者，且诊断结果可靠性有限。

Method: 提出了两种生成式模型：G-IRT和G-NCDM，通过生成过程分离认知状态推断和响应预测。

Result: 实验显示新方法在可扩展性和可靠性上显著优于传统方法，诊断速度提升100倍。

Conclusion: 生成式范式为认知诊断在人工智能中的应用开辟了新途径。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [305] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: TVE是一种新颖的预训练框架，通过基于模式遍历图的集合聚合构建预测监督信号，显式建模关系动态，优于传统预训练方法。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的通用预训练策略因任务异质性而面临挑战，需要一种能考虑任务感知表示的有效框架。

Method: 提出任务向量估计（TVE）框架，通过模式遍历图的集合聚合建模关系动态，并利用信息论视角形式化方法。

Result: 在RelBench基准测试中，TVE表现优于传统预训练基线。

Conclusion: 预训练目标应编码任务异质性和时间结构，以优化关系数据库的预测建模。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [306] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新的自动提示优化（APO）框架，通过改进反馈机制（结合正负强化）和引入反馈多样化技术，显著提升了提示优化的效果和效率，并解决了模型迁移中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法主要关注错误修正，忽视了正确预测中的有用信息，限制了其效果和效率。此外，模型版本或API提供商的快速变化使得提示迁移成为实际挑战。

Method: 提出结合正负强化的反馈机制，引入反馈多样化技术以减少噪声，并形式化持续提示优化（CPO）以解决提示迁移问题。

Result: 实验表明，该方法在准确率、收敛速度和计算成本上均优于基线，尤其在提示迁移场景中表现突出。

Conclusion: 通过改进反馈机制和解决迁移问题，该方法显著提升了提示优化的性能和实用性。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [307] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Schedule-Free (SF)方法，用于大规模语言模型训练，无需显式衰减阶段或额外内存开销，并通过理论和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据集的规模迅速扩大，传统的预训练策略（如固定计算预算的余弦学习率调度）已不足以应对大规模训练需求。

Method: 重新审视了Schedule-Free (SF)方法，并提出了改进的SF-AdamW变体，通过理论和实证分析揭示其隐式权重平均特性。

Result: SF-AdamW在损失景观中有效导航，无需衰减阶段或额外内存开销，改进后的变体在动量和大批量训练下表现更优。

Conclusion: SF方法是一种实用、可扩展且理论支持的语言模型训练方法。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [308] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出了一种基于任务先验的概率空间框架，用于评估模型在所有可能下游任务上的性能，解决了当前固定评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的评估方法依赖于固定的下游基准测试，限制了研究的进展。论文旨在通过定义任务先验和任务分布，提供一个更灵活的评估框架。

Method: 采用任务分布和任务先验定义概率空间，评估模型在所有可能下游任务上的平均性能和方差。

Result: 框架首次能够回答模型在所有任务上的平均性能和方差问题，为SSL研究提供了新的评估标准。

Conclusion: 任务先验框架不仅改进了评估方法，还将加速自监督学习的研究进展。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [309] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench是一个标准化的大规模基准测试，用于评估非侵入式脑机接口（BCI）中的基础模型，涵盖7个关键应用领域，并提供多维评估指标和工具。


<details>
  <summary>Details</summary>
Motivation: 非侵入式BCI信号的高噪声和任务特定数据有限限制了其解码能力，当前缺乏全面、实用且可扩展的基准测试来评估公共基础模型的实用性。

Method: 提出AdaBrain-Bench，一个包含多样化BCI解码数据集的基准测试，集成了任务适应管道和多维评估指标。

Result: 通过AdaBrain-Bench评估了一系列公开可用的脑基础模型，并提供了在不同场景下选择合适模型的实践建议。

Conclusion: AdaBrain-Bench为促进稳健和通用的神经解码解决方案提供了持续演进的平台。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [310] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG是一种针对ECG信号的基础模型，能够抗噪声并适应任意子集的12导联ECG，通过对比学习和自监督学习框架训练，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决ECG信号因噪声或导联缺失导致的诊断误差或不确定性。

Method: 结合对比学习和自监督学习框架，学习ECG信号表示及其文本报告描述。

Result: 在PTB-XL和MIT-BIH数据集上表现最佳或次佳。

Conclusion: TolerantECG在多种ECG信号条件下表现优异，具有实际应用潜力。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [311] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: 论文提出NeuTSFlow框架，通过神经操作符学习连续函数族间的转换路径，提升时间序列预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将时间序列视为离散序列，忽略了其作为连续过程噪声样本的本质，导致预测不准确。

Method: 提出NeuTSFlow框架，利用神经操作符进行流匹配，学习历史与未来函数族间的转换路径。

Result: 实验表明NeuTSFlow在多种预测任务中表现优越，验证了函数族视角的有效性。

Conclusion: NeuTSFlow通过直接建模函数级特征，显著提升了时间序列预测的性能。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [312] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 提出了一种名为scSGC的软图聚类方法，用于解决单细胞RNA测序数据中硬图构建的局限性，通过非二值边权重更准确地表征细胞间的连续相似性。


<details>
  <summary>Details</summary>
Motivation: 硬图构建方法（基于阈值相似性矩阵）在单细胞RNA测序数据中存在信息丢失和聚类混淆的问题，限制了聚类准确性。

Method: scSGC框架包含三个核心组件：基于ZINB的特征自编码器、双通道切割信息软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种现有方法。

Conclusion: scSGC能够显著提升单细胞RNA测序数据分析的准确性，有助于更深入地理解细胞异质性。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [313] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 论文研究了RNN在流式奇偶任务中的学习动态，揭示了其从有限训练数据中实现无限泛化的机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在超出训练分布范围时的泛化能力，特别是RNN如何通过算法学习实现无限泛化。

Method: 通过流式奇偶任务（一种非线性序列任务）训练RNN，分析其学习动态和相变现象。

Result: RNN在有限训练后表现出完美无限泛化的相变，并通过隐式表示合并构建了有限自动机。

Conclusion: 研究揭示了神经网络从有限经验中实现无限泛化的一种机制，为算法学习提供了理论支持。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [314] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种结合依赖树和Transformer模型的新方法DepBERT，用于提取句子中的因果关系短语，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法未充分利用依赖树等语言学工具，限制了因果关系提取的性能。

Method: DepBERT扩展了Transformer模型，将句子的依赖树整合到模型框架中。

Result: 在三个数据集上的实验表明，DepBERT优于多种先进的监督方法。

Conclusion: DepBERT通过结合依赖树和深度学习模型，显著提升了因果关系提取的效果。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [315] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种解释大型语言模型（LLM）在核工程领域内部推理过程的新方法，通过低秩适应技术调整模型，并识别关键神经元，验证其对任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为了在核工程等安全关键领域部署LLM，需要理解其内部知识编码和推理过程，以满足核监管框架的验证要求。

Method: 采用低秩适应技术（LoRA）对通用LLM进行微调，通过神经元激活模式比较和神经元沉默技术，分析关键神经元的作用。

Result: 沉默单个关键神经元影响不显著，但集体沉默会导致任务性能显著下降，且模型生成技术信息的准确性受损。

Conclusion: 该方法为提升LLM透明性提供了可行路径，有助于实现核级AI保障，满足核监管要求。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [316] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文提出MemSinks方法，通过设计隔离记忆内容，解决大语言模型记忆重复序列的隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易记忆重复序列，引发隐私和版权担忧，现有后处理方法效果有限。

Method: 提出MemSinks范式，利用序列标识符激活特定记忆神经元，实现记忆内容的隔离。

Result: 在十亿参数和十亿标记规模上实现有效隔离和强泛化能力。

Conclusion: MemSinks首次证明在真实数据上同时实现泛化和隔离是可行的。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [317] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 论文提出了一种动态调整神经元数量的方法，以解决类别不平衡问题，通过实验验证其优于固定网络。


<details>
  <summary>Details</summary>
Motivation: 受生物启发，人脑在学习过程中会动态生成和修剪神经元，而传统深度学习网络的神经元数量固定，无法适应类别不平衡的数据集。

Method: 在训练过程中定期添加和移除神经元，动态调整网络容量，同时保持最终网络结构不变。

Result: 在三个数据集和五个模型上的实验表明，该方法优于固定网络，并与其他不平衡处理技术结合时效果更佳。

Conclusion: 动态网络设计能有效提升类别不平衡数据的性能，具有实际部署的潜力。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [318] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg通过合成数据增强和弱标签生成，显著提升了HLS预测模型的泛化能力，在少样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在HLS硬件设计预测中泛化能力不足的问题。

Method: 采用合成数据增强（Iceberg），结合LLM生成程序和弱标签，并通过上下文模型架构实现元学习。

Result: 在六个实际应用中，模型准确率提升86.4%；在两种测试数据集上，离线DSE性能分别提升2.47倍和1.12倍。

Conclusion: Iceberg有效提升了HLS预测模型的泛化能力，代码已开源。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [319] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种新的表示学习和分类模型，用于在线招聘中的职位分类，通过嵌入层次化行业类别到潜在空间，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在线招聘中，职位分类的准确性对推荐系统、搜索排名和劳动力市场分析至关重要，传统方法难以充分利用行业类别的层次结构。

Method: 结合标准职业分类系统（SOC）和内部层次化分类法Carotene，将职位和行业类别嵌入潜在空间，捕捉图和层次关系。

Result: 在大规模职位数据集上的实验表明，该模型显著优于现有方法，解决了冷启动问题并提升了动态匹配效果。

Conclusion: 该研究为提升职位分类准确性提供了稳健框架，支持招聘行业更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [320] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种新的潜在空间距离估计方法，通过行和列距离近似潜在空间距离，并引入校正项减少噪声影响。基于此提出了径向邻域估计器（RNE），在模拟和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在潜在空间中具有低秩结构，但如何有效定义和测量潜在空间中的距离以捕捉用户和物品关系是一个关键挑战。

Method: 通过行和列距离近似潜在空间距离，引入基于经验方差估计器的校正项，提出径向邻域估计器（RNE）构建邻域并提高插补精度。

Result: RNE在模拟和真实数据集上表现优于现有协同过滤和矩阵分解方法，并能缓解冷启动问题。

Conclusion: 论文提出的距离估计方法和RNE为潜在空间距离定义和推荐系统性能提升提供了新视角。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [321] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 该论文探讨了地理神经网络加权回归（GNNWR）中的归纳偏差问题，提出了通过结合卷积神经网络、循环神经网络和Transformer来改进模型的方法，并在合成数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有GNNWR方法在建模空间非平稳性时受限于固定的距离方案和有限的归纳偏差，需要更灵活的模型来捕捉复杂的空间关系。

Method: 通过引入局部感受野、序列上下文和自注意力机制，扩展了GNNWR，结合了多种神经网络架构的优势。

Result: 在合成数据集上，改进后的GNNWR在捕捉非线性空间关系方面优于传统方法，且模型性能与数据特性密切相关。

Conclusion: 归纳偏差在空间建模中至关重要，未来研究方向包括可学习的空间加权函数、混合神经架构和提高非平稳空间数据模型的解释性。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [322] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL是一种结合因果推理的源自由域泛化方法，通过文本驱动生成视觉表示和因果干预网络提取域不变特征，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统域泛化方法数据需求高的问题，以及现有源自由域泛化方法对域特定混杂因素处理不足的局限性。

Method: 1. 使用数据增强生成风格词向量，结合类别信息生成文本嵌入以模拟视觉表示；2. 训练因果干预网络，利用混杂因素词典提取域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet数据集上取得最先进性能。

Conclusion: TDCRL通过因果学习机制有效提取域不变特征，显著提升源自由域泛化的鲁棒性。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [323] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息高斯过程的网格无关框架，用于解决合规性最小化问题，通过共享多输出神经网络和参数化设计，提高了效率和设计控制。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在合规性最小化问题中存在特征边界模糊、计算成本高和缺乏设计复杂性控制机制的问题。

Method: 采用高斯过程先验参数化设计和状态变量，共享多输出神经网络作为均值函数，结合PGCANs架构，通过最小化合规性、总势能和体积分数约束残差来估计参数。

Result: 方法能生成超分辨率拓扑结构，收敛速度快，合规性和灰度区域分数优于传统数值方法，并能控制细尺度特征。

Conclusion: 该方法在效率和性能上优于传统和基于机器学习的方法，提供了更好的设计控制和解释性。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [324] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 该论文研究了图结构对神经网络性能的影响，特别是社区结构的作用，发现密集互连的社区能提升学习能力。


<details>
  <summary>Details</summary>
Motivation: 探索图结构（尤其是社区结构）对神经网络预测性能的影响，弥补现有研究的局限性。

Method: 使用随机网络、无标度网络和生物神经网络进行比较分析，研究结构属性对图像分类任务的影响。

Result: 网络结构确实影响性能，具有密集互连社区的神经网络表现更优。

Conclusion: 研究为网络科学和机器学习提供了新见解，可能启发更具生物启发性的神经网络设计。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [325] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 本研究开发了首个用于预测亚利桑那州山谷热发病率的图神经网络（GNN）模型，整合了环境预测因子和病例数据，揭示了疾病传播的关键环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 山谷热在西南美国流行地区是重要的公共卫生问题，需要更有效的预测方法以支持早期预警和资源分配。

Method: 采用图神经网络（GNN）模型，结合病例数据和环境变量（如土壤条件、大气变量等），探索变量间的相关性，并引入滞后效应捕捉疾病进展的延迟。

Result: GNN模型成功预测了山谷热的发病率趋势，并识别了关键的环境驱动因素。

Conclusion: 该模型为早期预警系统和疾病预防资源分配提供了科学依据。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [326] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 论文探讨了LLMs在单细胞数据分析中的性能驱动因素，并提出了结合scGPT和LLMs的scMPT模型，展示了互补方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 单细胞基础模型（如scGPT）无法利用生物学中的文本信息，而LLMs虽然表现优异，但其性能驱动因素尚不明确。研究旨在探索LLMs在单细胞数据中的生物学见解，并开发互补模型。

Method: 提出scMPT模型，结合scGPT和LLMs的单细胞表示，并通过不同融合方法验证其性能。

Result: scMPT在性能上优于其组件模型，且表现更稳定。融合方法展示了结合专业推理模型的潜力。

Conclusion: LLMs可以补充单细胞基础模型，提升单细胞分析性能。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [327] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 本文提出了一种高效且可持续的对抗性鲁棒决策树训练流程，包括扰动大小自动选择、对抗训练和鲁棒性验证三个阶段。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在工业中的快速应用，其可信赖性受到关注，但鲁棒训练流程的效率和可持续性仍需改进。

Method: 1. 自动选择数据集扰动大小；2. 训练先进的对抗训练方法；3. 验证模型的鲁棒性。

Result: 发现验证时间与训练时间无关，验证时间是影响流程效率的关键因素。

Conclusion: 该流程显著提高了对抗性鲁棒决策树训练的效率和可持续性。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [328] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 提出一种基于控制理论H²模型降阶技术的高效参数缩减方法，用于压缩线性SSM模型的参数量，实验表明其优于现有方法且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 线性SSM模型在捕捉长程依赖时参数量大，难以部署在资源受限设备上。

Method: 应用控制理论中的H²模型降阶技术缩减线性SSM组件的参数。

Result: 在LRA基准测试中，参数量缩减至1/32，性能优于平衡截断方法。

Conclusion: 该方法高效且保持模型性能，适用于资源受限场景。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [329] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO是一种新颖的表格数据合成管道，通过数据修剪和列重排序优化合成数据的监督学习效用，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据在监督学习中表现不佳的问题，尤其是类别不平衡和数据关系被忽视的挑战。

Method: 结合数据修剪和列重排序技术，优化合成数据的信号噪声比和类别分布。

Result: 在22个公共数据集上，PRRO显著提升预测性能，最高达871.46%，并改善类别分布相似性43%。

Conclusion: PRRO促进了数据合成与监督学习的无缝集成，提升了数据分析和可访问性。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [330] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [331] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经ODE、图注意力、小波变换和自适应频率学习的神经网络框架，用于能源供需预测，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 能源供需预测对可持续能源系统优化至关重要，但可再生能源的波动性和动态消费模式增加了预测难度。

Method: 模型整合了神经ODE、图注意力、多分辨率小波变换和自适应频率学习，使用Runge-Kutta方法求解ODE，并通过SHAP分析提升可解释性。

Result: 在七个数据集（如ETTh1、Solar等）上，模型在多种预测指标上均优于现有方法。

Conclusion: 该模型能有效捕捉复杂时间依赖关系，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [332] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+是一个高效、可扩展的联邦学习框架，专为工业环境中的客户端适应设计，解决了数据标注、协变量偏移和资源限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上实现隐私保护的分散学习，但面临数据标注成本高、协变量偏移和资源限制等问题。

Method: 框架基于预训练源模型，冻结主干和分类器，仅调整域适应线性层，并支持流数据处理的扩展。

Result: 实验证明FedAcross+在低端设备上能有效适应目标域，解决域偏移问题，并支持资源受限环境中的零星模型更新。

Conclusion: FedAcross+为实际工业部署提供了高效、实用的解决方案。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [333] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 本文旨在通过实例和直观可视化解释张量网络（TN）分解中TN秩的概念，帮助读者理解其在不同结构中的意义和应用。


<details>
  <summary>Details</summary>
Motivation: TN秩是TN分解效率和表达能力的关键，但其缺乏统一解释，常被作为超参数调整。本文希望通过直观方法揭示TN秩的意义，指导实际应用。

Method: 通过实际案例和图形化方法，展示如何利用领域知识选择TN秩，并揭示TN秩与张量展开矩阵秩的关系。

Result: 提供了一种直观理解TN秩的方法，支持领域驱动的TN设计，避免复杂的多指标张量代数。

Conclusion: 本文为读者提供了对TN秩的清晰统一理解，支持在实际应用和教育中更有效地选择和解释张量方法。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [334] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 使用无监督CNN-LSTM自动编码器模型直接从低级别游戏轨迹数据中提取潜在表示，减少对领域知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 通过识别游戏风格提供设计洞察并实现自适应体验，同时改进游戏代理。

Method: 采用CNN-LSTM自动编码器模型处理MicroRTS中的低级别游戏轨迹数据。

Result: 潜在空间能够有效区分不同游戏代理，减少领域偏见。

Conclusion: 该方法为探索多样游戏风格提供了新途径，减少了对领域专业知识的依赖。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [335] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文提出了T-GRAB基准测试，用于评估时序图神经网络（TGNNs）在捕捉周期性、因果性和长程依赖等核心时序模式上的能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管时序图神经网络（TGNNs）在建模动态关系数据方面表现出色，但其是否真正捕捉到核心时序模式（如周期性、因果性和长程依赖）仍不明确。

Method: 作者设计了T-GRAB基准测试，包含一系列合成任务，用于系统评估TGNNs在计数/记忆周期性、推断延迟因果效应以及捕捉时空长程依赖等方面的能力。

Result: 对11种时序图学习方法的评估显示，它们在泛化时序模式方面存在显著不足。

Conclusion: 研究揭示了当前模型的局限性，为开发具有更强时序推理能力的架构提供了方向。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [336] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出一种对抗性表示学习方法，通过引入焦点熵来保护用户隐私，同时保持高预测能力。


<details>
  <summary>Details</summary>
Motivation: 在保持高预测能力的同时保护用户隐私，解决现有熵方法潜在的信息泄露问题。

Method: 使用对抗性表示学习和焦点熵（一种熵的变体）来净化敏感内容。

Result: 在多个基准测试中验证了方法的可行性，结果显示高目标效用和中等隐私泄露。

Conclusion: 该方法在保护隐私的同时有效维持了预测能力，适用于实际应用。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [337] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 论文通过图变量和统计充分性分析神经网络，证明在无限宽度限制下，层输出对输入具有统计充分性，并扩展到有限宽度网络。


<details>
  <summary>Details</summary>
Motivation: 旨在通过图论和统计理论为神经网络提供新的统计理解，连接深度学习和传统统计方法。

Method: 将神经网络层解释为基于图的变换，神经元作为输入与锚点间的成对函数，分析层输出的统计充分性条件。

Result: 证明在无限宽度限制下，层输出具有统计充分性，且可通过有限宽度网络实现。

Conclusion: 该框架为神经网络提供了新的统计理论基础，适用于多种网络架构。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [338] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM是一种基于RBF的自适应扩展方法，结合贝叶斯优化和最小二乘优化，解决了PI-ELM在捕捉尖锐梯度时的局限性，并在多个PDE问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: PI-ELM虽然速度快，但其固定输入层限制了捕捉尖锐梯度的能力，因此需要一种自适应方法提升性能。

Method: 引入轻量级贝叶斯优化框架，优化输入层参数的统计分布，结合最小二乘优化输出层参数。

Result: KAPI-ELM在多个PDE基准测试中表现优异，尤其在尖锐梯度问题上，准确率优于现有方法。

Conclusion: KAPI-ELM是一种可扩展、可解释且通用的物理信息学习框架，适用于复杂PDE问题。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [339] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T是一个基于生物背景的化学建模框架，通过条件生成模型优化分子设计，显著提升药物发现的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成化学语言模型（CLMs）在药物发现中缺乏可靠奖励信号和输出可解释性不足的问题。

Method: SAFE-T通过建模基于片段的分子序列在生物提示下的条件似然，支持虚拟筛选、药物-靶标相互作用预测等任务，并实现目标导向的分子生成。

Result: 在零样本评估中，SAFE-T在预测和生成任务上表现优于或与现有方法相当，且速度更快，同时能捕捉已知的结构-活性关系。

Conclusion: SAFE-T展示了条件生成CLMs在统一评分与生成方面的潜力，可加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [340] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 本文研究了层次k-中值聚类问题的平均敏感性，提出了一种高效算法，并证明了其低敏感性和高质量聚类。


<details>
  <summary>Details</summary>
Motivation: 现代算法应用中，数据集通常大且动态，若层次聚类对数据扰动敏感，算法实用性将降低。

Method: 分析算法在随机删除数据点时的输出变化，提出高效层次k-中值聚类算法，并进行理论证明。

Result: 算法具有低平均敏感性和高聚类质量，而单链接聚类和CLNSS变体敏感性高。

Conclusion: 实验验证了算法的鲁棒性和有效性。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [341] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba是一种基于状态空间模型的自动痴呆分类框架，通过语音记录进行认知评估，性能优于现有方法21%，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 早期痴呆检测对及时干预和改善患者预后至关重要，传统神经心理学测试依赖人工评分，自动分类系统可提高效率和准确性。

Method: 提出Demenba框架，基于状态空间模型，内存和计算随序列长度线性增长，训练数据来自Framingham心脏研究的1000多小时认知评估语音。

Result: Demenba在精细痴呆分类中性能提升21%，参数更少，且与大型语言模型融合后表现更优。

Conclusion: Demenba为透明、可扩展的痴呆评估工具提供了新方向，结合大型语言模型潜力巨大。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [342] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 论文分析了联邦学习（FL）中客户端随机参与的问题，提出了对Agnostic FedAvg算法的收敛性证明，适用于非均匀参与情况，并展示了其优于加权聚合方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端非均匀参与的实际挑战，现有方法通常假设全设备参与或已知参与分布，而实际中这些假设不成立。

Method: 研究Agnostic FedAvg算法在随机且变大小客户端参与下的优化问题，并分析其收敛性。

Result: 在凸且可能非光滑损失函数下，算法达到标准收敛速率$\mathcal{O}(1/\sqrt{T})$，且无需知道参与分布。

Conclusion: Agnostic FedAvg在非均匀参与情况下表现优于加权聚合方法，即使服务器知道参与权重。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [343] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 本文系统评估了IMU运动捕捉数据缺失值的填补方法，发现多变量方法优于单变量方法，并提出首个公开数据集。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据缺失问题影响其应用，但缺乏系统性评估，本文填补这一空白。

Method: 比较统计、机器学习和深度学习方法，模拟三种缺失机制，使用53名空手道练习者的数据集。

Result: 多变量方法显著优于单变量方法，如GAIN和迭代填补器在复杂缺失场景下表现最佳。

Conclusion: 为未来研究提供基准，并建议提升运动捕捉数据完整性和鲁棒性的实用方法。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [344] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 论文研究了ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数逼近误差，提出了宽度和深度相关的超逼近误差界。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在高维函数逼近中的表现，克服维度灾难问题。

Method: 利用稀疏网格有限元和比特提取技术，分析网络逼近误差。

Result: 在$L_p$范数中得到了$2m$阶，在$W^1_p$范数中得到了$2m-2$阶的超逼近误差界。

Conclusion: 神经网络的表达能力不受维度灾难显著影响，逼近效果优于经典方法。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [345] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 提出一种算法，加速SO(3)流形上的扩散过程，通过数值Picard迭代实现，实验显示速度提升4.9倍且任务奖励无损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的顺序性导致去噪耗时，需加速SO(3)流形上的扩散过程。

Method: 采用数值Picard迭代方法，应用于解决姿态模糊问题的现有扩散模型。

Result: 算法速度提升达4.9倍，生成单样本延迟显著降低，任务奖励无下降。

Conclusion: 算法有效加速扩散过程，适用于姿态模糊问题，性能无损失。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [346] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedFD的特征蒸馏方法，用于解决模型异构联邦学习中知识蒸馏的不稳定性和性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注logit蒸馏，无法解决异构模型带来的知识偏差问题。

Method: 提出基于特征蒸馏的FedFD方法，通过正交投影对齐特征信息，减少知识偏差。

Result: 实验表明FedFD在性能上优于现有方法。

Conclusion: FedFD通过特征蒸馏和正交投影，有效提升了异构联邦学习的稳定性和性能。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [347] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal-Aligned Transformer（TAT）的多时间范围预测模型，用于提升电商和实体零售商在高峰需求期间的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的需求预测对供应链管理至关重要，尤其是在销售高峰期间，传统方法难以准确预测需求峰值。

Method: TAT模型结合了编码器和解码器，并引入了Temporal Alignment Attention（TAA）机制，利用已知的上下文变量（如节假日和促销信息）进行预测。

Result: 在两个大型电商数据集上的实验表明，TAT在高峰需求预测上提升了30%的准确性，同时整体性能优于其他先进方法。

Conclusion: TAT通过上下文对齐机制显著提升了高峰需求预测的准确性，为供应链管理提供了有力支持。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [348] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets在岩土工程中的应用有限，本研究评估了几种DeepONet架构在一维固结问题中的表现，提出了一种改进的Trunknet傅里叶特征增强DeepONet（Model 4），显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索DeepONets在岩土工程中的应用潜力，解决传统方法在复杂系统中的效率问题。

Method: 比较了三种DeepONet架构（标准型和物理启发型），并提出了一种改进的Trunknet傅里叶特征增强DeepONet（Model 4）。

Result: Model 4表现最佳，计算速度提升1.5至100倍，适用于快速变化的函数。

Conclusion: DeepONets在岩土工程中具有高效、通用的潜力，推动了科学机器学习在该领域的早期集成。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [349] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于云和LLM的共享电动出行平台，结合移动应用提供个性化路线推荐，并通过优化模块和RAG框架在不同场景下进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务的兴起，用户对端到端解决方案的需求增加，需要更先进的技术支持。

Method: 开发了一个云基LLM驱动的共享电动出行平台，集成了移动应用和优化模块，并采用RAG框架进行模式级评估。

Result: 优化模块在旅行时间和成本方面表现良好；RAG框架在系统操作员查询和用户查询上的执行准确率分别为0.81和0.98。

Conclusion: 该平台通过LLM和云技术有效支持了共享电动出行服务的智能化和个性化需求。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [350] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义标签的依赖感知Transformer模型TagBERT，用于电子商务查询重构任务，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索引擎需要解决查询与用户意图之间的语义鸿沟问题，现有方法未充分利用查询令牌的语义标签。

Method: 将查询重构任务建模为令牌分类问题，设计依赖感知的Transformer模型TagBERT，利用语义标签学习查询短语嵌入。

Result: 在大规模真实电子商务数据集上，TagBERT表现优于BERT、eBERT和序列到序列Transformer模型。

Conclusion: TagBERT通过利用语义标签显著提升了查询重构任务的性能。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [351] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 提出了一种针对复杂反应（如环化反应）的机制搜索策略，结合图枚举和机器学习筛选，使用神经网络势能（AIMNet2-rxn）评估反应路径。


<details>
  <summary>Details</summary>
Motivation: 复杂反应（如多步协同键变化）的机制搜索困难，需高效方法以探索此类反应。

Method: 结合图枚举和机器学习筛选，利用神经网络势能（AIMNet2-rxn）评估反应路径。

Result: 验证了NNP在活化能估计、立体选择性预测及天然产物合成关键步骤中的有效性。

Conclusion: 该方法为复杂反应机制搜索提供了高效且经济的解决方案。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [352] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 提出了一种用于算子学习不确定性量化的新框架——随机算子网络（SON），结合了随机神经网络（SNN）和DeepONet的概念。


<details>
  <summary>Details</summary>
Motivation: 解决算子学习中的不确定性量化问题。

Method: 将分支网络建模为随机微分方程（SDE），并通过伴随BSDE反向传播，用随机极大值原理中的哈密顿量梯度替代损失函数梯度进行SGD更新。

Result: SON能够通过扩散参数学习算子中的不确定性，并在2D和3D噪声算子复制中表现出色。

Conclusion: SON是一种有效的算子学习不确定性量化方法。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [353] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 研究探讨了如何在AI/ML模型中平衡能效与性能，通过知识蒸馏训练紧凑的DeepRX学生模型，降低能耗同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型在能效与性能之间的平衡问题，特别是在DeepRX接收器中。

Method: 使用知识蒸馏（KD）训练紧凑的DeepRX学生模型，比较不同模型大小和KD超参数，评估FLOPs/Watt和FLOPs/clock等指标。

Result: 蒸馏模型在SINR水平下表现出更低的错误率，验证了KD在实现高效能AI解决方案中的有效性。

Conclusion: 知识蒸馏可有效降低DeepRX模型的能耗，同时保持性能，为能效优化提供了可行方案。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [354] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL是一种训练方案，通过结合多选择学习（MCL）和低秩适应（LoRA），在语言模型中解码多样且合理的句子延续。


<details>
  <summary>Details</summary>
Motivation: 传统语言建模是一个病态问题，同一上下文可能有多个合理的未来。LoRA-MCL旨在高效处理这种模糊性。

Method: 结合多选择学习（MCL）、Winner-Takes-All（WTA）损失和低秩适应（LoRA），假设数据来自混合分布。

Result: 在视觉和音频字幕生成任务中，生成的输出具有高多样性和相关性。

Conclusion: LoRA-MCL通过多选择学习和低秩适应，有效解决了语言建模中的模糊性问题，并生成多样且合理的输出。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [355] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过最优传输视角研究分布偏移下的共形预测，提出了一种估计和缓解覆盖率损失的方法。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下可能失效，现有方法需预先知道偏移类型，本文旨在解决这一问题。

Method: 利用最优传输理论，估计分布偏移对覆盖率的影响并设计缓解策略。

Result: 证明了在分布偏移下仍能估计和保持共形预测的覆盖率。

Conclusion: 通过最优传输视角，本文为分布偏移下的共形预测提供了新的解决方案。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [356] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA是一种新型自监督学习策略，用于在线持续学习，通过对齐当前与过去的表征来减少遗忘，提升训练收敛速度，并在相同计算预算下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线持续学习中数据以小批量到达、计算预算固定且无任务边界时，自监督学习技术不足的问题。

Method: 提出Continual Latent Alignment (CLA)，通过对齐当前模型学习到的表征与过去表征来减少遗忘。

Result: CLA加速了在线场景下的训练收敛，优于现有方法；还发现CLA作为预训练协议能提升最终性能。

Conclusion: CLA是一种有效的在线持续学习自监督策略，兼具训练效率和性能优势。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [357] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文探讨了当前最先进的视觉语言模型（VLMs）在基础视觉任务中的局限性，通过设计一系列测试揭示其设计缺陷，并对比不同组件的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多任务处理中表现出色，但仍缺乏一些基础视觉理解能力。本文旨在揭示这些局限性，并为改进提供指导。

Method: 通过构建测试任务，对比分析视觉编码器、视觉-语言投影层和LLM解码器的性能表现。

Result: 研究发现VLMs在视觉信息处理中存在不足，并提出了关于其能力和鲁棒性的重要观察。

Conclusion: 本文的发现为VLMs的进一步改进提供了重要参考。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [358] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 论文探讨了优化问题中的梯度下降变体，特别是通过Polyak-Łojasiewicz不等式（PLI）实现损失函数指数收敛速率的研究，并比较了连续时间与离散时间LQR问题的差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决连续时间LQR问题中收敛速率随初始条件变化而消失的现象，以及探索广义PLI条件以弥补连续时间与离散时间行为的差距。

Method: 方法包括应用PLI及其广义条件，分析梯度估计误差的影响，并采用输入到状态稳定性（ISS）分析方法。

Result: 研究揭示了连续时间与离散时间LQR问题的收敛行为差异，并提出了广义PLI条件以应对梯度估计误差的影响。

Conclusion: 结论强调了广义PLI条件的重要性，并指出其在理解梯度误差的瞬态和渐近效应中的关键作用。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [359] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish是一种高效的非负矩阵和张量分解框架，结合加权中位数变换提升鲁棒性，同时保持Fast-HALS算法的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统加权NMF方法对异常值鲁棒但收敛慢，需改进以兼顾速度和鲁棒性。

Method: 采用加权中位数变换自适应平滑数据，保持Fast-HALS的高效加法更新结构。

Result: 在图像数据集上，Target Polish在精度上媲美或超越现有鲁棒NMF方法，计算时间减少一个数量级。

Conclusion: Target Polish在鲁棒性和计算效率上取得显著改进，适用于噪声数据分解。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [360] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 论文研究了弹性权重巩固（EWC）在持续学习中的表现，验证了其减少遗忘的效果，但略微牺牲新任务学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中因灾难性遗忘而难以保留旧知识的问题。

Method: 使用PermutedMNIST和RotatedMNIST基准，比较EWC与L2正则化和SGD的表现。

Result: EWC显著减少遗忘，但新任务学习效率略有下降。

Conclusion: EWC是神经网络终身学习的可行解决方案。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [361] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens结合FSS和U形SL，提升数据隐私保护，减少通信和计算成本，同时抵御更多攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管SL在保护客户端数据方面有潜力，但其易受攻击，需要更安全的解决方案。

Method: 采用FSS和U形SL结合的方法，隐藏公共输入并保护标签数据。

Result: 实验显示，该方法减少了训练时间和通信成本，同时保持准确性。

Conclusion: SplitHappens提供了更高的安全性，适用于多种攻击场景。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [362] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 论文探讨了人工智能在生物学中的应用，指出缺乏标准化跨领域基准的问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决生物学中AI模型缺乏标准化基准的问题，以提升模型的稳健性和可信度。

Method: 通过召集机器学习与计算生物学专家研讨会，分析数据异质性、噪声、可重复性等瓶颈，并提出建议。

Result: 提出高质量数据管理、标准化工具、全面评估指标等建议，以推动AI驱动的虚拟细胞基准发展。

Conclusion: 标准化基准将促进生物学AI模型的严谨性、可重复性和生物相关性，推动新发现和细胞系统理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [363] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 研究发现，尽管强化学习（RL）能提升大语言模型（LLMs）的推理能力，但Qwen2.5模型在常用基准测试中的表现可能因数据污染而不可靠。通过生成无泄漏的合成数据集RandomCalculation，证实仅准确奖励信号能稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨RL方法在提升LLMs推理能力时的有效性，并揭示Qwen2.5模型在基准测试中可能存在的数据污染问题。

Method: 引入生成器创建完全合成的算术问题数据集RandomCalculation，用于评估RL方法的真实效果。

Result: 在无污染数据集上，仅准确奖励信号能提升性能，而噪声或错误信号无效。

Conclusion: 建议在无污染的基准测试和多样化模型家族中评估RL方法，以确保结论的可靠性。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [364] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 论文分析了在重尾类别不平衡分布下，常见隐私学习优化算法的优化行为，发现DP-GD在低频类别学习中表现不佳，而DP-AdamBC通过消除DP偏差显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究在重尾类别不平衡分布下，隐私学习优化算法的表现差异，尤其是对低频类别的影响。

Method: 通过理论模型和实验对比，分析了DP-GD和DP-AdamBC在优化过程中的表现。

Result: DP-AdamBC在消除DP偏差后，对低频类别的训练准确率提升了约8%（控制实验）和5%（真实数据）。

Conclusion: DP-AdamBC能有效缓解重尾类别不平衡导致的优化问题，提升低频类别的学习效果。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [365] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出了一种图世界模型（GWM），支持多模态数据和图结构状态，通过通用消息传递算法处理结构化信息，并在多领域任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要关注非结构化数据，无法利用普遍存在的图结构数据；而图基础模型又局限于图学习任务。GWM旨在填补这一空白，支持多模态和跨学科任务。

Method: GWM采用通用消息传递算法，通过两种方式处理多模态数据：转换为文本（GWM-T）或使用模态特定编码器（GWM-E），并引入动作节点以支持多样化任务。

Result: 在六个不同领域的任务中，GWM表现优于或匹配领域专用基线，并展现出零样本/少样本学习能力。

Conclusion: GWM是一种通用的世界模型，能够有效处理多模态和图结构数据，适用于多样化任务，具有广泛的应用潜力。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [366] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: FusionBench和FusionFactory提出了一种系统化的LLM融合框架，通过多级融合（查询级、思维级、模型级）显著提升了任务性能，优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 现有应用多依赖单一LLM，无法充分利用多样化模型的互补优势，导致性能和成本效率低下。LLM路由数据未被充分挖掘，揭示了模型在不同任务中的比较优势。

Method: 提出FusionBench基准（14任务，5领域，20个开源LLM），并设计FusionFactory框架，包含查询级、思维级和模型级融合策略。

Result: 实验表明，FusionFactory在所有14个基准上均优于最佳单一LLM，且最优融合配置因任务而异。

Conclusion: 系统化的LLM融合能有效利用互补优势，显著提升整体性能，为复杂任务提供更高效的解决方案。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [367] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 论文提出了一种新的解耦方法，通过拆分嵌套规则的节点，提升神经DNF模型的性能，使其在分类任务中表现更接近翻译前的模型。


<details>
  <summary>Details</summary>
Motivation: 神经DNF模型在翻译过程中性能下降，部分原因是未能解耦学习到的知识，导致表现不佳。

Method: 提出解耦方法，拆分嵌套规则的节点为独立节点，以保留模型性能。

Result: 在多种分类任务中，解耦方法提供了紧凑且可解释的逻辑表示，性能更接近翻译前的模型。

Conclusion: 解耦方法有效解决了神经DNF模型在翻译过程中的性能下降问题。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>
